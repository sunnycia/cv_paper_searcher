|Index|Title|sentence|
|---|---|---|
|1|Yantao_Shen_Person_Re-identification_with_ECCV_2018_paper|Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information.|
|||The GNN propagates messages on a graph structure.|
|||Compared with manifold learning and re-ranking, GNN incorporates graph computation into the neural networks learning, which makes the training end-to-end and benefits learning the feature representation.|
|||Different from most existing GNN approaches, our proposed approach exploits the training data label supervision for generating more accurate feature fusion weights in the graph message passing.|
|||The Similarity Guided GNN (SGGNN) and deep messages propagation for person re-identification will be presented in Section 3.2.|
|||Compared with conventional setting of GNN Eq.|
|||Visual features by our proposed SGGNN outperforms the compared base model and conventional GNN setting significantly, which demonstrates that SGGNN also learns more discriminative and robust features.|
||7 instances in total. (in eccv2018)|
|2|Zihang_Meng_Efficient_Relative_Attribute_ECCV_2018_paper|Graph neural networks were proposed by [11, 19], where the authors describe GNN as a parameterized message passing scheme which can be trained.|
|||[9] generalized the GNN using message passing neural network and demonstrated state-of-the-art results on molecular prediction benchmarks.|
|||With this data, a generalized GNN is trained where both the node features (representations of the images) and edge weights are learned.|
|||The core architecture of our GNN is shown in Fig.|
|||The weights in the entire framework including those in the CNN and GNN are trained end-to-end.|
|||The architectural details of our GNN which remains the same for both RAL and BAP.|
||6 instances in total. (in eccv2018)|
|3|Qi_3D_Graph_Neural_ICCV_2017_paper|Graph Neural Networks  In this section, we briefly review Graph Neural Networks (GNN) [9, 33] and their variants, e.g., gated GNN [20], and discuss their relationship with existing models.|
|||3DGNN for RGBD Semantic Segmentation  In this section, we propose a special GNN to tackle the  problem of RGBD semantic segmentation.|
|||The initial learning rates of the pre-trained unary CNN and GNN are 0.001 and 0.01 respectively.|
|||We now compare our 3DGNN to the unary CNN in order to investigate how GNN can be enhanced by leveraging 3D geometric information.|
||4 instances in total. (in iccv2017)|
|4|cvpr18-Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs|The global energy proposed by [13] is defined with respect to the 10-nearest neighbor adjacency graph Gnn = (C, Enn) of the point cloud (note that this is not the SPG).|
|||In this paper, we use two different adjacency graphs between points of the input clouds: Gnn in Section 3.1 and Gvor in Section 3.2.|
||2 instances in total. (in cvpr2018)|
|5|Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper|A very recent GNN work captured the structure of point clouds by superpoint graph, which partitioned various objects into simple shapes and assigned segmentation labels to each part as a whole [27].|
||1 instances in total. (in eccv2018)|
|6|cvpr18-Surface Networks|[10, 19] study molecular fingerprints using variants of the GNN architecture, and [12] further develop the model by combining it with set representations [38], showing state-of-the-art results on molecular prediction.|
||1 instances in total. (in cvpr2018)|
|7|Graph-Structured Representations for Visual Question Answering|This enables the GNN to exploit (1) the unordered nature of scene elements (the objects in particular)  The task of visual question answering has received increasing interest since the seminal paper of Antol et al.|
||1 instances in total. (in cvpr2017)|
|8|cvpr18-Learning to Act Properly  Predicting and Explaining Affordances From Images|The GNN model then takes semantic feature representation of each object as its initial node representation, and iteratively updates its hidden vectors by propagating messages among the neighbors in the graph.|
||1 instances in total. (in cvpr2018)|
|9|Chenyang_Si_Skeleton-Based_Action_Recognition_ECCV_2018_paper|The GNN formulation at time step t is defined as follows:  are utilized to update the hidden state st  i is computed with st  i  mt  i = fm ({st1 i i, st1 i = fs (mt st ot i = fo (st i)  i  |i  {1, ..., |vi|}) )  (1)  (2)  (3)  where mt i is the sum of all the messages that the neighbors vi send to node vi, fm is the function to compute the incoming messages, fs is the function that expresses the state of a node and fo is the function to produce the output.|
||1 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
