|Index|Title|sentence|
|---|---|---|
|1|Qi_Volumetric_and_Multi-View_CVPR_2016_paper|Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations.|
|||[30] propose volumetric CNN architectures on volumetric grids for object classification and retrieval.|
|||[29], who augment their 2D CNN with pre-training from ImageNet RGB data [6].|
|||These results indicate that existing 3D CNN architectures and approaches are unable to fully exploit the power of 3D representations.|
|||Moreover, we show how to reduce the gap between volumetric CNNs and multi-view CNNs by efficiently augmenting training data, introducing new CNN architectures in 3D.|
|||We propose two volumetric CNN network architectures that signficantly improve state-of-the-art of  5648  volumetric CNNs on 3D shape classification.|
|||CNNs on Depth and 3D Data With the introduction of commodity range sensors, the depth channel became available to provide additional information that could be incorporated into common CNN architectures.|
|||Recently, a CNN architecture has been proposed where the RGB and depth data are processed in two separate streams; in the end, the two streams are combined with a late fusion network [8].|
|||The resulting representation is a 3D binary voxel grid, which is the input to a CNN with 3D filter banks.|
|||Current state-of-the-art uses multiple rendered views, and trains a CNN that can process  5649  all views jointly [29].|
|||This multi-view CNN (MVCNN) is pre-trained on ImageNet [6] and uses view-point pooling to combine all streams obtained from each view.|
|||Analysis of state-of-the-art 3D Volumetric  CNN versus Multi-View CNN  Multi-Vie(cid:449) CNN (cid:894)sta(cid:374)dard re(cid:374)deri(cid:374)g(cid:895)  Multi-Vie(cid:449) CNN (cid:894)sphere-(cid:1007)0 re(cid:374)deri(cid:374)g(cid:895)  (cid:1013)(cid:1006).0  (cid:1012)(cid:1013).|
|||(cid:1009)  Volu(cid:373)etric CNN (cid:894)(cid:448)olu(cid:373)e (cid:1007)0x(cid:1007)0x(cid:1007)0(cid:895)  (cid:1012)(cid:1008).|
|||Yellow and blue bars: Performance drop of multi-view CNN due to discretization of CAD models in rendering.|
|||This indicates that the network of the volumetric CNN is weaker than that of the multiview CNN.|
|||A volumetric CNN based on voxel occupancy (green) is 7.3% worse than a multi-view CNN (yellow).|
|||The multi-view CNN downsamples each rendered view to 227  227 pixels (Multiview Standard Rendering in Fig 1); to maintain a similar computational cost, the volumetric CNN uses a 303030 occupancy grid (Volumetric Occupancy Grid in Fig 1)2.|
|||As shown in Fig 1, the input to the multi-view CNN captures more detail.|
|||To this end, we feed the multi-view CNN with renderings of the 30  30  30 occupancy grid using sphere rendering3, i.e., for each occupied voxel, a ball is placed at its center, with radius equal to the edge length of a voxel (Multi-View Sphere Rendering in Fig 1).|
|||The accuracy of this multi-view CNN is reported in blue.|
|||As shown in Fig 2, even with similar level of object detail, the volumetric CNN (green) is 4.8% worse than the multi-view CNN (blue).|
|||An image CNN is then appended to classify the 2D projection.|
|||3It is computationally prohibitive to match the volumetric CNN resolu tion to multi-view CNN, which would be 227  227  227.|
|||The main innovation is that we add auxiliary tasks to predict class labels that focus on part of an object, intended to drive the CNN to more heavily exploit local discriminative features.|
|||Network 1: Auxiliary Training by Subvolume  Supervision  We observe significant overfitting when we train the volumetric CNN proposed by [30] in an end-to-end fashion (see supplementary).|
|||When the volumetric CNN overfits to the training data, it has no incentive to continue learning.|
|||Inspired by its success, we design a neural network archi 5651  A(cid:374)isotropi(cid:272) Pro(cid:271)i(cid:374)g  (cid:1007)(cid:1004)  (cid:1005) (cid:1005)  (cid:1007)(cid:1004)  (cid:1007)(cid:1004)  (cid:1007)(cid:1004)  (cid:1007)(cid:1004)  (cid:1005) (cid:1005)  (cid:1009)  (cid:1009)  (cid:1007)(cid:1004)  (cid:1007)(cid:1004)  (cid:1007)(cid:1004)  (cid:1005)  (cid:1005)  (cid:1009)  (cid:1009)  (cid:1007)(cid:1004)  (cid:1007)(cid:1004)  I(cid:373)age-(cid:271)ased CNN  (cid:894)Network I(cid:374) Network(cid:895)  Soft(cid:373)ax  Loss  (cid:1008)(cid:1004)  Figure 4.|
|||Similar to Su-MVCNN [29] which aggregates information from multiple view inputs through a view-pooling layer and follow-on fully connected layers, we sample 3D input from different orientations and aggregate them in a multi-orientation volumetric CNN (MO-VCNN) as shown in Fig 5.|
|||A volumetric CNN is firstly trained on single rotations.|
|||Left: Volumetric CNN (single orientation input).|
|||Right: Multi-orientation volumetric CNN (MO-VCNN), which takes in various orientations of the 3D input, extracts features from shared CNN1 and then pass pooled feature through another network CNN2 to make a prediction.|
|||Multi-View Convolutional Neural Networks  The multi-view CNN proposed by [29] is a strong alternative to volumetric representations.|
|||Although the multi-view CNN presented by [29] produces compelling results, we are able to improve its performance through a multi-resolution extension with improved data augmentation.|
|||In the following, we discuss the results within volumetric CNN methods and within multi-view CNN methods.|
|||For reference of multi-view CNN performance at the same  5653  (cid:894)Wu et al.|
|||(cid:1012)  Volu(cid:373)etri(cid:272) CNN at resolutio(cid:374) (cid:1007)(cid:1004)(cid:1012)(cid:1008).|
|||(cid:1011) Multi-Vie(cid:449) CNN sphere-(cid:1007)(cid:1004) re(cid:374)deri(cid:374)g (cid:1012)(cid:1013).|
|||(cid:1009)  Multi-Vie(cid:449) CNN sta(cid:374)dard re(cid:374)deri(cid:374)g  (cid:1013)(cid:1006).|
|||Our volumetric CNNs have matched the performance of multi-view CNN at 3D resolution 30 (our implementation of Su-MVCNN [29], rightmost group).|
|||3D resolution, we also include Ours-MVCNN-Sphere-30, the result of our multi-view CNN with sphere rendering at 3D resolution 30.|
|||Moreover, they both match the performance of our multiview CNN under the same 3D resolution.|
|||Effect of 3D Resolution over Performance  Sec 6.2 shows that our volumetric CNN and multi-view CNN performs comparably at 3D resolution 30.|
|||Bottom: performance of image-based CNN and volumetric CNN with increasing 3D resolution.|
|||Fig 9 shows the performance of our volumetric CNN and multi-view CNN at different 3D resolutions (defined at the beginning of Sec 6).|
|||Due to computational cost, we only test our volumetric CNN at 3D resolutions 10 and 30.|
|||The observations are: first, the performance of our volumetric CNN and multi-view CNN is on par at tested 3D resolutions; second, the performance of multiview CNN increases as the 3D resolution grows up.|
|||To further improve the performance of volumetric CNN, this experiment suggests that it is worth exploring how to scale volumetric CNN to higher 3D resolutions.|
|||More Evaluations  Data Augmentation and Multi-Orientation Pooling We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [30], to train and test on three variations of augmented data (Table 1).|
|||Similar trend is observed for other volumetric CNN variations.|
|||Comparison of performance of volumetric CNN architectures.|
|||Comparison of Volumetric CNN Architectures The architectures in comparison include VoxNet [21], E2E-[30] (the end-to-end learning variation of [30] implemented in Caffe [14] by ourselves), 3D-NIN (a 3D variation of Network in Network [20] designed by ourselves as in Fig 3 without the Prediction by partial object branch), SubvolumeSup (Sec 4.2) and AniProbing (Sec 4.3).|
|||Therefore, it is worth exploring the design of efficient volumetric CNN architectures that scale up to higher resolutions.|
||52 instances in total. (in cvpr2016)|
|2|Switching Convolutional Neural Network for Crowd Counting|Current state-of-the art approaches tackle these factors by using multi-scale CNN architectures, recurrent networks and late fusion of features from multi-column CNN with different receptive fields.|
|||Patches from a grid within a crowd scene are relayed to independent CNN regressors based on crowd count prediction quality of the CNN established during training.|
|||The independent CNN regressors are designed to have different receptive fields and a switch classifier is trained to relay the crowd scene patch to the best CNN regressor.|
|||It is observed that the switch relays an image patch to a particular CNN column based on density of crowd.|
|||Crowd counting as a computer vision problem has seen drastic changes in the approaches, from early HOG based head detections [6] to CNN regressors [18, 19, 9] predicting the crowd density.|
|||We build on the performance of CNN based architectures for crowd counting and propose Switching Convolutional Neural Network (Switch-CNN) to map a given crowd scene to its density.|
|||Independent CNN crowd density regressors are trained on patches sampled from a grid in a given crowd scene.|
|||The independent CNN regressors are chosen such that they have different receptive fields and field of view.|
|||A particular CNN regressor is trained on a crowd scene patch if the performance of the regressor on the patch is the best.|
|||A switch classifier is trained alternately with the training of multiple CNN regressors to correctly relay a patch to a particular regressor.|
|||To summarize, in this paper we present:  A novel generic CNN architecture, Switch-CNN trained end-to-end to predict crowd density for a crowd scene.|
||| Switch-CNN maps crowd patches from a crowd scene to independent CNN regressors to minimize count error and improve density localization exploiting the density variation within a scene.|
|||An Alexnet [7] style CNN model is trained by [15] to regress the crowd count.|
|||In [9], a multi-scale CNN architecture is used to tackle the large scale variations in crowd scenes.|
|||Multi-column CNN used by [2, 19] perform late fusion of features from different CNN columns to regress the density map for a crowd scene.|
|||In [19], shallow CNN columns with varied receptive fields are used to capture the large variation in scale and perspective in crowd scenes.|
|||Both the model fuse the feature maps from the CNN columns by weighted averaging via a 11 convolutional layer to predict the density map of the crowd.|
|||We build on the performance of multi-column CNN and incorporate a patch based switching architecture in our proposed architecture, SwitchCNN to exploit local crowd density variation within a scene (see Sec 3.1 for more details of architecture).|
|||Traditional convolutional architectures have been modified to model the extreme variations in scale induced in dense crowds by using multi-column CNN architectures with feature fusion techniques to regress crowd density.|
|||In this paper, we consider switching CNN architecture (Switch-CNN) that relays patches from a grid within a crowd scene to independent CNN regressors based on a switch classifier.|
|||The independent CNN regressors are chosen with different receptive fields and field-of-view as in multi-column CNN networks to augment the ability to model large scale variations.|
|||A particular CNN regressor is trained on a crowd scene patch if the performance of the regressor on the patch is the best.|
|||A switch classifier is trained alternately with the training of multiple CNN regressors to correctly relay a patch to a particular regressor.|
|||This patch is relayed to one of the three CNN regressor networks based on the CNN label inferred from Switch.|
|||Switch(cid:173)CNN  Our proposed architecture, Switch-CNN consists of three CNN regressors with different architectures and a classifier (switch) to select the optimal regressor for an input crowd scene patch.|
|||Feeding patches as input to the network helps in regressing different regions of the image independently by a CNN regressor most suited to patch attributes like density, background, scale and perspective variations of crowd in the patch.|
|||3  We use three CNN regressors introduced in [19], R1 through R3, in Switch-CNN to predict the density of crowd.|
|||These CNN regressors have varying receptive fields that can capture people at different scales.|
|||The architecture of each of the shallow CNN regressor is similar: four convolutional layers with two pooling layers.|
|||Density maps ease the difficulty of regression for the CNN as the task of predicting the exact point of head annotation is reduced to predicting a coarse location.|
|||Pretraining  The three CNN regressors R1 through R3 are pretrained separately to regress density maps.|
|||Individual CNN regressors are trained to minimize the Euclidean distance between the estimated density map and ground truth.|
|||Let DXi (; ) represent the output of a CNN regressor with parameters  for an input image Xi.|
|||The loss Ll2 is optimized by backpropagating the CNN via stochastic gradient descent (SGD).|
|||Though we optimize the l2-loss between the estimated and ground truth density maps for training CNN regressor, factoring in count error during training leads to better crowd counting performance.|
|||Hence, we measure CNN performance using count error.|
|||For example, a CNN regressor with large receptive field capture high level abstractions like background elements and faces.|
|||While the backpropagation of independent regressor Rk is still done with l2-loss, the choice of CNN regressor for backpropagation is based on the count error.|
|||Coupled Training  Differential training on the CNN regressors R1 through R3 generates a multichotomy that minimizes the predicted count by choosing the best regressor for a given crowd scene patch.|
|||In switched differential training, the individual CNN regressors are trained using crowd scene patches relayed by  switch for one epoch.|
|||Testing  We evaluate the performance of our proposed architecture, Switch-CNN on four major crowd counting datasets At test time, the image patches are fed to the switch classifier which relays the patch to the best CNN regressor Rk.|
|||The selected CNN regressor predicts a crowd density map for the relayed crowd scene patch.|
|||Because of the two pooling layers in the CNN regressors, the predicted density maps are 1  th size of the input.|
|||Switch-CNN with CNN regressors R1 and R3 has lower MAE than Switch-CNN with regressors R1R2 and R2R3.|
|||Switch-CNN with all three regressors outperforms both the models as it is able to model the scale and perspective variations better with three independent CNN regressors R1, R2 and R3 that are structurally distinct.|
|||Switch-CNN leverages multiple independent CNN regressors with different receptive fields.|
|||Switch Multichotomy Characteristics  The principal idea of Switch-CNN is to divide the training patches into disjoint groups to train individual CNN regressors so that overall count accuracy is maximized.|
|||We see that the density of crowd in the patches increases from CNN regressor R1R3.|
|||Figure 5 displays some sample patches that are relayed to each of the CNN regressors R1 through R3.|
|||The density of crowd in the patches increases from CNN regressor R1 through R3.|
|||We utilize the inherent structural and functional differences in multiple CNN regressors capable of tackling large scale and perspective variations by enforcing a differential training regime.|
||51 instances in total. (in cvpr2017)|
|3|Yang_Convolutional_Channel_Features_ICCV_2015_paper|CCF transfers low-level features from pretrained CNN models to feed the boosting forest model.|
|||With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods.|
|||We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without finetuning the whole network to each task by achieving stateof-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.|
|||Combinations such as Haar-like feature with boosting [49], HOG feature with SVM [9], multiple channel features with boosting [11] and fine-tuned high-level CNN features with SVM [21] have largely improved the object detection.|
|||Among these combinations, the multiple channel features with boosting and the fine-tuned high-level CNN features with SVM [21] show the most promising performances on various detection tasks.|
|||The fine-tuned high-level CNN feature with SVM has recently shown extreme power [21, 40] in challenging tasks.|
|||Typically, a CNN model previously trained on ImageNet classification task is fine-tuned for new task and then the mid-level or high-level features of the CNN are extracted and fed into a SVM classifier.|
|||The main advantage of this kind of approach is that the CNN has large capacity to handle large-scale training data and the learning process is endto-end.|
|||However, currently CNN is often accompanied by huge computation complexity in inference and learning, and the model size is usually large (e.g., more than 500MB for widely used VGG net [45]).|
|||Specifically, we extend the multiple channel features to low-level feature maps transferred from a CNN model trained on ImageNet image classification task, or equivalently speaking, replace the high-level connections in CNN with a boosting forest.|
|||The advantages are twofold: the transferred feature maps from CNN improve the representative capacity in channel features, while the boosting forest absolves the painstaking fine-tuning of high-level connections in CNN during the adaptation to various classification/regression problems.|
|||From the CNN perspective, CCF shows that: 1) highlevel connections in conventional CNN models, like convolutional and fully-connected layers, can be discarded and replaced with a boosting forest model learned with respect to different tasks, resulting in a more efficient algorithm without loss in accuracy; 2) the low-level feature representation in pre-trained CNN feature hierarchies can generalize well to diverse tasks, ranging from edge detection to pedestrian detection, without even fine-tuning to each domain.|
|||CCF sets new records in 3 out of 4 vision tasks (not all metrics in edge detection), beating not only channel features variants, but also CNN based methods which either fine-tune pre-trained models to new tasks or delicately design special architecture/loss.|
|||Two features of CNN models play the key role in its success: 1) The feature representation is learned in a hierarchical way, making it more representative.|
|||2) CNN features have a good generalization ability.|
|||Specifically, [16] shows that high-level CNN features can be transferred to generic recognition tasks without fine-tuning.|
|||Through finetuning, CNN features learned on large-scale recognition tasks have been successfully transferred to various vision tasks like object detection [21], semantic segmentation [36] and action detection [23], but they suffer from heavy computation cost caused by high-level connections [16, 20].|
|||As the above two models are developed individually, we want to build a bridge between them, by replacing the hand-crafted channel features with low-level CNN features.|
|||They both use the output of the last convolutional layer as feature representation and concatenate the CNN features with another structural model.|
|||There is also work that harnesses low-level CNN features, like [31] that uses the output of first convolutional layer to learn a two-class object classifier for proposal generation, then the RCNN approach is used for object detection.|
|||Our work differs from these in that: 1) we use low-level CNN features as a universal representation (without fine-tuning) for vision tasks ranging from edge detection to pedestrian detection; 2) we apply boosting forest model directly on the feature maps to solve vision problems, which owns the advantages of fewer parameters, smaller model size and faster inference, compared with conventional CNN methods.|
|||In our case, as different layers in CNN often have different number of pooling layers under them, we set a minimum down-sampling of 4 for all cases.|
|||The pipeline of Convolutional Channel Features (CCF), which is a concatenation of low-level CNN feature extraction and boosting forest model learned with respect to diverse tasks.|
|||1, CCF is composed by concatenating two individual components, the CNN feature extraction part and the boosting forest part.|
|||The CNN feature extraction part extracts feature representation, using only the first few layers from a pre-trained CNN model.|
|||In the following part of this section, we introduce the selection of CNN features used for CCF, techniques to accelerate the feature extraction process under the sliding window setting and how to learn boosting forest models for different vision tasks.|
|||Selection of feature representation  We compare various feature choices among several popular CNN models, which are AlexNet (ANet for short) [33], VGG net [45] and GoogLeNet (GNet for short) [46].|
|||We adopt hard negative mining strategy in training baseline models, and all collected negative samples by ACF model are stored for all experiments of CNN feature selection for the sake of training speed.|
|||Since channel features are general-purposed and dense feature representation, it is reasonable to prefer low-level features in CNN to the task-specific, sparse high-level one.|
|||While pyramid construction of channel features can be quite efficient, the same process of CNN features can be expensive due to restriction of a fixed input size of CNN models.|
|||Recent works on CNN framework also borrow the same idea [28, 43].|
|||As the input image size of a CNN model is usually fixed due to implementation restrictions, the idea of patchwork is straightforward that images at different scales can be put together to form a large input image for feature extraction (as  85  shown in Fig.|
|||The input image size of the CNN model is therefore set relatively large (say 1.5 of average image size).|
|||If the image is larger than the CNN input image size, we segment the image into small regions to fit into CNN input size rather than warp it.|
|||Pedestrian and face detection: Decision trees are learned directly on pixel lookups of the given CNN feature maps like the way in [11, 50].|
|||Experiments  One reason that we select the low-level CNN features is that it is a good balance between feature representativeness and generalization ability.|
|||gain, while some of these CNN models are designed sophisticatedly.|
|||CCF also has a great edge over CNN based methods JointDeep [41], SDN [37] and AlexNet+ImageNet [27], beating the best one with 6.00%  For face detection, we use the same training data as [50] and similar boosting paradigm (the depth of decision tree is changed from 2 to 3).|
|||What should be noted is that CCF considerably outperforms another CNN based method DDFD [19] which fine-tunes the CNN model to face detection, showing the effectiveness of boosting forest as a way to ap 87  Method  Human DeepNet [32] SE [15] SE+ms [15] MCG [2] DeepEdge [5] CCF CCF+ms CCF+ms+CF  ODS  0.80 0.738 0.739 0.746 0.747 0.753 0.741 0.744 0.745  OIS  0.80 0.759 0.759 0.767 0.779 0.772 0.761 0.767 0.768  AP   0.758 0.792 0.803 0.759 0.807 0.808 0.809 0.807  Table 4.|
|||ply pre-trained CNN models to specific domains.|
|||Speed  By definition, CCF is apparently slower than channel features approach, and faster than end-to-end CNN methods (e.g.|
|||R-CNN) as high level connections in CNN take up roughly half the whole computation time [16].|
|||To be specific and accurate, we compare the speed of CCF with channel features variants and a popular CNN approach R-CNN in the task of pedestrian detection.|
|||In this paper, we revisit the popular channel features approach and Convolutional Neural Networks approach, and propose an integrated method called Convolutional Channel Features (CCF) by combining the low-level CNN features and boosting forest model together.|
|||CCF benefits from the rich representative capacity of CNN to guarantee outstanding performance in various vision tasks, as well as the efficiency in inference and learning from boosting forest model.|
|||Deformable part models with cnn features.|
||46 instances in total. (in iccv2015)|
|4|cvpr18-Divide and Grow  Capturing Huge Diversity in Crowd Images With Incrementally Growing CNN|We tackle this problem with a growing CNN which can progressively increase its capacity to account for the wide variability seen in crowd scenes.|
|||Our model starts from a base CNN density regressor, which is trained in equivalence on all types of crowd images.|
|||This hierarchical training leads to a CNN tree, where the child regressors are more fine experts than any of their parents.|
|||Typical CNN density regressors [2, 10, 26] are optimized over an entire dataset containing images of all densities.|
|||Hence, we propose an Incrementally Growing CNN (IG-CNN) for crowd counting.|
|||IG-CNN starts from a base CNN density regressor which is trained on the entire dataset.|
|||Then we replicate the base CNN into two child networks by copying the weights of the parent.|
|||This procedure is done recursively forming a hierarchical CNN tree where each node has two child nodes which are more specialized than their parent.|
|||At the end of the growing, a set of experts are created at the leaf nodes of the CNN tree.|
|||With deep learning, CNN based regressors become popular and give better performance than classical models.|
|||The counting CNN introduced in [25], is trained by alternatively  optimizing both crowd density loss as well as crowd count loss.|
|||The deep CNN employed by [19] directly regresses the crowd count instead of a density map.|
|||But such approaches prevent the CNN from acquiring good feature detectors and lead to lower performance than training for density map prediction.|
|||[2] propose a VGG based deep CNN along with a shallow CNN.|
|||Multi-column network from [26] has three CNN columns, each having different receptive fields and hence can capture crowds at multiple scales.|
|||[13] address this issue by performing a differential training procedure to accentuate the specialization between CNN columns of varied architecture.|
|||Few works like [9] grow a CNN progressively by adding new neurons in a data-dependent fashion.|
|||[1] jointly train specialty branches along with a generalist CNN which can classify the specialties.|
|||The algorithm proposed in [22] can learn a CNN tree where the nodes down the tree capture progressively fine-grained features.|
|||Our model also hierarchically grows a CNN tree, but it is employed only as a method to create a set of experts without any manually specified specialization criteria.|
|||In contrast to works like [22], the hierarchy is discarded in IG-CNN after training and only the networks at the leaf nodes of the CNN tree are kept.|
|||Our incrementally growing CNN or IG-CNN architecture is shown in Figure 2.|
|||IG-CNN training begins with a base CNN regressor, which is trained on the full dataset to estimate crowd density.|
|||Regressors are recursively replicated and specialized forming a CNN tree.|
|||The two way replicating and specialization process is recursively continued forming a CNN tree.|
|||Growing CNN Architecture  The hierarchical differential training procedure results in the creation of a set of regressors at the leaf nodes of the CNN tree.|
|||This helps CNN to train better as otherwise it would have to predict exactly at the annotation point.|
|||Then, the l2 loss function is defined as  Ll2 () =  1 2N  N  X  i=1  kMXi (x; )  M GT  Xi (x)k2 2,  (1)  where  refers to the trainable parameters of the CNN and N is the total number of training samples.|
|||After pretraining of the base CNN, a CNN tree is progressively built where each node represents a regressor finetuned on a subset of the dataset.|
|||A region of interest (RoI) of size RW RH is defined for the patch on which the CNN regressor predicts the crowd density.|
|||Pretraining of Base CNN  The base CNN is trained on the entire dataset to regress crowd density map.|
|||Since the CNN is pretrained with l2 loss, it has good initial features and finetuning with count loss provides complementary information.|
|||At every increment of the growing process, regressors at the leaf nodes of the CNN tree are split and new expert regressors are created.|
|||input : Dataset D0 = {Xi, M GT }N i=1 (image & map) Xi output: Parameters {r} of experts and classifier c Random initialize 0 for base CNN R0; Pretrain R0; Rleaf = {R0}; Dleaf [R0] = D0; /* Hierarchical Differential Training */ for l = 0 to max tree depth do  /* Replicate R twice */ for R in Rleaf do  Rchild[R] = {R, R};  end /* Differential Training */ /* R predicts count C R for i = 1 to max iterations do  i while C GT  i  is the actual */  for Rl in Rleaf do  for X, M in Dleaf [Rl] do  r = argmin  RRchild[Rl]  |C R  X  C GT  X |;  Fine-tune Rr with X to update r;  end  end Break if validation Oracle MAE stagnates;  end /* Dataset division for leaf regressors */ Dleaf = []; for X, M in D0 do  for Rl in Rleaf do r = argmin  RRchild[Rl]  |C R  X  C GT  X |;  Add (X, M ) to Dleaf [r];  end  end /* Training of Expert Classifier */ Initialize c with VGG-16 weights; for X, M in D0 do |C R  X  C GT  X |;  r = argmin RRleaf  Add (X, r) to Dc;  end Train classifier with Dc and update c; Break if validation Actual MAE stagnates;  end  Algorithm 1: IG-CNN training algorithm.|
|||Figure 4 shows density maps predicted by IG-CNN and the base CNN along with the corresponding ground truths.|
|||3623  760.0216.0664.6186.4764.5224.2Input ImageGround TruthBase CNN PredictionIG-CNN PredictionMethod  Scene1  Scene2  Scene3  Scene4  Scene5 Average  Zhang et al.|
|||Table 4 lists count errors for the base CNN along with that of the IG-CNN at different levels of growth.|
|||There is significant improvement of MAE for IG-CNN at higher levels than the base CNN but saturates after level 3.|
|||In the same setting as that of IG-CNN, we use VGG-16 classifier as gating CNN to output softmax confidences.|
|||The 8 regressors are initialized with base CNN weights and their outputs are multiplied by the classifier confidences.|
|||The base CNN is replicated into two child regressors, each of which are imposed specialization with differential training and recursively divided again forming a CNN tree.|
|||Learning to count with CNN boostIn European Conference on Computer Vision, pages  ing.|
|||Learning fine-grained features via a CNN tree for large-scale classification.|
||43 instances in total. (in cvpr2018)|
|5|Gidaris_Object_Detection_via_ICCV_2015_paper|Object detection via a multi-region & semantic segmentation-aware CNN model  Spyros Gidaris  Nikos Komodakis  Universite Paris Est, Ecole des Ponts ParisTech  Universite Paris Est, Ecole des Ponts ParisTech  gidariss@imagine.enpc.fr  nikos.komodakis@enpc.fr  Abstract  We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features.|
|||We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model.|
|||[10] with the R-CNN framework..  Overfeat [24] uses two CNN models that apply in a sliding window fashion on multiple scales of an image.|
|||This success of R-CNN comes from the fact that hand-engineered features like HOG [3] or SIFT [22] are replaced with the high level object representations produced from the last layer of a CNN model.|
|||By employing an even deeper CNN model, such as the 16layers VGG-Net [25], they boosted the performance another 7 points [10].|
|||In order to achieve our goal, we propose a multi-component CNN model, called multiregion CNN hereafter, each component of which is steered to focus on a different region of the object thus enforcing diversification of the discriminative appearance factors captured by it.|
|||Note that this property, which is highly desirable for detection, contradicts with the builtin invariances of CNN models, which stem from the use of max-pooling layers.|
|||To that end, we extend the above CNN model such that it also learns novel CNNbased semantic segmentation-aware features.|
|||Besides object representation, our work is also motivated by the observation that, due to the remarkable classification capability of the recent CNN models [17, 30, 25, 16, 13, 26], the bottleneck for good detection performance is now the accurate object localization.|
|||In order to prove our belief, we attempt to built a more powerful localization system that combines our multiregion CNN model with a CNN-model for bounding box regression, which are used within an iterative scheme that alternates between scoring candidate boxes and refining their coordinates.|
|||As features they use the outputs of HOG [3]+SVM classifiers trained on each region separately and the 1000class predictions of a CNN pre-trained on ImageNet.|
|||To summarize, our contributions are as follows: (1) We develop a multi-region CNN recognition model that yields an enriched object representation capable of capturing a diversity of discriminative appearance factors and of exhibiting localization sensitivity that is de(2) We sired for the task of accurate object localization.|
|||furthermore extend the above model by proposing a unified neural network architecture that also learns semantic segmentation-aware CNN features for the task of object detection.|
|||(3) We show how to significantly improve the localization capability by coupling the aforementioned CNN recognition model with a CNN model for bounding box regression, adopting a scheme that alternates between scoring candidate boxes and refining their locations, as well as modifying the post-processing step of non-maximum-suppression.|
|||The remainder of the paper is structured as follows: We describe our multi-region CNN model in 2.|
|||Multi-Region CNN Model  The recognition model that we propose consists of a multi-component CNN network, each component of which is chosen so as to focus on a different region of an object.|
|||We call this a Multi-Region CNN model.|
|||To that end, in order to facilitate the description of our model we introduce a general CNN architecture abstraction that decomposes the computation into two different modules:  Activation maps module.|
|||max pooling    fully connected     fully connected      C r o p p i n g       L a y e r  C o n c a t e n a t e   L a y e r  O b j e c t    R e p r e s e n t a t i o n  Figure 2: Multi Region CNN architecture.|
|||The above architecture can be extended to also learn semantic segmentation-aware CNN features (see section 3) by including additional activation-maps and region-adaptation modules that are properly adapted for this task (these are not shown here due to lack of space).|
|||Under this formalism,  the architecture of the MultiRegion CNN model can be seen in Figure 2.|
|||Region  Figure 3: Illustration of the regions used in the Multi-Region CNN model.|
|||We tested such a hypothesis by conducting an experiment where we trained and tested two Multi-Region CNN models that consist of two regions each.|
|||Semantic Segmentation-Aware CNN Model  To further diversify the features encoded by our representation, we extend the Multi-Region CNN model so that it also learns semantic segmentation-aware CNN features.|
|||In the context of our multi-region CNN network, the incorporation of the semantic segmentation-aware features is done by adding properly adapted versions of the two main modules of the network, i.e., the activation-maps and region-adaptation modules:   Activation maps module for semantic segmentation aware features.|
|||Object Localization  As already explained, our Multi-Region CNN recognition model exhibits the localization awareness property that is necessary for accurate object localization.|
|||Specifically, let Bt i=1 denote the set of Nc,t bounding boxes generated on iteration t for class c and image X. the boxes from the previous iteration Bt1 i,c = Frec(Bt1 i,c |c, X) by our recognition model and refined i,c = Freg(Bt1 into Bt i,c |c, X) by our CNN regression model, thus forming the set of candidate detections Dt c = i,c)}Nc,t {(st i,c, Bt i=1 .|
|||After the last iteration T , the candidate detections {Dt t=1 produced on each iteration t are merged together Dc = T c. Because of the multiple regression steps, the generated boxes will be highly concen t=1Dt  c}T  Figure 4: Illustration of the weakly supervised training of the FCN [21] used as activation maps module for the semantic segmentation aware CNN features.|
|||The reason that we do not repeat the same regions as in the initial Multi-Region CNN architecture is for efficiency as these are already used for capturing the appearance cues of an object.|
|||We combine the Multi-Region CNN features and the semantic segmentation aware CNN features by concatenating  In practice T = 2 iterations were enough for convergence.|
|||Implementation Details  For all  the CNN models involved in our proposed system, we used the publicly available 16-layers VGG model [25] pre-trained on ImageNet [4] for the task of image classification.|
|||Multi-Region CNN model.|
|||https://gist.github.com/ksimonyan/  Semantic segmentation-aware CNN model.|
|||In order to train it, we used the same procedure as for the region components of the Multi-Region CNN model.|
|||The activation maps module used as input in this case is common with the Multi-Region CNN model.|
|||The Multi-Region CNN model without the semantic segmentation aware CNN features (MR-CNN), achieves 66.2% mAP, which is 4.2 points higher than RCNN with VGG-Net (62.0%) and 4.5 points higher than the Original candidate box region alone (61.7%).|
|||Extending the Multi-Region CNN model with the semantic segmentation aware CNN features (MR-CNN & S-CNN), boosts the performance of our recognition model another 1.3 points and reaches the total of 67.5% mAP.|
|||Significant is also the improvement that we get when we couple our recognition model with the CNN model for bounding box regression under the iterative localization scheme pro posed (MR-CNN & S-CNN & Loc.).|
|||Region CNN model instead of the Original Candidate Box region alone, a considerable reduction in the percentage of false positives due to bad localization is achieved.|
|||Our overall system involves the MultiRegion CNN model enriched with the semantic segmentation aware CNN features and coupled with the CNN based  More experiments that demonstrate the localization sensitivity of our  model are presented in section 7.3 of technical report [9].|
|||Middle column: Multi-Region CNN model without the semantic segmentation aware CNN features.|
|||Object detection via a multiregion & semantic segmentation-aware cnn model.|
||42 instances in total. (in iccv2015)|
|6|Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper|Depth-aware CNN for RGB-D Segmentation  Weiyue Wang[0000000281148271] and Ulrich Neumann  University of Southern California, Los Angeles, California  {weiyuewa,uneumann}@usc.edu  Abstract.|
|||To address these issues, we present Depth-aware CNN by introducing two intuitive, flexible and effective operations: depth-aware convolution and depth-aware average pooling.|
|||Keywords: Geometry in CNN, RGB-D Semantic Segmentation  1  Introduction  Recent advances [29,37,4] in CNN have achieved significant success in scene understanding.|
|||Taking advantages of the two complementary modalities with CNN is able to improve the performance of scene understanding.|
|||However, CNN is limited to model geometric variance due to the fixed grid computation structure.|
|||Incorporating the geometric information from depth images into CNN is important yet challenging.|
|||Depth-aware CNN incorporate the geometric relations of pixels in both convolution and pooling.|
|||To address the aforementioned problems, in this paper, we present an end-toend network, Depth-aware CNN (D-CNN), for RGB-D segmentation.|
|||The main advantages of depth-aware CNN are summarized as follows:   By exploiting the nature of CNN kernel handling spatial information, geom etry in depth image is able to be integrated into CNN seamlessly.|
|||ABCRGBDepth-aware CNNDepth-Aware ConvolutionGround TruthDepthABCDepth-aware CNN for RGB-D Segmentation  3   Depth-aware CNN does not introduce any parameters and computation com plexity to the conventional CNN.|
|||Depth-aware CNN is a general framework that bonds 2D CNN and 3D geometry.|
|||These advances in 2D CNN and the availability of depth sensors enables progresses in RGB-D segmentation.|
|||[27] built a 3D k-nearest neighbor (kNN) graph neural network on a point cloud with extracted features from a CNN and achieved the state-of-the-art on RGB-D segmentation.|
|||Instead of using 3D representations, we use the raw depth input and integrate 3D geometry into 2D CNN in a more efficient and flexible fashion.|
|||Deformable CNN [7] learns kernel offsets to augment the spatial sampling locations.|
|||For each pixel location p0 on y, the output of standard 2D convolution is  *Conv KernelDepthInput FeatureDepth SimilarityDepthInput FeatureDepth SimilarityDepth-aware CNN for RGB-D Segmentation  5  y(p0) = XpnR  w(pn)  x(p0 + pn),  (1)  where R is the local grid around p0 in x and w is the convolution kernel.|
|||FD  3.3 Understanding Depth-aware CNN  A major advantage of CNN is its capability of using GPU to perform parallel computing and accelerate the computation.|
|||However, this limits the ability of CNN to model geometric variations.|
|||By augmenting the convolution kernel with a depth similarity term, depth-aware CNN captures geometry with transformable receptive field.|
|||In deformable CNN [7], Dai et al.|
|||demonstrate that learning receptive field adaptively can help CNN achieve better results.|
|||To get a better understanding of how depth-aware CNN captures geometry with depth, Figure 3 shows the effective receptive field of the given input neuron.|
|||With the depth-aware term incorporated, they are ad Depth-aware CNN for RGB-D Segmentation  7  justed by the geometric variance.|
|||3.4 Depth-aware CNN for RGB-D Semantic Segmentation  In this paper, we focus on RGB-D semantic segmentation with depth-aware CNN.|
|||Depth-aware CNN based on DeepLab outlined in Table 2 is evaluated to validate the effectiveness of our approach and this is referred as D-CNN in the paper.|
|||To make fair comparison, we also build depth-aware CNN with this two-stream fashion and denote this as D-CNN+HHA.|
|||4.1 Main Results  Depth-aware CNN is compared with both its baseline and the state-of-the-art methods on NYUv2 and SUN-RGBD dataset.|
|||Depth-aware CNN for RGB-D Segmentation  9  RGB  Depth  GT  Baseline  HHA  D-CNN  DCNN+HHA  Fig.|
|||Depth-aware CNN for RGB-D Segmentation  11  [18]  [27] HHA D-CNN D-CNN+HHA  mAcc (%) 48.1 55.2 mIoU (%) 42.0   50.5 40.2  51.2 41.5  53.5 42.0  Table 6.|
|||Using depth images, D-CNN is able to achieve 4% IoU over CNN while preserving the same number of parameters and computation complexity.|
|||Depth-aware CNN To verify the functionality of both depth-aware convolution and depth-aware average pooling, the following experiments are conducted.|
|||Depth-aware CNN for RGB-D Segmentation  13  Acc (%)  mAcc (%)  mIoU (%)  fwIoU (%)  Baseline HHA VGG-1 VGG-2 VGG-3 VGG-4 59.5 37.3 26.6 43.8  50.1 59.1 60.3 56.0 23.9 30.8 39.3 32.2 15.9 21.9 27.8 22.4 34.2 43.0 44.9 40.2  59.3 39.2 27.4 44.0  VGG-1 ResNet-50 D-ResNet-50 69.4 Acc (%) mAcc (%) 53.6 mIoU (%) 41.0 fwIoU (%) 54.5  69.6 53.3 41.5 54.4  68.9 50.2 38.8 54.4  Table 8.|
|||Moreover, we observe depth-aware CNN has a faster convergence than baseline, especially trained from scratch.|
|||Depth similarity helps preserving edge details, however, when depth values vary in a single object, depth-aware CNN may lose contextual information.|
|||Without increasing any model parameters, D-CNN is able to incorporate geometric information in CNN efficiently.|
|||5 Conclusion  We present a novel depth-aware CNN by introducing two operations: depthaware convolution and depth-aware average pooling.|
|||Depth-aware CNN augments conventional CNN with a depth similarity term and encode geometric variance into basic convolution and pooling operations.|
|||Depth-aware CNN provides a general framework for vision tasks with RGBD input.|
|||Moreover, depth-aware CNN takes the raw depth image as input and bridges the gap between 2D CNN and 3D geometry.|
|||In future works, we will apply depth-aware CNN on various tasks such as 3D detection, instance segmentation and we will perform depth-aware CNN on more challenging dataset.|
|||Depth-aware CNN for RGB-D Segmentation  15  References  1.|
|||Hazirbas, C., Ma, L., Domokos, C., Cremers, D.: Fusenet: incorporating depth into  semantic segmentation via fusion-based cnn architecture.|
||42 instances in total. (in eccv2018)|
|7|Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper|To address these problems, this paper proposes a novel and  effective  method  to  learn  a  rotation-invariant  and  Fisher  discriminative CNN (RIFD-CNN) model.|
|||This is achieved  by introducing and learning a rotation-invariant layer and  a Fisher discriminative layer, respectively, on the basis of  the existing high-capacity CNN architectures.|
|||Specifically,  the  rotation-invariant  layer  is  trained  by  imposing  an  explicit regularization constraint on the objective function  that  enforces  invariance  on  the  CNN  features  before  and  after rotating.|
|||The Fisher discriminative layer is trained by  imposing  the  Fisher  discrimination  criterion  on  the  CNN  features  so  that  they  have  small  within-class  scatter  but  large  between-class  separation.|
|||While the CNN features have shown impressive success  for object detection, the problems of object rotation, within-class  variability, and between-class similarity still remain several major  challenges.|
|||The  proposed rotation-invariant and Fisher discriminative CNN model  provides a possible solution to address these problems.|
|||Then, R-CNN uses AlexNet model [2]  to  extract  CNN  features  from  object  proposals  and  classifies  them  as  objects  or  non-objects  by  using  class specific linear support vector machines (SVMs), where the  AlexNet  CNN  model,  with  more  than  60  million  parameters,  was  first  pre-trained  on  an  auxiliary  task  of  image classification in the ImageNet ILSVRC challenge [3]  and then transferred and fine-tuned on a small set of images  annotated  for  the  detection  task.|
|||The success of R-CNN method [5] is largely attributed to  the ability of CNN model to extract more richer high-level  object  representation  features  as  opposed  to  hand engineered low-level features such as SIFT [31] and HOG  [32].|
|||However,  while  the  CNN  features  have  shown  impressive success for object detection tasks, they are still  difficult  to  effectively  deal  with  the  challenges  (as  illustrated  in  Figure  1)  of  object  rotation,  within-class  variability,  and  between-class  similarity,  which  are  some  important sources of detection error.|
|||To  address  these  problems  and  to  further  improve  the  state-of-the-arts,  in  this  paper  we  propose  a  novel  and  effective  method  to  learn  a  rotation-invariant  and  Fisher  discriminative CNN (RIFD-CNN) model.|
|||Our main contributions are summarized as follows: First,  we build on the existing high-capacity CNN architectures  [2, 9] to train a rotation-invariant CNN (RI-CNN) model by  adding  and  learning  a  new  rotation-invariant  layer.|
|||Second, we propose a new method to train a Fisher  discriminative CNN (FD-CNN) model by introducing and  learning  a  Fisher  discriminative  layer.|
|||The  Fisher  discriminative  layer  is  trained  by  imposing  the  Fisher  discrimination  criterion  on  the  CNN  features  so  that  they  have  small  within-class  scatter  but  large  between-class   separation.|
|||The introduction of the R-CNN framework [5] opens the  door to extract rich features through deep CNN models to  improve object detection performance.|
|||In the work of [5],  AlexNet CNN [2] was used to extract a set of deep features  from  category-independent  region  proposals  provided  by  selective  search  [29]  and  then  class-specific  linear  SVMs  were  adopted  to  classify  them.|
|||By  adopting  the  R-CNN  framework  [5]  with  a  deeper  16-layers  VGGNet  CNN  model [9], the performance was further boosted.|
|||[17]  addressed  the  inaccurate  localization problem by using a search algorithm based on  Bayesian optimization that sequentially proposes candidate  regions for an object bounding box and training the CNN  with  a  structured  loss  that  penalizes  the  localization  inaccuracy explicitly.|
|||Our  method  is  also  built  upon  the  remarkable  R-CNN  framework [5] with the existing CNN architecture such as  AlexNet  [2]  and  VGGNet  [9].|
|||However,  different  from  previous work, this paper mainly focuses on enriching the  power  of  the  CNN  feature  representations  via  imposing  rotation  invariance  and  fisher  discriminative  criterion  on  the  objective  function  of  our  new  RIFD-CNN  model.|
|||In the second step, we build on the existing  high-capacity CNN architectures to train our rotation-invariant CNN model by adding and learning a new rotation-invariant layer.|
|||Proposed method   The  goal  of  our  method  is  to  learn  a  rotation-invariant  and Fisher discriminative CNN model in order to advance  the  performance  of  object  detection.|
|||This  is  achieved  by  introducing and learning a rotation-invariant layer (Figure  2) and a Fisher discriminative layer (Figure 3), respectively,  on  the  basis  of  the  existing  high-capacity  CNN  architec tures.|
|||The  Fisher  discriminative  layer  is  trained  by  imposing  the  Fisher  discrimination  criterion  on  the  CNN  features  so  that  they  have  small  within-class  scatter  but  large  between-class  separation.|
|||In the remainder of this section we first describe  how to learn rotation-invariant CNN model and next detail  the training of Fisher discriminative CNN model.|
|||Learning rotation-invariant CNN model   The framework of the proposed rotation-invariant CNN  (RI-CNN)  model  training  is  illustrated  in  Figure  2.|
|||In  the  second  step,  we  build  on  the  existing  popular  CNN  architectures,  such  as  AlexNet  [2]  and  VGGNet [9], to train our rotation-invariant CNN model by  adding and learning a new rotation-invariant layer.|
|||Learning Fisher discriminative CNN model   As illustrated in Figure 3, our Fisher discriminative CNN  (FD-CNN)  model  is  designed  by  adding  a  new  Fisher  discriminative  fully-connected  layer  FCc  that  uses  the  output of layer FCm or FCa (in this situation by combining  RI-CNN  and  FD-CNN  together  we  can  obtain  a  more  powerful RIFD-CNN model) as input.|
|||isher discriminative layer is trained via imposing the Fisher  discrimination criterion on the CNN features to enforce the  learned  FD-CNN  features  have  small  within-class  scatter  and large between-class separation.|
|||(10) is a discrimination  regularization  constraint  imposed  on  the  CNN  features.|
|||(16),  the  new  discriminative  objective  function  not  only  minimizes  the  classification  loss,  but  also  imposes  a  regularization  constraint  to  make  the learned CNN features be more discriminative.|
|||2889  AP (%) Model  72.7  RC-RBM IHOF [46] + linear SVM  74.7  Gradients IHOF [46] + linear SVM  RC-RBM [46] + Gradients IHOF [46] + linear SVM  77.6  Standard HOG [32] + slot kernel structured SVM [52]  75.7  Rotation-invariant HOG [51] + linear SVM  82.6  84.2  Rotation-invariant HOG [51] + Random Forest  90.2  Fine-tuned CNN with AlexNet [2] + linear SVM  Our RI-CNN with AlexNet [2] + linear SVM  94.6  Table 1 The detection result comparison for different methods on  the aerial car detection dataset.|
|||To  adapt  it  to  the  new  aerial  car  detection  task,  we  first  perform  SGD  fine-tuning  of  the  whole  CNN  parameters  with a 2-way softmax classification layer (one for car and  the  other  for  background)  using  augmented  training  samples obtained from the aerial dataset [33].|
|||As can be seen from Table 1, using a simple linear  SVM  classifier,  our  RI-CNN  model  can  1)  significantly  improve  the  performance  of  the  traditional  CNN  model  with  AlexNet  architecture  [2]  fine-tuned  on  the  aerial  car  dataset,  which  is  also  the  baseline  of  our  method,  and  2)  outperform all other recent publications [46, 51, 52] which  address the rotation problem in different ways, where [52]  uses  slot  kernel  structured  SVM  and  the  standard  HOG  feature, [46] focuses on learning rotation-invariant feature  and  descriptor  called  RC-RBM  and  IHOF,  respectively,  and [51] presents rotation-invariant HOG descriptors using  Fourier analysis in polar and spherical coordinates.|
|||We  build  on  the  high-performance  VGGNet  CNN  model  [9],  that  was pre-trained on Image Net [3] and then fine-tuned on PASCAL VOC 2007 dataset  to  train  our  RI-CNN  model,  FD-CNN  model,  and  RIFD-CNN  model,  respectively.|
|||Following  the  R-CNN  framework  [5],  we  use  our  trained  models to extract new CNN features from object proposals  provided  by  selective  search  method  [29],  classify  them  with class-specific linear SVMs (trained using ground-truth  positive  samples  and  negative  samples  obtained  via  hard  negative  mining  [30]),  and  then  perform  non-maximum  suppression and bounding box regression [30].|
|||Table  2 reports the detection performance of our improved CNN  models  including  RI-CNN  model,  FD-CNN  model,  and  their  combination  (RIFD-CNN  model).|
|||Detection performance on PASCAL VOC 2007 test set using our improved CNN models including RI-CNN model, FD-CNN  model, RIFD-CNN model, and the baseline method of R-CNN [5] with VGGNet [9] and bounding box regression (BB).|
|||Model   car  cat  chair cow table dog horse mbike person  plant  sheep  sofa train  aero bike  bird  boat  bottle bus  tv  mAP  DPM v5 [30]   DPM ST [34]   33.2 60.3  10.2  16.1  27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1  48.2  43.2  12.0  21.1  36.1 46.0 43.5 33.7  23.8 58.2  10.5  8.5  27.1 50.4 52.0 7.3  19.2 22.8 18.1 8.0  55.9  44.8  32.4  13.3  15.9  22.8 46.2 44.9 29.1  DPM HSC [38]   32.2 58.3  11.5  16.3  30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1  51.6  39.9  12.4  23.5  34.4 47.4 45.2 34.3  CNN-DPM-BB [22]   50.9 64.4  43.4  29.8  40.3 56.9 58.6 46.3 33.3 40.5 47.3 43.4 65.2  60.5  42.2  31.4  35.2  54.5 61.6 58.6 48.2  E2E-DPM [23]   DP-DPM [20]   49.3 69.5  31.9  28.7  40.4 61.5 61.5 41.5 25.5 44.5 47.8 32.0 67.5  61.8  46.7  25.9  40.5  46.0 57.1 58.2 46.9  44.6 65.3  32.7  24.7  35.1 54.3 56.5 40.4 26.3 49.4 43.2 41.0 61.0  55.7  53.7  25.5  47.0  39.8 47.9 59.2 45.2  Sliding-window CNN [12]   64.1 72.3  62.8  44.0  44.2 66.4 72.5 67.7 35.2 68.9 35.9 62.7 69.0  65.7  65.8  36.2  60.1  50.3 63.2 66.0 58.6  R-CNN with AlexNet   64.2 69.7  50.0  41.9  32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6  66.8  54.2  31.5  52.8  48.9 57.9 64.7 54.2  R-CNN with VGGNet   71.6 73.5  58.1  42.2  39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9  69.6  59.3  35.7  62.1  64.0 66.5 71.2 62.2  R-CNN with AlexNet & BB   68.1 72.8  56.8  43.0  36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1  68.6  58.7  33.4  62.9  51.1 62.5 64.8 58.5  R-CNN with VGGNet & BB   73.4 77.0  63.4  45.4  44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1  73.1  64.2  35.6  66.8  67.2 70.4 71.1 66.0  + FGS [17]   74.2 78.9  67.8  51.6  52.3 75.7 78.7 76.6 45.4 72.4 63.1 76.6 79.3  70.7  68.0  40.3  67.8  61.8 70.2 71.6 67.2  + StructObj [17]   73.1 77.5  69.2  47.6  47.6 74.5 78.2 75.4 44.5 76.3 64.9 76.7 76.3  69.9  68.1  39.4  67.0  65.6 68.7 70.9 66.6  + StructObj + FGS [17]   74.1 83.2  67.0  50.8  51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4  75.1  70.1  41.4  69.6  60.8 70.2 73.7 68.5  + RIFD-CNN (ours)   77.4 80.8  70.7  49.9  47.2 77.6 79.9 82.7 48.6 76.1 67.1 81.9 80.2  74.6  71.9  40.9  69.5  70.7 73.5 74.2 69.8  78.9 82.5  69.6  54.2  49.7 78.3 82.0 83.4 51.1 76.0 69.0 82.2 80.7  77.2  73.1  42.6  70.3  70.4 74.2 74.1 71.0  + RIFD-CNN + FGS (ours)  Table 3.|
|||Rows 1-7 show sliding window detectors that  employ  different  features,  where  the  first  [30]  uses  only  HOG,  the  next  two [34,  38]  use  different  feature  learning  approaches  to augment or  replace HOG,  and  the  last  four  [12, 20, 22, 23] employ CNN features.|
|||As  shown  in  Table  3,  we  achieve  state-of-the-art results for 17 out of 20 categories compared      2891  In this paper, we proposed an effective method to boost  the performance of object detection in R-CNN framework  [5] by learning a rotation-invariant and Fisher discrimina tive  CNN  model.|
|||Deformable  Part  Models with  CNN  Features.|
||41 instances in total. (in cvpr2016)|
|8|Wang_Modality_and_Component_CVPR_2016_paper|Modality and Component Aware Feature Fusion for  RGB-D Scene Classification  Anran Wang1, Jianfei Cai1, Jiwen Lu2, and Tat-Jen Cham1  1 School of Computer Engineering, Nanyang Technological University, Singapore  2 Department of Automation, Tsinghua University, Beijing, China  Abstract  While convolutional neural networks (CNN) have been excellent for object recognition, the greater spatial variability in scene images typically meant that the standard full-image CNN features are suboptimal for scene classification.|
|||In this paper, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV) encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead.|
|||The CNN features are computed from an augmented pixel-wise representation comprising multiple modalities of RGB, HHA and surface normals, as extracted from RGB-D data.|
|||After a substantial leap in object recognition [13] performance using convolutional neural networks (CNN) trained on large-scale object-centric datasets such as ImageNet [4], a scene-centric dataset known as Places [36] was introduced for investigating the utility of CNN in scene classification tasks.|
|||Although there was a reported performance improvement using a scene-centric CNN, it became obvious that  Proposal CNN features  Fisher Vector  Global CNN   feature  Modality and Component Aware Feature Fusion  Bedroom  Figure 1.|
|||Then for all the proposals and the full image, we derive CNN features from different modalities: RGB, HHA and surface normal (SN).|
|||For each image, the proposal based CNN features for each modality are encoded by Fisher Vector (FV), and the resulted multi-modal FV features are regarded as the input to our modality and component aware feature fusion.|
|||Finally we combine the regression results of the proposal based FV features and the full-image based CNN features to get the final classification result.|
|||global CNN features extracted from full images were too spatially rigid to be optimal for scene classification.|
|||They share a similar pipeline: CNN features were densely extracted at different locations and scales of  5995  an image, encoded as a combined feature representation (e.g.|
|||Results show that the local features are competitive when compared to fullimage based CNN features and provide important complementary information.|
|||In particular, we use an existing object proposal extractor to generate region proposals from each RGB-D image, representing each proposal by the corresponding local CNN features obtained from different modalities.|
|||In order to more explicitly capture geometric information, we also extract CNN features from an additional modality of surface normals (SN).|
|||For each modality, we use FV to encode the regionproposal-based CNN features.|
|||Finally, by learning and combining regressors for  both proposal-based FV features and full-image CNN features, we were able to achieve state-of-the-art performance on the SUNRGBD Dataset [23] and NYU Depth Dataset V2 [17].|
|||To adapt the current CNN techniques for scene classification, Zhou et al.|
|||[36] introduced a large scene-centric dataset called Places and showed significant performance improvement on scene classification using a CNN trained on this dataset, as compared to directly applying the CNN pretrained on the object-centric dataset ImageNet [4].|
|||Although a scene-centric dataset more appropriately captures the richness and diversity of scene imagery, the typical way of extracting global CNN features from full images may not adequate handle the geometric variability of complex indoor scenes.|
|||[5] proposed densely extracting multi-scale CNN activations, aggregating the activations of each scale via vector of locally aggregated descriptors (VLAD) [11], and concatenating the multi-scale VLAD features together as the final feature representation.|
|||[30] approached the multi-label image classification problem through multi-view learning, where they derived a feature view by extracting CNN features from object proposals followed by the FV encoding, and constructed a label view using strong labels.|
|||There is a recent work in which CNN is used as the feature extraction method for RGB-D data [8], where Gupta et al.|
|||This makes it possible to directly apply the CNN model pre-trained on RGB images, which also have three channels, to HHA to extract CNN features for depth.|
|||For scene classification, they directly used pre-trained CNN in [36] to extract CNN features from RGB and HHA.|
|||[14] proposed to include a regularization on semantic segmentation to improve scene classification performance, where their cost function to train CNN contains both the loss of scene classification and the loss of semantic segmentation.|
|||Multi-modal Proposal-based Global Feature  Representation  In our framework,  local information is incorporated through the use of region proposals and their corresponding local CNN features.|
|||Since all three modalities comprise three channels each, we start with the same 8-layer CNN model pretrained on the Places Dataset [36] for each modality, but then fine-tuned independently.|
|||layer 6 in the 8layer CNN) in each modality as the CNN features for each proposal.|
|||In order to reduce computational complexity, the number of dimensions in the CNN full6 activation vectors is reduced from 4096 to d = 400 per modality via PCA.|
|||In this way, given an RGB-D image with J extracted object proposals, each proposal in each modality is represented by its corresponding CNN feature vector f ij  Rd.|
|||The CNN features for all proposals within a single RGBD image is then encoded with the standard Fisher Vector (FV) [18, 20] approach.|
|||Fine-tuning: Our starting point is the current state-ofthe-art CNN model (Places-CNN) for scene classification, pre-trained on the Places Dataset [36] (2.5 million RGB images with 205 scene categories).|
|||To better adapt the pre-trained CNN network for RGB-D data, especially for the HHA and surface normal modalities, we fine-tuned the  5999  Figure 4.|
|||After one of the fine-tuned CNNs has operated on a region proposal in an image, the CNN activation vector of the first fully connected layer (full6) is extracted.|
|||Comparing the classification results of the proposal based FV features and the full-image based CNN features under different modalities with linear SVM classifier on SUNRGBD Dataset.|
|||Results on SUNRGBD Dataset  We first compare the linear SVM classification results of the proposal-based FV features and the full-imagebased CNN features obtained from different combinations of modalities, without including our proposed regularization terms.|
|||We considered three baselines: 1) Full (SVM): the full-image-based CNN features with SVM; 2) FV (SVM): the proposal-based FV features with SVM; and 3) FV+Full (SVM): concatenating the full-image features and the FV features prior to linear SVM classification.|
|||Comparing the classification results of the proposal based FV features and the full-image based CNN features under different modalities with linear SVM classifier on NYU Depth Dataset V2.|
|||Here we consider seven other settings: 1) FV (L1): using our framework only on proposal-based CNN features and with only the R3 (L1-norm) regularization term active; 2) FV (Modality+L1): proposal-based features only with R2 and R3 active; 3) FV (Component+L1): proposal-based features only with R1 and R3 active; 4) FV (Modality+Component+L1): proposal-based features only with R1, R2 and R3 active; 5) Full (L1): using our framework only on full-image based CNN features, with only R3 active; 6) Full (Modality+L1): fullimage features only with R2 and R3 active; 7) Combine FV and Full: combined regression using both FV (Modality+Component+L1) and Full (Modality+L1), which is our final result.|
|||Although Full (Modality+L1) does not outperform Full (SVM) (mainly because the full-image based CNN features are not of high dimensions), the combination of the regression results of the FV features and the full-image based features (Combine FV and Full) achieves the best performance.|
|||Table 4 compares the classification results of the proposal-based FV features and the fullimage-based CNN features with linear SVM classifier.|
|||By combining the regression results of the proposal based multi-modal FV features and the full-image based multimodal CNN features, we achieved state-of-the-art scene classification performance on the SUNRGBD Dataset and the NYU Depth Dataset V2.|
||41 instances in total. (in cvpr2016)|
|9|Xu_A_Discriminative_CNN_2015_CVPR_paper|A Discriminative CNN Video Representation for Event Detection  Zhongwen Xu Yi Yang Alexander G. Hauptmann  QCIS, University of Technology, Sydney SCS, Carnegie Mellon University zhongwen.xu@student.uts.edu.au yee.i.yang@gmail.com alex@cs.cmu.edu  Abstract  In this paper, we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available.|
|||The focus of this paper is to effectively leverage deep Convolutional Neural Networks (CNNs) to advance event detection, where only frame level static descriptors can be extracted by the existing CNN toolkits.|
|||This paper makes two contributions to the inference of CNN video representation.|
|||[22] is the only attempt to apply CNN features in TRECVID MED 2013.|
|||CNNavg are our results from the average pooling representation of frame level CNN descriptors.|
|||performance of CNN based video representation is worse than the improved Dense Trajectories in TRECVID MED 2013 [22, 3], as shown in Table 1.|
|||Firstly, CNN requires a large amount of labeled video data to train good models from scratch.|
|||Lastly, given the frame level CNN descriptors, we need to generate a discriminative video level representation.|
|||Average pooling is the standard approach [32, 3] for static local features, as well as for the CNN descriptors [22].|
|||Table 1 shows the performance comparisons of the improved Dense Trajectories and CNN average pooling representation.|
|||We can see that the performance of CNN average pooling representation cannot get better than the hand-crafted feature improved Dense Trajectories, which is fairly different from the observations in other vision tasks [12, 6, 13].|
|||First, this is the first work to leverage the encoding techniques to generate video representation based on CNN descriptors.|
|||Second, we propose to use a set of latent concept descriptors as frame descriptors, which further diversifies the output with aggregation on multiple spatial locations at deeper stage of  1However, with certain modification of the CNN structure, e.g.|
|||With these two contributions, the proposed video CNN representation achieves more than 30% relative improvement over the state-of-the-art video representation on the large scale MED dataset, and this can be conducted on a single machine in two days with 4 GPU cards installed.|
|||Though the structure in [37] is much deeper than the classic CNN structure in [21, 6, 12], the subscripts of pool5, fc6 and fc7 notations still correspond if we regard the convolution layers between the max-pooling layers as a compositional convolutional layer [37].|
|||Video CNN Representation  We begin by extracting the frame level CNN descriptors using the Caffe toolkit [18] with the model shared by [37].|
|||We then need to generate video level vector representations on top of the frame level CNN descriptors.|
|||Average Pooling on CNN Descriptors  As described in state-of-the-art complex event detection systems [3, 32], the standard way to achieve image-based video representation in which local descriptor extraction relies on individual frames alone, is as follows: (1) Obtain the descriptors for individual frames; (2) Apply normalization on frame descriptors; (3) Average pooling on frame descriptors to obtain the video representation, i.e., xvideo = 1 i=1 xi, xi is the frame-level descriptor and N is the total number of frames extracted from the video; (4) Re-normalization on video representation.|
|||Video Pooling on CNN descriptors  Video pooling computes video representation over the whole video by pooling all the descriptors from all the frames in a video.|
|||To our knowledge, this is the first work on the video pooling of CNN descriptors and we broaden the encoding methods from local descriptors to CNN descriptors in video analysis.|
|||, K}, where k, k, k are the mean, variance and prior parameters of k-th component learned from the training CNN descriptors in the frame level, respectively.|
|||, xN ) of CNN descriptors extracted from a video, we have mean and covariance deviation vectors for the k-th component as:  uk =  vk =  1  N2k  N  Xi=1  N  1  Xi=1  k (cid:19) qki(cid:18) xi  k Nk qki"(cid:18) xi  k  1# , k (cid:19)2  (1)  where qki is the posterior probability.|
|||By concatenation of the uk and vk of all the K components, we form the Fisher vector for the video with size 2DK, where D is the dimension of CNN descriptor xi after PCA pre-processing.|
|||3.2.3 Quantitative Analysis  Given the above three approaches, we need to find out which one is the most appropriate for the CNN descriptors.|
|||To this end, we conduct an analytic experiment on the MEDTest 14 training set [2] to study the discriminative ability of three types of video representations, i.e., average pooling, video pooling with Fisher vector, and video pooling with VLAD on the CNN descriptors.|
|||From the above analytic study, we can see that VLAD is the most fit for the CNN descriptors because the VLAD representation has the best discriminative ability, which is also consistent with the experimental results in Section 5.1.|
|||After extracting the CNN latent concept descriptors for all spatial locations of each frame in a video, we then apply video pooling to all the latent concept descriptors of that video.|
|||As in [14], we apply four different CNN maxpooling operations and obtain (6  6), (3  3), (2  2) and (1  1) outputs for each independent convolutional filter, a total of 50 spatial locations for a single frame.|
|||We sample every five frames in the videos and follow the pre-processing of [21, 6] on CNN descriptor extraction.|
|||Results for Video Pooling of CNN descriptors  In this section, we show the experiments on video pooling of fc6, fc6 relu, fc7 and fc7 relu.|
|||Unlike local descriptors such as HOG, MBH, which have dimensions less than 200-D, the CNN descriptors have much higher dimensions (4,096-D).|
|||Trajectories (IDT) and average pooling on CNN descriptors in Figure 3.|
|||We can see very clearly that VLAD encoded CNN features significantly outperform IDT and average pooling on CNN descriptors over all settings.|
|||Results for CNN Latent Concept Descriptors  with Spatial Pyramid Pooling  We evaluate the performance of latent concept descriptors (LCD) of both the original CNN structure and the structure with the Spatial Pyramid Pooling (SPP) layer plugged in to validate the effectiveness of SPP.|
|||LCDVLAD is VLAD encoded LCD from the original CNN structure, while LCDVLAD + SPP indicates VLAD encoded LCD with SPP layer plugged in.|
|||The aggregation at a deeper stage to generate multiple levels of spatial information via multiple CNN max-pooling demonstrates advantages over the original CNN structure while having only minimal computation costs.|
|||Impact of dimensions of CNN descriptors after PCA, with fixed K = 256 in VLAD.|
|||We provide results for average pooling on CNN descriptors with late fusion of three layers as well, denoted as CNNavg.|
|||Using Convolutional Neural Network (CNN) representation seems to be a good solution, but generating video representation from CNN descriptors has different characteristics from image representation.|
|||We are the first to leverage encoding techniques to generate video representation from CNN descriptors.|
|||And we propose latent concept descriptors to generate CNN descriptors more properly.|
||41 instances in total. (in cvpr2015)|
|10|Li_Mid-Level_Deep_Pattern_2015_CVPR_paper|Specifically, we find that for an image patch, activation extracted from the first fully-connected layer of a CNN have two appealing properties which enable its seamless integration with pattern mining.|
|||Patterns are then discovered from a large number of CNN activations of image patches through the wellknown association rule mining.|
|||Our approach also outperforms or matches recent works using CNN for these tasks.|
|||In this sense, an extra bonus of our formulation lies in that we are now relying on CNN activations, a more appealing alternative than the hand-crafted HOG, as indicated in recent works [17, 18, 19, 20, 21, 22].|
|||Thanks to two appealing properties of CNN activations (Sec.|
|||We present two properties of CNN activations that allow seamless integration with association rule mining, avoiding the limitations of pattern mining algorithms.|
|||To extract CNN activations, we rely on the publicly available caffe [23] reference model which was pre-trained on the ImageNet [24].|
|||We extract its non-negative 4096-dimensional CNN activation from the sixth layer fc6 (the first fully-connected layer) after the rectified linear unit (ReLU) transformation.|
|||In the benchmark CNN architecture of Krizhevsky et al.|
|||Properties of CNN activations of patches  In this section we provide a detailed analysis of the performance of CNN activations on the MIT Indoor dataset [34], from which we are able to conclude two important properties thereof.|
|||Then, for each image patch, we extract its 4096-dimensional CNN activation using caffe.|
|||car) and the natural world, we represent each as a transaction after extracting their CNN activation.|
|||The classification accuracies achieved by the two proposed strategies for keeping the k largest magnitudes of CNN activations of image patches on the MIT Indoor dataset.|
|||The feature representation for an image is the outcome of max pooling on these binarized CNN activations.|
|||In comparison, our baseline feature, which is the outcome of max pooling on CNN activations of all patches in an image, gives an accuracy of 65.15%.|
|||In this work, we rely on its CNN activation which has two appealing properties (Sec.|
|||More specifically, we treat each dimension index of a CNN activation as an item (4096 items in total).|
|||Specifically, due to the sparse nature of CNN activations (sparse property in Sec.3), each integer vector transaction calculated as described contains only k items, and k can be set to be small (20 in all of our experiments).|
|||Therefore, each complete transaction has k + 1 items, consisting of indices of k largest CNN magnitudes plus one class label.|
|||For example, if we set k equals three, given a CNN activation of an image patch from the target category which has 3 largest magnitudes in its 3rd, 100th and 4096th dimensions, the corresponding transaction will be {3,100,4096,pos}.|
|||After extracting their CNN activations from caffe, a transaction database D is created containing a large number of transactions created using the proposed technique above.|
|||In the MergingTrain function, we begin by selecting the element covering the maximum number of training images, followed by training a Linear Discriminant  and corresponding element detectors D  Algorithm 1: Ensemble Merging Pseudocode Input: A set of partially redundant visual elements V Output: A set of clean mid-level visual elements V(cid:48) Initialize V(cid:48)  , Dc  ; while V (cid:54)=  do [Vt,d]  MergingTrain(V); V  V \ Vt; V(cid:48)  V(cid:48)  {  V Vt D  D  {d};  V };  end return V(cid:48), D; Function MergingTrain(V)  Select V   V which covers the maximum number of training images; Initialize Vt  {V }, S   ; repeat  Vt  Vt  S; Train LDA detector d using Vt; S  {V  V \ Vt|Score(V,d) > T h} where Score(V,d) = 1|V | pre-defined threshold, x is a CNN activation of an image patch in V );  xV dT x (T h is a  (cid:80)  until S = ; return Vt, d;  Analysis (LDA) detector [36].|
|||For each image, we resize its smaller dimension to 256 while maintaining its aspect ratio, then we sample 128128 patches with a stride of 32 pixels, and calculate the CNN activations using caffe.|
|||Thanks to the fact that CNN activations from caffe are invariant to horizontal flipping [22], we avoid adding right-left flipped images (c.f .|
|||As we are the first to use CNN activation for mid-level visual elements discovery, a natural question is that what is the performance of previous works if CNN  Method D-patch [1] BoP [3] miSVM [5] MMDL [6] D-Parts [4] RFDC [7] DMS [2] LDA-Retrained LDA-Retrained LDA-KNN LDA-KNN Ours Ours  #elements Acc(%) 38.10 46.10 46.40 50.15 51.40 54.40 64.03 58.78 62.30 59.14 63.93 68.24 69.69  210 50 20 11 73 50 200 20 50 20 20 20 50  Table 2.|
|||Classification results of methods using CNN activations on the MIT Indoor dataset.|
|||To answer this question, we implement two baselines using CNN activations as image patch representations.|
|||As shown in Table 2, we report much better results than both baselines in the same setting, which verifies that the proposed MDPM is an essential step for achieving good performance when using CNN activations for mid-level visual element discovery.|
|||Comparison to methods using CNN activation.|
|||In Table 3, we compare our approach to others in which CNN activation is involved.|
|||Our work is most closely related to MOP-CNN [22] and SCFVC [43] in the sense that all these works rely on off-the-shelf CNN activations of image patches.|
|||To encode these local CNN activations, MOP-CNN [22] rely on the classical VLAD encoding [44], whereas SCFVC [43] is a new Fisher encoding [45] strategy for encoding high-dimensional local features.|
|||Fine-tuning has been shown to be beneficial when transferring pre-trained CNN models on the ImageNet to another Jittered CNN features (e.g., crops, dataset [19, 20, 21].|
|||After concatenating with CNN activations of the whole image (both normalized to unit norm), our performance increases to 70.46%, outperforming all previous works using CNN on this dataset.|
|||Given pre-computed CNN activation from about 0.2 million patches, the baseline method LDA-Retrained takes about 9 hours to find visual elements in a class.|
|||Comparison to methods using CNN activation.|
|||Table 4 reports our results along with those of other recent methods based on CNN activation.|
|||Classification results of methods using CNN activations on the PASCAL VOC 2007 dataset.|
|||75.2% mean average precision (mAP), significantly outperforming the baseline that using CNN activations as a global feature (67.3%), as well as its average pooling and max pooling counterparts.|
|||In the process we have shown not only that it is profitable to apply pattern mining technique to mid-level visual element discovery, but also that, from the right perspective, CNN activations are particularly well suited to the task.|
|||[18] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, Cnn features off-the-shelf: An astounding baseline for recognition, in Proc.|
||41 instances in total. (in cvpr2015)|
|11|Diba_DeepCAMP_Deep_Convolutional_CVPR_2016_paper|In particular, we train a newly designed CNN (DeepPattern) that learns discriminative patch groups.|
|||Our discriminative mid-level mining CNN obtains state-of-theart results on these datasets, without a need for annotations about parts and poses.|
|||The paper presents a newly designed CNN to extract such information by identifying informative image patches.|
|||At the end of the training, the CNN has become an expert in detecting those image patches that distinguish human actions and attributes.|
|||The CNN comes with the features extracted from those patches.|
|||Section 3 describes our framework and new CNN for the mining and detection of discriminative patches for human action and attribute classification.|
|||ject detection scheme (RCNN) by extracting CNN features from object region proposals.|
|||[37] used HOGposelets to train a part-based CNN model for attribute classification.|
|||The part-based RCNN can discriminate birds by learned models of their parts, by fine-tuning a CNN trained on ImageNet.|
|||[5] trained a deep CNN with prepared HOG poselets as training data and detected humans based on the resulting deep poselets.|
|||for the head and torso, based on CNN pool5 feature sliding window search and combined them with the whole body box to train a CNN jointly.|
|||One of the state-of-the-art contributions in mid-level element mining is [22], which applies pattern mining to deep CNN patches.|
|||To the best of our knowledge, the use of CNN mid-level elements for action and attribute classification, as is the case in this paper, is novel.|
|||In the first part of this section we talk about the motivation and  3558  ... ...  Deep  Pattern  Mining  Input Image  Initial Patch Feature extraction and Clustering  Patch CNN Training  ...  ...  Fc7    Embedding  Action,  Attribute   Classification  Patch Harvesting  Patch Cluster   Updating   and Classifier   Training  Figure 2.|
|||We design an iterative algorithm where in each iteration, we improve the embedding by training a new CNN to classify cluster labels obtained in the previous iteration.|
|||This approach in MDPM makes it an interesting method because the specific properties of activation extracted from the fully-connected layer of a CNN allow them to be seamlessly integrated with association rule mining, which enables the discovery of category-specific patterns from a large number of image  3559  patches.|
|||Finally, we pass the new patches with associate cluster labels to learn a new CNN based representation.|
|||Training patch clusters CNN Our main insight is that the representation of image patches plays an important role in clustering.|
|||Assuming that the initial clustering is reasonable, in this block, we train a new CNN to improve the representation.|
|||The new CNN is trained so that given patch images, it predicts their cluster label.|
|||This is in contrast to the initial CNN that was learned to classify bounding box images to different action categories.|
|||Input: Image set (I , L) Extract dense patche: Pi Extract initial features Fi while Convergence do  j (jth patch of ith image) j and initial cluster labels Ci  j  CN NP atch = Train CNN(P,C) F  Extract CNN Feature(CN NP atch,P) C  Update Cluster(F,C) W = Train Patch Classifier(F,C) S= Compute Score(W,F) for all patches do  if Si < th then j Eliminate P i j  end  end  end Output: Mid-Level Pattern Clusters (C)  3.3.|
|||Mid(cid:173)level Deep Patterns Network  In updating the representation, we train a CNN to predict the cluster labels for given image patches.|
|||Using fast RCNN helps us to have an efficient, fast and computationally low cost CNN layers calculations, since convolutions are applied at an image-level and are afterward reused by the ROI-specific operations.|
|||Our network is using a pretrained CNN model on ImageNet-1K with Alex-Net[18] architecture, to perform fine-tuning and learn patch network.|
|||Finally, we pass the whole bounding box through overall CNN trained on action or attribute la 3560  ...  Conv1  Conv4  Patch Conv5  256  256  Human Conv5  512  Action1_Cluster1   Action1_Cluster2   .|
|||To train this CNN for mid-level discriminative patches, we concatenate the conv5 layers of patch and the person regions to abstract the visual distinctive information of the patch with holistic clue of the person who is performing an action or has a specific attribute.|
|||Experimental Setup  Common properties of the networks All of the networks have been trained using the caffe CNN training package [17] with back-propagation.|
|||We set the learning rate of CNN training to 0.0001, the batch size to 100.|
|||The new obtained CNN representations for patches help updating clusters to be performed more precise.|
|||For the action Classification task, we evaluate our midlevel pattern mining pipeline and proposed patch CNN network performances on PASCAL VOC and Stanford 40 action datasets.|
|||The training and fine-tuning of the initial CNN and pattern CNN, have been done only on PASCAL VOC dataset.|
|||The two first rows are baselines of our method, which are results of training CNN on pascal and using MDPM to classify them.|
|||The ours Alex methods rows are the results of iterating 1,2 and 3 times in the iterative process of pipeline using the Alex-Net architecture as patch CNN training block.|
|||The used initial CNN and patch CNNs in this section are trained on the PASCAL VOC dataset, and we use these networks to extract patches form Images of Stanford40 dataset.|
|||ing patch CNN networks on a dataset with less classes than the test dataset is to evaluate discrimination power of our proposed methods extracted patches.|
|||The baseline CNN in the first row of table is AlexNet trained on PASCAL VOC dataset using SVM on  the fc7 layer features.|
|||Finally the last three rows are same as previous ones with 1 to 3 iterations applying our proposed pattern CNN architecture.|
|||We can conclude from the table that our proposed iterative pipeline and newly proposed CNN architecture can improve the result independently, so the combined method outperform either one alone.|
|||As we can see in the table the mean accuracy of our proposed method with the proposed PatternNet outperforms all the previous 8 layer CNN network based methods.|
|||As we mentioned in the implementation details we evaluate the Stanford40 actions dataset using our final pipeline mid-level patterns CNN PatternNet iter3 which is trained on PASCAL VOC, and report the results in Table 3.|
||41 instances in total. (in cvpr2016)|
|12|Semantic Regularisation for Recurrent Image Annotation|Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN.|
|||Specifically, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN.|
|||A CNN is used to encode the image into a fixed length vector, which is then fed into an RNN that either decodes it into a list of tags (multi-label) or sequence of words composing a sentence (captioning).|
|||Existing work differs slightly in how the CNN and RNN models are interfaced (see Figs.|
|||However, they share a key characteristic: the image embedding that provides the CNN-RNN interface is the final feature layer of the CNN [14, 22, 31] (e.g.|
|||First, since the CNN output feature is not explicitly semantically meaningful, both the label prediction and label correlation/grammar modelling tasks now need to be shouldered by the RNN model alone.|
|||(a) CNN encodes an image (I) to a feature representation (F).|
|||(b) The image CNN output features F set the LSTM hidden states [14].|
|||(c) The image CNN output feature layer is integrated with the LSTM output via late fusion [22, 31].|
|||The CNN model is regularised by the ground truth semantic concepts s, which serve as strong deep supervision to guide the learning of the CNN layers.|
|||The CNN prediction layer s is used as image embedding which is used to set the LSTM initial states.|
|||In addition, joint training of CNN and RNN has to be carried out very carefully to prevent noisy gradients back propagated from the RNN from corrupting the CNN model.|
|||With the unary CNN taking the responsibility of concept prediction, the relational RNN model is better able to focus on learning concept correlations/sentence generation.|
|||It thus allows for better and more efficient fine-tuning of the CNN module, as well as fast convergence in end-to-end training of the full CNN-RNN model.|
|||(2) Our proposed semantic regularisation enables reliable fine-tuning of the CNN image encoder as well as the fast convergence of end-to-end CNN-RNN training.|
|||[36] present an end-to-end structured model that combines the CNN model with a CRF.|
|||[2] to a deep model which combines MRFs and CNN to model output correlations, and is applied to multi-label classification.|
|||Our model is related to [14, 31] in that it follows the CNN-RNN design pattern; however, it uses a semantically regularised image embedding layer as the interface layer rather than an unregularised CNN feature layer.|
|||where ct and ht are the models cell and hidden states, it, ft, ot are the activation of input gate, forget gate,  Despite these differences, existing CNN-RNN models have a key common characteristic: The image embedding  2875  Ie that acts as the interface between the CNN and RNN models is taken to be a layer of weak and implicit semantics, e.g., CNN feature layer, or its transform.|
|||Concretely, instead of using a CNN feature layer as embedding Ie, we use the CNN label prediction layer, e.g., concept prediction layer of an Inception net [28].|
|||2, in our Semantically regularised CNN-RNN (S-CNN-RNN), the CNN part takes an image I as input, and predicts the likelihood of the semantic concepts s  Rk1 where k is the number of semantic concepts1.|
|||CNN For pretraining of the CNN model (Fig.|
|||Joint CNN-LSTM After the CNN and RNN models are pretrained, the whole model can be jointly trained by simultaneously optimising the deeply supervised joint loss L. For inference, we condition on the image by setting the initial state, then feed a start signal and recurrently sample model predictions of the previous step as input until an end signal is generated.|
|||Implementation details For fair comparisons with previous work, in our S-CNN-RNN model, we use the caffe reference net [8] as our unary CNN subnet on the NUSWIDE dataset [5], and VGG16 on MS COCO.|
|||For pretraining the CNN subnet, the learning rate is set to 1e-4 for NUS-WIDE and 1e-3 for MS COCO.|
|||In all compared models, the same CNN and RNN modules are used.|
|||1(c)) to merge CNN output features and RNN outputs [31].|
|||RIA: In this CNN-RNN model [14], the CNN output features are used to set the LSTM hidden state (Fig.|
|||Since the results reported by SINN [13] and TagNeighbour [15] were based on ImageNet-pretrained CNN models, for direct comparison we train a variant of our model that fixes the weights of the CNN subnet without finetuning (Ours+Tag1K Fix).|
|||(3) The gaps between Ours and CNN-RNN [31] and RIA [14] show clearly the importance of adding semantic regularisation to the CNN embedding layer.|
|||Implementation details For our S-CNN-RNN, we use Inception v3 [28] as the CNN subnet, and an LSTM network is used as RNN subnet.|
|||For the CNN pretraining, we initially just learn the prediction layer, and then tune all the parameters for 30,000 iterations with a batch size of 32 and learning rate of 1e-4.|
|||mRNN: The multimodal recurrent neural network [22] uses a multimodal layer to combine the CNN and RNN.|
|||V2L: The V2L model [32] use a CNN based attribute detector to firstly generate 256 attributes, and then feed as initial input to a LSTM model to generate captions.|
|||All five models use a CNN and a RNN, but only NICv2 does end-to-end training.|
|||NIC-F: removing the semantic regularisation and use the CNN output feature layer as the inference Ie to RNN.|
|||This gives us the standard NIC model [29] with the same Inception v3 as CNN subnet.|
|||As a result, the CNN feature representation benefits from the deep supervision (rather than distal supervision via the RNN), but the specific embedding used as the RNN interface is not directly semantically meaningful.|
|||For example, pretraining our Inception v3 [28] CNN only needs 30,000 iterations with a batch size of 32.|
|||The semantic regularisation makes the CNN-RNN interface semantically meaningful, distributes the label prediction and correlation tasks between the CNN and RNN models, and importantly the deep supervision makes training the full model more stable and efficient.|
||40 instances in total. (in cvpr2017)|
|13|Yang_Exploit_Bounding_Box_CVPR_2016_paper|However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal.|
|||Our framework is extensively compared with state-of-the-art handcrafted feature based methods and CNN based methods on two multi-label benchmark datasets.|
|||However, conventional CNN features may not generalize well for images containing multiple objects as the objects can be in different locations, scales, occlusions and categories.|
|||Since multi-label recognition task is more general and practical in real world applications, many CNN related methods [18, 19, 26] have been proposed to address the problem.|
|||A well known fact is that image-level labels can be utilized to fine-tune a pre-trained CNN model and produce good global representations [22, 23, 19].|
|||To address the problems with global CNN representations, following the recent works [19, 26, 18, 12], we incorporate local information via extracting object proposals using general object detection techniques such as selective search [25] for multi-label object recognition.|
|||3), the usual CNN representations might not be good enough for discrimination.|
|||[18] directly uses ground-truth bounding boxes to fine tune the CNN model, but its performance is not better than other proposal-based methods [19, 26] that do not utilize ground-truth bounding boxes.|
|||The main contribution of this research lies in the proposed multi-view multi-instance framework, which utilizes bounding box annotations (strong labels) to encode the label view and combine it with the typical CNN feature representation (feature view) for multi-label object recognition.|
|||Related Works  Our paper mainly relates to the topics of CNN based recognition, multi-view and multi multi-label object instance learning and local and metric learning.|
|||Many works [12, 22, 18, 19, 26] have demonstrated that CNN models pre-trained on a large dataset such as ILSVRC can be used to extract features for other applications without enough training data.|
|||The other is a standard CNN feature as the feature view.|
|||Instead, we only need to identify whether there exist target objects in the proposals, which has been proven to be the forte of CNN features [12].|
|||From Global Representation to Local Simi larity  Once we obtain object proposals for each image, we can naturally use CNN features to represent these proposals.|
|||With the proposals represented by CNN features, one baseline is to encode each image (bag) at the feature view by Fisher vector as discussed in Sec.|
|||However, the goal of CNN is usually to minimize the classification error using loss functions such as the logistic loss, which may not be suitable for local encoding.|
|||Details of training and fine-tuning the LMNN CNN can be found in Section 4.3.|
|||The output of the proposed LMNN network is a low-dimensional feature that shares the good semantic abstraction power of conventional CNN feature and the good  neighborhood property of large-margin metric learning.|
|||We then build the candidate pool with the LMNN CNN features extracted from ground truth objects.|
|||Network Configurations and Implementation  Details  Our framework consists of two networks, a large-margin nearest neighbor (LMNN) CNN and a standard CNN.|
|||For the standard CNN used for feature view, we only fine-tune the network with weak labels on the whole image.|
|||For the large-margin NN (LMNN) CNN used for label view, we execute a three-step fine-tuning.|
|||This method employed OverFeat [22], which is pre-trained on ImageNet, to get CNN activations as the off-the-shelf features.|
|||[18] proposed to transfer image representations learned with CNN on ImageNet to other visual recognition tasks with limited training data.|
|||The parameters of the first seven layers of CNN are then fixed and the last fully-connected layer is replaced by two adaptation layers.|
|||The first step is to pre-train a CNN on ImageNet data.|
|||[24] densely extracts multiple CNN features across multi-scales of the image with very-deep networks (16-layer and 19-layer).|
|||Our Setup and Parameters  It is difficult to make a completely fair comparison among different CNN based methods as the CNN configurations, the data augmentation and the pre-training could substantially influence the results.|
|||All CNN based methods can benefit from extra training data and more powerful networks as shown in [18, 26, 19, 5, 24].|
|||To fairly evaluate our proposed framework, we develop our system based on the common 8-layer CNN pretrained on ILSVRC 2012 dataset with 1000 categories.|
|||Once the LMNN CNN and the standard CNN (see Fig.|
|||For VOC 2007 and 2012 datasets, after PCA, the standard CNN features is reduced to around 450-d. After PCA, we generate 128 GMM codewords and encode each image with IFV similar to [20].|
|||In the lower part of Table 2, we list the results of HCP2000C [26], which uses additional 1000 categories from ImageNet that are semantically close to VOC 2007 categories for CNN pre-training, and VeryDeep [24], which densely extracts multiple CNN features from 5 scales and combines two very-deep CNN models (16-layer an 19layer).|
|||The upper part shows the results of the hand-crafted feature based methods and the CNN based methods trained with 8-layer CNN and ILSVRC 2012 dataset.|
|||The lower part shows the results of the methods trained with very-deep CNN or with additional training data.|
|||The upper part shows the results of the hand-crafted feature based methods and the CNN based methods trained with 8-layer CNN and ILSVRC 2012 dataset.|
|||The lower part shows the results of the methods trained with very-deep CNN or with additional training data.|
|||This suggests that our proposalbased framework and the multi-scale CNN extracted from the whole image are complement to each other.|
|||Similar to Table 2, we compare with the hand-crafted feature based methods and the CNN based methods pre-trained on ILSVRC 2012 using 8layer CNN model in the upper part and the methods trained with additional data or very-deep CNN models in the lower part.|
|||Our framework that uses only the feature view (FeV) already outperforms the state-of-the-art hand-crafted feature method (NUS-PSL) by 1.8% and the state-of-the-art proposal-based CNN method (HCP-1000C) by 2.3%.|
||40 instances in total. (in cvpr2016)|
|14|cvpr18-Robust Classification With Convolutional Prototype Learning|Despite its high accuracies, CNN has been shown to be easily fooled by some adversarial examples, indicating that CNN is not robust enough for pattern classification.|
|||In this paper, we argue that the lack of robustness for CNN is caused by the softmax layer, which is a totally discriminative model and based on the assumption of closed world (i.e., with a fixed number of categories).|
|||One example is the existence of adversarial samples [37], when we add small noises or make some small changes to the initial samples, CNN will give different predictions for these samples with high confidence, al Figure 1.|
|||Feature representation learned by traditional CNN model on MNIST.|
|||This greatly limits the application of CNN in real worlds.|
|||The main reasons for these problems include two aspects: First, CNN is a purely discriminative model, it essentially learns a partition of the whole feature space, therefore, the samples from unseen classes are still predicted to some specific regions under the partition, and CNN still views these samples as some known classes with high confidence.|
|||This explains why the rejection ability of CNN is poor; Second, from the perspective of representation learning, the learned representation of CNN is linear separable, see Fig.|
|||1 for an illustration, and under this kind of representation, the interclass distance is sometimes even smaller than the intra-class distance, this significantly reduces the robustness of CNN in real and complicated environments.|
|||Several methods have been proposed to improve the ro 3474  bustness of CNN and most of them concentrate on designing better loss functions.|
|||[38] proposed the center loss to improve the performance of softmax-based CNN, however, the centers can not be learned jointly with the CNN and are only updated according to some pre-defined rules rather than learned directly from data.|
|||Moreover, previous work of [26] and [25] also made improvements and extensions for the softmax based loss but they still kept the softmax layer with the traditional framework of CNN for classification.|
|||We design multiple loss functions for this framework, making the CNN feature extractor and the prototypes being learned jointly from the raw data.|
|||Experiments on several datasets demonstrate that the CPL framework can achieve comparable or even better classification accuracies compared with traditional CNN models.|
|||Among these methods, [36] combined the cross entropy loss and contrastive loss [5] to train the CNN, the cross entropy loss can increase the inter-personal variations while the contrastive loss can reduce the intrapersonal variations, and both losses guide the CNN to learn more discriminative representations.|
|||[33] designed a triplet loss for CNN to learn representations in a compact Euclidean space where distances directly correspond to a measure of similarity, and the learned representation performs well on several tasks including recognition, verification, and clustering.|
|||[38] proposed a center loss and combine it with cross entropy loss to train the CNN for learning more discriminative features, and they also propose a mini-batch based update method for the centers, which was proved to be useful for face recognition and verification.|
|||Thus, we use a CNN as feature extractor in our framework, which is denoted as f (x; ), x and  denote the raw input and parameters of the CNN respectively.|
|||Different from the traditional CNN which use softmax layer for linear classification on the learned features, we maintain and learn several prototypes on the features for each class and use prototype matching for classification.|
|||The CNN feature extractor f (x; ) and the prototypes {mij} are jointly trained from data.|
|||Feedforward for prediction  Given an input pattern x, we first get its abstract representation by the CNN feature extractor, then we compare the abstract feature with all prototypes and classify it to the category where the nearest prototype belongs to:  x  class arg  C  max i=1  gi(x)  where gi(x) is the discriminant function for class i:  gi(x) =   K min j=1  kf (x; )  mijk2 2  (1)  (2)  The trainable parameters in our framework are composed by two parts.|
|||One is the parameters of the CNN extractor, which is denoted as ; the other is the prototypes in each class, which is denoted as M = {mij|i = 1, ..., C; j = 1, ..., K}.|
|||(11)  Note that f is the output of the CNN feature extractor, according to the error back propagation algorithm, the derivatives with regard to the parameters of CNN can be calculated start from l/f .|
|||Similarly, the derivatives with regard to the parameters of CNN can also be calculated starting from l/f by the chain rule.|
|||Like traditional CNN framework for classification, the CPL still separates the whole feature space and the learned representation is still linearly separable.|
|||Most rejection strategies are based on the probabilities (confidences) produced by the softmax layer of CNN model.|
|||Experiments and analysis on MNIST [17]  In this experiment, the architecture of CNN feature extractor is the same as the network used in [38] with the ReLU activation function.|
|||The output of the CNN feature vector is set to be two, thus we can directly plot the features on the 2-D surface for visualization.|
|||The accuracy of different CNN structures and different models on CIFAR-10  loss function  accuracy (%)  soft-max  97.55 [40]  MCE MCL GMCL DCE  97.35 97.61 97.36 97.58  Table 3.|
|||The accuracy of GCPL on OLHWDB dataset  performance of the traditional softmax based classification, CPL, and GCPL methods under these CNN structures.|
|||We use the same CNN structure as [40] and make little modifications with batch normalization and ReLU to improve training process.|
|||Note that our purpose is not to achieve significantly better accuracy than previous softmax-based CNN model, we only want to show that, from the accuracy perspective, our framework can match or work slightly better than tradition 3479  Figure 3.|
|||From table 1, we can see that our new proposed CPL framework can achieve comparable performance with traditional softmax based CNN under the same structure.|
|||Experiments and analysis on CIFAR(cid:173)10  We realized several CNN structures on CIFAR-10 [14], including the model C appeared in [35], a modified version of model C that adds batch normalization layers after each convolutional and fully connected layers in model C, residual net 20 and residual net 32 in [8].|
|||al CNN model.|
|||We use the same structure of CNN feature extractor as section 5.1.|
|||For comparison, we also give the test accuracy of the traditional softmax-based CNN framework under the same architecture and training data.|
|||Actually, CNN is very powerful for feature extraction, even though the initial intra-class distribution may be very complex, after CNN transformation, it can still be well modeled with a single Gaussian distribution (a single prototype).|
|||CPL can achieve comparable or even better classification accuracy than softmax-based CNN models.|
|||The GCPL has great advantage compared with traditional CNN models in the perspectives of outlier rejection and class-incremental learning.|
||39 instances in total. (in cvpr2018)|
|15|Chunrui_Han_Face_Recognition_with_ECCV_2018_paper|In current face recognition approaches with convolutional neural network (CNN), a pair of faces to compare are independently fed into the CNN for feature extraction.|
|||Extensive experiments on the challenging LFW, and IJB-A show that our proposed contrastive convolution significantly improves the vanilla CNN and achieves quite promising performance in face verification task.|
|||The most effective solutions for the face verification at present are employing the powerful CNN models.|
|||To verify whether a given pair of faces A and B are of the same identity, most CNN-based methods generally first feed the two faces into a CNN to obtain their feature representations.|
|||Since the parameters of convolutional kernels are fixed once the training of CNN is completed, all faces are processed with identical kernels and thus mapped into a common discriminative feature space.|
|||Inspired by this observation, we propose a novel CNN structure with what we referred to as contrastive convolution, whose kernels are carefully designed and mainly focus on those distinct characteristics, i.e., contrastive features, between the two faces for better verification of them.|
|||To demonstrate the effectiveness of the proposed contrastive convolution, extensive experiments are performed on the challenging LFW and IJB-A dataset, and our contrastive CNN achieves quite promising performance.|
|||In [27], a CNN is proposed to extract deep features of the faces that are aligned to frontal through a general 3D shape model and performs better than many traditional face recognition methods.|
|||Specifically, the whole verification model, referred to as Contrastive CNN, consists of a trunk CNN and a kernel generator, forming a successive architecture.|
|||The truck CNN C is designed for base feature representation, which is shared between the two images for efficiency although it can be generally different.|
|||Overall, the objective function of our CNN with contrastive convolution can  be formulated as follows:  min  C,G,W,H  L1 + L2  (16)  The  is a balance parameter, and is set as 1 in our experiments in addition to special instructions.|
|||This objective can be easily optimized by using the gradient decent same as most CNN based methods.|
|||4 Experiments  In this section, we will evaluated our proposed CNN with contrastive convolution w.r.t.|
|||Architectures of the CNN used in our method with 4, 10, 16 layers respectively.|
|||We also compare our contrastive CNN with the conventional CNN, which is constructed by adding additional layer to the base CNN (referred to as L-Vanilla CNN) so that it has the same network structure as ours for fair comparison.|
|||Our contrastive CNN is designed with 3 sub-generators which generate 9, 4, and 1 contrastive kernels respectively, i.e.|
|||4.2 Ablation study of contrastive convolution  Effectiveness of contrastive convolution To show the improvement of our contrastive convolution, we compare our contrastive CNN with what we referred to  Face Recognition with Contrastive Convolution  11  Table 2.|
|||Comparion between the vanilla CNN and our contrastive CNN.|
|||Method  Loss  mAcc on LFW (%)  L-VanillaCNN  Contrastive CNN  Pairwise Loss  L-VanillaCNN Pairwise Loss  Contrastive CNN +Softmax Loss  91.80  95.20  97.50  98.20  TAR(%)@FAR on IJB-A  0.1  0.01  0.001  64.13  22.43  5.88  78.73  52.51  31.37  88.43  71.51  52.72  90.24  74.55  58.04  Table 3.|
|||Performance of our Contrastive CNN with different number of sub-generators on LFW in terms of mean accuracy (mAcc) and IJB-A in terms of TAR (%) at FAR = 0.1, 0.01, and 0.001.|
|||# sub-generator mAcc on LFW  1  2  3  97.83  98.17  98.20  TAR(%)@FAR on IJB-A  0.1  0.01  0.001  87.06  64.95  37.32  89.92  75.08  57.08  90.24  74.55  58.04  as L-Vanilla CNN, which is constructed by adding additional layers that have similar structure with our kernel generator to the base CNN so that it has the same network structure as ours.|
|||As can be seen, for both vanilla CNN and our contrastive CNN, the results with softmax loss+pairwise loss are better than that only with pairwise loss, demonstrating the superiority of the softmax loss as that in [33].|
|||These comparison clearly and convincingly show that our contrastive CNN can significantly improve the conventional CNN.|
|||Results of our Contrastive CNN with different base CNNs on LFW in terms of mean accuracy (mAcc) and IJB-A in terms of TAR at FAR = 0.1, 0.01, 0.001.|
|||Three base CNN structures with layers 4, 10, 16 are evaluated respectively, with architecture detailed in Table 1.|
|||# Layers of base CNN mAcc on LFW(%)  4  10  16  98.20  98.93  99.12  TAR(%)@FAR on IJB-A  0.1  0.01  0.001  90.24  74.55  58.04  93.17  80.35  61.83  95.31  84.01  63.91  A  B1  B2  B3  B4  B5  B6  B7  B8  B9  B10  Contrastive   CNN  Vanilla  CNN  Low response  High response  Fig.|
|||Feature maps from our Contrastive CNN and Vanilla CNN for a given image A when comparing to images B1  B10.|
|||These feature maps for contrastive CNN mainly focus on the region of eyes and eyebrows.|
|||spectively and accordingly there are 9, 13, 14 contrastive kernels orderly in the 4-layer CNN shown in Table 1.|
|||The performance of Contrastive CNN with different number of sub-generator can be found in Table 3, where the performance is constantly improved with the increasing of number of sub-generator.|
|||We visualize those feature maps from our contrastive CNN and vanilla CNN in Fig.|
|||Illustration of feature maps from contrastive CNN and vanilla CNN on the toy data for A1 comparing with B1, and A2 comparing with B2.|
|||[8]  LargeMargin [18]  SphereFace [17]  Contrastive CNN (ours)  3  1  1  1  1  1  1  1  1  7  5  15  14  10  14  17  64  16  4M*  300K*  2.6M  200M*  WebFace  WebFace  WebFace  WebFace  WebFace  mAcc on  LFW  97.35  98.70  98.95  99.65  97.73  98.43  98.71  99.42  99.12  high response of conventional CNN scatters over the whole image.|
|||Moreover, a toy experiment with images filled in simple geometry patterns is designed for more obvious illustration of feature maps from our contrastive CNN and conventional CNN, and the visualization is shown in Fig.|
|||Both experiments clearly demonstrate that our contrastive CNN can focuses on the distinct characteristics between the two faces to compare as claimed.|
|||In this experiment, contrastive CNN with 16 layers is used for fair comparison as most existing methods are equipped with large architectures.|
|||Methods  OPENBR  GOTS  ReST [32]  FastSearch [29]  PAM [20]  DR-GAN [19]  Deep Multi-pose [1]  Triplet Similarity [24]  Joint Bayesian [5]  TAR(%)@FAR on IJB-A  0.1  43.3  62.7   89.3    91.1  94.5  96.1  0.01  23.6  40.6  63.0  72.9  73.3  75.5  78.7  79.0  81.8  0.001  10.4  19.8  54.8  51.0  55.2  51.8   59.0   Contrastive CNN (ours)  95.31  84.01  63.91  trained on WebFace with reasonable number of layer.|
|||5 Conclusion  In this work, we propose a novel CNN architecture with what we referred to as contrastive convolution for face verification.|
|||The proposed contrastive convolution can be incorporated into any kind of CNN architecture.|
||39 instances in total. (in eccv2018)|
|16|3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation From Single Depth Images|Our proposed 3D CNN taking a 3D volumetric representation of the hand depth image as input can capture the 3D spatial structure of the input and accurately regress full 3D hand pose in a single pass.|
|||In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data.|
|||Corresponding author  Figure 1: Overview of our proposed 3D CNN based hand pose estimation method.|
|||3D CNN is trained in an end-to-end manner to map the 3D volumetric representation to 3D hand joint relative locations in the 3D volume.|
|||For example, in [17], the initial result of 2D CNN is very poor, and it is iteratively refined by a feedback loop to incorporate 3D information from a generative model.|
|||1991  In this work, we propose a 3D CNN based hand pose estimation approach that can capture the 3D spatial structure of the input and accurately regress full 3D hand pose in a single pass, as illustrated in Figure 1.|
|||To our knowledge, this is the first work that applies such a 3D CNN in hand pose estimation in order to understand hand pose structure in 3D space and infer 3D hand joint locations efficiently and robustly.|
|||Compared to previous CNN based methods for hand pose estimation, our proposed 3D CNN based method has the following advantages:   Our proposed 3D CNN has the ability to effectively learn 3D features from the 3D volumetric representation for hand pose estimation.|
|||Compared to the 2D CNN regressing 3D joint locations from 2D features [5, 16, 17, 40], the 3D CNN can directly regress 3D joint locations from 3D features in a single pass without adopting any iterative refinement process, and can achieve superior estimation performance.|
|||We design a relatively shallow architecture for the 3D CNN which contains only three 3D convolutional layers and three fully-connected layers.|
|||Comprehensive experiments show that our proposed 3D CNN based method for 3D hand pose estimation outperforms state-of-the-art methods on both datasets, with runtime speed of over 215 fps on a standard computer with a single GPU.|
|||Limited by the hand-crafted features, random forests based methods are difficult to outperform current CNN based methods in hand pose estimation.|
|||Our work is related to the CNN based data-driven approach.|
|||In this work, we lift the 2D CNN to 3D CNN which can understand 3D spatial information and extract 3D features for 3D hand pose estimation.|
|||3D CNN 3D CNNs have been successfully applied in video and dynamic hand gesture analysis for recognition tasks [6, 35, 13], which regard time as the third dimension.|
|||[19] show that the 3D CNN with low input volume resolution can still achieve good object classification accuracy by applying subvolume supervision and anisotropic probing.|
|||Song and Xiao [24] propose to use 3D CNN for 3D object detection in RGB-D images.|
|||Maturana and Scherer [12] propose VoxNet, a 3D CNN that can process LiDAR, RGB-D and CAD data for object recognition.|
|||They also apply the 3D CNN for landing zone detection [11].|
|||These 3D volumes will be fed into the 3D CNN for learning 3D features and regressing 3D hand joint locations.|
|||(b) Visualization of extracted 3D features output from layers L1, L2 and L3 during the forward pass in a fully trained CNN model.|
|||Network Architecture  Our proposed 3D CNN takes three volumes of the projective D-TSDF as inputs and outputs a column vector containing 3  K elements corresponding to the K 3D hand joint relative locations in the volume.|
|||In Figure 3b, we visualize some extracted 3D features output from layers L1, L2 and L3 during the forward pass using a fully trained 3D CNN model.|
|||In order to make the 3D CNN model robust to different orientations and sizes, we propose to perform 3D data augmentation on the training data.|
|||The 3D CNN model is implemented within the Torch7 [3] framework.|
|||As shown in Figure 6, our 3D CNN based method outperforms state-of-the-art methods by large margin on the MSRA dataset.|
|||In our experiments, the 3D CNN is trained to estimate a subset of 14 hand joints, following previous work in [34, 17].|
|||We first compare our 3D CNN based hand pose estima 1997  Figure 8: Qualitative results for MSRA dataset [28] and NYU dataset [34].|
|||We compare our 3D CNN based method (in the second line) with the multi-view CNN based method in [4] (in the first line).|
|||tion method with five state-of-the-art methods: the 2D CNN based heatmap regression method [34], the 2D CNN based direct regression method with a prior [16], the 2D CNN based regression method using feedback loop [17], the 2D CNN based hand model parameters regression method [42] and the deep feature based matrix completion method [23] on the NYU dataset.|
|||The processes of volume generation and 3D CNN forward propagation are performed on GPU.|
|||The coordinate trans formation that converts CNN output values to 3D locations in the cameras coordinate system is performed on CPU.|
|||In addition, our 3D CNN model takes about 500 MB of GPU memory during testing, while the multi-view CNNs in [4] take about 1.5 GB of GPU memory during testing.|
|||As can be seen, compared with the multi-view CNN based method in [4], our 3D CNN based method can better utilize the depth information and provide more accurate estimation.|
|||Conclusion  We present a novel 3D CNN based hand pose estimation method in this paper.|
|||We show that the 3D CNN mapping the 3D volumes to 3D joint locations in a single pass is easy to train.|
|||Experimental results indicate that our proposed 3D CNN based approach achieves state-of-the-art performance for 3D hand pose estimation in real-time on two challenging datasets.|
|||Robust 3D hand pose estimation in single depth images: from singleview CNN to multi-view CNNs.|
||38 instances in total. (in cvpr2017)|
|17|Chen_Webly_Supervised_Learning_ICCV_2015_paper|Specifically inspired by curriculum learning, we present a two-step approach for CNN training.|
|||We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories.|
|||We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012.|
|||We first train a CNN using easy images from Google (above).|
|||This CNN is then used to find relationships and initialize another CNN (below) for harder images.|
|||Specifically, one can bootstrap CNN training with easy examples first, followed by a more extensive and comprehensive learning procedure with similarity constraints to learn visual representations.|
|||On the other hand, we assume a vanilla CNN is robust to noise when trained with simple examples, from which a relationship graph can be learned, and this relationship graph provides powerful constraints when the network is faced with more challenging and noisier data.|
|||While it seems that CNNs are designed for big data small datasets plus millions of parameters can easily lead to over-fitting, we found it is still hard to train a CNN naively with random image text/tag pairs.|
|||In a similar manner, we first train our CNN model from scratch using easy images downloaded  1We tried to train a CNN with Google results of 7000 noun phrases randomly sampled from the web (5M images), but it does not converge.|
|||All the images are then fed directly into the CNN as training data.|
|||3In our experiments, we find with the same 1500 categories and close-to-uniform label distribution, a CNN converged on Google images yields an entropy 2.8, whereas Flickr gives 4.0.|
|||We do not re-train our CNN representations using these strategies.|
|||For this, we will do four experiments: 1) First, we will show that our learned CNN can be used for object detection.|
|||Here, we use the approach similar to R-CNN [19] where we will fine-tune our learned CNN using VOC data.|
|||This is followed by learning SVM-detectors using CNN features.|
|||Localizing Objects  Until now, we have focused on learning a weblysupervised CNN representation based on classification loss.|
|||Note that this is a non-trivial task, since: 1) the CNN is only trained to distinguish a closed set of classes, unnecessarily aware of all the negative visual world, e.g.|
|||We will also show that our CNN can be used to clean up the web data: that is, discover subcategories and localize the objects in web images.|
|||We will only use web images to train both the CNN and the subsequent SVMs.|
|||As baseline, we also report numbers for CNN learned using Flickr images alone (FlickrS) and combined Google+Flickr images (GFAll).|
|||PASCAL VOC Object Detection  Next, we test our webly trained CNN model for object detection on the PASCAL VOC.|
|||Second, we fine tune the CNN by back-propagating the error end-to-end using PASCAL trainval set.|
|||Several interesting notes:   Despite the search engine bias and the noise in the data, our two-stage CNN with graph regularization is on par with ImageNet-trained CNN.|
|||For example, FlickrS gives the worst performance and in fact when a CNN is trained using all the images from Google and Flickr it gives a mAP of 40.5, which is substantially lower than our mAP.|
|||Note that the original R-CNN paper fine-tuned the ImageNet CNN using train data alone and therefore reports lower performance [19].|
|||On the other hand, ImageNet CNN seems to outperform our network on animals [41] (e.g.|
|||To test if this hypothesis is true, we trained a separate CNN us ing NEIL images downloaded from Google before March 2013 (pre-CNN based image search era).|
|||Despite the data being noisier and less (450 per category), we observe 1% performance fall compared to a CNN trained with November 2014 data on the same categories.|
|||This indicates that the underlying CNN in Google image search has minimal effect on the training procedure and our network is quite robust to noise.|
|||For our approach, we try five different settings: 1) GoogleO: Features are based on GoogleO CNN and the bounding boxes are also extracted only on easy Google im 71437  VOC 2007 test aero bike bird boat bottle  bus  car  cat  chair  cow table dog horse mbike person plant  sheep sofa  train  tv  mAP  LEVAN [13]  14.0 36.2 12.5 10.3  9.2  35.0 35.9  8.4  10.0 17.5  6.5  12.9  30.6  27.5  GoogleO  GoogleA  30.2 34.3 16.7 13.3  29.5 38.3 15.1 14.0  FlickrG  32.6 42.8 19.3 13.9  FlickrG-EA  32.7 44.3 17.9 14.0  FlickrG-CE  30.2 41.3 21.7 18.3  6.1  9.1  9.2  9.3  9.2  43.6 27.4 22.6  44.3 29.3 24.9  46.6 29.6 20.6  47.1 26.6 19.2  44.3 32.2 25.5  6.9  6.9  6.8  8.2  9.8  16.4 10.0 21.3  15.8  9.7  22.6  17.8 10.2 22.4  18.3 10.0 22.7  21.5 10.4 26.7  25.0  23.5  26.7  25.0  27.3  35.9  34.3  40.8  42.5  42.8  6.0  7.6  9.7  11.7  12.0  12.6  1.5  18.8  10.3 23.5 16.4  17.1  9.3  12.7  14.0  12.7  13.3  21.8  21.4  19.0  22.2  20.4  17.3 31.0 18.1  15.8 33.4 19.4  19.0 34.0 21.9  20.9 35.6 18.2  20.9 36.2 22.8  20.7  21.5  22.9  23.0  24.4  Table 3.|
|||We use the learned CNN representation to discover subcategories and localize positive instances for different categories [9].|
|||From the results, we can see that in all cases the CNN based detector boosts the performance a lot.|
|||Scene Classification  To further demonstrate the usage of CNN features directly learned from the web, we also conducted scene classification experiments on the MIT Indoor-67 dataset [37].|
|||Fine-tuning on hard images enhanced the features, but adding scene-related categories gave a huge boost to 66.5 (comparable to the CNN trained on Places database [59], 68.2).|
|||This indicates CNN features learned directly from the web are generic and quite powerful.|
|||other than objects or scenes from the web, webly supervised CNN bears a great potential to perform well on many relevant tasks with the cost as low as providing a query list for that domain.|
|||First, we train CNN with easy images downloaded from Google image search.|
|||We show that our two-stage CNN comes close to the ImageNet pretrained-CNN on VOC 2007, and outperforms on VOC 2012.|
||38 instances in total. (in iccv2015)|
|18|Gadde_Semantic_Video_CNNs_ICCV_2017_paper|A key insight of this work is that fast optical flow methods can be combined with many different CNN architectures for improved performance and end-to-end training.|
|||The number of proposed CNN models for semantic image segmentation by far outnumbers those for video data.|
|||A naive way to use a single image CNN for video is to apply it frame-by-frame, effectively ignoring the temporal information altogether.|
|||Alternative approaches include the use of conditional random field (CRF) models on video data to fuse the predicted label information across frames or the development of tailored CNN architectures for videos.|
|||A separate CRF applied to the CNN predictions has the limitation, that it has no access to internal representations of the CNNs.|
|||Our implementation of NetWarp takes only about 2.5 milliseconds to process an intermediate CNN representation of 128  128 with 1024 feature channels.|
|||It is fully differentiable and can be learned using standard back propagation techniques during training of the entire CNN network.|
|||In addition, the resulting video CNN model with NetWarp modules processes the frames in an online fashion, i.e., the system has access only to the present and previous frames when predicting the segmentation of the present frame.|
|||Schematic of the proposed video CNN with NetWarp modules.|
|||The video CNN is applied in an online fashion, looking back only one frame.|
|||The CNN filter activations for the current frame are modified by the corresponding representations of the previous frame via NetWarp modules.|
|||[29] proposed a CRF model and an effiecient inference technique to fuse CNN unaries with long range spatiotemporal cues estimated by recurrent temporal restricted Boltzmann machine.|
|||We avoid the CRF construction and filter the intermediate CNN representations directly.|
|||These filtering techniques propagate information after the semantic labels are computed for each frame, whereas in contrast, our approach does filtering based propagation across intermediate CNN representations making it more integrated into CNN training.|
|||But, most CNN techniques work on single images.|
|||The resulting representation ez frame and and combines the warped representations with those of the present frame z t is then passed onto the remaining CNN layers for semantic segmentation.|
|||These works use pixel correspondences across frames to refine or propagate labels, whereas the proposed approach refines the intermediate CNN representations with a module that is easy to integrate into current CNN frameworks.|
|||Warping Image CNNs to Video CNNs  Our aim is to convert a given CNN network architecture, designed to work on single images into a segmentation CNN for video data.|
|||The main building block will be the NetWarp module that warps the intermediate (kth layer) CNN representations zk t1 of the previous frame and then combines with those of the present frame zk t denote the  t , where z1  t , z2  t ,    , zm  intermediate representations of a given image CNN with m layers.|
|||The authors of [39] showed that intermediate CNN representations change only slowly over adjacent frames, especially for deeper CNN layers.|
|||In [13], a bilateral inception module is constructed to average intermediate CNN representations for locations across the image that are spatially and photometrically close.|
|||There, the authors use super-pixels based on runtime considerations and demonstrated improved segmentation results when applied to different CNN architectures.|
|||Using pixel correspondences, provided by optical flow, to combine intermediate CNN representations of adjacent frames consistently improves semantic predictions for a number of CNN architectures.|
|||So, we use a small CNN to transform the pre-computed optical flow, which we will refer to as FlowCNN and denote the transformation as (Ft).|
|||Let us assume that we want to apply the NetWarp module on the kth layer of the image CNN and the filter activations for the adjacent frames are zk (t1) (as in Fig 2).|
|||Due to strided pooling, deeper CNN representations are typically of smaller resolution in comparison to the image signal.|
|||The parameters w1, w2 are learned via standard t is then passed on to the remaining image CNN layers.|
|||Note that backpropagating a loss from frame t will affect image CNN layers (those preceding NetWarp modules) for the present and also previous frames.|
|||We use shared filter weights for the image CNN across the frames.|
|||Adding NetWarp modules to CNN introduces very few additional parameters.|
|||This network consists of a standard CNN followed by a context module with 8 dilated convolutional layers.|
|||Table 3 shows the performance and runtime comparisons with the dilation CNN and other related techniques.|
|||Qualitatively, we find improved performance near boundaries compared to baseline CNN (see supplementary video).|
|||qualitative results with static image CNN and our video CNN are shown.|
|||The main concept is to transfer intermediate CNN filter activ 4459  Figure 6.|
|||The resulting video CNN is end-to-end trainable, runs in an online fashion and has only a small computation overhead in comparison to the frame-by-frame application.|
|||First, we demonstrate consistent performance improvements across different image CNN hierarchies.|
|||Memory optimizing the CNN training would alleviate some of the problems and enables training with many frames together.|
||38 instances in total. (in iccv2017)|
|19|cvpr18-Convolutional Image Captioning|(Figure 5, Table 2); c) We analyze the characteristics of CNN and LSTM nets and provide useful insights such as  CNNs produce more entropy (useful for diverse predictions), better classification accuracy, and do not suffer from vanishing gradients (Section 6 and Figure 6, 7 and 8).|
|||The paper is organized as follows: Section 2 gives our notation, Section 3 reviews the RNN based approach, Section 4 describes our convolutional method, Section 5 gives the details of CNN architecture, Section 6 contains results and Section 7 discusses related work.|
|||Our CNN with attention (attn) achieves comparable performance (equal CIDEr scores on MSCOCO test set) to [16] and outperforms LSTM+Attention baseline of [39].|
|||We start with a CNN comprising masked convolutions and fully connected layers only.|
|||feed-forward CNN module.|
|||The CNN module operates on the combined input and image embedding vector.|
|||The output of the CNN module after three layers is a 512-dimensional vector for each word.|
|||We use a linear layer to encode the 512-dimensional vectors obtained from the CNN module into a 256-dimensional representation per word.|
|||We use a cross-entropy loss on the probabilities pi,w(yi|y<i, I) to train the CNN module and the embedding layers.|
|||Attention  In addition to the aforementioned CNN architecture, we also experiment with an attention mechanism, since attention benefited [9, 35].|
|||Our results show that with beam size= 3 our CNN outperforms LSTM [16] on all metrics.|
|||Beam = 1 is same as the test set results reported in Table 1.  c5 (Beam = 1)  c40 (Beam = 1)  B1 B2 B3 B4 M R  C  B1 B2 B3 B4 M R  C  .704 .528 .384 .278 .241 .517 .876 .880 .778 .656 .537 .321 .655 .898 LSTM CNN+Attn .708 .534 .389 .280 .241 .517 .872 .883 .786 .667 .545 .321 .657 .893  c5 (Beam = 3)  c40 (Beam = 3)  B1 B2 B3 B4 M R  C  B1 B2 B3 B4 M R  C  .710 .537 .399 .299 .246 .523 .904 .889 .794 .681 .570 .334 .671 .912 LSTM CNN+Attn .715 .545 .408 .304 .246 .525 .910 .896 .805 .694 .582 .333 .673 .914  Table 3: Above, we show that CNN outperforms LSTM on BLEU metrics and gives comparable scores to LSTM on other metrics for test split on MSCOCO evaluation server.|
||| Adding attention to our CNN gives improvements on metrics and we outperform the LSTM+Attn baseline [39] (Table 1).|
||| We analyze the CNN and RNN approaches and show that CNN produces (1) more entropy in the output probability distribution, (2) gives better word prediction accuracy (Figure 6), and (3) does not suffer as much from vanishing gradients (Figure 8).|
||| In Table 4, we show that a CNN with 1.5 more parameters can be trained in comparable time.|
|||LSTM: a cat is laying down on a bed CNN: a polar bear is drinking water from a white bowl GT: A white polar bear laying on top of a pool of water  LSTM: a bear is standing on a rock in a zoo CNN: two bears are walking on a rock in the zoo GT: two bears touching noses standing on rocks  LSTM: a box of donuts with a variety of toppings CNN: a box of doughnuts with sprinkles and a sign GT:A bunch of doughnuts with sprinkles on them  LSTM: a dog and a dog in a field CNN: two cows are standing in a field of grass GT: A dog and a horse standing near each other  Figure 4: Captions generated by our CNN are compared to the LSTM and ground-truth caption.|
|||Typically we observe that CNN and LSTM captions are of similar quality.|
|||perform beam search for both LSTM and our CNN methods.|
|||Qualitative Comparison  See Figure 4 for a qualitative comparison of captions generated by CNN and LSTM.|
|||Analysis of CNN and RNN  In Table 4 we report the number of trainable parameters and the training time per epoch.|
|||Table 1, 2 and 3 show that we obtain comparable performance from both CNN and RNN/LSTM-based methods.|
|||We find that the loss is higher for CNN than RNN.|
|||Less peaky posterior distributions provided by a CNN may be indicative of CNNs being more capable of predicting diverse captions.|
|||This plot shows that for the CNN we have higher unique words for more word positions and consistently higher 2/4-grams than LSTM.|
|||Method  # Parameters Train time per epoch  LSTM [16] Our CNN  Our CNN+Attn  13M 19M 20M  1529s 1585s 1620s  Table 4: We train a CNN faster per parameter than the LSTM.|
|||This is because CNN is not sequential like the LSTM.|
|||If learning is stalled, for larger datasets than the ones we currently use for image captioning, the performance of RNN and CNN may differ significantly.|
|||[37] jointly train a vision (or image) CNN with a language RNN to generate sentences, [39] extends [37] with additional attention parameters and learns to identify salient objects for caption generation.|
|||These recurrent neural nets have found widespread use for captioning because they  5567  (a) CNN gives higher cross-entropy loss on train/val set of MSCOCO compared to LSTM.|
|||But, as we show in (c), CNN obtains better % word accuracy than LSTM.|
|||(b) The entropy of the softmax layer (or posterior probability distribution) of our CNN is higher than the LSTM.|
|||(c) Even though the CNN training loss is higher than LSTM, its word prediction accuracy is better than LSTM on train set.|
|||On val set, the difference in accuracy between LSTM and CNN is small (only  1%).|
|||Blue line denotes our CNN and red denotes the LSTM based method [16].|
|||(a) Unique words  (b) Unique 2-grams  (c) Unique 4-grams  Figure 7: We perform beam search of beam size 10 with our best performing LSTM and CNN models.|
|||5568  051015202530Epochs1.21.41.61.82.02.22.42.62.83.0Cross-Entropy Loss ValueCross-Entropy Loss  (or Negative Log-Likelihood)LSTM on Train SetLSTM on Val SetOur CNN on Train SetOur CNN on Val Set051015202530Epochs1.21.41.61.82.02.22.42.62.83.0EntropyEntropy after Softmax (last layer)  (Entropy of Posterior)LSTM on Train SetLSTM on Val SetOurs CNN on Train SetOur CNN on Val Set051015202530Epochs42444648505254565860% Word accuracyWord accuracy in %LSTM on Train SetLSTM on Val SetOur CNN on Train SetOur CNN on Val Set12345678910111213Word Position0100200300400500600700CountsUnique words at every positionCNNLSTM12345678910111213Starting word position for 2-gram0500100015002000CountsUnique 2-grams at every positionCNNLSTM1234567891011Starting word position for 4-gram01000200030004000500060007000CountsUnique 4-grams at every positionCNNLSTM051015202530Epochs10-210-1100101Gradient NormGradient Norm at Embedding  and Classification LayerLSTM Embed.|
|||LayerOur CNN Embed.|
|||LayerOur CNN Classif.|
||38 instances in total. (in cvpr2018)|
|20|cvpr18-Learning Structure and Strength of CNN Filters for Small Sample Size Training|Learning Structure and Strength of CNN Filters for Small Sample Size Training  Rohit Keshari, Mayank Vatsa, Richa Singh  Afzel Noore  IIIT-Delhi, India  Texas A&M University-Kingsville, USA  {rohitk, mayank, rsingh}@iiitd.ac.in  Afzel.Noore@tamuk.edu  Abstract  Convolutional Neural Networks have provided state-ofthe-art results in several computer vision problems.|
|||To address the challenge of small sample size, researchers have proposed algorithms focusing on CNN initialization tricks and modifications to CNN architecture.|
|||For example, And en and Mallat [1] propose Scattering network (ScatNet) which is a CNN like architecture where pre-defined Morlet filter bank is utilized to extract features.|
|||[35] have proposed hybrid network, where they have utilized ScatNet feature followed by CNN architecture.|
|||[12] propose a PCA-based Convolutional Network (PCN) which has the influence of both CNN [20] and PCANet [4].|
|||[5] propose DeConv loss for CNN architecture that helps in training small databases.|
|||This paper focuses on two novel ways to develop CNN based feature representation algorithm for small sample size problems: (i) associating strength parameter to control the effect  of each pre-trained filter, and (ii) utilizing a generalizable approach that pre-learns the structure of the filters using small training samples.|
|||Further, unlike CNN approaches where we update the weights in every iteration, we introduce strength of the filter and update only the strength parameter not the filters.|
|||To mitigate these challenges, we propose a novel approach, termed as Structure and Strength Filtered CNN (SSF-CNN), which has two components: (i) structure of the filter and (ii) strength of the filter.|
|||It is our hypothesis that structure of the CNN filters can be learned from either domain specific larger databases or from other representation learning paradigms that require less training data for instance, dictionary learning [40, 41].|
|||If we represent CNN filters using dictionary, it can provide the structure; however, it may not be well optimized for the classification task.|
|||We introduce strength parameter t for the CNN filters W which allows the network to assign weight for each filter based on its structural importance.|
|||In CNN model,  9350  Algorithm 1 Hierarchical Dictionary Filter Learning  1: Notation: N is a number of training samples, n num ber of extracted patches, y is a patch from Y  2: Input: XN 3: Output: D 4: for each layer l := 1 to numLayer do 5:  [xn]N  extractP atch(XN ) Y  reshape([xn]N ) n Pn minDRmk  i=1 mini ( 1  1  2 ||yi  Dli||2  2 +  Figure 2.|
|||Filters (e) to (h) illustrate the change due to the proposed strength parameter in CNN architecture.|
|||In this research, we utilize dictionary learning to pre-train the filters of CNN in a hierarchical manner.|
|||ReLu) used in CNN models.|
|||In this manner, the number of dictionary layers is same as the number of convolutional layers In Algorithm 1 extractP atch function in CNN models.|
|||strength and structural parameters t and W can be learned in two ways: 1) pre-train W, use it in CNN by freezing the values of W followed by learning the strength t, and 2) pre-train W which is used to initialize the CNN model followed by learning t and W iteratively.|
|||While the second approach which simultaneously learns both structure and strength may be desirable, the first approach requires very few parameters to be trained in CNN model.|
|||The algorithm can be divided into two steps: 1) learn hierarchical dictionary filters and utilize trained dictionary filters to initialize the CNN, and (2) train CNN with dictionary initialized filters.|
|||Hierarchical Dictionary Filter Learning: Dictionary learning focuses on learning a sparse representation of the input data in the form of a linear combination of basic ele Training CNN with Dictionary Initialized Filters: Typically, CNN has multiple convolutional layers, each layer has multiple filters, and these filters are trained using stochastic  9351  (a)(b)(c)(d)(e)(f)(g)(h)Addition3x3 Dictionary Filterst*WUnsupervised dictionary filter trainingResNet architectureReLuBNAddition3x3 Conv3x3 Dictionary Filters3x3 ConvBNReLut*WFigure 4.|
|||For input X and convolutional filter W, the convolutional function of the CNN can be defined as f (X, W, b) = X  W + b, where  is the convolutional operation and b is the bias.|
|||A CNN architecture is designed by stacking multiple convolutional and pooling layers.|
|||These deep CNN architectures are trained in two passes: 1) forward pass and 2) backward pass.|
|||In backward pass, the error l j for each layer l on node j is computed with respect to the cost and the weights of the CNN filters are updated accordingly.|
|||In traditional CNN learning, the weights are initialized in different ways such as Xavier [13], or MSRA [16] approach and even randomly.|
|||In this research, we propose initialization of the CNN filters using dictionary learned filters as discussed above.|
|||Experimental Results  The effectiveness of the proposed algorithm is evaluated on multiple databases with state-of-the-art CNN architectures including ResNet [17] and DenseNet [18].|
|||The proposed model utilizes a dictionary and pre-trained model to initialize and train the CNN filters.|
|||These dictionaries are layered in a similar manner as CNN layers and are referred to as hierarchical dictionary.|
|||Figure 4 shows the first and second layer filters trained on CIFAR-10 database: (a) & (c) showcase filters with two existing initialization techniques in CNN architecture, (b) & (d) trained CNN filters on 1000 training samples, and (e) trained dictionary filters on 1000 training samples.|
|||In Figure 4, it can be observed that dictionary trained filters have less noisy patterns compare to CNN trained filters on small data.|
|||Also, when we use training images of only 10 newborns to train filter of CNN models from scratch, the test accuracies are extremely low.|
|||As shown in Table 2, we have observed that learning strength of the filters improves the performance of CNN models compared to conventional fine-tuning approach.|
|||In this research, we propose Structure and Strength Filtered CNN as a framework for learning a CNN model with small training databases.|
|||We propose to initialize the filters of CNN using dictionary filters which can be trained with small training samples.|
|||The proposed CNN has the flexibility to work for small as well as large databases.|
|||A light CNN for deep face representation with noisy labels.|
||38 instances in total. (in cvpr2018)|
|21|Lu_Square_Localization_for_ICCV_2015_paper|In the training phase, square CNN models and object co-presence priors are learned.|
|||In the testing phase, sliding CNN models are applied which produces a set of response maps that can be effectively filtered by the learned co-presence prior to output the final bounding boxes for localizing an object.|
|||Empirical recognition accuracy of the CNN model [14] and human recognition on the VOC 2007 dataset.|
|||With more careful parameter tuning of the CNN we believe the above accuracy can be further improved.|
|||Specifically, We train a 20 classes plus background CNN model respectively on the whole images and CSO images, and evaluate the accuracies by both computer (CNN model) and human.|
|||Thanks to current GPU technology, GPU implementation of CNN is extremely fast.|
|||Large scale empirical studies confirm that only O[log(min{H, W})] rounds of sliding CNN testing is run in practice, though the worse complexity is still O(min{H, W}) which is still impressive compared with the exhaustive sliding windows.|
|||As we will show, our sliding CNN is category scalable.|
|||Then, deep CNN classifier is applied on each candidate bounding box to label the object which is time consuming.|
|||In this paper, we propose a novel strategy for localizing compact squares images which enables the CNN to operate in a sliding window manner efficiently.|
|||We first train square CNN models on compact square object images.|
|||Given square CNN models, we implement sliding CNN testing to test everywhere in the image to produce response maps at each scale.|
|||Square CNN Model Learning  In the training phase, unlike previous work that trains the CNN models on the whole object image, we train the models on compact square object images, or CSO images for short, and thus call the trained model square CNN model.|
|||We train a n + 1 classes ways (square) CNN models on the square images extracted from the training data.|
|||A sliding CNN example.|
|||We choose the square CNN model u = 128 from {256, 128, 64} whose input size is closest to s. Then, we resize the original image with u/s.|
|||Sliding CNN testing is applied on the resized image.|
|||Sliding CNN  Now, we want to apply the learned square CNN models on a testing image at different square scales.|
|||Recall that each square CNN model requires the input size to be of size {256  256, 128  128, 64  64}.|
|||We choose a square CNN model whose input size u (u = {256, 128, 64}) is closest to s, and resize the testing image with factor u/s.|
|||After applying sliding CNN using the chosen square CNN model on the resized image, the output of the square CNN model represents a s  s square region in the original image.|
|||The difference is that the denseNet only operates on the first five convolutional layers of a network, whereas we operate on the fully-connected layers in a convolutional manner, and use the CNN scores for indicating square regions.|
|||Scale Selection by Binary Search  We have described above how to use square CNN model given a testing window of arbitrary size.|
|||Then, given two scales L and S, we denote M i S as the sliding CNN response output for the ith category.|
|||For each scale, the learned square CNN is run as a universal region on the resized image and outputs n classes response maps, where each pixel value indicates likelihood score of a given category in a square region.|
|||However, in practice, we often find pixels around indicator pixels to have high responses as well, since an object image with a minor shift in location can also produce a large response by the CNN model or any reasonable recognition technique.|
|||In testing square CNN models, the computation overhead for each additional category only occurs in the last layer of CNN which is extremely small.|
|||Since our focus is localization, for the recognition part, we use an established CNN recognizer to predict the category label and score, instead of checking the corresponding response map, since our square CNN model  is trained on CSO images rather than whole object images.|
|||The reason of using another CNN is as follows: although in our square framework we can output the object score directly upon concatenating the full object box, we found a 23mAP difference in comparison with assigning score using a trained CNN geared to whole object images.|
|||Note that the number of output boxes is small after our localization and thus the extra time is minor for applying a whole-object CNN classifier.|
|||We will report the mAP performance of both square CNN and an additional pass of applying whole-object CNN.|
|||Since the data size of VOC is not large compared with more recent big datasets, we first train three square CNN models with different input  sizes{2562, 1282, 642} on the ILSVRC 2014 object detec tion dataset which has 200 object categories.|
|||Then, three square CNN models with 21 classes ways for VOC 2007 (2012) are fine-tuned on them respectively.|
|||To make the comparison fair, since we focus on localization, we use the same recognition CNN models provided in the RCNN website.|
|||2  As we can see, our localization has made improvement in comparison to standard selective search [20] and edge box [21]) given the same CNN recognition model.|
|||Here whole-object CNN refers to testing the whole object image using trained object CNN.|
|||Time Complexity and Scalability  The main computation for the sliding CNN occurs in the first 6 layers inference.|
||37 instances in total. (in iccv2015)|
|22|Learning Deep CNN Denoiser Prior for Image Restoration|Learning Deep CNN Denoiser Prior for Image Restoration  Kai Zhang1,2, Wangmeng Zuo1, , Shuhang Gu2, Lei Zhang2  1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China  2Dept.|
|||To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems.|
|||This paper aims to train a set of fast and effective discriminative denoisers and integrate them into modelbased optimization methods to solve other inverse problems. Rather than learning MAP inference guided discriminative models, we instead adopt plain convolutional neural networks (CNN) to learn the denoisers, so as to take advantage of recent progress in CNN as well as the merit of GPU computation.|
|||Particularly, several CNN techniques, including Rectifier Linear Units (ReLU) [37], batch normalization [32], Adam [36], dilated convolution [63] are adopted into the network design or training.|
|||The contribution of this work is summarized as follows:   We trained a set of fast and effective CNN denoisers. With variable splitting technique, the powerful denoisers can bring strong image prior into model-based optimization methods.|
||| The learned set of CNN denoisers are plugged in as a modular part of model-based optimization methods to tackle other inverse problems.|
|||Learning Deep CNN Denoiser Prior  Basically, to plug the denoiser prior into the optimization procedure of Eqn.|
|||Why Choose CNN Denoiser?|
|||By considering the speed, performance and discriminative color image prior modeling, we choose deep CNN to learn the discriminative denoisers.|
|||First, the inference of CNN is very efficient due to the parallel computation ability of GPU.|
|||Second, CNN exhibits powerful prior modeling capacity with deep architecture.|
|||Third, CNN exploits the external prior which is complementary to the internal prior of many existing denoisers such as BM3D.|
|||Fourth, great progress in training and designing CNN have been made during the past few years and we can take advantage of those progress to facilitate discriminative learning.|
|||The Proposed CNN Denoiser  The architecture of the proposed CNN denoiser is illustrated in Figure 1.|
|||Batch normalization and residual learning which are two of the most influential architecture design techniques have been widely adopted in recent CNN architecture designs.|
|||Due to the characteristic of convolution, the denoised image of CNN may introduce annoying boundary artifacts without proper handling.|
|||We adopt the zero padding strategy and wish the designed CNN has the capacity to model image boundary.|
|||The main reason lies in the fact that, rather than using training patches of large size, cropping them into small patches can enable CNN to see more boundary information.|
|||However, the proposed CNN denoiser can have a PSNR gain of about 0.2dB over those three methods.|
|||Table 2 shows the color image denoising results of benchmark CBM3D and our proposed CNN denoiser, it can be seen that the proposed denoiser consistently outperforms CBM3D by a large margin.|
|||As one can see, the proposed CNN denoiser prior based optimization method achieves very promising PSNR results.|
|||Thus, in order to thoroughly evaluate the flexibility of the CNN denoiser prior based optimization method as well as the effectiveness of the CNN denoisers, following [45], this paper considers three typical image degradation settings for SISR, i.e., bicubic downsampling (default setting of Matlab function imresize) with two scale factors 2 and 3 [15, 21] and blurring by Gaussian kernel of size 77 with standard deviation 1.6 followed by downsampling with scale factor 3 [22, 45].|
|||The proposed deep CNN denoiser prior based SISR method is compared with five state-of-the-art methods, in cluding two CNN-based discriminative learning methods (i.e., SRCNN [21] and VDSR [35]), one statistical prediction model based discriminative learning method [45] which we refer to as SPMSR, one model based optimization method (i.e., NCSR [22]) and one denoiser prior based method (i.e., SRBM3D [24]).|
|||Since the source code of SRBM3D is not available, we also compare two methods which replace the proposed CNN denoiser with BM3D/CBM3D denoiser.|
|||Third, both of the gray and color CNN denoiser prior based optimization methods can produce promising results.|
|||As a result, this figure is mainly used to show the flexibility advantage of the proposed deep CNN denoiser prior based optimization method over discriminative learning methods.|
|||Conclusion  In this paper, we have designed and trained a set of fast and effective CNN denoisers for image denoising.|
|||Extensive experimental results have demonstrated that the integration of model-based optimization method and discriminative CNN denoiser results in a flexible, fast and effective framework for various image restoration tasks.|
|||On the one hand, different from conventional model-based optimization methods which are usually time-consuming with sophisticated image priors for the purpose of achieving good results, the proposed deep CNN denoiser prior based optimization method can be implemented effectively due to the plug-in of fast CNN denoisers.|
|||On the other hand, different from discriminative learning methods which are specialized for certain image restoration tasks, the proposed deep CNN denoiser prior based optimization method is flexible in handling various tasks while can produce very favorable results.|
|||In addition, this work has shown that learning expressive CNN denoiser prior is a good alternative to model image prior.|
|||While we have demonstrated various merits of plugging powerful CNN denoiser into model-based optimization methods, there also remain room for further study.|
|||First, it will be interesting to investigate how to reduce the number of the discriminative CNN denoisers and the number of whole iterations.|
|||Second, extending the proposed CNN denoiser based HQS framework to other inverse problems such as inpainting and blind deblurring would be also interesting.|
|||Finally, and perhaps most interestingly, since the HQS framework can be treated as a MAP inference, this work also provides some insights into designing CNN architecture for task-specific discriminative learning.|
|||Meanwhile, one should be aware that CNN has its own design flexibility and the best CNN architecture is not necessarily inspired by MAP inference.|
||36 instances in total. (in cvpr2017)|
|23|Cai_Learning_Complexity-Aware_Cascades_ICCV_2015_paper|The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages.|
|||Second, CompACT seamlessly integrates handcrafted and CNN features in a unified detector.|
|||the CNN features of [17, 29]).|
|||CNN: In addition to operators defined over the ACF channels, we consider a set of CNN features.|
|||The CNN is a smaller version of the popular model of [17], with five convolutional layers and one fully connected layer.|
|||For feature extraction, we only use the output of the 5h convolutional layer, which can be seen as CNN feature channels, similar  3365  to ACF.|
|||The complexity of CNN features is of a different nature than that of ACF features.|
|||Second, while the CNN features are computed on an as needed basis, the structure of the network makes it inefficient to compute each feature individually.|
|||If the CNN features are needed to classify a certain image window, it is significantly more efficient to compute the 5h layer responses over the whole window than repeatedly applying the network to subwindow regions.|
|||We account for these difficulties by setting a trigger complexity   for CNN features.|
|||That is, in (23), CNN features have  =   if no CNN feature has been used by the previous cascade stages to classify the current patch.|
|||Once the CNN features are computed, the complexity of using any CNN feature is 1, similar to ACF, while CNNCB features have complexity 4.|
|||Embedding Large CNN Models  Large CNN models [17, 29] are now popular in computer vision.|
|||Instead, we limited the use of a large CNN to the final cascade stage.|
|||Upon learning the cascade, we simply used a large CNN classifier as the final weak learner  of (21).|
|||The CNN is simply a descent direction of (18) unavailable to prior stages.|
|||It differs from the standard proposal+CNN approach in that 1) not only the bounding boxes but also the confidence scores of the cascade are forwarded to the deep CNN stage, and 2) the combination of the proposal mechanism (cascade) and large CNN is optimal under the well defined risk of (8).|
|||An NVIDIA Tesla K40M GPU was used for CNN computations.|
|||More surprisingly, the CNN features were also rarely selected, with CNNCB dominating the late cascade stages.|
|||Recall that, while the CNN features are a little more efficient, CompACT boosting weighs complexity less heavily than discrimination in the late cascade stages.|
|||Method  MR  time (s)  SS  CB  Single Type  ACF 42.6 34.29 37.89 37.15 28.07 0.07 0.87  CompACT LDA CNN CNNCB ACF CNN 32.15 23.82 0.11 0.28  26.93 2.05  0.08  0.23  0.16  Table 2.|
|||This was used to produce standard cascades of ACF, SS, CB, LDA, CNN and CNNCB features.|
|||Finally, CNNCB has the best detection results, but only a marginal gain over CNN and much higher computation.|
|||The two sides of Table 2 differ in that only ACF-based features were used on the left, while both these and the small CNN model were used on the right.|
|||Note also the introduction of the small CNN model enables substantially better cascades.|
|||Large CNN models  While the previous experiments only use small models, a number of experiments were performed with large models.|
|||In all these variants, the large CNN is computed only on windows selected by CompACT.|
|||The Embedded columns report to the use of the large CNN as the last stage of the cascade, as discussed in Section 4.2.|
|||Finally, the Intermediate columns report to an intermediate between these two, in which the large CNN stage was only applied to the CompACT output, after non-maximum suppression (NMS).|
|||Second, the embedding of the large CNN on the CompACT model achieved the best results in all cases.|
|||This shows that the ComPACT cascade score contains information that complements that of the CNN scores.|
|||For both CNN models, it was better to combine scores with the CompACT cascade than to consider the latter simply as a proposal mechanism.|
|||Finally, the theoretically more sound embedding of the large CNN before NMS (Embedding)  3367     e  t  a r   s s m  i  1  .80  .64  .50  .40  .30  .20  .10  .05     94.7% VJ 77.2% ConvNet 68.5% HOG 34.6% InformedHaar 29.8% ACFCaltech+ 24.8% LDCF 23.3% RCNN 22.5% Katamari 21.9% SpatialPooling+ 18.5% Checkerboards 18.9% CompACT 11.7% CompACTDeep  3  10  2  10  1  10  0 10  1 10  false positives per image  i  i  n o s c e r p  1  0.75  0.5  0.25  0    0  KITTI Pedestrian (moderate)     DPM DADPM RCNN FilteredICF pAUCEnsT Regionlets CompACT CompACTDeep  0.2  0.4  0.6  0.8  1  recall  Figure 3.|
|||CompACT refers to the model using ACF + small CNN features, and CompACTDeep to the model with the embedded VGG model in the last stage.|
|||Note that it uses approximately the same number of feature channels (including the CNN model) as pAUCEnsT [23] and FilteredICF [37], which are both much less accurate and slower.|
|||R-CNN [15, 12], the only CNN detector on KITTI, is also substantially weaker than CompACT-Deep (difference larger than 8 points).|
||36 instances in total. (in iccv2015)|
|24|Mining Object Parts From CNNs via Active Question-Answering|Mining Object Parts from CNNs via Active Question-Answering  Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu  University of California, Los Angeles  Abstract  Given a convolutional neural network (CNN) that is pretrained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts.|
|||As an interpretable model, the AOG associates different CNN units with different explicit object parts.|
|||We use an active humancomputer communication to incrementally grow such an AOG on the pre-trained CNN as follows.|
|||Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge.|
|||However, CNN methods still face two issues in real-world applications.|
|||Semanticizing knowledge in a pre-trained CNN via active question-answering (QA).|
|||We develop our method based on the following three ideas: 1) When a CNN is pre-trained using objects of a category with object-box annotations, most appearance knowledge of the target category may have been encoded in convlayers of the CNN.|
|||As shown in Fig 2, the AOG has four layers, which encode a clear semantic hierarchy ranging from semantic part, part templates, latent patterns, to CNN units.|
|||Each latent pattern in the third layer (OR node) naturally corresponds to a certain range of units within a CNN conv-slice.|
|||We select a CNN unit within this range to account for geometric deformation of the latent pattern.|
|||Inputs and outputs of QA-based learning: Given a pre-trained CNN and its training samples (i.e.|
|||1) We transfer patterns in a pre-trained object-level CNN to the target part concept, instead of learning all knowledge from scratch.|
|||The pattern-mining process purifies the CNN knowledge for better representation of the target part.|
|||1) We mine and represent latent patterns hidden in a pre-trained CNN using an AOG.|
|||Related work  Passive CNN visualization vs. active CNN semanticization: In order to explore the hidden semantics in the CNN, many studies visualized and analyzed patterns of CNN units [44, 23, 33, 1, 21].|
|||However, from the perspective of semanticizing CNN units, CNN visualization and our active QA go in two opposite directions.|
|||given a query of modeling/refining certain object parts, can we efficiently discover certain patterns that are related to the part concepts, within the pre-trained CNN from its complex neural activations?|
|||Given CNN feature maps, Zhou et al.|
|||discovered objects [30] from CNN activations in an unsupervised manner, and learned part concepts in a supervised fashion [32].|
|||Such a white-box representation of the CNN knowledge also guided further active QA.|
|||[12, 37] discovered objects and identified actions  347  pattern (OR node), and CNN unit.|
|||Given an image I 1, we use the CNN to compute neural activations on I in its conv-layers, and then use the AOG for hierarchical part parsing.|
|||And-Or graph grown on the pre-trained CNN as a semantic branch.|
|||The AOG associates certain CNN units with certain image regions.|
|||Instead of directly building new models from active QA, our method uses the QA to semanticize the CNN and transfer the hidden knowledge to the AOG.|
|||Thus, we crop I to only contain the object and resize I for CNN inputs to simplify the scenario of learning for part localization.|
|||348         TerminalsDeformation rangeConv-layerConv-layerConv-layer Input image(AND) part template(OR) latent patterns(OR) semantic part(AND) part template(OR) latent patterns(OR) semantic partoutputFCFCoutputFCFCHeatmap of the selected CNN units in all conv-layers Image reconstructed using CNN responsesVisualizationsub-partcontextVsemVtmpVlatVuntvalue of V unt and its local deformation level.|
|||Objects with large gains usually correspond to unexplained or not well explained CNN neural activations.|
|||fI  denotes CNN features of I  at the top conv-layer after ReLu operation, and M is a diagonal matrix representing the prior reliability for each feature dimension3.|
|||i  )], where V unt  i  is the CNN unit corre 350  Annotation  number  05 10 15 20 25 30  Layer 1:  Layer 3: semantic part part template latent pattern  Layer 2:  3.15 5.95 8.52 11.16 13.55 15.83  3791.5 3804.8 3760.4 3778.3 3777.5 3837.3  91.6 93.9 95.5 96.3 98.3 99.2  Table 1.|
|||CNN-PDD selected a conv-slice in a CNN (pretrained using ImageNet ILSVRC 2012 dataset) to represent and localize the part on well cropped objects.|
|||The third column indicates whether the baseline used all object-box annotations in the category to pre-finetune a CNN before learning the part (object-box annotations are more than part annotations).|
|||The fourth column indicates whether the baseline used all object annotations (more than part annotations) in the category to pre-fine-tune a CNN before learning the part.|
|||Second, less model drift: Instead of learning/fine-tuning new CNN parameters, our method just used limited part annotations to mine reliable patterns and organize their spatial relationships to represent the part concept.|
|||Summary and discussion  In this paper, we aim to pursue answers to the following three questions: 1) whether we can represent a pre-trained CNN using an interpretable AOG model, which reveals semantic hierarchy of objects hidden in the CNN, 2) whether the representation of the CNN knowledge can be clear enough to let people directly communicate with middle-level AOG nodes, and 3) whether we can let the computer directly learn from weak supervision of active QA, instead of strongly supervised end-to-end learning.|
||35 instances in total. (in cvpr2017)|
|25|A New Representation of Skeleton Sequences for 3D Action Recognition|More specifically, each frame of the generated clips is fed to a deep CNN to extract a CNN feature.|
|||Then the three CNN features of the three clips at the same timestep (See Figure 1) are concatenated into one feature vector.|
|||(2) We introduce a MTLN to process all the CNN features of the frames in the generated clips, thus to learn the spatial structure and the temporal information of the skeleton sequence.|
|||The generated clips are then fed to a deep CNN model to extract CNN features which are used in a MTLN for action recognition.|
|||A deep CNN model (c) and a temporal mean pooling (TMP) layer (d) are used to extract a compact representation from each frame of the clips (see Figure 3 for details).|
|||The output CNN representations of the three clips at the same timestep are concatenated, resulting four feature vectors (e).|
|||An advantage of this method is that for any skeleton sequence of any length, the generated clips contain the same number of frames and the long-term temporal information of the original skeleton sequence can be effectively captured with the powerful CNN representations of the frame images in the generated clips.|
|||Then the CNN features of all frames of the generated clips are jointly processed in parallel using multi-task learning, thus to utilize their intrinsic relationships to learn the spatial temporal information for 3D action recognition.|
|||3.2.1 Temporal Pooling of CNN Feature Maps  To learn the features of the generated clips, a deep CNN is firstly employed to extract a compact representation of each frame of the clips.|
|||Given the generated clips, the CNN feature of each frame is extracted with the pre-trained VGG19 [38] model.|
|||The pre-trained CNN model is leveraged as a feature extractor due to the fact that the CNN features extracted by the models pre-trained with ImageNet [34] are very powerful and  e m a r f  512  ...  512  ...  joint  (a)  (b)  (c)  (d)  Figure 3.|
|||Temporal mean pooling of the CNN feature maps.|
|||The CNN models trained on the large image dataset can be used as a feature extractor to extract representations of the patterns in matrices.|
|||The main ideas of the proposed method Clips + CNN + MTLN are 1) generating three clips (each clip consists of four frames) from a skeleton sequence, 2) using CNNs to learn global long-term temporal information of the skeleton sequence from each frame of the generated clips, and 3) using MTLN to jointly train the CNN features of the four frames of the clips to incorporate the spatial structural information for action recognition.|
|||Frames + CNN In this baseline, the CNN features of single frames instead of the entire generated clips are used for action recognition.|
|||Clips + CNN + Concatenation In this baseline, the CNN features of all frames of the generated clips are concatenated before performing action recognition.|
|||Clips + CNN + Pooling In this baseline, max pooling is applied to the CNN features of all frames of the generate clips before performing action recognition.|
|||Same as Clips + CNN + Concatenation, this baseline is also used to show the benefits of using MTLN.|
|||As shown in Table 1, Frames + CNN achieves an accuracy of about 75.73% and 79.62% for the two testing protocols, respectively.|
|||Compared to extracting temporal features of skeleton sequences with FTP and native 3D coordinates, using CNN to learn the temporal information of skeleton sequences from the generated frames is more robust to noise and temporal variations due to the convolution and pooling operators, resulting in better performances.|
|||From Table 1, it can also be seen that Frames + CNN also performs better than the previous state-of-the-art method.|
|||It clearly shows the effectiveness of the CNN features of the proposed clip representation.|
|||The performances are improved by learning entire clips with CNN and MTLN (i.e., Clips + CNN + MTLN).|
|||It can also be seen that the proposed MTLN (i.e., Clips + CNN + MTLN) performs better than feature concatenation (i.e., Clips + CNN + concatenation) and pooling (i.e., Clips + CNN + pooling).|
|||Frames + CNN, Clips + CNN + concatenation and Clips + CNN + pooling can be viewed as a single-task method, while using MTLN to process multiple frames of the generated clips in parallel utilizes their intrinsic relationships and incorporates the spatial structural information, which improves the performance of the single 3293  Table 1.|
|||Methods  Accuracy  Cross Subject Cross View  Lie Group [42]  Skeletal Quads [7]  Dynamic Skeletons [16]  Hierarchical RNN [6]  Deep RNN [37] Deep LSTM [37]  Part-aware LSTM [37]  ST-LSTM [26]  ST-LSTM + Trust Gate [26]  Coordinates + FTP  Frames + CNN  Clips + CNN + Concatenation  Clips + CNN + Pooling Clips + CNN + MTLN  50.1% 38.6% 60.2% 59.1% 59.3% 60.7% 62.9% 65.2% 69.2% 61.06% 75.73% 77.05% 76.37% 79.57%  52.8% 41.4% 65.2% 64.0% 64.1% 67.3% 70.3% 76.1% 77.7% 74.64% 79.62% 81.11% 80.46% 84.83%  task method for action recognition.|
|||Similar to the NTU RGB+D dataset, CNN features perform better than FTP to learn the temporal information.|
|||When incorporating the CNN features of the entire clips using concatenation and pooling methods, the performance is improved by about 2%.|
|||It clearly shows the benefit of using MTLN to learn the CNN features entire clips.|
|||In [26], a Trust Gate is introduced to remove the noisy  Methods  Accuracy  CHARM [25]  Hierarchical RNN [6]  Deep LSTM [54]  Raw Skeleton [53] Joint Feature [18]  49.7% 86.9% 83.9% 80.35% 86.03% Deep LSTM + Co-occurrence [54] 90.41% 88.6% 93.3% 79.75% 90.88% 92.86% 92.26% 93.57%  Clips + CNN + Pooling Clips + CNN + MTLN  Coordinates + FTP  Frames + CNN  Clips + CNN + Concatenation  ST-LSTM [26]  ST-LSTM + Trust Gate [26]  Table 3.|
|||Methods  Hierarchical RNN [6]  Deep LSTM [54]  Deep LSTM + Co-occurrence [54]  Coordinates + FTP  Frames + CNN  Clips + CNN + Concatenation  Clips + CNN + Pooling Clips + CNN+ MTLN  Accuracy  CMU subset CMU 83.13% 75.02% 86.00% 79.53% 88.40% 81.04% 83.44% 73.61% 91.53% 85.36% 90.97% 85.76% 90.66% 85.56% 93.22% 88.30%  joints and this improves the accuracy from 88.6% to 93.3%.|
|||Each frame is duplicated three times to formulate a color image for CNN feature learning.|
|||A simple alternative is to generate a color clip with three channels of the cylindrical coordinates, and then extract a single CNN feature from the color frame for action recognition.|
|||The CNN features of the three clips at the same time-step are concatenated in a single feature vector, which describes the temporal information of the entire skeleton sequence and one particular spatial relationship between the joints.|
|||Recurrent highway networks arXiv preprint  with language cnn for image captioning.|
||35 instances in total. (in cvpr2017)|
|26|cvpr18-Interpretable Convolutional Neural Networks|The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e.|
|||what patterns are memorized by the CNN for prediction.|
|||Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN.|
|||without any additional human supervision, can we modify a CNN to make its conv-layers obtain interpretable knowledge representations?|
|||We expect the CNN to have a certain introspection of its representations during the end-to-end learning process, so that the CNN can regularize its representations to ensure high interpretability.|
|||Our problem of improving CNN interpretability is different from conventional off-line visualization [34, 17, 24, 4, 5, 21] and diagnosis [2, 10, 18] of pre-trained CNN representations.|
|||1 shows the difference between a traditional CNN and our interpretable CNN.|
|||In contrast, the filter in our interpretable CNN is activated by a certain part.|
||| We propose to slightly revise a CNN to improve its interpretability, which can be broadly applied to CNNs with different structures.|
||| The interpretable CNN does not change the loss function on the top layer and uses the same training samples as the original CNN.|
|||Method: We propose a simple yet effective loss to push a filter in a specific conv-layer of a CNN towards the representation of an object part.|
|||a CNN may use an unreliable contexteye featuresto identify the lipstick attribute of a face image).|
|||Therefore, human beings usually cannot fully trust a network, unless a CNN can semantically or visually explain its logic, e.g.|
|||In this study, we expect the CNN to explain its logic at the object-part level.|
|||end-to-end learning a CNN whose representations in high conv-layers are interpretable.|
|||Related work  Our previous paper [39] provides a comprehensive survey of recent studies in exploring visual interpretability of neural networks, including 1) the visualization and diagnosis of CNN representations, 2) approaches for disentangling CNN representations into graphs or trees, 3) the learning of CNNs with disentangled and interpretable representations,  and 4) middle-to-end learning based on model interpretability.|
|||up-convolutional nets [4] were used to invert CNN feature maps to images.|
|||They also disentangled CNN representations in an And-Or graph via active question-answering [37].|
|||Model diagnosis: Many statistical methods [28, 33, 1] have been proposed to analyze CNN features.|
|||[27] learned CNN representations with better object compositionality, and Liao et al.|
|||[16] learned compact CNN representations, but they did not make filters obtain explicit part-level or texture-level semantics.|
|||We can understand the interpretable CNN from the per 8828  R(cid:28664)(cid:28639)(cid:28648)  C(cid:28674)(cid:28673)v  (cid:28639)(cid:28674)(cid:28678)(cid:28678) (cid:28665)(cid:28674)(cid:28677) (cid:28665)i(cid:28671)(cid:28679)(cid:28664)(cid:28677) (cid:28612) (cid:28639)(cid:28674)(cid:28678)(cid:28678) (cid:28665)(cid:28674)(cid:28677) (cid:28665)i(cid:28671)(cid:28679)(cid:28664)(cid:28677) (cid:28613) (cid:28639)(cid:28674)(cid:28678)(cid:28678) (cid:28665)(cid:28674)(cid:28677) (cid:28665)i(cid:28671)(cid:28679)(cid:28664)(cid:28677) (cid:28614)  (cid:28647)(cid:28677)(cid:28660)(cid:28663)i(cid:28679)i(cid:28674)(cid:28673)(cid:28660)(cid:28671)  C(cid:28674)(cid:28673)v-(cid:28671)(cid:28660)y(cid:28664)(cid:28677)  I(cid:28673)(cid:28679)(cid:28664)(cid:28677)(cid:28675)(cid:28677)(cid:28664)(cid:28679)(cid:28660)(cid:28661)(cid:28671)(cid:28664)  C(cid:28674)(cid:28673)v-(cid:28671)(cid:28660)y(cid:28664)(cid:28677)  xmasked  (cid:28640)(cid:28660)(cid:28678)(cid:28670)(cid:28678)  x  R(cid:28664)(cid:28639)(cid:28648)  C(cid:28674)(cid:28673)v  Figure 2.|
|||During the forward propagation, given each input image I, the CNN computes a feature map x of the filter f after the ReLu operation, where x is an nn matrix, xij  0.|
|||Our method estimates the potential position of the object part on the feature map x as the neural unit with the strongest activation  = argmax=[i,j]xij, 1 i, j  n. Then, based on the estimated part position , the CNN assigns a specific mask with x to filter out noisy activations.|
|||The CNN selects different templates for different images.|
|||Learning  We train the interpretable CNN via an end-to-end manner.|
|||During the forward-propagation process, each filter in the CNN passes its information in a bottom-up manner, just like traditional CNNs.|
|||We initialized parameters of fully-connected (FC) layers and the new conv-layer, and loaded parameters of other conv-layers from a traditional CNN that was pre-trained using 1.2M ImageNet images in [12, 25].|
|||We then fine-tuned parameters of all layers in the interpretable CNN using training images in the dataset.|
|||Then, [2] scaled up low-resolution valid map regions to the image resolution,  4We considered the output yc for each category c independent to outputs for other categories, thereby a CNN making multiple independent single-class classifications for each image.|
|||It is because compared to other CNN semantics discussed in [2] (such as colors and textures), object-part semantics requires a stricter criterion.|
|||However, when a filter in an ordinary CNN does not have consistent contours, it is difficult for [40] to align different images to compute an average RF.|
|||When we use an interpretable CNN to classify a large number of categories simultaneously, filters in a conv-layer are assigned with different categories, which makes each category corresponds to only a few filters.|
|||Examining cnn repre sentations with respect to dataset bias.|
||34 instances in total. (in cvpr2018)|
|27|Zhang_Detecting_Faces_Using_ICCV_2017_paper|In addition, we introduce a two-stream contextual CNN architecture that leverages body part information adaptively to enhance face detection.|
|||It leverages Inside Cascaded Structure (ICS) to encourage the CNN to handle difficult samples at deep layers, and utilizes the two-stream contextual CNN to exploit the body part information adaptively.|
|||(c) Illustration of two-stream contextual CNN and Body Part Sensitive Learning (BPSL).|
|||The key part of recent CNN-based face detection methods is to train a powerful CNN as a face/non-face classifier.|
|||Different from previous works, we notice that different layers of CNN can learn features of different perceptions that are suitable for discriminating face/non-face examples of different difficulties.|
|||To relieve this difficulty, we propose a two-stream contextual CNN that joint body parts localization and face detection in an optimal way.|
|||(2) We propose to jointly optimize body part localization and face detection in a two-stream contextual CNN that exploits body information to assist face detection by learning filters sensitive to the body parts.|
|||Convnet [14] integrates a CNN and a 3D mean face model in an end-to-end multi-task learning framework.|
|||How to use CNN with cascade structure is widely studied.|
|||Cascaded CNN based methods [13, 29, 18] treat CNN as a face/non-face classifier and use hard sample mining scheme to construct a cascade structure outside CNNs.|
|||However, filters inside a CNN are stacked layer by layer and these methods ignore the correlation among these cascaded filters.|
|||However, it separates the CNN optimization and cascaded classifiers optimization.|
|||Overall Framework  We use a cascaded CNN framework as our basic due to its good performance and runtime efficiency [13, 29, 18].|
|||Different from these works, for the CNN-based face/nonface classifier, we introduce the Inside Cascaded Structure (ICS) and combine contextual CNN for more robust face detection.|
|||P-Net is a fully convolutional CNN that quickly produces candidate windows through a sliding scan on the  3172  Figure 2.|
|||(b) An example of inside cascaded two-stream contextual CNN structure.|
|||R-Net-1 and R-Net-2 are the inside cascaded two-stream contextual CNN (shown in Fig.|
|||Each pooling layer of the CNN is connected to an ERC that predicts the probability of a sample being a face for each sample.|
|||An example of neural network in ERC and CNN architectures of P-Net, R-Net-1 and R-Net-2.|
|||The CNN with ICS can be optimized using regular stochastic gradient descent [10] and the optimization of different layers are different due to the different training samples sets selected by the DR layers.|
|||Two-stream Contextual CNN  In this section, we will introduce the proposed twostream contextual CNN and Body Part Sensitive Learning (BPSL) that jointly optimizes body parts localization and face detection to help the CNN to exploit body information adaptively in large visual variations.|
|||These two inputs are fed to face CNN and body CNN separately.|
|||In this way, CNN can exploit not only the face but also body information.|
|||Hence, we propose to use a body CNN to model the appearance of the body parts.|
|||In particular, we aim to learn the CNN filters that are sensitive to the body parts and showing discriminative appearance in convolutional features.|
|||Implementation details  For body part localization, using CNN to generate body part score map is very prevalent [3, 1, 2] and thus we use the body part score map as supervision signal in our methods.|
|||It will encourage CNN to learn visual body appearance related filters and naturally formulates the cases where the body parts are occluded or even whole body region is absent. Specifically, in training processing, after the last convolutional layer in body CNN, there is a deconvolutional layer that generates multiple body part score maps (each score map indicates a kind of body part, see Fig.|
|||The face CNN and body CNN are trained jointly.|
|||This is because the body CNN will focus more on localizing body part and less on exploiting contextual information for face detection.|
|||The evaluation results of only using face CNN and using two-stream CNNs with/without BPSL (i.e., localize body part in training) are shown in Fig.|
|||Conclusion  In this paper, we develop two new strategies to improve the performance of cascaded CNN for face detection.|
|||First, we propose the inside cascaded structure (ICS) that constructs cascaded layer classifies inside a CNN to rejects negative samples layer wise.|
|||In addition to ICS, we propose to jointly optimize body part localization and face detection in a two-stream contextual CNN to improve the robustness of our model.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||34 instances in total. (in iccv2017)|
|28|Deep Self-Taught Learning for Weakly Supervised Object Localization|To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process.|
|||Given image-level supervision, seed positive proposals are first obtained as initial positive samples for a CNN detector.|
|||The CNN detector is then trained with self-taught learning which alternates between training and online supportive sample harvesting relying on the relative improvement of CNN scores predicted by the detector.|
|||Recent WSL methods [11, 12, 13, 14, 9] also combine deep convolutional neural network (CNN) models [15, 16, 17] with MIL, considering that CNN architectures can provide more powerful image representations.|
|||However, the representation provided by a CNN tailored to classification does not contain any specific information about object spatial locations and is thus not suitable for object-level localization tasks, leading to marginal benefits for learning a high-quality object detector.|
|||Moreover, such methods only perform off-line MIL to mine confident class-specific object proposals before train 1377  ing the detector, where the strong discriminating power of the learned object-level CNN detector is not fully leveraged to mine high-quality proposals for detector learning.|
|||By fully exploiting the strong discriminating ability of the regional CNN detector (e.g., Fast R-CNN [3]), supportive samples of higher quality can be identified, compared with the ones provided by the conventional CNN plus MIL approaches.|
|||However, one key problem with the above online supportive sample harvesting strategy for self-taught learning is that some poor seed positive samples may be easily fitted by the CNN detector due to its strong learning ability and hence trap the CNN detector in poor local optima.|
|||We propose a novel deep self-taught learning approach to progressively harvest high-quality positive samples  1Throughout this paper, response and CNN score refer to the final prob ability output after softmax normalization to the target class.|
|||Related Work  Previous works on WSL can be roughly categorized into  MIL based methods and end-to-end CNN models.|
|||End-to-end CNN models are also used for WSL.|
|||[24] proposed an end-to-end CNN model with two  1378  streams, one for classification and the other for localization, which outputs final scores for the proposals by the elementwise multiplication on the results of the two streams.|
|||[25] proposed a context-aware CNN model trained with contrast-based contextual guidance, resulting in refined boundaries of detected objects.|
|||Additionally, SVM is used in MIL in [9], which has the inferior discriminating ability to the regional CNN detector.|
|||In contrast, our approach overcomes this weakness by performing image-to-object transferring during multi-label image classification and online supportive sample harvesting in regional CNN detector learning.|
|||Then, online supportive sample harvesting is presented, which progressively improves the quality of the positive samples, where the detector dynamically harvests the most informative positive samples during learning, guided by the relative CNN score improvement from the detector itself.|
|||Considering that each positive image contains at least one positive object proposal that contributes significantly to each class, we train a multi-label classification CNN model as the first step to identify seed samples.|
|||Online Supportive Sample Harvesting  After obtaining the seed positive proposals, we further seek higher-quality positive samples by taking advantage of the object-level CNN detector.|
|||In particular, we implement self-taught learning to improve the ability of the object-level regional CNN detector progressively.|
|||Fast R-CNN is used as our regional CNN detector.|
|||We observe that a regional CNN detector (Fast R-CNN) trained on seed samples is sufficiently powerful for selecting the most confident tight positives for further training itself.|
|||To address this issue, we propose to online select the most confident and tight positive samples based on relative improvement (RI) of output CNN scores, instead of relying on the static absolute CNN score at certain training iterations.|
|||First, it can provide an adaptive number of proposals  the increasing of the generalization power of the CNN model in early epochs (e.g., 1+ to 2-), but decreases in later epochs (e.g., 3+ to 4-, 4+ to 5-) when the CNN gains strong discrimination between the target class and background.|
|||First, standard detection mean average precision (mAP) defined by [28] is evaluated on the PASCAL 2007 test set,  1381  Figure 4: CNN score on the target class vs. number of epochs during training Fast R-CNN for different proposals. The training proposals are the seed positive samples to train Fast R-CNN.|
|||1and 1+ indicate the CNN score right before and after training on this image in the 1st epoch, respectively.|
|||It can be seen that later OSSH has a less benefit to CorLoc than the OSSH in the 2nd epoch, showing that high-quality positive proposals gain consistent CNN score improvements in each of these epochs and thus can be easily picked out in the first time of OSSH.|
|||To validate the advantage of using relative CNN score improvement, we conduct comparison experiments with using absolute CNN scores to harvest confident positive samples in OSSH.|
|||From Table 6, it is found that relative score improvement consistently outperforms absolute CNN scores in all cases, especially when OSSH is performed in more epochs.|
|||Using absolute CNN scores, the improvements of OSSH in the later two epochs are much less than using relative score improvement.|
|||This further demonstrates that the detector is more easily trapped in poor local optima when selecting positive samples based on absolute CNN scores, since the detector highly overfits seed positive samples and thus seed positive samples can obtain high predicted scores after the first 2 epochs.|
|||Table 6: Correct localization (CorLoc) (%) on the PASCAL 2007 trainval set of using relative CNN score improvement and absolute CNN score in OSSH.|
|||Epochs of OSSH  1  2  3  absolute CNN score  48.8 52.3 53.2  relative score improvement 50.2 54.9 56.1  4.5.|
|||Then by virtue of online supportive sample harvesting augmented with a new relative CNN score improvement metric, our approach can successfully detect positive samples of improved quality.|
|||Hcp: A flexible cnn framework for multi-label image classification.|
||34 instances in total. (in cvpr2017)|
|29|cvpr18-Feature Generating Networks for Zero-Shot Learning|To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network (GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution.|
|||Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method.|
|||Figure 1: CNN features can be extracted from: 1) real images, however in zero-shot learning we do not have access to any real images of unseen classes, 2) synthetic images, however they are not accurate enough to improve image classification performance.|
|||f-CLSWGAN, to generate CNN features of unseen classes.|
|||(1) We propose a novel conditional generative model f-CLSWGAN that synthesizes CNN features of unseen classes by optimizing the Wasserstein distance regularized by a classification loss.|
|||Hence, we propose a novel GAN architecture to directly generate CNN features that can be used to train a discriminative classifier for zeroshot learning.|
|||In this paper, we propose to tackle generalized zero-shot learning by generating CNN features for unseen classes via a novel GAN model.|
|||The main insight of our proposed model is that by feeding additional synthetic CNN features of unseen classes, the learned classifier will also explore the embedding space of unseen classes.|
|||Hence, the key to our approach is the ability to generate semantically rich CNN feature distributions conditioned on a class specific semantic vector e.g.|
|||Let S = {(x, y, c(y))|x  X , y  Y s, c(y)  C} where S stands for the training data of seen classes, x  Rdx is the CNN features, y denotes the class label in Y s = {y1, .|
|||Given the train data S of seen classes, we aim to learn a conditional generator G : Z  C  X , which takes random Gaussian noise z  Z  Rdz and class embedding c(y)  C as its inputs, and outputs a CNN image feature  x  X of class y.|
|||f-WGAN does not guarantee that the generated CNN features are well suited for training a discriminative classifier, which is our goal.|
|||Instead, the first two terms in Equation 2 approximate the Wasserstein distance, and the third term is the gradient penalty which enforces the gradient of D to have unit norm along the straight line  Given c(u) of any unseen class u  Y u, by resampling the noise z and then recomputing  x = G(z, c(u)), arbitrarily many visual CNN features  x can be synthesized.|
|||As real CNN features, we extract 2048-dim toplayer pooling units of the 101-layered ResNet [21] from the entire image.|
|||As synthetic CNN features, we generate 2048-dim CNN features using our f-xGAN model.|
|||While the discriminator of f-GAN has one hidden layer with 1024 hidden units in order to stabilize the GAN training, the discriminators of f-WGAN and f-CLSWGAN have  1We denote our f-GAN, f-WGAN, f-CLSWGAN as f-xGAN  2as ImageNet is used for pre-training the ResNet [21]  5545  Zero-Shot Learning  Generalized Zero-Shot Learning  CUB FLO SUN AWA  CUB  FLO  SUN  AWA  Classifier  FG  T1  T1  T1  T1  u  s  H  u  s  H  u  s  H  u  s  H  DEVISE [14]  SJE [3]  LATEM [45]  ESZSL [40]  ALE [2]  Softmax  52.0 none f-CLSWGAN 60.3 53.9 none f-CLSWGAN 58.4 49.3 none f-CLSWGAN 60.8 53.9 none f-CLSWGAN 54.7 54.9  54.2 23.8 53.0 32.8 45.9 56.5 9.9 44.2 16.2 16.9 27.4 20.9 13.4 68.7 22.4 66.9 52.2 42.4 46.7 45.0 38.6 41.6 38.4 25.4 30.6 35.0 62.8 45.0 60.4 60.9 65.6 23.5 59.2 33.6 13.9 47.6 21.5 14.7 30.5 19.8 11.3 74.6 19.6 53.4 53.7 66.9 48.1 37.4 42.1 52.1 56.2 54.1 36.7 25.0 29.7 37.9 70.1 49.2 67.4 56.5 40.4 55.3 55.1 15.2 57.3 24.0 7.3 71.7 13.3 60.8 61.3 69.9 53.6 39.2 45.3 47.2 37.7 41.9 42.4 23.1 29.9 33.0 61.5 43.0 58.2 12.6 63.8 21.0 11.4 56.8 19.0 11.0 27.9 15.8 51.0 54.5 6.6 75.6 12.1 63.9 36.8 50.9 43.2 25.3 69.2 37.1 27.8 20.4 23.5 31.1 72.8 43.6 54.3 54.0 48.5 58.1 59.9 23.7 62.8 34.4 13.3 61.6 21.9 21.8 33.1 26.3 16.8 76.1 27.5 none f-CLSWGAN 61.5 71.2 62.1 68.2 40.2 59.3 47.9 54.3 60.3 57.1 41.3 31.1 35.5 47.6 57.2 52.0  6.6 47.6 11.5 14.7 28.8 19.5  none f-CLSWGAN 57.3                                  67.2 60.8  68.2 43.7 57.7 49.7 59.0 73.8 65.6 42.6 36.6 39.4 57.9 61.4 59.6  Table 2: ZSL measuring per-class average Top-1 accuracy (T1) on Y u and GZSL measuring u = T1 on Y u, s = T1 on Y s, H = harmonic mean (FG=feature generator, none: no access to generated CNN features, hence softmax is not applicable).|
|||The accuracy boost can be attributed to the strength of the f-CLSWGAN generator learning to imitate CNN features of unseen classes although not having seen any real CNN features of these classes before.|
|||Furthermore, we would like to emphasize that the simple softmax classifier beats all the models and is now applicable to GZSL thanks to our CNN feature generation.|
|||Our first observation is that for both ZSL and GZSL settings all generative models improve in all cases over none with no access to the synthetic CNN features.|
|||We conclude from these experiments that generating CNN features to support the classifier when there is missing data is a technique that is flexible and strong.|
|||Analyzing f-xGAN Under Different Conditions  In this section, we analyze f-xGAN in terms of stability, generalization, CNN architecture used to extract real CNN features and the effect of class embeddings on two fine-grained datasets, namely CUB and FLO.|
|||Using the pre-trained model, we generate CNN features of unseen classes.|
|||We then train a softmax classifier using these synthetic CNN features of unseen classes with real CNN features of seen  5547  CNN  FG  u  s  H  GoogLeNet  ResNet-101  20.2 35.7 25.8 none f-CLSWGAN 35.3 38.7 36.9 23.7 62.8 34.4 none f-CLSWGAN 43.7 57.7 49.7  Table 3: GZSL results with GoogLeNet vs ResNet-101 features on CUB (CNN: Deep Feature Encoder Network, FG: Feature Generator, u = T1 on Y u, s = T1 on Y s, H = harmonic mean, none= no generated features).|
|||Effect of CNN Architectures.|
|||The aim of this study is to determine the effect of the deep CNN encoder that provides real features to our f-xGAN discriminator.|
|||Besides, most importantly, with both CNN architectures we observe that our f-xGAN outperforms the none by a large margin.|
|||This is because generated CNN features help us explore the space of unseen classes whereas the state of the art learns to project images closer to seen class embeddings.|
|||This is due to the fact that stc leads to high quality features [35] reflecting the highly descriptive semantic content language entails and it shows that our f-CLSWGAN is able to learn higher quality CNN features given a higher quality conditioning signal.|
|||These results show that our f-CLSWGAN is able to generate high quality CNN features also with Word2Vec as the class embedding.|
|||With these results we emphasize that with a supervision as weak as a Word2Vec signal, our model is able to generate CNN features of unseen classes and operate at the ImageNet scale.|
|||image features extracted from 256  256 synthetic images generated by StackGAN [48] and CNN feature, i.e.|
|||On the other hand, generating CNN features leads to a significant boost of accuracy, e.g.|
|||Our framework is generalizable as it can be integrated to various deep CNN architectures, i.e.|
||33 instances in total. (in cvpr2018)|
|30|Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper|Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.|
|||Applying CNN directly for WSI classification has several drawbacks.|
|||Second, it is possible that a CNN might only learn from one of the multiple discriminative patterns in an image, resulting in data inefficiency.|
|||Therefore, one solution is to train a CNN on high resolution image patches and predict the label of a WSI based on patch-level predictions.|
|||Top: A CNN is trained on patches.|
|||We propose using a patch-level CNN and training a decision fusion model as a two-level model, shown in Fig.|
|||The first-level (patch-level) model is an Expectation Maximization (EM) based method combined with CNN that outputs patch-level predictions.|
|||We train a CNN model that outputs the cancer type probability of each input patch.|
|||In [50] a pretrained CNN model extracts features on patches which are then aggregated for WSI classification.|
|||Aggregating patchlevel CNN predictions for WSI classification significantly outperforms patch-level CNNs with max-pooling or vot(2) We propose a new EM-based model that identiing.|
|||fies discriminative patches in high resolution images automatically for patch-level CNN training, utilizing the spatial relationship between patches.|
|||Patch extraction and segmentation  To train the CNN model, we extract patches of size 500500 from WSIs (examples in Fig.|
|||To prevent the CNN from overfitting, we perform three kinds of data augmentation in every iteration.|
|||CNN architecture  The architecture of our CNN is shown in Tab.|
|||We used the CAFFE tool box [25] for the CNN implementation.|
|||Layer  Filter size, stride Output WHN  Input Conv ReLU+LRN Max-pool Conv ReLU+LRN Max-pool Conv ReLU Conv ReLU Max-pool FC ReLu+Drop FC ReLu+Drop FC Softmax   10  10, 2   6  6, 4 5  5, 1   3  3, 2 3  3, 1   3  3, 1   3  3, 2   400  400  3 196  196  80 196  196  80 49  49  80 45  45  120 45  45  120 22  22  120 20  20  160 20  20  160 18  18  200 18  18  200  9  9  200  320 320 320 320  Dataset dependent Dataset dependent  Table 1: The architecture of our CNN used in glioma and NSCLC classification.|
|||CNN-SMI: CNN followed by max-pooling.|
|||CNN-LR: CNN followed by logistic regression.|
|||One tenth of the patches in each image is held out from the CNN to train the second-level multi-class logistic regression.|
|||CNN-SVM: CNN followed by SVM with RBF kernel  instead of logistic regression.|
|||EM-Finetune-CNN-LR/SVM: Similar  to EM-CNNLR/SVM except that instead of training a CNN from scratch, we fine-tune a pretrained 16-layer CNN model [46] by training it on discriminative patches.|
|||SMI-CNN-SMI: CNN with max-pooling at both discriminative patch identification and image-level prediction steps.|
|||For the patch-level CNN training, in each WSI only one patch with the highest confidence is considered discriminative.|
|||But pretrained 16-layer CNN model to extract features from patches.|
|||Max-pooling: results by CNN with the SMI assumption (SMI-CNN-SMI).|
|||EM: results by our EM-based patch-level CNN (EM-CNNVote/SMI/LR).|
|||A CNN cannot be applied to gigapixel images directly because of computational limitations.|
|||CNN-Image: We apply the CNN on image scale directly.|
|||In particular, we train the CNN on 400400 regions randomly extracted from images in each iteration.|
|||At test time, we apply the CNN on five regions (top left, top right, bottom left, bottom right, center) and average the predictions.|
|||The CNN used in this experiment has a similar achitecture to the one described in Tab.|
|||Moreover, results using CNN features extracted on patches (Pretrained CNN-FeaSVM) are better than results with CNN features extracted on images (Pretrained-CNN-ImageFea-SVM).|
|||We proposed an Expectation-Maximization (EM) based method that identifies discriminative patches automatically for CNN training.|
||33 instances in total. (in cvpr2016)|
|31|Dixit_Scene_Classification_With_2015_CVPR_paper|This is the CNN analog of a Fisher vector mapping.|
|||Beyond SIFT Fisher vectors and CNN layers, there exists a different class of image mappings known as semantic representations.|
|||In this work, we argue that highly accurate classifiers, such as the ImageNET trained CNN of [16] eliminate the first problem.|
|||It follows that the semantic FV can be implemented as a classic (Gaussian Mixture) FV of pre-softmax CNN outputs.|
|||Due to the invariance of this representation, which is a direct result of semantic abstraction, it is shown to outperform Fisher vectors of lower layer CNN features [10] as well as a classifier obtained by fine-tuning the CNN itself [9].|
|||Bag of Features  Both the SIFT-FV classifier and the CNN are special cases of the general architecture in Figure 1, commonly  known as the bag of features (BoF) classifier.|
|||While Fisher vectors derived using mixture based encoding are invariant by design, a CNN embedding learned from almost centered object images is unlikely to cope with the variability in scenes.|
|||We argue, that the recent availability of robust classifiers such as the CNN of [16], trained on large scale datasets, such as ImageNET [7], effectively solves the problem of noisy semantics.|
|||This is because an ImageNET CNN is, in fact, trained to classify objects which may occur in local regions or patches of a scene image.|
|||CNN embedding  For the CNN of [16],  the mapping F consists of 5 convolutional layers.|
|||Comparison  We compared the CNN and FV embeddings, on two popular object recognition (Caltech 256 [11]) and scene classification (MIT Indoors [26]) datasets, with the results shown in the top half of Table 1.|
|||For the CNN embedding, 7th fully connected layer features were obtained with Caffe [14].|
|||Comparison of the ImageNET CNN and FV embeddings on scene and object classification tasks.|
|||Method  MIT Indoor Caltech 256  fc 7  conv5 + FV  fc7 + FV  59.5 61.43  65.1  68.26 56.37  60.97  invariant enough to represent images containing single objects, the CNN embedding cannot cope with the variability of the scene images.|
|||The process is illustrated in Figure 3 for a CNN classifier.|
|||Hence, the  mapping from  to  is the softmax transformation commonly implemented at the CNN output.|
|||This implies that  the CNN is learning how to discriminate the data in the natural parameter space of the multinomial distribution, which is a generalization of a natural binomial space shown in Figure 4 d).|
|||FVs of layer 7 activations  The proposed representation, when computed with mapping (2) of (15), as discussed above, acts directly on the outputs of the 8th layer (fc8) of ImageNET CNN [16].|
|||Besides their explicit semantic nature also ensures a higher level of abstraction, as a result of which they can generalize better than lower CNN layer features.|
|||The Places CNN  Recent efforts of improving scene classification have relied on a pre-trained imageNET CNN [8, 32, 10, 22].|
|||propose a more direct approach that does not rely on the ImageNET CNN at all.|
|||They simply learn a new CNN on a large scale database of scene images known as the Places dataset [39].|
|||Although the basic architecture of their Places CNN is the same as that of the ImageNET CNN, the type of features learned are very different.|
|||While the convolutional units of ImageNET CNN respond to object-like occurrences, those in Places CNN are selective of landscapes with more spatial features.|
|||The CNN features were extracted with the Caffe library [14].|
|||For FVs, the relevant CNN features (fc7 or fc8) were extracted from local PP image patches on a uniform grid.|
|||In this experiment, the CNN features were extracted at multiple scales (globally as well as from patches of size 80, 96, 128 and 160 pixels).|
|||First, when compared to the approach of extracting CNN features globally [8], the localized representations have far better performance.|
|||Third, recent arguments for the use of intermediate CNN features should be revised.|
|||As expected, the pioneering DeCaf [8] representation is vastly inferior to all other methods since it describes complex scene images with a globally extracted descriptor using an object CNN [16].|
|||Comparison with a CNN trained on Scenes [39] MIT Indoor MIT SUN 54.4 + 0.3  ImgNET fc8-FV (Our)  Method  Places fc7 [39]  Combined  54.34 + 0.14 61.72 + 0.13  scene dataset of interest and lasts about 5-10 hours on a single GPU.|
|||An alternative to using pre-trained object classification CNNs [16, 31] for scenes is to learn a CNN directly on a large scale scene dataset.|
|||Table 5 indicates a comparison between a scene representation obtained with the Places CNN and our ImageNET based semantic FV.|
||33 instances in total. (in cvpr2015)|
|32|Lee_Recursive_Recurrent_Nets_CVPR_2016_paper|Furthermore, the manually defined N-gram CNN model has a large number of output nodes (10k output units for N = 4), which increases the training complexity  requiring an incremental training procedure and heuristic gradient rescaling based on N-gram frequencies.|
|||The three main contributions of the work presented in this paper are:  (1) Recursive CNNs with weight-sharing, for more effective image feature extraction than a vanilla CNN under the same parametric capacity.|
|||[9] first used a CNN with multiple position-sensitive character classifiers for street number recognition.|
|||We refer to this baseline method as Base CNN (and labeled in Figure 3 as Base CNN) in the rest of the paper.|
|||Our proposed system is built upon this Base CNN model; we     g n i l e d o M    e g a u g n a L    l e v e l r e t c a r a h C     n o i t c a r t x E   e r u t a e F   e g a m  I  y1   y2   y3   <eow>   R   R   R   A   A   A   Attention   modeling   R   R   R   <sow>   y1   y2   R   A   R   yN-1   Fully connected6   Fully connected5   Recursive conv4   Recursive conv3   Recursive conv2   Recursive conv1   Figure 1: Recursive recurrent nets with attention modeling (R2AM) approach: the model first passes input images through recursive convolutional layers to extract encoded image features, and then decodes them to output characters by recurrent neural networks with implicitly learned character-level language statistics.|
|||One possible way to improve upon this Base CNN model to enable even longer range contextual dependencies for character prediction would be to consider using a larger kernel size for each convolutional layer or a deeper network, increasing the corresponding receptive field size.|
|||This allows the  3 iterations   2 iterations   3 iterations   2 iterations   =(cid:885)  =(cid:884)  =(cid:883)  =(cid:882)   Recursive  CNN   Recurrent  CNN   Figure 2: Illustration of the proposed untied recursive and recurrent convolutional layers.|
|||In the experiment section we observed that both recursive and recurrent versions of Base CNN model significantly improve the performance on many recent standard benchmarks such as Synth90k, SVT, and ICDAR13.|
|||For this reason, we choose the recursive version of Base CNN model for our overall system pipeline as shown in the bottom part of Figure 1.|
|||RNNs for character-level language modeling  The proposed untied recursive character sequence model in Section 2.2 can already serve as an end-to-end trainable  2233     ... C B A     ... C B A  ...     ... C B A  y1   y2   y3   y1   y2   y3   R  R  R  ...  R  R  R  ...  CNN  CNN  CNN  Baseline Character CNN   Single Layer, Captioning Style   Base CNN   Base CNN + RNN  1c   y1   y2   y3   y1   y2   y3   R  R  R  R  R  R  ...  ...  R  R  R  R  R  R  ...  ...  CNN  CNN  Single Layer, Unfactored   Base CNN + RNN  1u   y1   y2   y3   ...  ...  R  A  R  A  R  R  R  A  R  CNN  Two Layers, Unfactored   Base CNN + RNN  2u   Two Layers, Factored   Base CNN + RNN  2f   Two Layers, Attention Modeling   Base CNN + RNN  Atten   Figure 3: Five variations of the recurrent in time architecture that we experimentally evaluate for photo OCR task.|
|||Nonetheless, we observed that the Base CNN model (either plain CNNs, recursive CNNs, or recurrent CNNs) trains each character position independently by using multiple loss functions.|
|||Another way to access such character-level language information is to directly model all possibilities using a CNN  as in the bag-of-N-grams component of [17].|
|||Such a CNN model requires pre-defined Ngrams from a dictionary and uses a huge number of output nodes in which each node represents an element in N-gram combinations (e.g.|
|||The encoded image feature I is extracted from the last fullyconnected layer of the a CNN model.|
|||This CNN models can be either plain CNN, recurrent CNN, or recursive CNN.|
|||We will show how different CNN models perform in the experiment section.|
|||We detail four variants in this section and will explain the last variant (including attention modeling) in the next section: Base CNN: Baseline character sequence CNN trained with multiple loss functions where each loss function focuses on one character position as described in Section 2.1.|
|||Base CNN + RNN1c: A single-layer RNN inspired from image-captioning work [45].|
|||This variant serves as an good sanity check and helps us validate the capability of our RNN to perform character-level language modeling given an initial CNN representation.|
|||Base CNN + RNN1u: An unfactored single-layer RNN receiving image feature I at every time step  therefore the character predictions are conditioned on both image feature and previous hidden state at all time.|
|||Base CNN + RNN2u: An unfactored two-layer RNN using two stacks of RNNs.|
|||Base CNN + RNN2f: A factored two-layer RNN that uses two stacks of RNNs.|
|||We now describe our attention modeling function illustrated in Figure 3 as Base CNN + RNNAtten.|
|||Implementation details  The network architecture for our Base CNN model is shown in Table A1.|
|||Notice that each of the even number convolutional layer (conv2, conv4, conv6 or conv8) use its own shared weight matrix that has exactly the same input and output dimensionality, and so projects feature maps to the same space multiple times within one recursive convolutional layer under the same parametric capacity as Base CNN model.|
|||Method  CHAR [17] Base CNN  Recurrent CNN (2 iter) Recurrent CNN (3 iter)  Recursive CNN (2 iter) Recursive CNN (3 iter)  Synth90k  SVT  ICDAR13  87.3 91.9  92.6 93.5  93.3 94.2  68.0 75.1  75.8 76.9  77.1 78.9  79.5 85.7  86.1 87.4  87.3 88.5  Table 1: Unconstrained (lexicon-free) text recognition accuracies on recent benchmarks.|
|||Method  Synth90k  SVT  ICDAR13  CHAR [17] Base CNN Base CNN + RNN1c Base CNN + RNN1u Base CNN + RNN2u Base CNN + RNN2f Base CNN + RNNAtten  87.3 91.9 93.4 93.5 93.7 94.0 94.3  68.0 75.1 76.2 76.9 77.9 78.8 79.1  79.5 85.7 86.4 87.2 87.6 88.0 88.9  Table 2: Unconstrained (lexicon-free) text recognition accuracies on recent benchmarks.|
|||Our combined model R2AM (Recursive CNN + RNNAtten) significantly outperforms previous state-of-the-art methods in [17].|
|||that is due to architectural variations from that which might simply come from having more parameters, we first gradually increase the depth of the baseline CHAR model in [17] from 5 conv layers until we reach the performance plateau at 8 conv layers shown as Base CNN in Table A1.|
|||In an effort to decouple the performance improvement  Table 1 shows the effectiveness of the proposed untied recursive and recurrent CNNs over Base CNN model on unconstrained text recognition tasks.|
|||We observed an immediate performance boost by using any kind of the proposed RNN variants atop the Base CNN network which has already hit its performance plateau.|
|||RNN1c serves as a good sanity check module because the image features from the Base CNN are only fed to the RNN at the first time step, and then RNN1c is able to predict the first and the following character correctly based on the previously predicted character and the hidden state information.|
|||The comparison of RNN1u and RNN1c results indicates that feeding image feature from Base CNN to a RNN at every time step can further improve the performance, as  RNN1u has access not only to the previously predicted character and hidden state information, but also the raw image feature during inference.|
||33 instances in total. (in cvpr2016)|
|33|cvpr18-CNN Driven Sparse Multi-Level B-Spline Image Registration|Experimental results show that multi-grid configurations produced in this fashion using our CNN based approach provide registration quality comparable to L1norm constrained over-parameterizations in terms of exactness, while exhibiting significantly reduced computational requirements.|
|||Control points ill-suited for parameterization of the deformation transform under recovery are learned by a CNN using training data generated from L1-norm constrained multi-level B-spline grid registrations.|
|||Once such a CNN is trained to recognize the support required to express the deformation across all regions of the image, control point coefficients deemed superfluous by the CNN are constrained to zero while remaining coefficients are optimized to arrive at the transform best describing the image deformation.|
|||[23, 22] introduce a multi-level B 9282  Positive training Negative training  Original   !xed  CNN Layers  Di(cid:127)erence  Optical "ow  mag  Training set  Registration with   Sparse control   L1-norm and   bending energy  point coe#cients  Softmax   classi!cation  Original   !xed  Di(cid:127)erence  Registration with  bending energy  Grid with locked  control points  Optical "ow  mag  Testing set                 Candidate Grid Every grid owns a CNN model  Figure 2: Complete workflow.|
|||Most recently, CNN architectures have begun to inspire new approaches to image registration.|
|||Instead of optimizing parametric transformation model parameters based on a cost function, the proposed method uses CNN regressors to extract image features, which are used to predict transformation parameters for image correspondence.|
|||The second CNN has a more limited FOV and is tasked with performing finer local alignment.|
|||In this paper, we propose a CNN driven multi-grid Bspline method that focuses on learning the most suitable parameterization for describing the deformable transform between two images prior to the parameter optimization process.|
|||To this end, the proposed CNN is trained using deformable registrations normalized by the L1-norm in a fashion similar to that performed by Shi et al.|
|||Learning Grid Configurations with CNNs             ) 3 X 6 1 x 4 6 x 4 6 (           t u p n  I                              ) 3 x 3 x 3 (        D 3 v n o C     U L e R  g n  i l     o o p x a M     D 3 v n o C             ) 3 x 3 x 3 (                      ) 3 X 8 x 2 3 x 2 3 (        t u p n  I                              ) 3 x 3 x 3 (                      ) 3 X 4 x 6 1 x 6 1 (        D 3 v n o C     t u p n  I                      U L e R  U L e R  g n  i l     o o p x a M     D 3 v n o C             ) 3 x 3 x 3 (        U L e R  g n  i l     o o p x a M     Level1 CNN model  g n  i l     o o p x a M     D 3 v n o C             ) 3 x 3 x 3 (        U L e R  g n  i l     o o p x a M     Level2 CNN model          ) 3 x 3 x 3 (        D 3 v n o C     U L e R  D 3 v n o C             ) 3 x 3 x 3 (        U L e R  g n  i l     o o p x a M     Level3 CNN model  n e t t a F  l  e s n e D  U L e R     n e t t a F  l  e s n e D  U L e R        m r o n h c t a B     m r o n h c t a B  t u o p o r D               .  )|
|||5 0 = p  (                x a m  t f o S     t u p t u O  Figure 4: CNN architecture for each B-spline control grid level.|
|||The CNN for level 1 accepts 3 input channels of dimensions 64 64 16.|
|||The CNN for level 2 accepts 3 input channels of of 32  32  8.|
|||The CNN for level 3 accepts 3 input channels of 16164.|
|||To this end, a CNN architecture is trained using the B-spline control point coefficients and grid configurations produced by L1-norm regularized registrations.|
|||The resulting sparse B-spline control grids consist largely of zero valued control point coefficients, which we will call locked control points, as well as a minority of non-zero valued coefficients, which we will call free control points. These free and locked designations for control points form the class labels for the CNN training data.|
|||Preprocessing is performed on the associated image volumes to produce feature channels that will serve as inputs to the CNN input layer.|
|||One CNN is trained per Bspline control grid layer.|
|||For example, the CNN architecture for the three layer grids experimentally validated in this paper is illustrated in Figure 4.|
|||Preprocessing & CNN Inputs  Input channels to the proposed CNN architecture consist of the unaltered fixed image F as well as two other input channels derived from the fixed and the moving images in combination.|
|||In total, the number of used training patches for grid Level 1 CNN is 64,564 (32,282 positive and 32,282 negative); 233,250 for grid Level 2 (again, evenly balanced); and 679,256 for grid Level 3 (evenly balanced).|
|||This optimization burden is greatly reduced by CNN control point classification.|
|||Dataset Before CNN After CNN Reduction 86% Set 01 Set 02 89% 93% Set 01 93% Set 02 94% Set 01 Set 02 95%  128,625 128,625 902,289 902,289 6,744,273 6,744,273  17,246 13,618 59,506 60,512 397,404 323,880  L1  L2  L3  Table 1: Average number of B-spline parameters requiring optimization with and without CNN control point classification.|
|||Control point centric image patches extracted from four pairs of images from Set 01 and four pairs of images from Set 02 are used as CNN training data.|
|||Consequently, the number of training patches corresponding to the locked  (a)  (b)  (c)  (d)  (e)  (f)  (g)  (h)  (i)  (j)  (k)  (l)  (m)  (n)  (o)  (p)  (q)  (r)  Figure 7: Illustration of CNN control point classification accuracy.|
|||Prediction maps are gray when both the L1-norm and CNN agree the coefficient is free, cyan when the L1-norm votes free and the CNN votes locked, blue when both L1 norm and CNN agre that a coefficient is locked, and purple when the L1-norm votes locked and the CNN votes free.|
|||(e) is the difference between the fixed image and the transformed moving image for the CNN driven registration process, and (f) is the same difference image for the L1-norm driven process.|
|||specificity:  SP =  T N  F P + T N  avg Dataset  L1  L2  L3  Set 01 Set 02 Set 01 Set 02 Set 01 Set 02  SN 0.91 0.94 0.91 0.92 0.89 0.90  SP 0.94 0.97 0.97 0.97 0.95 0.96  AC 0.94 0.97 0.97 0.97 0.95 0.96  AUC 0.94 0.96 0.95 0.95 0.93 0.94  Table 2: CNN classification performance quantified in terms of sensitivity (SN), specificity (SP), accuracy (AC), and area under curve (AUC) for each of the three grid levels tested.|
|||Using the total number of true positive, true negative, false positive, and false negative control point classification instances, the classification performance of the CNN is expressed within Table 2 in terms of sensitivity:  SN =  T P  T P + F N  (6)  (7)  (8)  and accuracy:  AC =  T P + T N  T P + F N + F P + T N  The average sensitivity, specificity, accuracy, and area under the receiver operating characteristic curve (AUC) are shown in Table 2 for each of the three grid levels employed in our thoracic 4D CT validation study.|
|||Registration Results  Due to the decreased number of registration parameters requiring optimization under the proposed CNN driven algorithm coupled with the expressiveness of free class parameters, the SSD similarity metric decreased more rapidly  9287  L1-norm CNN     8000  6000  D S S  4000  2000  L1-norm CNN     8000  6000  D S S  4000  2000     L1-norm CNN  Acknowledgments: This material is based upon work supported by the National Science Foundation under Grant Numbers 1553436 (CAREER) and 1642380 (SI2-SSE).|
|||Each registration is performed using both L1-norm regularized registration (shown in blue) and the proposed CNN driven registration (shown in red).|
|||A CNN regression approach for real-time 2D/3D registration.|
||32 instances in total. (in cvpr2018)|
|34|Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper|Their success resulted from training a large CNN on 1.2 million labeled images, together with a few twists on LeCuns CNN (e.g., max(x, 0) rectifying non-linearities and dropout regularization).|
|||The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?|
|||This paper is the first to show that a CNN can lead to dra 1  1.|
|||Compute CNN featuresaeroplane?|
|||Classify regionswarped region...CNNR-CNN: Regions with CNN featuresmatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.1 Achieving this result required solving two problems: localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data.|
|||Instead, we solve the CNN localization problem by operating within the recognition using regions paradigm, as argued for by Gu et al.|
|||We use a simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the regions shape.|
|||Since our system combines region proposals with CNNs, we dub the method R-CNN: Regions with CNN features.|
|||We lobotomized the CNN and found that a surprisingly large proportion, 94%, of its parameters can be removed with only a moderate drop in detection accuracy.|
|||[6], who detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals.|
|||We extract a 4096-dimensional feature vector from each region proposal using the Caffe [21] implementation of the CNN described by Krizhevsky et al.|
|||In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatible with the CNN (its architecture requires inputs of a fixed 227  227 pixel size).|
|||We warp each proposal and forward propagate it through the CNN in order to read off features from the desired layer.|
|||First, all CNN parameters are shared across all categories.|
|||We discriminatively pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with image-level annotations (i.e., no bounding box labels).|
|||In brief, our CNN nearly matches the performance of Krizhevsky et al.|
|||To adapt our CNN to the new task (detection) and the new domain (warped VOC windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals from VOC.|
|||Aside from replacing the CNNs ImageNet-specific 1000-way classification layer with a randomly initialized 21-way classification layer (for the 20 VOC classes plus background), the CNN architecture is unchanged.|
|||For final results on the VOC 2010-12 datasets, we fine-tuned the CNN on VOC 2012 train and optimized our detection SVMs on VOC 2012 trainval.|
|||Each row in Figure 3 displays the top 16 activations for a pool5 unit from a CNN that we fine-tuned on VOC 2007 trainval.|
|||Rows 4-6 show results for the CNN pre-trained on ILSVRC 2012 and then fine-tuned (FT) on VOC 2007 trainval.|
|||We start by looking at results from the CNN without fine-tuning on PASCAL, i.e.|
|||all CNN parameters were pretrained on ILSVRC 2012 only.|
|||We now look at results from our CNN after having fine-tuned its parameters on VOC 2007 trainval.|
|||Compared with DPM (see [20]), significantly more of our errors result from poor localization, rather than confusion with background or other object classes, indicating that the CNN features are much more discriminative than HOG.|
|||Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification.|
|||[13] recently demonstrated good results on several dense scene labeling datasets (not including PASCAL) using a CNN as a multi-scale per-pixel classifier.|
|||The first strategy (full) ignores the regions shape and computes CNN features directly on the warped window, exactly as we did for detection.|
|||Therefore, the second strategy (fg) computes CNN features only on a regions foreground mask.|
|||Column 1 presents O2P; 2-7 use our CNN pre-trained on ILSVRC 2012.  always outperforms fc7 and the following discussion refers to the fc6 features.|
|||Without any fine-tuning, our CNN achieves top segmentation performance, outperforming R&P and roughly matching O2P.|
||31 instances in total. (in cvpr2014)|
|35|Zhang_Real-Time_Action_Recognition_CVPR_2016_paper|Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter.|
|||One successful example along this line is the two-stream framework [23] which utilizes both RGB CNN and optical flow CNN for classification and achieves the state-of-the-art performance on several large action datasets.|
|||Directly replacing optical flows with motion vectors will severely degrade recognition performance of CNN as observed in our experiments.|
|||Here our key insight to improve the recognition performance of motion vector CNN is that optical flow and motion vector share some similar characteristics which allows us to transfer the fine features/knowledge learned in optical flow CNN (OF-CNN) to that of motion vectors (MV-CNN).|
|||We call this new CNN as optical flow enhanced motion vector CNN.|
|||Firstly, we propose a real-time CNN based action recognition method which achieves comparable performance with the state-of-the-art two-stream approach [23].|
|||Finally, we propose techniques to transfer the knowledge of optical flow CNN to motion vector CNN, which significantly improves the recognition performance.|
|||Extensive experiments have demonstrated that CNN can achieve superior performance on various image and video classification tasks, e.g.|
|||These successes inspire researchers to extend CNN for video classification tasks [13, 23, 27].|
|||This fact can largely harm the recognition performance of CNN if we directly train networks with motion vectors.|
|||To relieve this problem, we propose several training methods to enhance motion vector CNN (MV-CNN) for better recognition performance.|
|||Clearly, in action recognition with CNNs, empty I-frame can hinder CNN training process and degrade the performance.|
|||The second component follows the two-stream architecture [23], which can be decomposed into spatial CNN (RGB-CNN) and temporal CNN (MV-CNN).|
|||The weight for spatial and temporal CNN are set as 1 and 2.|
|||We follow [23] by setting dropout ratio for spatial CNN to 0.9 to avoid over-fitting.|
|||Our temporal CNN is slightly different with ClarifaiNet by replacing all ReLU layers with PReLU [6] layers, which exhibits better performance and quick convergence.|
|||Temporal CNN is trained from scratch by stacking 10-frame motion vectors as input.|
|||This fact enables us to leverage the rich knowledge and fine features learned in optical flow CNN (OF-CNN) to enhance MV-CNN.|
|||Parameters for teacher CNN in optical flow domain is denoted by Tp = {T 1 p }, where n represents the total number of layers.|
|||As for student CNN (MV-CNN), its parameter is defined as Sp = {S 1 In this paper, we assume MV-CNN has the same network structure as OFCNN, while the techniques can be easily generalized to those with different structures.|
|||Teacher Initialization  As analyzed above, motion vectors lack fine details and contain noisy and inaccurate motion patterns, which makes training motion vector CNN (MV-CNN) more challenging.|
|||Formally, for a given frame I with optical flow feature o and motion vector v , we calculate the output of the last FC layer of teacher CNN as: T n(o) = softmax(T n1(o)), where softmax function is used to transform the feature T n1 to a probability score of multiple classes.|
|||Comparison of temporal CNN performance for Optical Flow based approach and Motion vector based Method on UCF101 (Split1).|
|||As videos in THUMOS14 are untrimmed and have large number of frames, we conduct CNN testing at every 20 frames.|
|||Furthermore, following [35]1, we use a scale jittering strategy to help CNN to learn robust features.|
|||Our teacher CNN is trained on TV-L1 optical flow [21] that achieves 81.6% on UCF101 Split1, which is comparable with the performance in the original paper 81.2% [23].|
|||Evaluation of MV(cid:173)CNNs and EMV(cid:173)CNNs  In this subsection, we compare and analyze different training strategies for motion vector CNN (MV-CNN) on UCF101 Split1 and THUMOS14.|
|||In addition, combining EMV-based with spatial CNN exhibits only a minor performance loss compared with OF-based CNN on UCF101.|
|||EMV+RGB-CNN stands for twostream based CNN with EMV-CNN and RGB-CNN.|
|||EMV+RGB-CNN stands for twostream based CNN with EMV-CNN and RGB-CNN.|
|||To relieve this problem, we developed three knowledge transfer techniques to adapt the models of optical flow CNN to motion vector CNN, which significantly boost the recognition performance of the latter.|
||31 instances in total. (in cvpr2016)|
|36|cvpr18-Egocentric Basketball Motion Planning From a Single First-Person Image|Next, we use our proposed inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations by optimizing the following objectives: (1) minimize the difference between the refined configurations and initial configurations predicted by future CNN while also (2) maximizing the goal verifier network output.|
|||First, we feed the first-person image through our proposed future CNN to predict an initial sequence of future 12D camera configurations.|
|||First, we use our proposed future CNN to predict an initial sequence of 12D camera configurations, which aim to capture how real players move during a one-on-one basketball game.|
|||After that, we use our proposed inverse synthesis procedure to generate a refined camera configuration sequence that (1) matches the initial camera configuration sequence predicted by the future CNN and (2) maximizes the output of the goal verifier network, which is trained to verify whether a given 12D camera configuration aligns with realistic one-on-one basketball goals.|
|||Egocentric Camera (EgoCam) CNN  An egocentric camera (EgoCam) CNN is used to map a given first-person image xi into a 12D camera configuration yi  R112 that encodes the 3D location and 3D orientation of the camera on a players head.|
|||We implement our EgoCam CNN using a popular ResNet-101 [12] architecture.|
|||The network is optimized using the following L2 loss:  5891  The basket  The basket  Input  Future CNN Output  Input  Future CNN Output  Figure 3: An illustration of the camera configurations generated by our future CNN, which we visualize in a sparsely reconstructed 3D space (best viewed in color).|
|||We note that our future CNN is able to produce a diverse set of intermediate configurations, which allows us to generate a wide array of different sequences at a later step in our model.|
|||Our EgoCam CNN produces similar outputs to the actual SfM algorithm.|
|||We also report that we tried retrieving missing 12D configurations using a nearest neighbor algorithm, but observed that our EgoCam CNN produced better results (2.54 vs. 1.97 L2 error in the normalized 12D space).|
|||Future CNN  Given a first person image xi from the beginning of a sequence, the future CNN encodes it into multiple configurations (xi)  Rk12, which represent intermediate states that capture how real players move during a one-on-one game.|
|||k. Each branch in the future CNN is responsible for generating its own intermediate state.|
|||The jth branch in the future CNN is then optimized to predict camera configurations, with s values falling in the interval  k , j  [ j1 k ], where k is the number of branches in the future CNN.|
|||We also point out that all input images xi that are used to train the future CNN have values s  [0, 0.1] to make sure that the future CNN generates accurate intermediate configurations from a first-person image at the beginning of a sequence.|
|||The future CNN is then trained using the following loss function:  Lf ut =  k  X  j=1  ||j(xi)  yi0 (xi0 )||2  2  (2)  where j denotes the output from branch j of a future CNN, and yi0 is the output of an EgoCam CNN for an input image xi0 .|
|||In the second and third columns, we visualize the activations of a Future CNN from the res4a and res5c layers.|
|||Subsequently, the goal verifier, which is a two-layer network, takes a 12D camera configuration as its input and is trained using a cross-entropy loss:  Lg = g(yi) log (yi) + (1  g(yi)) log (1  (yi)) (3)  where yi  R112 is the 12D camera configuration output from an EgoCam CNN (associated with an image xi), and (yi)  R11 is the output of a goal verifier network  that indicates whether a given 12D camera configuration accurately represents the final goals of real players.|
|||Our approach is partially inspired by the idea of synthesizing the preferred stimuli in a CNN [42, 26, 27], which has been mostly used to visualize activations in the CNNs [42, 26, 27].|
|||In the previous sections, we solved these two problems using the future CNN and the goal verifier network, respectively.|
|||However, we now want to invert these two problems: given (1) a set of intermediate states predicted by the future CNN and (2) a trained goal verifier network, our goal is to generate a smooth basketball motion sequence in the form of 12D camera configurations.|
|||To do this, we frame the inverse synthesis problem as a search problem in the 12D camera space, where our goal is to find a 12D configuration h that (1) matches intermediate states generated by the future CNN j(xi) and (2) maxi 5893  GAN [11] LSTM [14] RNN [16]  Evaluation Tasks PFM  62.31 5.66 5.82 5.36 Ours w/o GV 4.91 Ours w/ GV 4.93  CG  0.329 0.678 0.612  NN   0.671 0.776  Figure 5: A figure illustrating the 3D locations of every camera configuration from our dataset mapped on a 2D court (best viewed in color).|
|||To select the branch from a future CNN whose output j we are trying to match, we compute j = f loor(c/(N/k)), where j is the index of a selected branch, c is the iteration counter, and k is the number of branches in the future CNN.|
|||We designed our future CNN to have 4 distinct branches, which we experimentally discovered to work well.|
|||To implement this idea, we first use our EgoCam CNN to extract camera configurations from every frame in every real sequence in the testing dataset.|
|||We conjecture that this happens because the future CNN is trained using a non 5895  discriminative L2 loss, while the goal verifier network is trained to discriminate which configurations represent the final goals of the players.|
|||Qualitative Results  Future CNN Visualization.|
|||To better understand how the future CNN works, we visualize its outputs in Figure 3.|
|||Note that the future CNN produces a wide array of different configurations, allowing us to generate diverse sequences.|
|||Furthermore, in Columns 2 and 3 of Figure 4, we visualize the activations of a future CNN from the res4a and res5c layers, respectively.|
|||Furthermore, we also observe that, in the res5c layer, the future CNN learns to recognize open spaces in the court that could be used by a player to navigate the court and get away from a defender.|
||30 instances in total. (in cvpr2018)|
|37|Xiao_Learning_Deep_Feature_CVPR_2016_paper|When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one.|
|||The CNN model we designed consists of several BN-Inception [16, 37] modules, and its capacity well fits to the scale of the mixed dataset.|
|||This carefully designed CNN model provides us a fairly strong baseline, but the simple joint learning scheme does not take full advantages of the variations of multiple domains.|
|||[1] designed CNN models specifically to the Re-ID task and achieved good performance on large-scale datasets.|
|||Method  Our proposed pipeline for learning CNN features from multiple domains consists of several stages.|
|||As shown in Figure 2, we first mix the data and labels from all the domains together, and train a carefully designed CNN from scratch on the joint dataset with a single softmax loss.|
|||Then we replace the standard Dropout layer with the proposed Domain Guided Dropout layer, and continue to train the CNN model for several more epochs.|
|||With the guidance of which neurons being effective for each domain, the CNN learns more discriminative features for all of them.|
|||At last, if we want to obtain feature representations for a specific domain, the CNN could be further fine-tuned on it, again with the Domain Guided Dropout to improve the performance.|
|||In our approach, we train a CNN to recognize the identity of each person, which  i  is also adopted in the face verification work [36].|
|||Joint learning objective and the CNN structure  When mixing all the D domains together, a straightforward solution is to employ a multi-task objective function, i.e., learning D softmax classifiers f1, f2, .|
|||Since pedestrian images are usually quite small and are not of square-shapes, it is not appropriate to directly use the ImageNet pretrained CNN models, which are trained with object images of high resolution and abundant details.|
|||For training the CNN from scratch, we randomly dropout 50% neurons of the fc7 layer.|
|||For the person re-identification problem, we first train a CNN jointly on all six domains.|
|||We propose a Domain Guided Dropout algorithm to discard useless neurons for each domain during the training process, which drives the CNN to learn better feature representations on all the domains simultaneously.|
|||The structure of our proposed CNN for person re-identification  3.3.|
|||Domain Guided Dropout  Given the CNN model pretrained by using the mixed dataset, we identify for each domain which neurons are effective.|
|||Specifically, let g(x)  Rd denote the d-dimensional CNN feature vector of an image x.|
|||In order to validate our approach, we first obtain a baseline by training the CNN individually on each domain.|
|||Next, we improve the learned CNN with the proposed deterministic Domain Guided Dropout (JSTL+DGD).|
|||To show our best possible results, we further finetune the CNN separately on each domain with the stochastic Domain Guided Dropout (FT-JSTL+DGD).|
|||We first evaluate the effectiveness of the proposed CNN structure.|
|||When the network is trained only with the CUHK03 dataset, which is large enough for training CNN from scratch, we improve the state-of-theart result by more than 10% to 72.6% (row 2 of Table 3).|
|||A two-stream network is used in [1] to compute the verification loss given a pair of images, while we opt for learning a single CNN through an ID classification task and directly computing Euclidean distance based on the features.|
|||When the training set is large enough, this classification objective makes the CNN much easier to train.|
|||However, when the dataset is quite small, it would be insufficient to learn such a large capacity network from scratch, which is demonstrated in Table 3 by the results of training the CNN only on each of the VIPeR, 3DPeS, and iLIDS datasets.|
|||This indicates that it is effective to regularize the network specifically for different domains, which maximizes the discriminative power of the CNN on all the domains simultaneously.|
|||Relative performance gain with respect to the number of neurons having negative impact scores on specific domain in the deterministic Guided Dropout scheme  of the stochastic Domain Guided Dropout when fine-tuning the CNN model.|
|||This again indicates that we should not treat all the domains equally when using all their data, but rather regularize the CNN properly for each of them.|
|||Conclusion  In this paper, we raise the question of learning generic and robust CNN feature representations from multiple domains.|
||30 instances in total. (in cvpr2016)|
|38|Wu_Harvesting_Discriminative_Meta_ICCV_2015_paper|Harvesting Discriminative Meta Objects with Deep CNN Features  for Scene Classification  Ruobing Wu1  Baoyuan Wang  Wenping Wang  Yizhou Yu   The University of Hong Kong   Microsoft Technology and Research  Abstract  Recent work on scene classification still makes use of generic CNN features in a rudimentary manner.|
|||In this paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification.|
|||We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches.|
|||We further apply discriminative clustering enhanced with local CNN finetuning to aggregate similar objects and parts into groups, called meta objects.|
|||In the context of scene classification, although a series of state-ofthe-art results on popular benchmark datasets (MIT Indoor 67[22], SUN397 [31]) have been achieved, CNN features are still used in a rudimentary manner.|
|||For example, recent work in [33] simply trains the classical Alexs net [14] on a scene-centric dataset (Places) and directly extracts holistic CNN features from entire images.|
|||The reason is that spatial aggregation performed by pooling layers in a CNN is too simple, and does not retain much information about local feature distributions.|
|||It has been shown in [8] that in addition to the entire image, it is consistently better to extract CNN features from multiscale local patches arranged in regular grids.|
|||In order to build a discriminative representation based on deep CNN features for scene image classification, we need to address two technical issues: (1) Objects within scene images could exhibit dramatically different appearances, shapes, and aspect ratios.|
|||In this paper, we present a novel pipeline built upon deep CNN features for harvesting discriminative visual objects and parts for scene classification.|
|||We apply a pretrained CNN to extract generic deep features from these patches.|
|||Locally aggregated CNN features are more discriminative than those global features fed into the fully connected layers in a single CNN.|
|||In summary, this paper has the following contributions: (1) We propose a novel pipeline for scene classification that is built on top of deep CNN features.|
|||(3) Instead of global fine-tuning, we locally fine-tune the CNN using the meta objects discovered from the target dataset.|
|||(d) Local fine-tuning is performed on Hybrid CNN [33], which decides which meta object a testing region belongs to.|
|||Feature Extraction We use the CNN model pre-trained on the Places dataset [33] as our generic feature extractor for all the image regions generated by MCG.|
|||Then each patch propagates through all the layers in the pre-trained CNN model, and we take the 4096-dimensional vector in the FC7 layer as the feature representation of the patch (see [14] and [33] for detailed information about the network architecture).|
|||We choose to fine-tune the pre-trained CNN on our meta objects, which include the collection of discriminative patches surviving the patch screening process.|
|||We perform stochastic gradient descent over the pre-trained CNN using the warped discriminative image patches and their corresponding meta object labels.|
|||The holistic Places CNN feature extracted from the whole image is also useful for training the scene image classifier since they also encode local as well as global information of the scene.|
|||The feature representation of a proposed region is set to the 4096-dimensional vector at the FC7 layer of the Hybrid CNN from [33].|
|||Discriminative clustering is performed on the augmented patches to produce 120 (40) meta objects for local fine-tuning in the bottom (top) level, which is performed on the Hybrid CNN by replacing the original output layer that performs ImageNet-specific 1000-way classification with a new output layer that does 121-way (41-way) classification while leaving all other layers unchanged.|
|||The image classification is done by a neural network with two fully-connected layers (200 nodes each) on the concatenated feature vector of VLAD pooling and the Hybrid CNN feature of the whole image.|
|||We also train a neural network with two fully-connected layers (200 nodes each) on the concatenated feature vector of VLAD pooling and the Hybrid CNN feature of the whole image to deal with image level classification.|
|||DeCAF [6] uses the global 4096D feature  1292  from a pre-trained CNN model on ImageNet.|
|||To compare, we start with the Places CNN network [33], and fine-tune this network on MIT Indoor 67.|
|||For a fair comparison, we use the Places CNN feature (FC7) to represent the visual elements in this work.|
|||We have also used the CNN locally fine-tuned on SUN397 to perform cross-dataset classification on MIT Indoor 67.|
|||Conclusions  We have introduced a novel pipeline for scene classification, which is built on top of pre-trained CNN networks via explicitly harvesting discriminative meta objects in a local manner.|
||29 instances in total. (in iccv2015)|
|39|Samuel_Schulter_Learning_to_Look_ECCV_2018_paper|We present a CNN that can hallucinate depth and semantics in areas occluded by foreground objects (marked in red and obtained via standard semantic segmentation), which gives an initial but noisy and incomplete estimate of the scene layout (middle).|
|||3.1, we propose a novel CNN that takes as input an image with occluded regions (corresponding to foreground objects) masked out, and estimates the segmentation labels and depth values over the entire image, essentially hallucinating distances and semantics in the occluded regions.|
|||Since there is no correspondence between actual images and simulated data, we employ an adversarial loss for teaching our CNN a generative aspect about typical layouts.|
|||3.1, we propose a CNN that takes I M as input and hallucinates the depth as well as the semantics of the entire image, including occluded pixels.|
|||3.3, we thus propose a CNN that learns priors from simulated data to further improve our representation.|
|||In order to inform the CNN about which pixels have to be in-painted, we apply the mask on the input RGB image and define each pixel in the masked input I M as  I M  ij =(  m,  Iij,  if Mij = 1 otherwise,  32  w  32 to h  8  w 8 .|
|||where  m is the mean RGB value of the color range, such that after normalization the input to the CNN is zero for those pixels.|
|||Similar to recent semantic segmentation literature [35], we use a larger stride in convolutions and dilation [32] to increase the feature map resolution from h  In addition to masking the input image, we explicitly provide the mask as input to the CNN for two reasons: (i) While the value m becomes 0 after centering the input of the CNN, other visible pixels might still share the same value and confuse the training of the CNN.|
|||We encode M cls with a small CNN and fuse the resulting feature with the one from the masked image, see Fig.|
|||(a) The inpainting CNN first encodes a masked image and the mask itself.|
|||(b) To train the inpainting CNN we ignore foreground objects as no ground truth is available (red) but we artificially add masks (green) over background regions where full annotation is already available  Fig.|
|||Training: We train the proposed CNN in a supervised way.|
|||In this way, we can teach our CNN to hallucinate occluded areas of the input image without acquiring costly human annotations.|
|||3.3 Refinement with a Knowledge Corpus  To remedy the above mentioned issues, we propose a refinement CNN that takes Binit and predicts the final output Bfinal  RklC bg , which has the same dimensions as Binit.|
|||The refinement CNN has an encoder-decoder structure with a fully-connected bottleneck layer, see Fig.|
|||The main difficulty in training the refinement CNN is the lack of semantic ground truth data in the birds eye view, which is very hard and costly to annotate.|
|||(c) The alignment CNN takes as input the initial BEV map and a crop of OSM data (via noisy GPS and yaw estimate given).|
|||Also, we can either train this CNN separately or jointly with the refinement CNN, thus providing different training signals for the refinement module.|
|||4.1 Occlusion Reasoning by Hallucination  Here we analyze our hallucination CNN proposed in Sec.|
|||Semantics & depth space versus RGB space: We compare our hallucination CNN with a baseline that takes the traditional approach of in-painting and operates in the RGB pixel space.|
|||The second CNN has the exact same architecture as our hallucination CNN and is trained without masking inputs but instead uses the already in-painted RGB images.|
|||Qualitative examples of our direct hallucination CNN are given in Fig.|
|||Note that the proposed refinement CNN recovers most errors for roads, while the relative performance drop for sidewalks is larger.|
|||Importance of depth prediction: We train a CNN that takes as input the RGB image in the perspective view and directly predicts the BEV map, without  Semantic Top-view Representations of Outdoor Scenes  13  Fig.|
|||The CNN extracts basic features with ResNet-50 [12], applies strided convolutions for further down-sampling, a fullyconnected layer resembling a transformation from 2D to 3D, and transposed convolutions for up-sampling into the BEV dimensions.|
|||We can also see that the proposed alignment CNN trained jointly with the refinement module provides the best training signal from OSM data.|
|||3.3, LBFG-S alignment failed for around 30% of the training data, which explains the superiority of the proposed CNN for predicting warping parameters.|
|||This requires solving the canonical challenge of hallucinating semantics and geometry in areas occluded by foreground objects, for which we propose a CNN trained using only standard annotations in the perspective image.|
|||Garg, R., G, V.K.B., Carneiro, G., Reid, I.: Unsupervised CNN for Single View  Depth Estimation: Geometry to the Rescue.|
||29 instances in total. (in eccv2018)|
|40|Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper|To handle an unseen target crowd scene, we present a data-driven method to finetune the trained CNN model for the target scene.|
|||After a CNN is trained with a fixed dataset, a data-driven method is introduced to fine-tune (adapt) the learned CNN to an unseen target scene, where training samples similar to the target scene are retrieved from the training scenes for fine-tuning.|
|||Our CNN model is trained for crowd scenes by a switchable learning process with two learning objectives, crowd density maps and crowd counts.|
|||Our CNN model learns crowdspecific features, which are more effective and robust than handcrafted features.|
|||The pre-trained CNN model is fine-tuned for each target scene to overcome the domain gap between different scenes.|
|||However, instead of directly transferring labels to the target scene like existing methods, we propose to use the training samples that fits the estimated crowd density distribution to fine-tune (adapt) the pre-trained CNN model to the target scene.|
|||Normalized crowd density map for training  The main objective for our crowd CNN model is to learn a mapping F : X  D, where X is the set of low-level features extracted from training images and D is the crowd density map of the image.|
|||Patches randomly selected from the training images are treated as training samples, and the density maps of corresponding patches are treated as the ground truth for the crowd CNN model.|
|||Crowd CNN model  An overview of our crowd CNN model with switchable objectives is shown in Figure 3.|
|||Then the patches are warped to 72 pixels by 72 pixels as the input of the Crowd CNN model.|
|||The main task for the crowd CNN model is to estimate the crowd density map of the input patch.|
|||Because two pooling layers exist in the CNN model, the output density map is down-sampled to 18  18.|
|||The two loss functions are defined as:  LD() =  LY () =  1 N  1 N  N(cid:2)  i  N(cid:2)  i  (cid:3)Fd(Xi; )  Di(cid:3)2,  (cid:3)Fy(Xi; )  Yi(cid:3)2,  (2)  (3)  where  is the set of parameters of the CNN model and N is the number of training samples.|
|||We set LD as the first objective loss to minimize, since the density map can introduce more spatial information to the CNN model.|
|||Algorithm 1: Training with iterative switching losses Input: Training set: size-normalized patches with  their counts and density maps from the whole training data  Output: Parameters  for crowd CNN model  1 set LD as the first objective; 2 for t = 1 to T do 3  BP to learn , until the validation loss drop rate L is less than the threshold  Switch the objective loss function  4 5 end  4.|
|||Nonparametric fine-tuning for target scene The crowd CNN model is pre-trained based on all training scene data through our proposed switchable learning process.|
|||These properties significantly change the appearance of crowd patches and affect the performance of the crowd CNN model.|
|||In order to bridge the distribution gap between the training and test scenes, we design a nonparametric fine-tuning scheme to adapt our pre-trained CNN model to unseen target scenes.|
|||Given a target video from the unseen scenes, samples with similar properties from the training scenes are retrieved and added to training data to fine-tune the crowd CNN model.|
|||Illustration of retrieving local patches similar to those in the test scene to fine-tune the crowd CNN model.|
|||With the pre-trained CNN model presented in Section 3.2, we can roughly predict the density and the total count for every patch of the target image.|
|||The fine-tuned crowd CNN model achieves better performance for the target scene.|
|||Our crowd CNN model is compared with  7  scene-specific methods.|
|||Our predicted density map from the CNN model can be treated as feature.|
|||Comparison with global regression methods for crowd counting on the UCSD dataset  Method  Kernel Ridge Regression [1]  Ridge Regression [6]  Gaussian Process Regression [4]  Cumulative Attribute Regression [5]  Our Crowd CNN Model  MAE MSE 2.16 7.45 7.82 2.25 7.97 2.24 6.86 2.07 1.60 3.31  Table 4 reports the errors with our methods and four other methods based on global regression.|
|||Our proposed crowd CNN model outperforms all the global regression based approaches for both metrics.|
|||The experiment results show that by incorporating the additional density information, our crowd CNN model boosts the accuracy of crowd counting significantly.|
|||Mean absolute errors of density regression methods and our approach on the UCSD dataset  Method  Density + RF [7]  Density + MESA [12] Codebook + RR [2]  Our Crowd CNN Model  max 1.7 1.7 1.24 1.70  down 2.16 1.28 1.31 1.26  up 1.61 1.59 1.69 1.59  min 2.2 2.02 1.49 1.52  5.3.|
|||[8]  Our Crowd CNN Model  MAE MSE 697.8 655.7 487.1 493.4 590.3 468.0 467.0 498.5  We compared three methods on the UCF CC 50 dataset.|
||29 instances in total. (in cvpr2015)|
|41|Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning|The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity.|
|||While there are many previous efforts that try to reduce the CNN model size or the amount of computation, we find that they do not necessarily result in lower energy consumption.|
|||To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses the energy consumption of a CNN to guide the pruning process.|
|||However, CNN processing incurs high energy consumption due to its high computational complex ity [2].|
|||For example, the trend is to simultaneously reduce the overall CNN model size and/or simplify the computation while going deeper.|
|||However, neither the number of weights nor the number of operations in a CNN directly reflect its actual energy consumption.|
|||A CNN with a smaller model size or fewer operations can still have higher overall energy consumption.|
|||This is because the sources of energy consumption in a CNN consist of not only computation but also memory accesses.|
|||In fact, fetching data from the DRAM for an operation consumes orders of magnitude higher energy than the computation itself [20], and the energy consumption of a CNN is dominated by memory accesses for both filter weights and feature maps.|
|||The total number of memory accesses is a function of the CNN shape configuration [21] (i.e., filter size, feature map resolution, number of channels, and number of filters); different shape configurations can lead to different amounts of memory accesses, and thus energy consumption, even under the same number of weights or operations.|
|||In addition, there is currently no way for researchers to estimate the energy consumption of a CNN at design time.|
|||5687  The key to closing the gap between CNN design and energy efficiency optimization is to directly use energy, instead of the number of weights or operations, as a metric to guide the design.|
|||In order to obtain realistic estimate of energy consumption at design time of the CNN, we use the framework proposed in [21] that models the two sources of energy consumption in a CNN (computation and memory accesses), and use energy numbers extrapolated from actual hardware measurements [22].|
|||We further propose a new CNN pruning algorithm with the goal to minimize overall energy consumption with marginal accuracy degradation.|
|||With the ability to directly estimate the energy consumption of a CNN, the proposed pruning method identifies the parts of a CNN where pruning can maximally reduce the energy cost, and prunes the weights more aggressively than previously proposed methods to maximize the energy reduction.|
|||In summary, the key contributions of this work include:   Energy Estimation Methodology: Since the number of weights or operations does not necessarily serve as a good metric to guide the CNN design toward higher energy efficiency, we directly use the energy consumption of a CNN to guide its design.|
||| Energy Consumption Analysis of CNNs: We evaluate the energy versus accuracy trade-off of widely-used or pruned CNN models.|
|||Our key insights are that (1) maximally reducing weights or the number of MACs in a CNN does not necessarily result in optimized energy consumption, and feature maps need to be factored in, (2) convolutional (CONV) layers, instead of fully-connected (FC) layers, dominate the overall energy consumption in a CNN, (3) deeper CNNs with fewer weights, e.g., GoogLeNet and SqueezeNet, do not necessarily consume less energy than shallower CNNs with more weights, e.g., AlexNet, and (4) sparsifying the filters can provide equal or more energy reduction than reducing the bitwidth (even to binary) of weights.|
|||Data reuse in a CNN arises in many ways, and is determined by the shape configurations of different layers.|
|||As shown in Fig 1, for each CNN layer, the framework calculates the energy consumption by dividing it into two parts: computation energy consumption, Ecomp, and data movement energy consumption, Edata.|
|||Potential Impact  With this methodology, we can quantify the difference in energy costs between various popular CNN models and methods, such as increasing data sparsity or aggressive bitwidth reduction (discussed in Sec.|
|||More importantly, it provides a gateway for researchers to assess the energy consumption of CNNs at design time, which can be used as a feedback that leads to CNN designs with significantly reduced energy consumption.|
|||There is a large body of work that aims to reduce the CNN model size by pruning weights while maintaining accuracy.|
|||2, where the input is a CNN model and the output is a sparser CNN model with lower energy consumption.|
|||Accuracy versus energy trade-off of popular CNN models.|
|||Conclusion  This work presents an energy-aware pruning algorithm that directly uses the energy consumption of a CNN to guide the pruning process in order to optimize for the best energy-efficiency.|
|||The energy of a CNN is estimated by a methodology that models the computation and memory accesses of a CNN and uses energy numbers extrapolated from actual hardware measurements.|
|||With the estimated energy for each layer in a CNN model, the algorithm performs layer-by-layer pruning, starting from the layers with the highest energy consumption to the layers with the lowest energy consumption.|
|||[23] CNN Energy Estimation Website. http://eyeriss.|
||29 instances in total. (in cvpr2017)|
|42|Zhang_Instance-Level_Segmentation_for_CVPR_2016_paper|Figure 1: Our approach densely samples patches of different sizes from the image (row 1) and exploits a CNN to provide a soft instance labeling of each patch (row 2).|
|||Our MRF connects all pixel pairs inside the patches (yellow curves in row 2), as well as all pixel pairs from far away connected components obtained from patch-level CNN predictions (yellow curves in row 1), to provide a globally consistent instance labeling of the image (row 3).|
|||In our approach we use a CNN trained to directly predict instance labeling in a stride of local patches, and then solve for a consistent instance labeling using our densely connected MRF, thus not requiring to explicitly perform object detection.|
|||[19] trained a multi-output CNN that jointly predicts pixel-level class labeling of the image as well as bounding box locations and object instance numbers.|
|||We build on [32] which trains a CNN on local patches to obtain a depth-ordered pixel-wise instance labeling of each patch.|
|||We then train a CNN to output a pixellevel instance labeling inside each patch.|
|||The CNN gives us (probabilistic) pixel-level predictions of instances at the patch level.|
|||Note that the CNN predicts up to 5 instances as well as background.|
|||The corresponding Gibbs energy E(y) of our MRF consists of three main terms: a pairwise smoothness term, a pairwise local CNN prediction term and a pairwise inter-connected component term, each encoding different intuitions about the problem:  E(y) = Esmo(y) + Ecnn(y) + Eicc(y).|
|||We cannot use the CNN output as a unary potential, as the label space of the local patches and the global image  are different, i.e., only 6 labels (including background) possible locally, and instance 2 in a local patch might be totally different from instance 2 in another patch far away.|
|||In our problem we use the combination of position and the output of the CNN to form our feature space.|
|||Our CNN is trained to differentiate between object instances, so the probability vectors that the CNN outputs are a very strong cue of how likely two pixels belong to the same object.|
|||(3)  ksmo(cid:16)f (z)  i  , f (z)  j (cid:17) = exp(cid:18)  kpz,i  pz,jk2 2  22 1    kdi  djk2 2  22 2  (cid:19) ,  i  where f (z) contains both the position di and the output of the CNN pz,i.|
|||We define the shifted pairwise potential (z,t,i,j)  (yi, yj) as a product of its weight, a compatibility function and a Gaussian kernel defined over pairs of shifted local CNN predictions:  cnn  3.2.2 Local CNN Prediction: Ecnn(y)  (z,t,i,j)  cnn  (yi, yj) = w(s(z))  cnn (t)  cnn(yi, yj)k(t)  cnn(ht(pz,i), ht(pz,j)),  Any given patch contains only a subset of the instances present in the full image.|
|||For example, if the CNN predicts that in patch z pixel i belongs to instance 1 while pixel j belongs to instance 2, then any global configuration with yi 6= yj should be encouraged.|
|||To encode this patch-image compatibility in our energy, we define the local CNN prediction term as a sum of patchspecific compatibility terms, each defined over all pixel pairs in the patch:  Ecnn(y) =Xz Xi,j:i,jPz ,i<j  (z,i,j)  cnn  (yi, yj).|
|||This could be simply encoded with a compatibility potential that yi and yj are encouraged to have the same label if the output of the CNN pz,i and pz,j are similar, and their relative ordering (yi > yj or vice versa) is respected if the CNN predicts them to be of different instances.|
|||Gaussian filtering is however crucial, since our local CNN prediction term is fully connected at patch level.|
|||Towards this goal, for each pixel i, we compute the probability that it belongs to foreground, by summing the output of local CNN predictions and re-normalizing.|
|||Local CNN Prediction.|
|||We generate surrogate ground truth for our training images with [5] and train our CNN as in [32].|
|||By changing the CNN architecture from the naive adaptation of VGG-16 by [25] to DeepLab-LargeFOV (denoted by w/ DeepLab [6] in our results), we observe substantial performance gain for our approach but a slight drop for the baselines.|
|||We run the extracted patches through the CNN to  674  obtain local instance predictions.|
|||Note that [32] and our method use the exact same CNN unaries and patch extraction method, so we evaluate the two different MRFs on equal footing.|
|||The second baseline Unary additionally adds the CNN energy potential in [32].|
|||Notice that without post-processing the model LocCNNPred which has only the local CNN prediction term performs much worse than our baselines, because it allows instances that do not coexist in any patch to have the same labeling.|
|||5, which are largely due to the CNN output.|
|||Conclusions  proposed work [32] which trains a CNN on local patches to obtain soft instance labelings.|
|||Our MRF exploits local CNN predictions, long-range connections between far apart instances, and contrast-sensitive smoothness.|
||29 instances in total. (in cvpr2016)|
|43|Edo_Collins_Deep_Feature_Factorization_ECCV_2018_paper|Our method, Deep Feature Factorization (DFF), allows us to see how a deep CNN trained for image classification would answer this question.|
|||These correspondences reflect semantic similarity as indicated by clusters in a deep CNN layer feature space.|
|||In this way, we allow the CNN to show us which image regions it thinks are similar or related across a set of images as well as within a single image.|
|||We show that when using a deep CNN trained to perform ImageNet classification [30], applying DFF allows us to obtain heat maps that correspond to semantic concepts.|
|||Since we use a pre-trained CNN to accomplish this, we refer to our method as performing weakly-supervised co-segmentation.|
|||We report results on several datasets and CNN architectures, showing the usefulness of our method across a variety of settings.|
|||2 Related work  2.1 Localization with CNN Activations  Methods for the interpretation of hidden activations of deep neural networks, and in particular of CNNs, have recently gained significant interest [25].|
|||We extract features from a deep CNN and view them as a matrix.|
|||2.2 CNN Features as Part Detectors  The ability of DFF to localize parts stems from the CNNs ability to distinguish parts in the first place.|
|||[2] the authors attempt to detect learned part-detectors in CNN features, to see if such detectors emerge, even when the CNN is trained with object-level labels.|
|||The availability of ground truth is essential to their analysis, yielding a catalog of CNN units that sufficiently correspond to labels in the dataset.|
|||3 Method  3.1 CNN Feature Space  In the context of CNNs, an input image I is seen as a tensor of dimension hI  wI  cI, where the first two dimensions are the height and the width of the image, respectively, and the third dimension is the number of color channels, e.g., 3 for RGB.|
|||As the the image gets processed layer by layer, the hidden activation at the lth layer of the CNN is a tensor we denote Al I of dimension hl  wl  cl.|
|||Since a feature map represents multiple patches (depending on the size of image I), we view them as points inhabiting the same cl-dimensional space, which we refer to as the CNN feature space.|
|||3.3 Non-negative Matrix Factorization on CNN Activations  Many modern CNNs make use of the rectified linear activation function, max(x, 0), due to its desirable gradient properties.|
|||These heat maps have the same spatial dimensions as the CNN layer which produced the activations, often low.|
|||4.2 Segmentation and localization methods  In addition to gaining insights into CNN feature space, DFF has utility for various tasks with subtle but important differences in naming:   Segmentation vs. Localization is the difference between predicting pixel wise binary masks and predicting bounding boxes, respectively.|
|||Most of these methods operate by selecting the best of a set of object proposals, produced by a pre-trained CNN [24] or an object-saliency heuristic [5, 19].|
|||The authors of [21] present a method for unsupervised object co-localization that, like ours, also makes use of CNN activations.|
|||Their approach is to apply k-means clustering to globally max-pooled activations, with the intent of clustering all highly active CNN filters together.|
|||Due to the low resolution of deep CNN activations, and hence of the heat map, we get blobs that do not perfectly align with the underlying region of interest.|
|||This behavior reveals a hierarchical structure in the clusters formed in CNN feature space.|
|||This observation also indicates the CNN has learned representation that explains these concepts with invariance to pose, e.g., leg positions in the 2nd, 3rd, and 4th columns.|
|||Our results show that clusters in CNN feature space correspond to coherent parts.|
|||More so, they indicate the presence of a cluster hierarchy in CNN feature space, where part-clusters can be seen as sub-clusters within object-clusters (See Figures 1, 2 and 3 for visual comparison.|
|||for (a) different VGG19 layers and (b) the deepest convolutional layer for other CNN architectures.|
|||Expectedly, different convolutional blocks show a clear difference in matching up with semantic parts, as CNN features capture more semantic concepts.|
|||We have shown that DFF can reveal interesting structures in CNN feature space, such as hierarchical clusters which correspond to a part-based decomposition at various levels of granularity.|
||28 instances in total. (in eccv2018)|
|44|Lin_Bilinear_CNN_Models_ICCV_2015_paper|Bilinear CNN Models for Fine-grained Visual Recognition  Tsung-Yu Lin  Aruni RoyChowdhury  Subhransu Maji  University of Massachusetts, Amherst  {tsungyulin,arunirc,smaji}@cs.umass.edu  Abstract  We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor.|
|||A bilinear CNN model for image classification.|
|||We present experiments demonstrating the effect of fine-tuning on CNN based Fisher vector models [7], the computational and accuracy tradeoffs of various bilinear CNN architectures, and ways to break the symmetry in the bilinear models using low-dimensional projections.|
|||The first is the hypercolumns of [17] that jointly considers the activations from all the convolutional layers of a CNN allowing finer grained resolution for localization tasks.|
|||Bilinear CNN models  A natural candidate for the feature function f is a CNN consisting of a hierarchy of convolutional and pooling layers.|
|||Another advantage of using only the convolutional layers, is the resulting CNN can process images of an arbitrary size in a single forward-propagation step and produce outputs indexed by the location in the image and feature channel.|
|||Computing gradients in the bilinear CNN model.|
|||Even when CNN features are  1451  pooled using FV method, training is usually not done endto-end since it is cumbersome to compute the gradients of the network since fA and fB both depend on the x.|
|||For Fisher vector CNN models we show that even when fine-tuning is done indirectly, i.e., using a different pooling method, the overall performance improves.|
|||I. CNN with fully-connected layers (FC-CNN) This is based on the features extracted from the last fully-connected layer before the softmax layer of the CNN.|
|||Fisher vector with CNN features (FV-CNN) This denotes the method of [7] that builds a descriptor using FV pooling of CNN filter bank responses with 64 GMM components.|
|||Bilinear CNN model (B-CNN) We consider several bilinear CNN models  (i) initialized with two M-nets denoted by B-CNN [M,M], (ii) initialized with a D-Net and an M-Net denoted by B-CNN [D,M], and (iii) initialized with two D-nets denoted by B-CNN [D,D].|
|||A more up-to-date set of results can be found in [21] who recently reported excellent performance using on this dataset leveraging more accurate CNN models with a method to train part detectors in a weakly supervised manner.|
|||This shows that domain specific finetuning can be useful even when early convolutional layers of a CNN are used as features.|
|||The bilinear CNN models are substantially more accurate than the corresponding FC and FV models.|
|||Comparison to previous work Two methods that perform well on this dataset when bounding-boxes are not available at test time are 73.9% of the part-based RCNN [38] and 75.7% of the pose-normalized CNN [2].|
|||We note that the accuracy of these methods can be improved by replacing the underlying AlexNet CNN [23] with the more accurate but significantly slower D-Net.|
|||Another recent approach called the spatial transformer networks reports 84.1% accuracy [19] using the Inception CNN architecture with batch normalization [18].|
|||FV-SIFT is the Fisher vector representation with SIFT features, FC-CNN uses features from the last fully connected layer of a CNN, and FV-CNN uses FV pooling of CNN filter banks [7].|
|||Once again the bilinear CNN models outperform all the other baselines with the B-CNN [D, M] model achieving 91.3% accuracy.|
|||Low dimensional bilinear CNN models  The bilinear CNN models that are symmetrically initialized will remain symmetric after fine-tuning since the gradients for the two networks are identical.|
|||Although this is good for efficiency since the model can be implemented with just a single CNN evaluation, this may be suboptimal since the model doesnt explore the space of solutions that can arise from different CNNs.|
|||The first is dropout [23] where during training a random subset of outputs in each layer are set to zero which will cause gradients of the CNN to differ.|
|||Our second idea is to project one of the CNN outputs to a lower dimension breaking the symmetry.|
|||This can be implemented by adding another layer of the CNN with a convolutional filter of size 11ND where N is the number of channels in the output of the CNN and D is the projected dimension.|
|||Our bilinear CNN models had two feature extractors whose processing pathways separated early, but some of the early processing in the CNNs may be shared.|
|||Conclusion  We presented bilinear CNN models and demonstrated their effectiveness on various fine-grained recognition datasets.|
|||The model is also efficient requiring only two CNN evaluations on a 448448 image.|
||28 instances in total. (in iccv2015)|
|45|Wang_Towards_Unified_Depth_2015_CVPR_paper|By allowing for interactions between the depth and semantic information, the joint network provides more accurate depth prediction than a state-of-the-art CNN trained solely for depth prediction [6].|
|||By inference using our joint global CNN, the depth prediction improves over the depth only CNN by an average 8% relative  1  Figure 1.|
|||1, given an image, we obtain regionwise and pixel-wise potential from a regional and a global CNN respectively.|
|||Recently, CNN have shown its effectiveness in both tasks.|
|||In [6], a two-level CNN is learned to directly predict the  depth maps, which significantly outperforms the previous state-of-the-arts.|
|||Inspired by these work, we also use CNN to train our model for joint global and local prediction.|
|||As illustrated in Fig.1, the pixel-level potential i(xi) is provided by a CNN trained globally on  the whole image, which jointly predicts pixel-wise depth values and probabilities of semantic labels.|
|||The details of the global CNN training and prediction will be introduced in Section 3.|
|||Joint Global Depth and Semantic Prediction In this section, we describe how we train a CNN with the whole image as input to predict pixel-wise depth and semantic maps, which are used as the pixel-level unary potentials in our HCRF model.|
|||Inspired by this work, we extend it to a CNN that directly predicts pixel-wise depth values jointly with semantic labels from the whole image.|
|||We follow the CNN structure in [6] in the earlier layers.|
|||Moreover, as will be shown in the experiments, the joint-prediction network after fine-tuning provides more accurate depth maps than the network trained to predict depth alone, which demonstrates that semantic information can regularize the CNN that benefit depth prediction.|
|||Joint Local Depth and Semantic Prediction While the depth and semantic maps predicted by the global CNN accurately capture the scene layout, they still lack details in local regions.|
|||CNN is learned through fine-tuning the global CNN in Section 3.|
|||First, the output of the global CNN in Sec.|
|||Therefore, for a segment, we take the f c6 layer output of the local CNN both from its bounding box and masked region, and concatenate it with the global prediction within the corresponding bounding box to form our feature vector.|
|||We use CNN as the local training model as well, which takes the warped bounding box of the segments as input, with loss function defined as the sum of sigmoid cross entropy loss over the affinities, i.e.|
|||Make3D [30], Depth Transfer [17], DC Depth [24], Canonical Depth [21] and Depth CNN [6].|
|||Joint HCRF, outperforms the state-of-the art Depth CNN [6] with a noticeable margin.|
|||The results of our Global Depth CNN are comparable to the one produced by [6].|
|||By fine-tuning the network to jointly predict depth and semantic labels, the joint global CNN is better than the depth-only CNN in 7 out of 8 metrics.|
|||It shows that the semantic labels regularized the depth prediction through the CNN training, which benefits the depth estimation.|
|||We also tried to use a global jointly trained CNN to directly predict the semantic labels.|
|||However, such a global prediction  3https://github.com/rbgirshick/rcnn  Image  DC Depth [24] D. CNN [6]  Ours depth  Depth GT  Ours semantic  Semantic GT  Figure 6.|
|||Criteria Make 3D [30] Depth Transfer [17] DC Depth [24] Canonical Depth [21] Depth CNN Coarse [6] Depth CNN Fine [6] Global CNN Depth only Global CNN Joint Joint HCRF  Lower is better Rel 0.349 0.350 0.335  Rel(sqr) Log10 0.492 0.539 0.442   0.134 0.127   0.228 0.215 0.207 0.226 0.220   0.223 0.212 0.216 0.208 0.210   0.104 0.095 0.094  RMSE(linear) RMSE(log)  Higher is better  < 1.25   < 1.252   < 1.253  1.214 1.1 1.06   0.871 0.907 0.823 0.750 0.745  0.409 0.378 0.362   0.283 0.285 0.284 0.266 0.262  0.447 0.460 0.475 0.542 0.618 0.611 0.550 0.593 0.605  0.745 0.742 0.770 0.829 0.891 0.887 0.861 0.889 0.890  0.897 0.893 0.911 0.940 0.969 0.971 0.969 0.976 0.970  Table 1.|
|||The segmentation from the joint global CNN is very blurry, while HCRF provides much clearer boundaries.|
|||8(a), for global prediction, by adding the semantic constraint, the distortion of depth CNN prediction is fixed because of the smoothness constraint enforced by the vertical label.|
|||We formulate the problem in a hierarchical CRF which embeds the potential from a global CNN and a local regional CNN.|
||28 instances in total. (in cvpr2015)|
|46|Shankar_Refining_Architectures_of_CVPR_2016_paper|However, a question of paramount importance is somewhat unanswered in deep learning research is the selected CNN optimal for the dataset in terms of accuracy and model size?|
|||In this paper, we intend to answer this question and introduce a novel strategy that alters the architecture of a given CNN for a specified dataset, to potentially enhance the original accuracy while possibly reducing the model size.|
|||Stretching increases the number of hidden units (nodes) in a given CNN layer, while a symmetrical split of say K between two layers separates the input and output channels into K equal groups, and connects only the corresponding input-output channel groups.|
|||Our procedure starts with a pre-trained CNN for a given dataset, and optimally decides the stretch and split factors across the network to refine the architecture.|
|||Out of the many CNN architectures, AlexNet [11], GoogleNet [22] and VGG [21] can be considered as the most popu lar ones, based on their impressive performance across a variety of datasets.|
|||He would typically try out famous CNN architectures like AlexNet, GoogleNet, VGG-11, VGG-16, VGG-19 and then select the one which gives maximum accuracy.|
|||We now formally define our problem statement as follows :  Given a pre-trained CNN for a specific dataset, refine the architecture in order to potentially increase the accuracy while possibly reducing the model size.|
|||Operations for CNN architecture refinement: One may now ask what is exactly meant by the refinement of a CNN architecture.|
|||In this paper, we consider the task of CNN architecture refinement on a broader level.|
|||Once the stretch and split factors are estimated, they are applied to original CNN for architectural refinement.|
|||We introduce a strategy that starts with a pre-trained CNN, and uses stretch and symmetric split (Fig 1) operations for CNN architecture refinement.|
|||However, learning an optimal CNN architecture for a given dataset is largely an open problem.|
|||Moreover, it is less known, how to find if the selected CNN is optimal for the dataset in terms of accuracy and model size or not.|
|||During transfer learning, the parameters of the CNN trained with base dataset are duplicated, and some additional layers are attached at the deep end of the CNN which are trained exclusively on the new dataset.|
|||However, none of the transfer learning techniques attempts to refine the CNN architecture ef 2213  Filter Concatenation  3x3 Conv  5x5 Conv  1x1 Conv  1x1 Conv  1x1 Conv  1x1 Conv  MAX POOL  Previous Layer  Layer 2  Layer 1  Figure 2.|
|||Low-rank and sparsification methods for CNNs:  Irrespective of whether a CNN has been hand-designed for a specific dataset or not, all the famous CNN architectures exhibit enormous redundancy in parameter space [12].|
|||Researchers have recently exploited this fact to speed up the inference speeds by estimating a highly reduced set of parameters which is sufficient to produce nearly the same accuracy as the original CNN does with the full set of parameters.|
|||Our approach instead modifies the architecture of the original CNN to potentially enhance the accuracy while possibly reducing the model size.|
|||Let the CNN architecture have L convolutional layers.|
|||l  l  Finding the inter-class separation: For a given dataset and a base CNN architecture, we first train the CNN on the dataset using a given loss function (such as softmax loss, sigmoid cross entropy loss, etc.|
|||The above explanation is made under the widely accepted notion that a CNN learns low-level type features (edges, color patterns, etc.)|
|||However, we thought this to be very intuitive, since objects are generally encoded in deeper layers, and thus, one would expect to reduce pa Architectures for Refinement: We choose GoogleNet [22] and VGG-11 [21] as the base CNN architectures, which we intend to alter using our approach.|
|||the CNN architecture is refined by only stretching some layers, but no symmetric splitting between layers is done.|
|||(c) L1 Sparsification We consider the L1 sparsification of a CNN as one of the important baselines.|
|||Here, the weights (parameters) of a CNN are regularized with an L1 norm, and the regularization term is added to the loss function.|
|||a CNN with a reduced model size.|
|||Training: For all datasets and CNN architectures, the networks are trained using the Caffe library [9].|
|||Conclusion  We have introduced a novel strategy that alters the architecture of a given CNN for a specified dataset for effecting a possible increase in original accuracy and reduction of parameters.|
||28 instances in total. (in cvpr2016)|
|47|Li_Visual_Saliency_Based_2015_CVPR_paper|Specifically, recently popular convolutional neural networks (CNN) are particularly well suited for this task because convolutional layers in a CNN resemble simple and complex cells in the human visual system [14] while fully connected layers in a CNN resemble higher-level inference and decision making in the human cognitive system.|
|||Inspired by this, we perform feature extraction using a CNN originally trained over the ImageNet dataset [10].|
|||In the rest of this paper, we call such features CNN features.|
|||Therefore, we extract multiscale CNN features for every image region from three nested and increasingly larger rectangular windows, which respectively encloses the considered region, its immediate neighboring regions, and the entire image.|
|||On top of the multiscale CNN features, our method further trains fully connected neural network layers.|
|||Concatenated multiscale CNN features are fed into these layers trained using a collection of labeled saliency maps.|
|||Thus, these fully connected layers play the role of a regressor that is capable of inferring the saliency score of every image region from the multiscale CNN features extracted from nested windows surrounding the image region.|
|||In summary, this paper has the following contributions:  A new visual saliency model is proposed to incorporate multiscale CNN features extracted from nested windows with a deep neural network with multiple fully connected layers.|
|||Note that object location cues and boundary-based background modeling are not neglected in our framework, but have been implicitly incorporated into our model through multiscale CNN feature extraction and neural network training.|
|||[11] pointed out that features extracted from Krizhevskys CNN trained on the ImageNet dataset [10] can be repurposed to generic tasks.|
|||Nevertheless, CNN features have not yet been explored in visual saliency research primarily because saliency cannot be solved using the same framework considered in [11, 30].|
|||This paper proposes a simple but very effective neural network architecture to make deep CNN features applicable to saliency modeling and salient object detection.|
|||Since an image region may have an irregular shape while CNN features have to be extracted from a rectangular region, to make the CNN features only relevant to the pixels inside the region, as in [15], we define the rectangular region for CNN feature extraction to be the bounding box of the image region and fill the pixels outside the region but still inside its bounding box with the mean pixel values at the same locations across all ImageNet training images.|
|||We warp the region in the bounding box to a square with 227x227 pixels to make it compatible with the deep CNN trained for ImageNet.|
|||The warped RGB image region is then fed to the deep CNN and a 4096-dimensional feature vector is obtained by forward propagating a mean-subtracted input image region through all the convolutional layers and fully connected layers.|
|||Again, this rectangular neighborhood is fed to the deep CNN after being warped.|
|||We call the resulting vector from the CNN feature B.|
|||To meet these demands, we use the deep CNN to extract feature C from the entire rectangular image, where the considered region is masked with mean pixel values for indicating the position of the region.|
|||Since our final feature vector is the concatenation of three CNN feature vectors, we call it S-3CNN.|
|||Neural Network Training  On top of the multiscale CNN features, we train a neural network with one output layer and two fully connected hidden layers.|
|||This network plays the role of a regressor that infers the saliency score of every image region from the multiscale CNN features extracted for the image region.|
|||Concatenated multiscale CNN features are fed into this  Figure 1: The architecture of our deep feature based visual saliency model.|
|||Multiscale Feature Extraction  We extract multiscale features for each image region with a deep convolutional neural network originally trained over the ImageNet dataset [10] using Caffe [21], an open source framework for CNN training and testing.|
|||The architecture of this CNN has eight layers including five convolutional layers and three fully-connected layers.|
|||Although this CNN was originally trained on a dataset for visual recognition, automatically extracted CNN features turn out to be highly versatile and can be more effective than traditional hand Conv_2Conv_5S-3CNNNN_Layer1Conv_2Conv_1Conv_5FC_6FC_7Conv_1FC_6               FC_7NN_Layer2  Conv_2Conv_1Conv_5FC_6            FC_7Output.|
|||Furthermore, the more challenging the dataset, the more obvious the advantages because our multiscale CNN features are capable of characterizing the contrast relationship among different parts of an image.|
|||Component-wise Efficacy Effectiveness of S-3CNN As discussed in Section 2.1, our multiscale CNN feature vector, S-3CNN, consists of three components, A, B and C. To show the effectiveness and necessity of these three parts, we have trained five additional models for comparison, which respectively take f eature A only, f eature B only, f eature C only, concatenated A and B, and concatenated A and C. These five models were trained on MSRA-B using the same setting as the one taking S-3CNN.|
|||These results demonstrate that the three components of our multiscale CNN feature vector are complementary to each other, and the training stage of our saliency model is capable of discovering and understanding region contrast information hidden in our multiscale features.|
||28 instances in total. (in cvpr2015)|
|48|Su_Multi-View_Convolutional_Neural_ICCV_2015_paper|We first present a standard CNN architecture trained to recognize the shapes rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors.|
|||We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.|
|||Another advantage of using 2D representations is that we can leverage (i) advances in image descriptors [22, 26] and (ii) massive image databases (such as ImageNet [9]) to pre-train our CNN architectures.|
|||toilet     3D shape model   rendered with  di(cid:31)erent virtual cameras  2D rendered  images  our multi-view CNN architecture  output class  predictions  Figure 1.|
|||Multi-view CNN for 3D shape recognition (illustrated using the 1st camera setup).|
|||3.2), we present new ideas for how to compile the information in multiple 2D views of an object into a compact object descriptor using a new architecture called multi-view CNN (Fig.|
|||Our multi-view CNN is related to jittering where transformed copies of the data are added during training to learn invariances to transformations such as rotation or translation.|
|||The multi-view CNN learns to combine the views instead of averaging, and thus can use the more informative views of the object for prediction while ignoring others.|
|||Even on traditional image classification tasks multi-view CNN can be a better alternative to jittering.|
|||For example, on the sketch recognition benchmark [11] a multi-view CNN trained on jittered copies performs better than a standard CNN trained with the same jittered copies (Sect.|
|||Pre-trained CNN models, data, and the complete source code to reproduce the results in the paper are available at http://vis-www.cs.umass.edu/mvcnn.|
|||In contrast our multi-view CNN architecture learns to recognize 3D shapes from views of the shapes using image-based CNNs but in the context of other views via a view-pooling layer.|
|||Our approach is to learn to combine information from multiple views using a unified CNN architecture that includes a view-pooling layer (Fig.|
|||All the parameters of our CNN architecture are learned discriminatively to produce a single compact descriptor for the 3D shape.|
|||We consider two types of image descriptors for each 2D view: a state-of-the-art hand-crafted image descriptor based on Fisher vectors [29] with multiscale SIFT, as well as CNN activation features [10].|
|||For our CNN features we use the VGG-M network  from [3] which consists of mainly five convolutional layers conv1,...,5 followed by three fully connected layers fc6,...,8 and a softmax classification layer.|
|||Both Fisher vectors and CNN features yield very good performance in classification and retrieval compared with popular 3D shape descriptors (e.g., SPH [16], LFD [5]) as well as 3D ShapeNets [37].|
|||The multi-view CNN (MVCNN) architecture outperforms the view-based methods, especially for retrieval.|
|||We design the multi-view CNN (MVCNN) on top of image-based CNNs (Fig.|
|||Using fc7 (after ReLU non-linearity) in an MVCNN as an aggregated shape descriptor, we achieve higher performance than using separate image descriptors from an image-based CNN directly, especially in retrieval (62.8%  70.1%).|
|||Using our CNN baseline trained on ImageNet in turn outperforms Fisher vectors by a significant margin.|
|||Finetuning the CNN on the rendered views of the training shapes of ModelNet40 further improves the performance.|
|||Our MVCNN outperforms all state-of-the-art descriptors as well as the Fisher vector and CNN baselines.|
|||With an off-the-shelf CNN (VGG-M from [3]), we are able to get 77.3% classification accuracy without any network fine-tuning.|
|||We apply the two CNN variants (regular CNN and MVCNN) discussed earlier for aggregating multiple views of 3D shapes, and get 85.5% (CNN w/o view-pooling) and 86.3% (MVCNN w/ viewpooling on fc7) classification accuracy respectively.|
|||Accuracy  query  top 10 retrieved 3D shapes  (1) FV [30]  (2) CNN M (3) CNN M, fine-tuned (4) CNN M, fine-tuned (5) MVCNN M, fine-tuned  (6) CNN VD (7) CNN VD, fine-tuned (8) CNN VD, fine-tuned (9) MVCNN VD, fine-tuned  (10) Human performance   6 6  6 6  n/a  79.0%  77.3% 84.0% 85.5% 86.3%  69.3% 86.3% 86.0% 87.2%  93.0%  Table 2.|
|||MVCNNs are better than CNN trained with data jittering.|
|||The results are shown with two different CNN architectures  VGG-M (row 2-5) and VGG-VD (row 6-9).|
||28 instances in total. (in iccv2015)|
|49|Li_A_Convolutional_Neural_2015_CVPR_paper|The proposed CNN cascade operates at multiple resolutions, quickly rejects the background regions in the fast low resolution stages, and carefully evaluates a small number of challenging candidates in the last high resolution stage.|
|||Compared with the previous hand-crafted features, CNN can automatically learn features to capture complex visual variations by leveraging a large amount of training data and its testing phase can be easily parallelized on GPU cores for acceleration.|
|||Considering the relatively high computational expense of the CNNs, exhaustively scanning the full image in multiple scales with a deep CNN is not a practical solution.|
|||To achieve fast face detection, we present a CNN cascade,  1  which rejects false detections quickly in the early, lowresolution stages and carefully verify the detections in the later, high-resolution stages.|
|||In this work, our contributions are four-fold:  we propose a CNN cascade for fast face detection;  we introduce a CNN-based face bounding box calibration step in the cascade to help accelerate the CNN cascade and obtain high quality localization;  we present a multi-resolution CNN architecture that can be more discriminative than the single resolution CNN with only a fractional overhead;  we further improve the state-of-the-art performance on the Face Detection Data Set and Benchmark (FDDB) [7].|
|||One of the recent CNN based detection method is the R-CNN by Girshick et al.|
|||It generates categoryindependent region proposals and extracts CNN features from the regions.|
|||Hence we benefit from the powerful features learned by the CNN to better differentiate faces from highly cluttered backgrounds.|
|||In practice, the CNN cascade can have varied settings for accuracycomputation trade off.|
|||3.2.2  12-calibration-net  12-calibration-net refers to the CNN after 12-net for bounding box calibration.|
|||3.2.1  12-net  12-net refers to the first CNN in the test pipeline.|
|||The structure of this CNN is shown in Figure 2.|
|||12-net is a very shallow binary classification CNN to quickly scan the testing image.|
|||Since the calibration patterns are not orthogonal to each other, we take the average results of the patterns of high confidence score as the adjustment  Test imageAfter 12-netOutput detectionsAfter 12-calibration-netAfter 24-netNMSAfter 24-calibration-netAfter 48-netBefore 48-calibration-netGlobal NMSNMSNMSNMSFigure 2: CNN structures of the 12-net, 24-net and 48-net.|
|||24-net  3.2.3 24-net is an intermediate binary classification CNN to further reduce the number of detection windows.|
|||The CNN structure is shown in Figure 2.|
|||The overall CNN becomes more discriminative and the overhead from the 12-net sub-structure is only a fraction of the overall computation.|
|||At this stage of the cascade, it is feasible to apply a more powerful but  3 channels 12x1216 3x3 filters stride 1ConvolutionallayerMax-poolinglayer3x3 kernel  stride 2Fully-connectedlayer16 outputsInput ImageLabels2 classesface / non-face3 channels 24x2464 5x5 filters stride 1ConvolutionallayerMax-poolinglayer3x3 kernel  stride 2Fully-connectedlayer128 outputsInput ImageLabels2 classesface / non-faceresize12-net24-netInput Image3 channels 48x4864 5x5 filters stride 13x3 kernel  stride 2Max-poolinglayer Convolutionallayer9x9 regionNormalizationlayer Convolutionallayer64 5x5 filters stride 13x3 kernel  stride 2Max-poolinglayer 9x9 regionNormalizationlayer Fully-connectedlayer256 outputs24-netresize2 classesface / non-faceLabels48-net12-netFully-connected layerFully-connected layer 0.75 0.8 0.85 0.9 0.95 1 0 5000 10000 15000Detection RateNumber of False Detectionswith multi-resolutionw/o multi-resolutionFigure 4: CNN structures of the 12-calibration-net, 24-calibration-net and 48-calibration-net.|
|||As a result, without the calibration step, the next CNN in the cascade will have to evaluate more regions to maintain a good recall.|
|||Instead of training a CNN for bounding boxes regression as in R-CNN, we train a multi-class classification CNN for calibration.|
|||We observe that a multi-class calibration CNN can be easily trained from limited amount of training data while a regression CNN for calibration requires more training data.|
|||We believe that the discretization decreases the difficulty of the calibration problem so that we can achieve good calibration accuracy with simpler CNN structures.|
|||At each stage in the cascade, the CNN is trained to address a sub-problem which is easier than addressing the face vs. non-face classification globally.|
|||Compared with the design to have one single CNN to scan the full image for faces, the cascade makes it possible to have simpler CNNs achieve the same or even better accuracy.|
|||2We use the cuda-convnet2 [1] CNN implementation.|
|||In this work, the CNN cascade can achieve very fast face detection.|
|||Conclusion  In this work, we present a CNN cascade for fast face detection.|
||27 instances in total. (in cvpr2015)|
|50|Shin_Learning_to_Read_CVPR_2016_paper|Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features.|
|||Most recent work also adapt recurrent neural networks (RNNs), using the rich deep CNN features to generate image captions.|
|||In order to circumvent the normal-vs-diseased cases bias, we adopt various regularization techniques in CNN training.|
|||In analogy to the previous works using ImageNet-trained CNN features for image encoding and RNNs to generate image captions, we first train CNN models with one disease label per chest x-ray inferred from image annotations, e.g., calcified granuloma, or cardiomegaly.|
|||Then we re-train CNNs with the obtained joint image/text contexts and generate annotations based on the new CNN features.|
|||With this recurrent cascade model, image/text contexts are taken into account for CNN training (images with calcified granuloma in right upper lobe and small calcified granuloma in left lung base will be assigned different labels), to ultimately generate better and more accurate image annotations.|
|||Nonetheless, it is impossible to assign a single image label based on MeSH and train a CNN to reproduce them, because MeSH terms seldom appear individually when describing an image.|
|||We rescale all CNN input training and testing images to a size of 256  256.|
|||Disease Label Mining  The CNN-RNN based image caption generation approaches [44, 54, 36, 14, 61, 15, 6, 62, 31] require a welltrained CNN to encode input images effectively.|
|||We then test whether our data can benefit from a more complex stateof-the-art CNN model, i.e.|
|||Regularization by Batch Normalization and  Data Dropout  Even when we balance the dataset by augmenting many diseased samples, it is difficult for a CNN to learn a good model to distinguish many diseased cases from normal cases which have many variations on their original samples.|
|||It was shown in [27] that normalizing via mini-batch statistics during training can serve as an effective regularization technique to improve the performance of a CNN model.|
|||Effect of Model Complexity  We also validate whether the dataset can benefit from a more complex GoogLeNet [58], which is arguably the current state-of-the-art CNN architecture.|
|||Annotation Generation with RNN  We use recurrent neural networks (RNNs) to learn the annotation sequence given input image CNN embeddings.|
|||We set the initial state of RNNs as the CNN image embedding (CNN(I)), and the first annotation word as the initial input.|
|||The output of the RNNs are the following annotation word sequences, and we train RNNs by minimizing the negative log likelihood of output sequences and true sequences:  L(I, S) =   NX  t=1  {PRNN(yt = st)|CNN(I)},  (11)  where yt is the output word of RNN in time step t, st the correct word, CNN(I) the CNN embedding of input image I, and N the number of words in the annotation (N = 5 with the end-of-sequence zero-padding).|
|||Equation 11 is not a true conditional probability (because we only initialize the  In sampling we again initialize the RNN state vectors with the CNN image embedding (ht=1=CNN(I)).|
|||We then use the CNN prediction of the input image as the first word as the input to the RNN, to sample following sequences up to five words.|
|||We use GoogLeNet as our default CNN model since it performs better than the NIN model in Sec.|
|||RNNs state vector (h) is initialized with the CNN image embedding (CNN(I)), and its unrolled over the annotation sequences with the words as input.|
|||Recurrent Cascade Model for Image Label ing with Joint Image/Text Context  In Section 5, our CNN models are trained with disease labels only where the context of diseases are not considered.|
|||Meanwhile, the RNNs trained in Section 6 encode the text annotation sequences given the CNN embedding of the image the annotation is describing.|
|||We therefore use the already trained CNN and RNN to infer better image labels, integrating the contexts of the image annotations beyond just the name of the disease.|
|||Note, that the state vector of RNN is initialized with the CNN image embeddings (CNN(I)), and the RNN is unrolled over the annotation sequence, taking each word of the annotation as input.|
|||age labels at 0th iteration) of CNN and RNN training.|
|||In the second CNN training round (1st iteration), we fine-tune from the previous CNNiter=0, by replacing the last classification layer with the new set of labels (17  57) and training it with a lower learning rate (0.1), except for the classification layer.|
|||We train the CNN once more with the additional labels (57, compared to 17 in Section 5), train the RNN with the new CNN image embedding, and finally generate image annotations.|
||27 instances in total. (in cvpr2016)|
|51|Qi_Hedged_Deep_Tracking_CVPR_2016_paper|However, as features from a certain CNN layer characterize an object of interest from only one aspect or one level, the performance of such trackers trained with features from one layer (usually the second to last layer) can be further improved.|
|||In this paper, we propose a novel CNN based tracking framework, which takes full advantage of features from different CNN layers and uses an adaptive Hedge method to hedge several CNN based trackers into a single stronger one.|
|||Empirical studies using a large object tracking benchmark show that the performance of CNN based trackers surpasses  Figure 1.|
|||Tracking results of using CNN features from different convolutional layers on a representative frame of four sequences with diverse challenges.|
|||In this paper, we propose a novel CNN based tracking algorithm, which first builds weak trackers from convolutional layers by applying correlation filters on the layer output, and then hedges all weak trackers into a single stronger one using an online decision-theoretical Hedge algorithm.|
|||The tracking result in the current frame is the weighted decisions of all experts, which combines advantages of all the considered CNN layers.|
|||The contributions of this paper are summarized below:  We propose a novel tracking algorithm that combines weak CNN based trackers from various convolutional layers into a single stronger tracker.|
|||Numerous methods have since been proposed to exploit CNN features [9, 30, 18] for visual tracking.|
|||Algorithmic Overview  As shown in Figure 2, the proposed approach consists of three steps: extracting CNN features, constructing weak trackers, and hedging weak trackers.|
|||The proposed algorithm consists of three components: 1) extracting CNN features from different convolutional layers using the pre-trained VGG-Net (Section 4.1); 2) constructing weak trackers using correlation filters where each one is trained with CNN features from one layer (Section 4.2); 3) hedging weak trackers into a stronger one using an improved Hedge algorithm (Section 4.3).|
|||All weak trackers are finally hedged into a stronger one using the proposed adaptive Hedge algorithm for visual tracking, which exploits the strength of all CNN layers.|
|||However, tracking methods using CNN features from any layer alone are less effective (see Figure 1 for example of tracking failures).|
|||Weak CNN based trackers  In this section, we first present the technical details of the proposed algorithm and then describe the online update scheme.|
|||Deep CNN features  CNN models, such as AlexNet [21], R-CNN [11], CaffeNet [19], and VGG-Net [26], have been developed for large-scale image classification and object recognition tasks.|
|||The deep VGG-Net facilitates features extracted  In this work, a module that makes use of correlation filters on CNN features extracted from one layer is used to build a weak tracker.|
|||Hedging CNN based trackers  The standard parameter-free Hedge algorithm [5] is proposed to tackle decision-theoretic online learning problems in a multi-expert multi-round setting.|
|||In the visual tracking scenario, it is natural to treat each CNN based tracker as an expert and then predict the target position in the t-th frame by  t ) = XK  wk  t , y  (x t ), is the weight of expert k and PK  where wk t = 1. t Once the ultimate target position is predicted, each expert will incur a loss.|
|||4.00 GHz CPU, 16GB RAM, and a GeForce GTX780Ti GPU card which is only used to compute the CNN features.|
|||We illustrating how the weights are assigned to the CNN based trackers by the proposed adaptive and the standard parameter-free Hedge methods.|
|||In particular, HDT outperforms other methods by a huge margin in handling low resolution, which can be attributed to CNN features with rich spatial details from first layers and features with semantics from last layers.|
|||But even the best component CNN based tracker VGG-16 still does not perform as good as CNN-SVM.|
|||When combining these six component CNN based trackers using the standard Hedge, the tracking performance is below the best performed component tracker.|
|||Conclusion  In this paper, we propose a novel CNN based tracking framework which uses an adaptive online decision learning algorithm to hedge weak trackers, obtained by correlation filters on CNN feature maps, into a stronger one to achieve better results.|
|||To the best of our knowledge, the proposed algorithm is the first to adaptively hedge features from different CNN layers in an online manner for visual tracking.|
|||Comparison among our HDT and several baselines: all its constituent CNN based trackers and the one combined by standard parameter-free Hedge.|
|||Since our HDT hedges several weak CNN based trackers that perform well in different environments, it can overcome these challenges much better than other trackers.|
|||Acknowledgments  To evaluate the effectiveness of the proposed adaptive Hedge method, we compare the HDT against its component CNN based trackers denoted by VGG-10, VGG11, VGG-12, VGG-14, VGG-15, and VGG-16, as well as the hedged CNN based tracker using the standard parameter-free Hedge [5], denoted by HDT-SH, on the benchmark [33].|
||27 instances in total. (in cvpr2016)|
|52|cvpr18-RoadTracer  Automatic Extraction of Road Networks From Aerial Images|We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct.|
|||The fundamental problem with a segmentation-based approach is that the CNN is trained only to provide local information about the presence of roads.|
|||However, it is not obvious how to train a CNN to learn to produce a graph from images.|
|||Our approach constructs the road network by adding individual road segments one at a time, using a novel CNN architecture to decide on the next segment to add given as input the portion of the network constructed so far.|
|||Training the CNN decision function is challenging because the input to the CNN at each step of the search depends on the partial road network generated using the CNN up to that step.|
|||Instead, we develop a dynamic labeling approach to produce training examples on the fly as the CNN evolves during training.|
|||In Section 4, we discuss the procedure used to train the CNN used in our solution.|
|||Because the CNN is trained with a loss function evaluated independently on each pixel, it will yield a noisy output in regions where it is unsure about the presence of a road.|
|||Including G in the input to the CNN is a noteworthy aspect of our method.|
|||First, this allows the CNN to understand which roads in the aerial imagery have been explored earlier in the search, in effect moving the problem of excluding these roads from post-processing to the CNN.|
|||Second, it provides the CNN with useful context; e.g., when encountering a portion of aerial imagery occluded by a tall building, the CNN can use the presence or absence of edges on either side of the building to help determine whether the building occludes a road.|
|||We noted earlier that our solution does not require complex post-processing heuristics, unlike segmentation-based methods where CNN outputs are noisy.|
|||The only postprocessing required in our decision function is to check a threshold on the CNN outputs and select the maximum index of the output vector.|
|||Iterative Graph Construction CNN Training  We now discuss the training procedure for the decision function.|
|||Training the CNN is non-trivial: the CNN takes as input a partial graph G (generated by the search algorithm) and outputs the desirability of walking at various angles, but we only have this ground truth map.|
|||A CNN trained on static training examples exhibits problematic behavior during inference.|
|||We can then train a CNN using gradient deho scent by back-propagating a cross entropy loss between walk = 1, a mean-squared action, and, if o Oaction and O error loss between Oangle and O angle.|
|||, o  walk, o  stopi, O  However, we found that although the CNN can achieve high performance in terms of the loss function on the training examples, it performs poorly during inference.|
|||During inference, however, the CNN may choose angles that are slightly off from the ones predicted by the oracle, resulting in small errors in G. Then, because the CNN has not been trained on imperfect inputs, these small errors lead to larger prediction errors, which in turn result in even larger errors.|
|||The CNN does not output the ideal angle at the turn; this causes it to quickly veer off the actual road because it never saw such deviations from the road during training, and hence it cannot correct course.|
|||Although this reduces the scale of the problem, the CNN still yields low performance at inference time, because the noise that we introduce does not match the characteristics of the noise introduced inherently by the CNN during inference.|
|||Dynamic Labels  We instead generate training examples dynamically by running the search algorithm with the CNN as the decision function during training.|
|||As the CNN model evolves, we generate new training examples as well.|
|||On each training step, as during inference, we feedforward the CNN to decide on an action based on the output  layer, and update G and S based on that action.|
|||In addition to deciding on the action, we also determine the action that an oracle would take, and train the CNN to learn that action.|
|||Otherwise, o o i = 1, where i is the closest walkable angle to . we set o Lastly, we compute a loss between O and O, and apply back-propagation to update the CNN parameters.|
|||Conclusion  On the face of it, using deep learning to infer a road network graph seems straightforward: train a CNN to recognize which pixels belong to a road, produce the polylines, and then connect them.|
||27 instances in total. (in cvpr2018)|
|53|Wang_Dictionary_Pair_Classifier_CVPR_2016_paper|The state-of-the-art object detection methods generally adopt the region-based CNN framework which includes three components: region proposal, feature extraction and object category classification.|
|||By far, many region proposal methods [31, 3, 24] and deep CNN architectures [13, 26, 28, 27, 16, 12] have been proposed, but not too many methods have been proposed for object category classification, where the SVM/softmax classifiers are dominantly used.|
|||These classifiers, however, are fully discriminative methods which directly learn an optimal mapping from the CNN features to the desired classification output.|
|||Combining of discriminative learning with representation or generative modeling is beneficial to exploit the complex structure of CNN features for improving object detection.|
|||In this work, we propose to design a dictionary pair classifier layer (DPCL) at the end of the CNN for object detection.|
|||Rather than learning the CNN and the dictionary pair separately, we adopt a joint training mechanism for simultaneous feature learning and classifier learning.|
|||A dictionary pair back propagation (DPBP) algorithm is proposed to jointly update the parameters of CNN and DPCL in an end-to-end learning manner.|
|||With DPBP, we can fine-tune the trained CNN to extract discriminative features specialized to DPCL.|
|||By integrating DPCL classifier training with CNN feature learning, the proposed method achieves about 3% / 2% mAP gain over the popular existing object detection frameworks (e.g., R-CNN [13], FRCN [12]) on PASCAL VOC 2007/2012 benchmarks, respectively.|
|||i) A novel deep architecture is developed by integrating DPCL with CNN for objection detection, and a DPBP algorithm is suggested for the endto-end learning of CNN and DPCL parameters.|
|||Deformable parts models (DPMs) can also be explained from the CNN perspective, and the integration of DPMs and CNNs has been investigated in [32, 14].|
|||(4)  In DPBP,  the partial derivatives with respect  to  {Pk, Dk} are defined as:  Lk(Pk, Dk)  Pk  Lk(Pk, Dk)  Dk  = 2Dk(I  DkPk)XkWkWT k  XT k  +2PkXkX  T k  = 2XkWk(I  DkPk)WT k  XT k  PT k  +2Dk  (8)  2141  k=0 Lk, the partial derivatives with respect  With L = PK  to Xk is then defined as:  L Xk  = 2(I  PT k  DT  k )(Xk  DkPkXk)WkWT  k  2PT  k Pk Xk  + Xk6=k  (9)  Once all L Xk back propagation [20] to update the CNN parameters.|
|||Object Detection on Test Image  Given a proposal I from the test image, we first extract the CNN feature x from I, and then define the reconstruction residual for the k-th category as:  L(x; Dk, Pk) = kx  DkPkxk2 F .|
|||1, the feature extracted by the CNN layers is duplicated and simultaneously fed into two DPCLs: the objectness DPCL layer and the categoryness DPCL layer.|
|||(14)  Let  denote the function of the CNN layers and Ii denote the input region with the category label yi, we have the feature x = (I, ).|
|||Then, the bounding box regression loss is defined as:  w, tk  w, t  x, tk  y, tk  x, t  y, t  Lloc(tk(I), t(I)) = Xix,y,w,h  2142  H1(tk  i  t  i ),  (16)  Algorithm 1 Multi-Task CNN+DPCL Learning Input:  Training samples I = [I1, I2, ..., IK , Ib] for K + 1 classes (Ib denotes background), pre-trained CNN layers parameters.|
|||Output:  Dictionary pairs {D, P} = {{D1, P1}, {D2, P2}, ..., {DK , PK }, {Do, Po}, {Db, Pb}}, fine-tuned CNN parameter , bounding box regressors r.|
|||Initialize CNN parameters  with pre-trained network; 2.|
|||Optimization  After obtaining the partial derivatives of Lmt with respect to Db, Pb, Do, Po, Dk, Pk, Xk, we can extend DPBP to fine-tune CNN+DPCL to update the dictionary pairs, CNN parameters and bounding box regressors.|
|||To optimize Lmt, we initialize the CNN parameters with some pre-trained network, e.g., AlexNet [19] or VGG [27], and initialize the dictionary pairs using the dictionary pair learning algorithm in Sect.|
|||Formally, we perform forward propagation to output the CNN feature (I, ) of the region, and then feed it into the ODP layer and the CDP layer, simultaneously.|
|||Following the same experiment settings in [13], the employed CNN parameters are firstly pretrained on ImageNet, and then fine-tuned on the corresponding VOC training and validation sets by stochastic gradient descent (SGD) with a 21-way softmax loss (20 object categories plus one background).|
|||Results and Comparisons  We denote by R-CNN(Alex/VGG) [13] and FRCN [12] the used CNN frameworks, by ODP and CDP the proposed objectiveness and categoryness dictionary pair layers, and by BB the Bounding Box regression in the R-CNN framework.|
|||trained CNN models (VGG [27]) under the FRCN framework, and perform component analysis on the VOC 2007 dataset.|
|||Conclusion  In this paper, we presented dictionary pair classifierdriven CNNs for object detection, where dictionary pair back propagation (DPBP) is proposed for the end-to-end learning of dictionary pair classifiers and CNN representation, and sample weighting is adopted to improve the localization performance.|
|||Object detection via a multiregion & semantic segmentation-aware cnn model.|
||26 instances in total. (in cvpr2016)|
|54|Hoshen_An_Egocentric_Look_CVPR_2016_paper|The CNN is trained on the optical-flow features described above.|
|||Using CNN improves the results over the LPC coefficients, yielding 90% identification rate (vs. accuracy of 3% in random) and verification EER (Equal Error Rate) of 8%.|
|||A diagram of our CNN architecture for photographer recognition from a given flow feature vector.|
|||The CNN learns descriptor and classifier end to end, and is able to take advantage both of dataset labels and the dependence between features when calculating filter coefficients.|
|||The CNN is a more general architecture, the LPC descriptor is a subset of descriptors learnable by the network.|
|||Due to the limited number of data points available in our datasets, we limit our CNN to only 2 hidden layers.|
|||b) Raw CNN probabilities.|
|||The output of this layer is the learned CNN descriptor.|
|||We compute the identity label (Lt) probability distribution for each feature vector Vt using LPC or CNN classifiers trained as de 4287  Input data:Size: 50 * 60 Depth: 2Convolution kernels:Size: 50*20*2128 Kernels Relu Non-Linearity Conv data:Size: 1 * 51 Depth: 128Max Pooling:Size: 1*20Stride: 15Max Pooling data:Size: 1 * 4 Depth: 128Fully Connected layer 1128 unitsOutput layer:Verification2 unitsIdentification:20/32 units Fully connected + sigmoid non-linearityFully connected + sigmoid non-linearityFigure 7.|
|||Classification accuracy vs. video length when one feature vector covers T = 4 seconds (Using CNN on the FPSI Dataset).|
|||The CNN further improves the performance with 90% (Top-1) and 93% (Top-2).|
|||High accuracy was achieved in both scenarios, same day CNN recognition accuracy is 90% (top 1) and 93% (top 2).|
|||The EER for both the CNN and LPC descriptors for videos of length 4s (one feature vector) and 12s (five feature vectors) is presented in Table.|
|||It can be seen from our results that high accuracy (low EER) can be obtained by both descriptors: LPC 14% (4s), 10% (12s) and CNN 11% (4s), 8% (12s).|
|||The CNN obtains better performance for both durations with a larger improvement for 4s.|
|||Verification equal error rates for LPC and CNN descriptors with 4s and 12s sequence duration.|
|||ROC curves for the verification performance of our method for LPC and CNN descriptors of 4s and 12s sequences.|
|||Discussion  Analysis of CNN features: In order to analyze the features learned by the CNN we visualize the filters learned by the first layer.|
|||identification CNN is trained on half the photographers in the training dataset.|
|||We choose a video by a target photographer (that was not used for training the CNN), and extract its CNN descriptors (as in Sec.|
|||Similarly we extract CNN descriptors from all video sequences of photographers not used for training the CNN, this forms our probe set (excluding the sequence used as gallery).|
|||In the CNN experiments, all optical flow values were divided by the square-root of their absolute value, this was found to help performance by decreasing the significance of extreme values.|
|||The CNN was trained by AdaGrad [5] with learning rate 0.01 on a GPU using the Caffe [9] package.|
|||The CNN classifier was shown to generalize and improve on the LPC hand-designed descriptor.|
|||The time-invariant CNN architecture presented here is quite general and can be used for other video classification tasks relying on coarse optical flow.|
||25 instances in total. (in cvpr2016)|
|55|Krull_Learning_Analysis-by-Synthesis_for_ICCV_2015_paper| We observe that the CNN does not specialize to the geometry or appearance of specific objects, and it can be used with objects of vastly different shapes and appearances, and in different backgrounds.|
|||Another CNN is used to predict the coarse pose of the object.|
|||This CNN is trained using pixel normals in images containing rendered synthetic objects.|
|||In contrast to the above approaches, we use a CNN to compare rendered and observed images.|
|||The output of our CNN is a single energy value, while in [10] In [7], a simthe output of the CNN is the object pose.|
|||A CNN with siamese architecture is used to map two faces to a feature space for comparison.|
|||Similarly, in [30] Wohlhart and Lepetit train a CNN to map image patches to a descriptor space, where pose estimation and object recognition is solved using the nearest neighbor method.|
|||The posterior distribution of the pose is modelled as a Gibbs distribution with a CNN as energy function.|
|||Zbontar and LeCun [31] train a CNN to predict how well two image patches match and use it to compute the stereo matching cost.|
|||While in [31] the CNN is used for comparing two image patches, our CNN is used to compare rendered and observed images.|
|||Different from aforementioned approaches, we model the posterior density of a particular object pose with a CNN that compares an observed and rendered image.|
|||They use a CNN as a part of probabilistic model.|
|||The CNN is fed in a sequential manner, first with the rendered image, then with the observed image.|
|||In contrast to [18], we jointly input the rendered and observed images into a CNN to produce an energy value.|
|||The major difference is that our CNN is trained, while they use a pre-trained CNN as feature extractor.|
|||The key difference is that while the energy function in [5] has only a few parameters which can be trained via discriminative crossvalidation, the CNN has around 600K, which we train with a maximum likelihood objective.|
|||While in a CRF the energy function is a sum of potential functions, we implement it by using a CNN which directly outputs the energy value.|
|||Our CNN then compares x with r(H) and outputs a value  (2)  f(cid:0)x, r(H); (cid:1).|
|||(g) Finally, the rendered and observed images are processed and fed into the CNN (Sec.|
|||Additionally, we observe that our CNN generalizes from a single training object to a set of 11 test objects, with large variability in appearance and geometry.|
|||The weights of the CNN were randomly initialized.|
|||We compare our method (using the CNN trained with the Samurai object) to our re-implementation of [5].|
|||Conclusion  We have presented a model for the posterior distribution in 6D pose estimation, which uses a CNN to map rendered and observed images to an energy value.|
|||It has been demonstrated that training on a single object is sufficient and the CNN is able to generalize to different objects and backgrounds.|
|||Considering the recent success of CNNs in recognition [2, 23] it might be possible for a CNN to learn to compare observed images to renderings of an idealized model representing an object class instead of an instance.|
||25 instances in total. (in iccv2015)|
|56|Gu_An_Empirical_Study_ICCV_2017_paper|An Empirical Study of Language CNN for Image Captioning  1 ROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore  Jiuxiang Gu1, Gang Wang2, Jianfei Cai3, Tsuhan Chen3  3 School of Computer Science and Engineering, Nanyang Technological University, Singapore  {jgu004, asjfcai, tsuhan}@ntu.edu.sg, gangwang6@gmail.com  2 Alibaba AI Labs, Hangzhou, China  Abstract  Language models based on recurrent neural networks have dominated recent image caption generation tasks.|
|||In this paper, we introduce a language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning.|
|||In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies in history words, which are critical for image captioning.|
|||To better model the hierarchical structure and long-term dependencies in word sequences, in this paper, we adopt a language CNN which applies temporal convolution to extract features from sequences.|
|||Such a method is inspired by works in NLP which have shown CNN is very powerful for text representation [18, 48].|
|||Unlike the vanilla CNN architecture, we drop the pooling operation to keep the relevant information for words representation and investigate the optimum convolutional filters by experiments.|
|||However, only using language CNN fails to model the dynamic temporal behavior.|
|||Our extensive studies show that adding language CNN to a recurrent network helps model sequences consistently and more effectively, and leads to improved results.|
|||Considering ConvNets can be stacked to extract hierarchical features over long-range contexts and have received a lot of attention in many tasks [10], in this paper, we design a language CNN to model words with long-term dependencies through multilayer ConvNets and to model the hierarchical representation through the bottom-up and convolutional architecture.|
|||Overall Framework  We study the effect of language CNN by combining it with Recurrent Networks.|
|||It consists of one deep CNN for image encoding, one CNN for sentence modeling, and a recurrent network for sequence prediction.|
|||In order to distinguish these two CNN networks, we name the first CNN for image feature extraction as CNNI , and the second CNN for language modeling as CNNL.|
|||Given an image I, we take the widely-used CNN architecture VGGNet (16-layer) [42] pre-trained on ImageNet [22] to extract the image features V  RK .|
|||Inspired by the recent success of CNNs in computer vision [10, 14], we adopt a language CNN with a hierarchical structure to capture the long-range dependencies between the input words, called CNNL.|
|||The architecture of language CNN for sentence modeling.|
|||In our study, we combine our language CNN with four  Traditionally, the simple RNN updates the recurrent state  r[t] of Equation 11 as follows:  r[t] = tanh(Wrr[t1] + Wzz[t] + b)  (13)  where z[t] is the input.|
|||Implementation Details  In the following experiments, we use the 16-layer VGGNet [42] model to compute CNN features and map the last fully-connected layers output features to an embedding space via a linear transformation.|
|||All weights are randomly initialized except for the CNN weights.|
|||Quantitative Results  We first evaluate the importance of language CNN for image captioning, then evaluate the effects of CNNL on two datasets (Flickr30K and MS COCO), and also compare with the state-of-the-art methods.|
|||We think the reason is that our language CNN takes all history words as input and explicitly model the long-term dependencies in history words, this could be regarded as an external memory cell.|
|||4.3.3 Results Using CNNL on Flickr30K  We also evaluate the effectiveness of language CNN on the smaller dataset Flickr30K.|
|||The results in Table 4 clearly indicate the advantage of exploiting the language CNN to model the long-term dependencies in words for image captioning.|
|||This demonstrates the effectiveness of our language CNN on the one hand, and also shows that our CNNL+RNN/RHN/GRU models are more robust and easier to train than LSTM networks when less training data is available.|
|||The results demonstrate that our model with language CNN can generate more humanlike sentences by modeling the hierarchical structure and long-term information of words.|
|||Conclusion  In this work, we present an image captioning model with language CNN to explore both hierarchical and temporal information in sequence for image caption generation.|
||25 instances in total. (in iccv2017)|
|57|cvpr18-CNN in MRF  Video Object Segmentation via Inference in a CNN-Based Higher-Order Spatio-Temporal MRF|Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object.|
|||This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step.|
|||In our model, we instead use a CNN to encode even higher-order spatial potentials over pixels.|
|||Given a labeled object mask in the first frame, we can train a mask refinement CNN for the object to refine a coarse mask in a future frame.|
|||Assuming that the mask refinement CNN is so reliable that it can consistently refine a coarse mask to a better one and keep a good mask unchanged, we can define an objective function based on the CNN to assess a given mask as a whole.|
|||When initialized with a simple one-shot segmentation CNN [13], our algorithm shows outstanding performance on challenging benchmarks like the DAVIS 2017 test-dev dataset [38].|
|||It is shown that a merely appearance-based CNN named OSVOS [13], trained on the first frame of a sequence and tested on each subsequent frame individually, achieves significant improvements over top-performing traditional methods (79.8% vs 68.0% accuracy on the DAVIS 2016 dataset [36]).|
|||CNN + MRF/CRF The idea of combining the best from both CNN and MRF/CRF is not new.|
|||We here briefly review some attempts to combine CNN and MRF/CRF for the segmentation task.|
|||The first idea to take advantage of the representation capability of CNN and the fine-grained probabilistic modeling capability of MRF/CRF is to append an MRF/CRF inference to a CNN as a separate step.|
|||Schwing and Urtasun [41] proposed to jointly train CNN and MRF by backpropagating gradient obtained during the MRF inference to CNN.|
|||[2] successfully demonstrated performance gains via a joint training of CNN and MRF, even with higher-order potentials modeled by object detection or superpixels.|
|||The CRF-RNN work [53] formulates the mean-field approximate inference for CRFs as a Recurrent Neural Network (RNN) and integrates it with a CNN to obtain an end-to-end trainable deep network, which shows an outstanding performance boost in an elegant way.|
|||We here resort to a feed-forward CNN to approximate x  c and define f () as follows  f (xc) = kxc  gCNN(xc)k2 2,  (7)  where gCNN() is a mask refinement CNN that accepts as input a given mask xc and outputs a refined mask.|
|||Fortunately, it is shown that such a CNN can be trained in a two-stage manner using the first frame of a given video and performs very reliably during the inference for the following frames [35].|
|||Intuitively, each time evaluating the total energy in the MRF, a feed-forward CNN pass for every frame in the video is required.|
|||Now we discuss the details of the CNN operator gCNN() in our algorithm.|
|||During the training, the input mask to the CNN is a contaminated version of the ground-truth mask with data augmentation  Parameters: number of outer iterations K, number of inner iterations L, number of pixels N , and number of frames C. Initialization: initial labeling x(0) = y(0).|
|||The CNN trained in this way can partially encode the appearances of an object of interest.|
|||The input image to our CNN is cropped around the object using the labeling from a previous iteration and then resized to 513  513.|
|||Runtime Analysis The main portion of the runtime is the online Lucid data augmentation and CNN training for a given video.|
|||The mask refinement step is a feed-forward pass of CNN and takes only a fraction of a second on GPU for each frame.|
|||As a comparison, the results from OSVOS [13] (without boundary snapping, multiple objects conflicts are handled by simply taking the object with the maximum CNN response value at each pixel) are also listed in the table.|
|||3, thanks to the representation power of CNN for encoding object appearances and shapes and the MRF modeling for establishing spatio-temporal connections.|
|||Different from previous efforts in combining MRFs and CNNs, our method explores the new direction to embed a feed-forward pass of a CNN inside the inference of an MRF model.|
||25 instances in total. (in cvpr2018)|
|58|Improving RANSAC-Based Segmentation Through CNN Encapsulation|Improving RANSAC-Based Segmentation Through CNN Encapsulation  Dustin Morley and Hassan Foroosh  University of Central Florida  Orlando, FL 32816  dustinrmorley@knights.ucf.edu, foroosh@cs.ucf.edu  Abstract  In this work, we present a method for improving a random sample consensus (RANSAC) based image segmentation algorithm by encapsulating it within a convolutional neural network (CNN).|
|||Our method for improving RANSAC segmentation performance by CNN encapsulation, shown with idealized intermediate outputs for an example problem of pupil segmentation.|
|||We seek to demonstrate that these approaches (or at least significant pieces of them) can in general be directly encapsulated into a CNN as-is, and that upon doing so the parameters can be fine-tuned through backpropagation using a novel error function which is directly tied to the propensity of RANSAC to choose the true segmentation over any false segmentation.|
|||Another interesting aspect of doing this is that a CNN constructed for a model-specific segmentation problem will generally be significantly smaller than the CNN architectures currently participating in the modern deep learning revolution.|
|||Thus, our work offers some validation of how well CNN concepts and techniques generalize to smaller problem sizes.|
|||We apply our CNN formulation to the problem of pupil segmentation in images of human eyes.|
|||In summary, our work makes the following contributions: we present a novel framework for model-specific segmentation that unifies CNN and RANSAC approaches using a loss function based on RANSAC outputs; we demonstrate success in using our framework to fine-tune a functional RANSAC segmentation algorithm through CNN training; we demonstrate robustness of our method through a multiplicity of experiments; and we demonstrate successful utilization of a CNN for a problem type and size that is very different from typical CNN work, thus providing significant additional validation of the adaptability and generalizability of the CNN framework.|
|||Although the fundamentals of the CNN technique can be said to have already existed for a few decades [13], it has only been in recent years that CPUs and GPUs have advanced far enough to allow these techniques to be applied to large sets of typically sized images.|
|||An important concept that has come to light with CNNs in recent years is the idea of fine-tuning, normally referring to the practice of taking as a starting point a CNN which has been pre-trained for some task and data set and applying it to a different task and/or data set.|
|||Success in doing so is well documented [28][21], with the pre-trained nets having at least reasonable performance right off the bat (due to the fact that filters within a pre-trained CNN exhibit positive responses to a large variety of useful features) and fantastic performance following training.|
|||Our work slots into this area in general, but with the important distinction of starting from a manually designed simple CNN rather than a pretrained deep CNN.|
|||This is in direct contrast to typical CNN segmentation approaches in which object boundaries are not direct outputs and must be found by applying additional algorithms to the CNN output (e.g.|
|||the CNN might output some kind of probability map from which boundary information must be extracted via algorithms like graph cut or RANSAC).|
|||In particular, we insert a custom layer into a CNN that performs the following forward and backward calculations on two input channels denoted gx and gy given a loss function L:  h = qg2  x + g2 y  L gx  L gy  =  =  L h  h gx  L h  h gy  =  =  L  h(cid:18) gx h (cid:19) h(cid:18) gy h (cid:19)  L  (1)  (2)  (3)  On the other hand, by utilizing more filters, one could just as well have a full bank of filters for different edge orientations and combine them with one of several possible methods, including Euclidean norm across all outputs, max across all outputs, average of all outputs, or another layer of convolutional filters applied to the outputs (which collapses to the averaging option in the limit that a single filter is used with the center value equal to 1 for all input channels and all other values set to zero).|
|||Regarding the other types of operations, a CNN can be constructed to have multiple largely independent channels entering this phase, which is  6340  significant because it is then possible to interpret some of these channels as focused on obtaining high feature strength for boundary pixels of the object to be segmented with the other channels instead focused on obtaining high feature strength for other objects or artifacts in the image.|
|||A CNN embodying this design philosophy is simply one in which the first few layers produce an output which is (ideally) the original image but with the undesired artifacts removed.|
|||Our RANSAC implementation for pupil detection operates directly on the output Z of the previous CNN layer according to the following steps (assuming a circular model): construct a list of all points (x , y) where Z (x , y) > 0 ; select three of these points at random and construct the unique circle C passing through these points; compute a score for that circle based on the values of Z at points sufficiently close to the circle, but assigning a score of 0 if the circle violates known geometric constraints; repeat the random point sampling and circle scoring steps for a fixed number of iterations, maintaining (and eventually returning) the highest scoring circle.|
|||An important point about the tolerance parameter in score computation is that the tolerance used for the forward and backward passes of the CNN does not necessarily have to be the same; for example, using a smaller value for the backward pass has the effect of being a bit more conservative with weight updates.|
|||The CNN we construct for these experiments is extremely tiny, containing only a few thousand free parameters.|
|||Table 1 shows the marginal but significant accuracy gains that were made in the ability to correctly identify the pupil center and radius through CNN training, while Figure 2 illustrates that the nature of much of this gain actually came in the form of removing directional bias.|
|||For all three geometric parameters governing the best fit circle for the pupil, the initial algorithm produced measurable biases in one direction or another, and optimizing the algorithm through CNN training significantly reduced these biases.|
|||The CNN was initialized to a pretty good performance prior to any training, and this performance was improved by training on our RANSAC loss function.|
|||The results were calculated over a test set of 1577 images, with the post-training CNN having been trained on 1051 different images.|
|||Conclusion  In this work, we successfully embedded a highperforming RANSAC segmentation algorithm for a practical problem into a CNN by hand, and achieved even better performance by fine-tuning the constructed CNN with backpropagation.|
|||We believe that our approach of CNN encapsulation and finetuning with our RANSAC loss function has general application to any computer vision problem where RANSAC has been proven to be a successful method, and we look forward to experimentally investigating this in the future.|
||25 instances in total. (in cvpr2017)|
|59|Sun_Learning_a_Convolutional_2015_CVPR_paper|We further extend the candidate set of motion kernels predicted by the CNN using carefully designed image rotations.|
|||To fully utilize the CNN, we propose to extend the candidate motion kernel set predicted by CNN using an image rotation technique, which significantly boost its performance for motion kernel estimation.|
|||Learning a CNN for Motion Blur Estimation We propose to estimate spatially-varying motion blur kernels using a convolutional neural network.|
|||P (m = (l, o)|p)  (a)	  Input	  image (b)	  Es1mated	  mo1on	  blur	  field	  by	  CNN (c)	  Result	  a<er	  deblurring m=(l,)l(a)	  Moon	  kernel	  represented	  by	  moon	  vector (c)	  Candidate	  moon	  kernel	  set	  for	  learning	  CNN 	   	   (b)	  Discrezing	  moon	  vector rFigure 3.|
|||Structure of CNN for motion kernels prediction.|
|||In Section 2.2 we will show how to extend the motion kernels of CNN to predict motion kernels outside the set S.  Given the candidate motion kernel set S, we next construct and learn CNN for predicting the motion distribution over S given a blurry patch.|
|||To train the CNN model, we generate a large set of training data T = {k, mk}K k=1, which are composed of blurry patch / motion kernel pairs.|
|||Using Caffe [5]2, we train the CNN model in one million iterations by stochastic gradient descent algorithm with batches of 64 patches in each iteration.|
|||Because the final layer of the CNN is a soft-max layer, we can predict the probabilities of motion kernels given an observed blurry patch  as  P (m = (l, o)|) =  c )T F 5())  exp((wS6 n exp((wS6  (cid:80)  n )T F 5())  ,  (3)  is the vector of weights on neuron connections where wS6 c from F5 layer to the neuron in S6 layer representing the motion kernel (l, o), c is the index of (l, o) in S. F 5() is the output features of F 5 layer of a blurry patch , which is a 1024-dimensional feature vector.|
|||In our implementation, we also tried to learn more complex CNN structures (e.g., with one more convolutional layer or more filters in convolutional layers), but the learning speed is significantly slower while the final prediction results are not significantly improved.|
|||Extending the Motion Kernel Set of CNN  Our learned CNN model can predict the probabilities of 73 candidate motion kernels in S. Obviously, they are not sufficiently dense in the motion space.|
|||We next extend the motion kernel set predicted by the CNN to enable the prediction for motion kernels outside S.  2http://caffe.berkeleyvision.org  3  	  	  	  	  	  	  	  30	  x	  30	  x	  3	  	  Blurry	  color	  patch	   Max-	  pooling Conv	  	  	  	  	  +	  ReLU Max-	  pooling Fully	  conn.	  	  	  	  	  +	  ReLU So?-	  max 96:	  7	  x	  7	  filters	  	  	  	  	  	  	  	  	  	  	  C1	   2	  x	  2,	  stride	  2	  	  	  	  	  	  	  	  	  	  M2 256:	  5	  x	  5	  filters	  	  	  	  	  	  	  	  	  	  	  	  C3	   2	  x	  2,	  stride	  2	  	  	  	  	  	  	  	  	  M4 1024	  neurons	  	  	  	  	  	  	  	  	  F5 73	  labels	  	  	  	  	  	  S6 ...... Conv	  	  	  	  	  +	  ReLU Learned	  filters	  in	  C1 p(I)p(RI)p(I)p(RI)m=(l,o)m=(l,o)IRI=24oFigure 5.|
|||Extension of motion kernel set predicted by CNN using rotated images.|
|||For an image I, we generate its rotated images R6 I, R12 I, R18 I, R24 I, then feed each patch and its rotated versions into the CNN to predict motion distributions.|
|||(4) Note that motion m = (l, o  ) may not belong to the motion kernel set of CNN (i.e., S).|
|||By carefully designing the image rotations, we can extend the motion kernel set of CNN as follows.|
|||Remember that the original CNN can predict probabilities of 73 motion kernels in S with orientations in So = {0, 30, 60, 90, 120, 150} with interval of 30.|
|||Note that this process does not require the CNN retraining, but just feed this image and its rotated versions to our learned CNN.|
|||Effect of CNN motion kernel set extension.|
|||Figure 6 shows an example of motion kernel estimation without and with CNN motion kernel set extension.|
|||Dense Motion Field Estimation by MRF  The CNN predicts distribution of motion kernels in an image at the patch level.|
|||ext (|So  ext  Predicng	  probabilies	  of	  moon	  kernels	  by	  CNN P(m=(l,o))P(m=(l,o+24o))lSl;oSoR6oIR12oIR18oIR24oIIp(I)p(R6oI)p(R12oI)p(R18oI)p(R24oI)P(m=(l,o+6o))P(m=(l,o+12o))P(m=(l,o+18o))(a)	  Blurry	  image (b)	  Es1ma1on	  without	  CNN	  mo1on	  kernel	  set	  extension	  (MSE	  =	  10.3) (c)	  Es1ma1on	  with	  CNN	  mo1on	  kernel	  set	  extension	  (MSE	  =	  8.4) Figure 7.|
|||Predicting dense motion blur for an image of size 300  400 takes  5  Orienta(on:	  0-180 Length:	  1-25 (a)	  Input	  blurry	  image (b)	  Examples	  of	  mo(on	  kernels	  probabili(es	  es(mated	  by	  CNN (c)	  Final	  mo(on	  blur	  es(ma(on (d)	  Ground-truth	  mo(on	  blur (a)	  Input	  blurry	  image 	  	  	  	  	  (b)	  Result	  without	  MRF	  	  	  	  	  (MSE_mo<on	  =	  22.93) 	  (c)	  Result	  using	  MRF	  (MSE_mo<on	  =	  10.94) (d)	  Ground-truth	  mo<on	  blur around 80 seconds using CPU including computing patchlevel motion distributions by CNN.|
|||Our CNN can better predict the different motion layers.|
|||In the future, we are interested in designing a CNN for estimating the general non-uniform blur kernels.|
||25 instances in total. (in cvpr2015)|
|60|Bilen_Weakly_Supervised_Deep_CVPR_2016_paper|Second, CNN training is data-hungry.|
|||Our method starts from a CNN pre-trained for image classification on a large dataset, e.g.|
|||[36], for example, uses a pre-trained CNN to describe image regions and then learn object categories as corresponding visual topics.|
|||While this method is currently state-of-the-art in weakly supervised object detection, it comprises several components beyond the CNN and requires signifiant tuning.|
|||Our method (section 3) starts from an existing network, such as AlexNet pre-trained on ImageNet data, and extends it to reason explicitly and efficiently about image regions R. In order to do so, given an image x, the first step is to efficiently extract region-level descriptors (x; R) by inserting a spatial pyramid pooling layer on top of the convolutional layers of the CNN [14, 11].|
|||Our two-stream CNN is also weakly related to the recent work of Lin et al.|
|||In section 4 we show that, when fine-tuned on the PASCAL VOC training set, this architecture achieves state-of-the-art weakly supervised object detection on the PASCAL data, achieving superior results to the current state-of-the-art [36] but using only CNN machinery.|
|||Since the system can be trained end-to-end using standard CNN packages, it is also as efficient as the recent fully-supervised Fast R-CNN detector of Girshick et al.|
|||[25] employ a pre-trained CNN to compute a mid-level image representation for images of PASCAL VOC.|
|||[26] modify a CNN architecture to coarsely localize object instances in image while predicting its label.|
|||[16] proposed a CNN architecture in which a subnetwork automatically pre-transforms an image in order to optimize the classification accuracy of a second subnetwork.|
|||First, we obtain a CNN pre-trained on a large-scale image classification task (section 3.1).|
|||Second, we construct the WSDDN as an architectural modification of this CNN (section 3.2).|
|||Pre(cid:173)trained network  We build our method on a pre-trained CNN that has been pre-trained on the ImageNet ILSVRC 2012 data [28] with only image-level supervision (i.e.|
|||We give the details of the used CNN architectures in section 4.|
|||We comprehensively evaluate our method with three pretrained CNN models in our experiments as in [11].|
|||The WSDDNs are trained on the PASCAL VOC training and validation data by using fine-tuning on all layers, a widely-adopted technique to improve the performance of a CNN on a target domain [3].|
|||We use the publicly available CNN toolbox MatConvNet [35] to conduct our experiments and share our code, models and data 1.|
|||Pre-trained CNN architectures.|
|||In addition to CNN features, Cinbis et al.|
|||Differently from the previous work, WSDDN is based on a simple modification of the original CNN architecture fine-tuned on the target data using back-propagation.|
|||The output of our model could also be used as input to one of the existing methods for weakly-supervised detection that use a CNN as a black-box for feature extraction.|
|||Conclusions  In this paper, we have presented WSDDN, a simple modification of a pre-trained CNN for image classification that allows it to perform weakly supervised detection.|
|||WSDDN is also shown to perform better than traditional fine-tuning techniques to improve the performance of a pre-trained CNN on the problem of image classification.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||25 instances in total. (in cvpr2016)|
|61|BigHand2.2M Benchmark_ Hand Pose Dataset and State of the Art Analysis|Training a CNN using the BigHand2.2M dataset, we demonstrate state-of-the-art performance on existing benchmarks, 1520mm average errors.|
|||Analysis of the state of the art  In this section we use the Holi CNN architecture [38] as the current state of the art.|
|||The input for the CNN model is the cropped hand area using the ground truth joint locations.|
|||The CNN is implemented using Theano and is trained on a desktop with an Nvidia GeForce GTX TITAN  4870  ICVL     <   r o r r e     h  t i  w   s e m a r f   f     o n o  i t r o p o r p  100%  90 %  80 %  70 %  60 %  50 %  40 %  30 %  20 %  10 %  0  %     <   r o r r e h     t i  w   s t n o  i  j   f     o n o  i t r o p o r p  Melax et al.|
|||A CNN trained on BigHand2.2M achieves state-of-the-art performance on ICVL and NYU, while the CNNs trained on ICVL, NYU, and MSRC do not generalize well to other benchmarks.|
|||The networks CNN MSRC, CNN ICVL, CNN NYU, and CNN BigHand are trained on the training set of MSRC, ICVL, NYU, and BigHand2.2M, respectively.|
|||When training the CNN model on BigHand2.2M, ICVL, NYU and MSRC, we keep the CNN structure and 1, 2,  of Adam unchanged.|
|||When the CNN model is trained on 1 2 , and all of the benchmark data, the test results on ICVL, NYU, MSRC, and BigHand2.2M keep improving.|
|||As pointed out in [27], in existing datasets, test poses remarkably resemble the training poses, and they proposed  4871  Figure 9: Generalization of the CNN trained on BigHand2.2M.|
|||The CNN generalizes to the ICVL dataset with a lower error than the original annotated ground truth.|
|||The performance when the CNN is trained on theBigHand2.2M training set is still high when evaluated on other datasets.|
|||This confirms that with high annotation accuracy and with sufficient variation in shape, articulation and viewpoint parameters, a CNN trained on a large-scale dataset is able to generalize to new hand shapes and viewpoints, while the nearest neighbor method showed poor cross-testing performance [27].|
|||When training the CNN on MSRC and testing on all real testing sets, the performance is worse than the CNN trained on NYU, and significantly worse than when trained on BigHand2.2M.|
|||Performance is to that of a CNN trained on ICVL which is only one-sixth in size compared to the MSRC training set.|
|||State(cid:173)of(cid:173)the(cid:173)art comparison  In this section we compare our CNN model trained on BigHand2.2M with 8 state-of-the-art methods including HSO [29], Sun et al.|
|||When the CNN model trained on BigHand2.2M is used for testing on NYU, it outperforms two recent methods, DeepPrior [13] and FeedLoop [14], and achieves comparable accuracy with Hier [38], even though the model has never seen any data from NYU benchmark, demonstrated in the right of Figure 7.|
|||The ICVL test error curve of the CNN model trained on BigHand2.2M is shown in Figure 7(left).|
|||Note that the mean estimation error for our CNN model is already as low as 14mm, which means that a small annotation discrepancy between training and test data will have a large influence on the result.|
|||The Holi CNN significantly outperforms the tracking-based methods FORTH [15] and Intel [3].|
|||(middle) the CNN trained on 90% of the BigHand2.2M data achieves high accuracy on the remaining 10% validation images.|
|||This dataset enabled us to train a CNN model resulting in performance competitive with that of third view hand pose estimation.|
|||In the experiment we train the CNN on nine subjects and test it on the remaining one.|
|||A CNN trained on BigHand2.2M achieves state-of-the-art performance in the egocentric-view pose estimation task.|
|||Baselines on BigHand2.2M  Three baselines are evaluated on our 37K-frame testing sequence, the CNN trained on BigHand2.2M, the Particle Swarm Optimization method (FORTH) [15] and the method by Intel [3].|
|||The CNN model outperforms the two generative methods, see the left plot of Figure 11.|
||25 instances in total. (in cvpr2017)|
|62|Liu_3DCNN-DQN-RNN_A_Deep_ICCV_2017_paper|The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds.|
|||Through supervised learning, the 3D convolutional neural network (CNN) has the ability to learn features about shape, spatial relationship, color and context of the points in the point cloud from multiple scales, and then encode them into a discriminative feature representation called 3D CNN feature.|
|||Once the class object is localized and enveloped, the 3D CNN feature, coordinate and color of each point within the eye window are combined into a vector which is input into the Residual recurrent neural network (RNN).|
|||Based on a 2D convolutional neural network, Maturana and Scherer proposed a 3D CNN for object binary classification task based on LiDAR data [16].|
|||A volumetric occupancy grid representation and a supervised 3D CNN are integrated to improve the performance.|
|||To make 3D CNN architectures fully exploit the power of 3D representations, Qi et al.|
|||introduced a fully connected network (FCN)-based end-to-end framework in which the trained CNN can perform pixel-wise prediction of class of images [14].|
|||in 5679  Action  Eye   Window  Conv 1  Conv 2  Deep Q Network  3DCNN  Conv 3  ...  R = [R1,R2]  Fully Connect  r = 2R1+1-R2  Conv 1  Conv 3  Conv 2  ...  ...  V  Value  Q  Qtg  A  Advantage  Optimization   goal  Fully  Connect  in the scene, and then apply the 3D CNN to recognize the data inside the eye window.|
|||We re-apply the 3D CNN to get the reward vector of the points in the new eye window.|
|||Finally, the 3D CNN features and all the points within the eye window are taken as the input of Network 2.|
|||To achieve a more purified classification result, we do not directly make use of the 3D CNN to parse the point clouds.|
|||All the features are taken as the 3D CNN feature vector of all the points within the eye window.|
|||3D CNN Structure The 3D CNN contains three convolutional layers and multiple fully-connected layers.|
|||3.1.2 Object Class Detection by the DQN  The 3D CNN predicts the probability of the points in the eye window belonging to a certain class rather than labels every point.|
|||Based on the analysis, we design a 3DCNN-DQN dythe 3D CNN is employed namic searching mechanism: to evaluate the current state and the result, i.e.|
|||a. Q Define Value and Advantage The 3D CNN is a hierarchical perceptive system whose parameters do not change after the training process.|
|||The 3D CNN acts as the input channel of the current state for the DQN.|
|||The concatenated vector [f1, f2, f3] is the encoded feature for all the points within the eye window and is called the 3D CNN feature.|
|||3.2.1 Residual RNN Structure  Every point Pk in the eye window corresponds to a reconstructed vector Vk = [xk, yk, zk, rk, gk, bk, f1, f2, f3], where xk, yk, zk are the coordinate of Pk; rk, gk, bk are the color of Pk; f1, f2, f3 are the 3D CNN feature vector of every point in the eye window.|
|||Experiments  We train the 3D CNN and RNN on two NVIDIA K40 GPUs.|
|||3D CNN is Not Well Trained  The main limitation of our method is that the 3D CNN is not trained well enough.|
|||The 3D CNN is hard to learn discriminative features from small point clouds.|
|||Based on the facts, we believe that training the 3D CNN on a large dataset can help to enhance the classification performance.|
|||The 3D CNN has the ability to learn the features about spatial distribution shapes, colors and contexts of the points in each voxel grid unit from multi-scales, and fuse the features into a 3D CNN feature representation.|
|||For further achieving highquality classification results, the 3D CNN feature and the point coordinate and color of each point in the eye window are concatenate into one vector which is taken as the input of the Residual RNN.|
||25 instances in total. (in iccv2017)|
|63|Wang_MMSS_Multi-Modal_Sharable_ICCV_2015_paper|We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multimodal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities.|
|||[29] outlined a framework which integrated Convolutional Neural Networks (CNN) and Recursive Neural Networks (RNN) to learn features from color and depth separately, where the single-layer CNN is pre-trained in an unsupervised manner to produce lower-level features while the RNN learns higher-level features.|
|||CNN layers are constructed to form the input to our multimodal feature learning framework, and the information of the multi-modal learning framework is back-propagated to the early CNN layers.|
|||In contrast, our multi-modal feature learning framework is based on matrix transformation which extracts both shared common patterns and modalspecific properties of different modalities, and is integrated with CNN based supervised deep learning for RGB-D object recognition.|
|||Recently, the generative power of CNN has been shown in many computer vision tasks [17, 10, 32, 15, 28, 11, 25, 30, 33, 16, 27].|
|||[12] proposed a CNN based approach which replaces the original depth map with three channels (horizontal disparity, height above ground, angle between point normal and inferred gravity) as the CNN input for RGB-D object detection and segmentation.|
|||Different CNN structures for RGB-D data.|
|||1, X = [x1, x2,    , xN ]  R M1N denotes the activations (with M1 dimensions) of the second fullyconnected layer of the CNN for color images in one data batch with N images.|
|||Similarly, Y  RM2N denotes the activations of the CNN for depth images in one data batch.|
|||Ti  (11)  (12)  In our framework, X and Y are the activations of the second fully-connected CNN layers.|
|||The results of the multi-modal learning will then be back-propagated to the lower layers of CNN by  F X  = 2p  1 hW T  1 (W1X  T1)  (W T  1 T1  X)i .|
|||Results on RGB(cid:173)D Object Dataset  Comparison with different baselines of using CNNs: We compare with five different CNN-based baselines: 1) CNN trained using RGB images only (Fig.|
|||2(d)), named Depth CNN; 3) RGB-D used as the four-channel input to a CNN (Fig.|
|||2(a)), named RGB-D CNN with 4-channel input; 4) CNN with separate training for color and depth at the lower layers, followed by concatenating the activations of the second fully-connected layer (fc7) and feeding them into the last fully-connected layer (Fig.|
|||2(b)), named RGB-D CNN connected at fc7; 5) Similar setting with 4), but two modalities are concatenated at the fifth convolutional layer (conv5), named RGB-D CNN connected at conv5.|
|||It can be seen that although simply adding depth as the fourth channel of the CNN input (RGB-D CNN with 4-channel input) greatly improves the performance of those only using one modality (RGB CNN and Depth CNN), extracting features separately from color and depth and connecting them at the later stage (RGB-D CNN connected at conv5) performs better with significant gain.|
|||Following [6], we also use surface normals to replace the depth map as the input, which results in another three baselines: 6) CNN trained using surface normals only, named Surface Normals (SN) CNN; 7) RGB and surface normals used as the six-channel input to a CNN, named RGB-SN CNN with 6-channel input; 8) CNN with separate training for color and surface normals at the lower layers, followed by concatenating the activations of the second fully-connected layer and feeding them into the last fullyconnected layer, named RGB-SN CNN connected at fc7; 9) Similar setting with 8), but two modalities are concatenated at conv5, named RGB-SN CNN connected at conv5.|
|||To further boost the performance, we use images of first 50 classes of ILSVRC2012 [26] to pretrain CNN layers of both color and surface normals, which leads to another baseline: 10) named RGB-SN CNN connected at conv5 (pretrained), achieving the best performance among all the  Table 1.|
|||Method  RGB CNN  Depth CNN RGB-D CNN with 4-channel input RGB-D CNN connected at fc7 RGB-D CNN connected at conv5  Surface Normal (SN) CNN RGB-SN CNN with 6-channel input RGB-SN CNN connected at fc7 RGB-SN CNN connected at conv5  RGB-SN CNN connected (pretrained)  at  conv5  Accuracy (%) 74.6  2.9 75.5  2.7 80.2  1.9 84.7  2.1 85.1  2.0 76.3  2.5 80.7  2.1 85.0  2.4 85.5  2.2 86.8  2.1  Table 2.|
|||Method  RGB-SN CNN connected (pretrained) Ours  at  conv5  Accuracy (%) 86.8  2.1  88.5  2.2  baselines.|
|||Table 2 compares the results of our proposed multi-modal learning with the best baseline, RGB-SN CNN connected at conv5 (pretrained).|
|||Note that our method also uses surface normals to replace depth images and uses pretrained CNN layers.|
|||[6]  RGB-SN CNN connected at conv5 (pretrained) Ours  Accuracy (%)  82.8 91.0  89.2 91.3  Figure 8.|
|||Table 4 shows the comparison between our method and the best baseline approach, RGB-SN CNN connected at conv5 (pretrained).|
|||The experimental results show that our method integrated with CNN layers greatly boosts the performance.|
||25 instances in total. (in iccv2015)|
|64|Weinzaepfel_Learning_to_Track_ICCV_2015_paper|The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features.|
|||The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features.|
|||Proposals are obtained by hierarchical merging of supervoxels [15], by maximizing an actionness score [46] or by relying on selective search regions and CNN features [11].|
|||Proposals are scored using CNN descriptors based on appearance and motion information [11].|
|||We then score the tracks with the CNN features as well as a spatio-temporal motion histogram descriptor, which captures the dynamics of an action.|
|||In addition, we classify the tracks using per-frame CNN features and spatio-temporal features.|
|||Object proposals from SelectiveSearch [42] are detected in each  frame, scored using features from a two-streams CNN architecture, and linked across the video.|
|||Moreover, we combine the perframe CNN features with descriptors extracted at a spatiotemporal level to capture the dynamics of the actions.|
|||Each proposal is represented with CNN features [11].|
|||The detector is based on the same CNN features as the first stage.|
|||The CNN features only contain information extracted at the frame level.|
|||The final score is obtained by combining CNN and STMH classifiers.|
|||We detect frame-level object proposals and score them with CNN action classifiers.|
|||We then score the tracks with CNN and spatio-temporal motion histogram (STMH) classifiers.|
|||Frame(cid:173)level proposals with CNN classifiers  Frame-level proposals.|
|||Recent work on action recognition [37] and localization [11] have demonstrated the benefit of CNN feature representations, applied separately on images and optical flows.|
|||We use the same set of CNN features as in [11].|
|||Illustration of CNN features for a region R. The CNN features are the concatenation of the fc7 layer from the spatialCNN and motion-CNN, i.e., a 2x4096 dimensional descriptor.|
|||For a region R, the CNN features we use are the concatenation of the fc7 layer (4096 dimensions) from the spatial-CNN and motion-CNN, see Figure 2.|
|||We then finetune the networks with back-propagation using Caffe [18]  3166  frame-level candidatesobject proposals and CNN action classifierstimescoringCNN + STMH classifierstemporal detectionsliding windowtracking best candidatesCNN featuresspatial-CNNmotion-CNNon the proposal regions for each dataset.|
|||The slidingwindow procedure using CNN features can be performed efficiently [10, 36].|
|||Let Sdesc(c, T ) be the average of the scores for all the chunks of length L from the track T for the action c.  Given a track T = {Rt}t=1..T , we score it by summing the scores from the CNN averaged over all frames, and the scores from the descriptors averaged over chunks:  S(T ) = (cid:16)Sdesc(c, T )(cid:17) + (cid:16)  T  X  t=1  SCNN(c, Rt)(cid:17) ,  (2)  where (x) = 1/(1 + ex).|
|||We also compute the mAP when scoring without STMH classifiers, i.e., the score is based on CNN features only, and observe a drop of 2%.|
|||The resulting tracks are scored by combining classifiers learned on CNN features and our proposed spatio-temporal descriptors.|
||24 instances in total. (in iccv2015)|
|65|DSAC - Differentiable RANSAC for Camera Localization|The success of deep learning began with systems in which a CNN processes an image in one forward pass to directly predict the desired output, e.g.|
|||[1] mapped the Vector of Locally Aggregated Descriptors (VLAD) [2] to a CNN architecture for place recognition.|
|||[23] trained a CNN to measure hypothesis consensus by comparing rendered and observed images.|
|||In this work, we adopt the idea of a CNN measuring hypothesis consensus, but learn it jointly with the scene coordinate regressor and in an end-to-end fashion.|
|||[20] demonstrated that a single CNN is able to directly regress the 6D camera pose given an RGB image, but its accuracy on indoor scenes is inferior to a RGB-based SCoRF pipeline [5].|
|||[5] in the fol lowing aspects:   Instead of a random forest, we use a CNN (called Coordinate CNN below) to predict scene coordinates.|
||| We score hypotheses using a second CNN (called Score CNN below).|
|||Instead of learning a CNN to compare rendered and observed images as in [23], our Score CNN predicts hypothesis consensus based on reprojection errors.|
|||Since the Coordinate CNN predicts only point estimates we do no further pose optimization using uncertainty.|
|||Given an RGB image, we let a CNN with parameters w predict 2D-3D correspondences, so called scene coordinates [36].|
|||From these, we sample minimal sets of four scene coordinates and create a pool of hypotheses h. For each hypothesis, we create an image of reprojection errors which is scored by a second CNN with parameters v. We select a hypothesis probabilistically according to the score distribution.|
|||Componentwise Training  Our pipeline contains two trainable components, namely the Coordinate CNN and the Score CNN.|
|||We train the Coordinate CNN using the following surrogate loss: lcoord(y, y) = ky  yk, where y is the scene coordinate prediction and y is ground truth.|
|||We synthetically created data to train the Score CNN in the following way.|
|||the ground truth pose will lead to large reprojection errors, and we want the  Score CNN to predict a small score.|
|||Poses close to ground truth will lead to small reprojection errors, and we want the Score CNN to predict a high score.|
|||Thus, we train the Score CNN to minimize the following loss: lscore(s, s) = |s  s|, where: s = lpose(h, h).|
|||We trained the Score CNN with a batch size of 64 reprojection error images of randomly generated poses.|
|||Hence, we initialize the Coordinate CNN and the Score CNN with componentwise training, see Sec.|
|||We use the following learning rate schedule for the Coordinate CNN: t = 104/(1 + 0.1t) where t is the learning rate at iteration t. For the Score CNN we use a fixed learning rate of 107.|
|||Instead of the Adam procedure, which was unstable, we use stochastic gradient descent with momentum [33] of 0.9, and we clamp all gradients to the range of -0.1 to 0.1, before passing them to the Score CNN or the Coordinate CNN.|
|||We study the effect of learning the Score CNN and the Coordinate CNN in an end-to-end fashion, in 6690  a)  A B L AT IO N S T U DY  Componentwise  +End-to-End Score  +End-to-End Coordinates  +End-to-End Score+Coordinates  % 5  .|
|||In the componentwise training, the Coordinate CNN learned to minimize the surrogate loss lcoord, i.e.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||24 instances in total. (in cvpr2017)|
|66|Shuai_Integrating_Parametric_and_2015_CVPR_paper|Furthermore, a large margin based CNN metric learning method is proposed for better global potential estimation.|
|||We introduce a CNN metric learning approach, and show that the learned features and metrics are beneficial in our non-parametric global belief estimation.|
|||Furthermore, [21] adopted a recurrent CNN to process the large size raw data.|
|||Compared with their hand-engineered features, we used the learned CNN features that are more compact and discriminative.|
|||The truncated CNN works as a feature extraction module.|
|||The local features are fed into two branches: (1), Local belief: they are independently classified based on the parametric CNN model; (2), Global belief: they are aggregated to generate the global feature, which are used to retrieve similar exemplars; the global belief is estimated based on them.|
|||The structure of our CNN is demonstrated in Figure 2.|
|||(3), instead of using sigmoid or tanh activation function, we use the ReLU: y(x) = max(0, x), which have been shown to converge faster during training in large scale object recognition task [13].The experiments will demonstrate that our CNN is able to achieve significantly better results than others when they are fed with same scale local context, while more efficient in terms of training and testing.|
|||Global Belief From Non-parametric Estimation The parametric CNN is capable of generating satisfactory results for the pixels with good local contextual support.|
|||Given an image I, the corresponding CNN feature tensor 2 F  RHWM is obtained by passing I to the truncated CNN.|
|||Concretely, the global belief is transferred from 2The output of FC-1 layer in our CNN (Figure 2).|
|||the statistics of pixel features in S(X), and it is calculated in a weighted K-NN manner:  (cid:80)  (cid:80)  S(X) G  P  (Xi, Yj) =  k (Xi, Xk)(Y (Xk) = Yj)  (5)  k (Xi, Xk)  where Xk is the k-th nearest neighbor of Xi among all the pixel features in S(X), Y (Xk) is the ground truth label for pixel Xk; (Y (Xk), Yj) is an indicator function; (Xi, Xk) measures the similarity between Xi and Xk, which is defined over spatial and feature space: (Xi, Xj) = exp(||xi  xj||)exp(||zi  zj||) (6) where xi = F (Xi) denotes the CNN pixel feature for Xi, zi is the normalized coordinate along the image height axis and ,  controls the belief exponential falloff.|
|||Our nonparametric global belief estimation is reminiscent of popular label transfer works[3, 16, 24, 25, 29], two differences need to be highlighted:   Instead of adopting hand-engineered low-level local and global features, we use more discriminative and compact features learned from CNN for label transfer.|
|||, Y (N ); Result: CNN parameters F , metric W ; train CNN-Softmax; while iter  MAXITER do  for i = 1 ... N do  S(X (i)) = NearestNeighbors(Xi); Ti = RandomPixels(S(X (i))); Fine tune CNN-Metric based on Ti;  end  end  Algorithm 1: CNN Training and fine tuning  of CNN-softmax.|
|||Instead of simply learning a metric based on the extracted CNN features, we further replace the softmax layer with our metric learning layer, so that the feature extraction parameters can also be adapted.|
|||We replace the softmax layer of previous CNN (CNN-softmax) with a fully connected layer parameterized by W (or more layers to learn non-linear metrics [10]) and fix the biases to be zero, which serves as a Mahalanobis metric (M = W T W ).|
|||Unlike other CNNs that take days or even weeks for training 4 , our CNN only takes 3  4 hours on a modern Telsa K40 GPU based on MatConvnet [30] implementation.|
|||Evaluation of Discriminative Power for Global  Features  The discriminative power of local CNN features has been presented in last section.|
|||Stanford  SiftFlow  singlescale convnet (4646) [4] Recurrent CNN (6767) [21]  Ours (4545) Ours (6565)  65.5%(20.8%) 73.5%(35.3%) 75.1% (38.2%)  The network has the same structure as 6565 CNN, except that the spatial dimensions for the first convolutional filter is 66.|
|||Our global feature pooled from CNN pixel features performs significantly better than its hand-engineered counterparts.|
|||In details, as our CNN has three subsampling layers (Figure 2), the dimension of the output feature map F is 1 8 of original image size: one fea SuperParsing[25]  Liu[16] Gould[7]  Ours  Ours+ metric tuning  Stanford 77.5% (-)   73.9% (63.2%) 79.0% (69.0%) 80.2% (69.9%)  Sift Flow  76.9% (29.4%)  74.8 % (-)   78.0% (33.5%) 78.2% (35.8%)  Table 3.|
|||We attribute the performance superiority to the highly discriminative CNN features that we adopt in the nonparametric framework.|
|||As evidenced by Table 4, our integration model is capable of significantly boosting the qualitative results (global pixel accuracy) of CNN local labeling by introducing global  5The auxiliary set consists of features from classes whose frequency is lower than 0.01.|
|||We may explore how to better apply discriminative CNN features to the current successful non-parametric models in the future.|
||24 instances in total. (in cvpr2015)|
|67|Li_Shape_Driven_Kernel_2015_CVPR_paper|Second, motivated by the intuition that different local facial regions may demand different adaptation functions, we further propose a tree-structured convolutional architecture to hierarchically fuse multiple local adaptive CNN subnetworks.|
|||Despite the great success, CNN based methods learn discriminant features mainly from texture information which may change significantly in real world conditions due to illumination and viewpoints variations.|
|||a set of facial landmarks, to help CNN based methods learning more powerful and robust face representation.|
|||Specifically, we propose a shape driven kernel adaptation for CNN and use automatically adapted kernels to more efficiently disentan (cid:18)(cid:148)(cid:139)(cid:137)(cid:139)(cid:144)(cid:131)(cid:142)(cid:3)(cid:12)(cid:143)(cid:131)(cid:137)(cid:135)(cid:3)  (cid:1835)   (cid:4666)(cid:1876)(cid:2869)(cid:481)(cid:1877)(cid:2869)(cid:4667) (cid:4666)(cid:1876)(cid:2870)(cid:481)(cid:1877)(cid:2870)(cid:4667)  (cid:4666)(cid:1876)(cid:3041)(cid:481)(cid:1877)(cid:3041)(cid:4667)   (cid:4)(cid:134)(cid:131)(cid:146)(cid:150)(cid:139)(cid:152)(cid:135)(cid:3)(cid:14)(cid:135)(cid:148)(cid:144)(cid:135)(cid:142)(cid:3)  (cid:1858)   convolve   convolve   convolve   pool   pool   pool   (cid:2032)  (cid:1845)  (cid:4666)(cid:1876)(cid:2869)(cid:481)(cid:1877)(cid:2869)(cid:4667) (cid:4666)(cid:1876)(cid:2870)(cid:481)(cid:1877)(cid:2870)(cid:4667) (cid:1709) (cid:4666)(cid:1876)(cid:3041)(cid:481)(cid:1877)(cid:3041)(cid:4667)       (cid:4)(cid:134)(cid:131)(cid:146)(cid:150)(cid:139)(cid:152)(cid:135)(cid:3)(cid:14)(cid:135)(cid:148)(cid:144)(cid:135)(cid:142)(cid:3)  (cid:1858)(cid:4593)   pool   pool   pool      convolve   convolve   convolve   Invariant Feature   (cid:1829)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3406)(cid:3)(cid:3)(cid:3)(cid:3)(cid:1829)(cid:4593)  (cid:1855)(cid:2869) (cid:1855)(cid:2870) (cid:1855)(cid:3041)  (cid:1855)(cid:2869)(cid:4593) (cid:1855)(cid:2870)(cid:4593)  (cid:1855)(cid:3041)(cid:4593)(cid:3)  (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) (cid:3406)  (cid:1858)(cid:3404)(cid:2032)(cid:4666)(cid:1845)(cid:4667)      Kernel Adaptation   (cid:2032)  (cid:1845)(cid:4593)  (cid:4666)(cid:1876)(cid:2869)(cid:4593)(cid:481)(cid:1877)(cid:2869)(cid:4593)(cid:4667) (cid:4666)(cid:1876)(cid:2870)(cid:4593)(cid:481)(cid:1877)(cid:2870)(cid:4593)(cid:4667) (cid:1709) (cid:4666)(cid:1876)(cid:3041)(cid:4593)(cid:481)(cid:1877)(cid:3041)(cid:4593)(cid:4667)       (cid:23)(cid:148)(cid:131)(cid:144)(cid:149)(cid:136)(cid:145)(cid:148)(cid:143)(cid:135)(cid:134)(cid:3)(cid:12)(cid:143)(cid:131)(cid:137)(cid:135)(cid:3)  (cid:1835)(cid:4593)   (cid:4666)(cid:1876)(cid:2869)(cid:4593)(cid:481)(cid:1877)(cid:2869)(cid:4593)(cid:4667)  (cid:4666)(cid:1876)(cid:2870)(cid:4593)(cid:481)(cid:1877)(cid:2870)(cid:4593)(cid:4667)  (cid:4666)(cid:1876)(cid:3041)(cid:4593)(cid:481)(cid:1877)(cid:3041)(cid:4593)(cid:4667)   (cid:3404)  (cid:3404)  Original Landmarks   Transformed Landmarks   Figure 1.|
|||An toy example for clarification of the basic idea of proposed kernel adaptation in a CNN framework.|
|||In summary, we propose a novel tree-structured kernel adaptive CNN to exploit shape information in the CNN framework for robust facial trait recognition.|
|||The contributions of the paper include: 1) propose shape driven kernel adaptation in CNN framework, which helps learning robust face representations that are invariant to non-rigid appearance variations; 2) propose a tree-structured deep convolutional fusion hierarchy, which further enhances the power of kernel adaptation and 3) achieve the state-of-the-art performance in various facial trait recognition tasks, including identity, age and gender recognition.|
|||In [34], a CNN based method was used to conduct facial age, gender and race classification.|
|||To this end, we propose a kernel adaptation mechanism for traditional CNN framework.|
|||Therefore, instead of using single adaptation function over the whole face, the kernel adaption is separately adopted in multiple local CNN subnetworks, indicated as Ci (i = 1, 2, ..., N ), over multiple local facial patches, indicated as Pi (i = 1, 2, ..., N ).|
|||Then the convolutional features of multiple local subnetworks are stacked and fed into the part-fusion subnetworks C 2 i (i =  (cid:1835)  (cid:1845)   (cid:2204)(cid:2779)(cid:3) (cid:2204)(cid:2778)  (cid:2204)(cid:2170)(cid:2778)(cid:2879)(cid:2778)(cid:3)(cid:2204)(cid:2170)(cid:2778)(cid:3) (cid:1874)(cid:2870)(cid:3) (cid:2204)(cid:2778)  (cid:1874)(cid:3015)(cid:3117)(cid:2879)(cid:2869)(cid:3) (cid:1874)(cid:3015)(cid:3117)(cid:3)  Convolve   ...   (cid:1499)  (cid:1842)(cid:2869)  (cid:1858)(cid:2869)(cid:2869)  (cid:1499)  (cid:1842)(cid:2870)  (cid:1858)(cid:2870)(cid:2869)  (cid:1499)  (cid:1842)(cid:3015)(cid:3117)   (cid:1858)(cid:3015)(cid:3117)(cid:2869)    ...  ...   ...   Pool   (cid:1373)  (cid:1829)(cid:2869)(cid:2869)  (cid:1829)(cid:2869)(cid:2869)  (cid:1373)  (cid:1829)(cid:2870)(cid:2869)  (cid:1829)(cid:2870)(cid:2869)  (cid:1373)  (cid:1829)(cid:3015)(cid:3117)(cid:2869)     (cid:1829)(cid:3015)(cid:3117)(cid:2869)    ...  ...     Convolve   (cid:1499)  (cid:1829)(cid:2869)(cid:2870)  (cid:1858)(cid:2869)(cid:2870)  (cid:1499)  (cid:1829)(cid:3015)(cid:3118)(cid:2870)   (cid:1858)(cid:3015)(cid:3118)(cid:2870)    ...     (cid:1829)(cid:2869)(cid:2869)  (cid:1829)(cid:2870)(cid:2869)  (cid:1829)(cid:3015)(cid:3117)(cid:2879)(cid:2869)  (cid:1829)(cid:3015)(cid:3117)(cid:2869)   (cid:2869)  ...     (cid:1499)  (cid:1829)(cid:2869)(cid:2870) (cid:1829)(cid:1829)(cid:3015)(cid:1829) (cid:3118)(cid:2870) Convolve (cid:1829)(cid:2871)  (cid:1829)(cid:2869)(cid:2870)  (cid:1829)(cid:3015)(cid:3118)(cid:2870)   (cid:1858)(cid:2871)   (cid:1850)   (cid:1877)   Normalized Face   Texture and Shape   Stage 1: Local Kernel  Adaptive Subnetworks   Stage 2: Part    Fusion Subnetworks   Stage 3: Global    Fusion Subnetwork   Logistic   Regression   i=1, multiple local kernel adaptive CNN subnetworks {C 1  Figure 2.|
|||Although the structure is different from the conventional CNN [16], the tree-structure kernel adaptive CNN can also be similarly trained with the back-propagation method.|
|||(9) and (10) can be further simplified as:  C 3  After removing the logistic regression layer of the trained local CNN model, the remaining stacked convolutionpooling modules are used as the first stage of tree-structured CNN.|
|||Optimization  Typically, CNN is trained in a pure supervised manner without unsupervised pre-training.|
|||Although our treestructured kernel adaptive CNN can also be trained with the back-propagation method as aforementioned, in practice, we adopt a more efficient optimization method.|
|||More specifically, the optimization of tree-structured kernel adaptive CNN can be divided into four steps.|
|||First, we train multiple kernel adaptive CNN models with Eqn.|
|||Evaluation on Kernel Adaptation  In order to clarify the effect of kernel adaptation with respect to facial poses, we compare CNN with or without kernel adaptation on MultiPIE database [7], indicated as aCNN and CNN respectively.|
|||(a) A reference face to clarify local patch positions; (b) Result of CNN without kernel adaptation; (c) Result of CNN with kernel adaptation.|
|||Evaluation on Tree-structured Architecture  To evaluate the tree-structured convolutional architecture, we compare our tree-structured CNN, denoted as treeCNN, with the conventional CNN for gender estimation on  e s o P w a Y     45o  30o  15o  0o  +15o  +30o  +45o  1  2  3  4  5  7  8 6 Kernel Index  9  10  11  12  95 90 85 80 75 70  +0.6  +0.4  +0.2  0  0.2  0.4  0.6  88.77   86.59   88.2   89.71   78.97   81.19   Two stages  Three stages  CNN  tree-CNN  tree-a-CNN  Figure 7.|
|||5, the convolutional fusion of local CNN can consistently improve the classification accuracy.|
|||8, the proposed tree-structured CNN outperforms conventional CNN with comparable filter number and the same network depth.|
|||Evaluation on Facial Trait Recognition  Finally, we present comprehensive results on WebFace [26], Morph II [22] and MultiPIE [7] in Table 1 to evaluate the effectiveness of propose shape driven kernel adaptive CNN for facial trait recognition.|
|||Different from age and gender recognition which only use the final output of the network, for face identification task we concatenate the output of local, part and global CNN to constitute a comprehensive representation of the face images.|
||24 instances in total. (in cvpr2015)|
|68|cvpr18-Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition|The OFS-CNN also beats the CNN using multiple filter sizes and more importantly, is much more efficient during testing with the proposed forward-backward propagation algorithm.|
|||While most of the earlier approaches employed handcrafted and general-purpose features; deep learning, especially CNN based methods, has shown great promise in recognizing facial expressions or AUs [7, 24, 19, 15, 9, 12, 34, 17, 30, 21].|
|||CNN-based methods employ predefined and fixed filter sizes in each convolutional layer, which is called the traditional CNN hereafter.|
|||In this work, we propose a novel and feasible solution  5070  in a CNN framework to automatically learn the filter sizes for all convolutional layers simultaneously from the training data along with learning the convolution filters.|
|||In particular, we proposed an Optimized Filter Size CNN (OFSCNN), where the optimal filter size of each convolutional layer is estimated iteratively using stochastic gradient descent (SGD) during the backpropagation process.|
|||During backpropagation, the filter size k will be updated, e.g., decreased when the partial derivative of CNN loss with respect to the filter size is positive, i.e., L  k > 0, and vice versa.|
|||Furthermore, the OFS-CNN also beats a deep CNN using multiple filter sizes with a remarkable improvement in time efficiency during testing, which is highly desirable for realtime applications.|
|||n o i t a g a p o r P d r a w k c a B     Decrease Filter size   Increase Filter size   Figure 2: An overview of the proposed method to optimize the convolution filter size k with the CNN loss backpropagation at the tth iteration.|
|||[17] experimentally compared facial expression recognition performance using different filter sizes and found that the CNN with 55, 44, and 55 filter sizes in the three convolutional layer, respectively, has the best performance on 4242 input images.|
|||In contrast, the proposed OFS-CNN is capable of learning and optimizing the filter sizes for all convolutional layers simultaneously in a CNN learning framework, which is desirable, especially when the CNNs go deeper and deeper.|
|||A Brief Review of CNNs  A CNN consists of a stack of layers such as convolutional layers, pooling layers, rectification layers, fully connected (FC) layers, and loss layers.|
|||In this work, k  R+ is defined as a continuous variable that can be learned and optimized during CNN training.|
|||Therefore, the time complexity does not increase compared with the traditional CNN in the forward training process as well as in the testing process.|
|||Hence, the partial derivative can be calculated as follows:  k  With the chain rule, the derivative of CNN loss w.r.t.|
|||xt  = wt(kt)  ij (kt  (21)  xt  +)  With the chain rule, the derivative of CNN loss w.r.t.|
|||kt, wt(kt xt, based on Eqs.14, 18, and 22, respectively Update kt+1, wt+1(kt+1 Update the bias using standard CNN backpropagation //Transformation: if kt+1 > kt + then kt+1  = kt + kt+1 + = kt + + 2 Expand the upper-bound and lower bound filters wt+1(kt+1  + ), and xt+1 based on SGD  + ) and wt+1(kt+1   ) as in Eq.|
|||All filter sizes are 5  5 in the original cifar10 quick [13] and will be used for the baseline CNN for comparison.|
|||AUs  AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AVE  CNN-Filter3 F1 2AFC 0.577 0.591 0.654 0.754 0.654 0.720 0.786 0.582 0.603 0.676 0.651 0.659  0.315 0.291 0.362 0.677 0.640 0.706 0.749 0.505 0.298 0.547 0.337 0.493  CNN-Filter5 F1 2AFC 0.578 0.573 0.649 0.775 0.658 0.728 0.805 0.597 0.599 0.683 0.658 0.664  0.313 0.277 0.358 0.693 0.643 0.726 0.763 0.517 0.296 0.550 0.348 0.499  CNN-Filter7 F1 2AFC 0.577 0.586 0.653 0.771 0.661 0.720 0.805 0.600 0.611 0.683 0.657 0.666  0.310 0.284 0.361 0.688 0.652 0.716 0.759 0.525 0.306 0.553 0.352 0.501  CNN-Filter9 F1 2AFC 0.583 0.575 0.661 0.773 0.659 0.723 0.791 0.593 0.622 0.678 0.659 0.665  0.315 0.279 0.367 0.689 0.646 0.711 0.750 0.523 0.316 0.544 0.350 0.499  1-layer OFS-CNN  3-layer OFS-CNN  F1  0.320 0.291 0.362 0.685 0.658 0.720 0.768 0.521 0.305 0.532 0.345 0.501  2AFC Converged Size 0.586 0.592 0.661 0.764 0.660 0.725 0.801 0.600 0.609 0.673 0.655 0.666  6.0 5.8 6.0 6.0 6.0 6.0 6.1 5.4 6.0 5.7 6.1 5.9  F1  0.348 0.312 0.376 0.723 0.634 0.739 0.799 0.532 0.300 0.542 0.355 0.515  2AFC Converged Size 0.628 0.626 0.673 0.811 0.652 0.758 0.855 0.635 0.607 0.694 0.659 0.691  5.2, 5.1, 5.1 5.2, 5.3, 4.9 5.1, 5.5, 4.8 5.1, 4.7, 4.7 5.0, 4.8, 4.7 4.6, 5.1, 4.8 4.8, 5.9, 4.8 5.1, 4.6, 4.5 5.2, 4.9, 4.8 4.9, 4.7, 4.6 5.4, 4.6, 4.7 5.0, 5.0, 4.8  Table 2: Performance comparison of the proposed OFS-CNN and the baseline CNN for varying image resolutions on the BP4D database [26] in terms of the average F1 score.|
|||Resolution  6448  128 96  256192  Layer AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AVE  CNN OFS-CNN CNN OFS-CNN CNN OFS-CNN 0.313 0.277 0.358 0.693 0.643 0.726 0.763 0.517 0.296 0.550 0.348 0.499  0.340 0.307 0.411 0.721 0.642 0.718 0.774 0.552 0.331 0.561 0.381 0.522  0.348 0.312 0.376 0.723 0.634 0.739 0.799 0.532 0.300 0.542 0.355 0.515  0.345 0.303 0.415 0.729 0.649 0.754 0.805 0.562 0.337 0.563 0.398 0.533  0.332 0.278 0.324 0.676 0.504 0.690 0.697 0.544 0.323 0.540 0.354 0.478  0.416 0.305 0.391 0.745 0.628 0.743 0.812 0.555 0.326 0.568 0.413 0.537  iteration, where most of the CNN models are converged in our experiments.|
|||This demonstrates that the proposed OFS-CNN is superior to the best CNN model obtained by exhaustive search.|
|||All the CNN models have similar CNN structure as described in Section 4.2.|
|||AUs AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AVE  % GoogLeNet OFS-CNN 128  96 OFS-CNN 256  192 23.1 17.9 22.7 46.0 52.6 59.6 55.8 52.1 18.0 32.6 17.0  0.416 0.305 0.391 0.745 0.628 0.743 0.812 0.555 0.326 0.568 0.413 0.537  0.345 0.303 0.415 0.729 0.649 0.754 0.805 0.562 0.337 0.563 0.398 0.533  0.369 0.267 0.498 0.746 0.657 0.768 0.836 0.503 0.325 0.511 0.376 0.531   Table 5: Performance comparison with the baseline CNN on the DISFA database [20] in terms of the average F1 score and the 2AFC score.|
|||Comparison with the baseline CNN on the DISFA database [20]: As illustrated in Table 5, the proposed OFSCNN also outperforms the baseline CNN with a notable margin in terms of the average F1 score on the DISFA database [20].|
|||Furthermore, the OFS-CNN will be applied to other applications such as object classification or detection as well as various CNN structures.|
||24 instances in total. (in cvpr2018)|
|69|CNN-Based Patch Matching for Optical Flow With Thresholded Hinge Embedding Loss|In this paper, we present a CNN based patch matching approach for optical flow estimation.|
|||Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods.|
|||For instance, it is important that CNN based features are not only able to distinguish between different patch positions, but the position should also be determined accurately.|
|||Furthermore, the top performing CNN architectures are very slow when used for patch matching as it requires matching several patches for every pixel in the reference image.|
|||A first solution to succeed in CNN based patch matching is to use pixel-wise batch normalization [12].|
|||Instead, we improve the CNN features themselves to a level that allows us to outperform existing approaches.|
|||Furthermore, we present a novel way to calculate CNN based features for the scales of Flow Fields [3], which clearly outperforms the original multi-scale feature creation  13250  approach, with respect to CNN based features.|
|||Doing so, an important finding is that low-pass filtering CNN based feature maps robustly improves the matching quality.|
|||A novel multi-scale feature creation approach tailored  for CNN features for optical flow.|
|||The CNN architecture used in our experiments.|
|||A first approach using patch matching with CNN based features is PatchBatch [12].|
|||Recently, several successful CNN based approaches for stereo matching appeared [35, 23, 24].|
|||They tried to solve the optical flow problem as a whole with CNNs, having the images as CNN input and the optical flow  Our approach is based on a Siamese architecture [6].|
|||We observed the same effect for CNN based features  even if the CNN is also trained on the lower resolutions.|
|||We use a CNN trained and applied only on the highest image resolution for the highest and second highest scale.|
|||Furthermore, we use a CNN trained on 3 resolutions (100%, 50% and 25%) to calculate the feature maps for the third and fourth scale applied at 50% and 25% resolution, respectively.|
|||Our modification of feature creation of the Flow Fields approach [3] for clearly better CNN performance.|
|||Comparison of CNN based multi-scale feature creation approaches.|
|||Comparison of CNN based Multi(cid:173)Scale Feature  Map Approaches  In Table 2, we compare the original feature creation approach (Figure 2 left) with our approach (Figure 2 right), with respect to our CNN features.|
|||Starting from a distance between p+ 2 of 9 pixels, CNN based features created on a 2x down-sampled image match more robustly than CNN based features created on the full image resolution.|
|||2 and p  One can argue that by training the CNN with more close2 ) more accuracy could be gained.|
|||Therefore, we use an extra CNN for the highest resolution.|
|||This is contrary to PatchBatch where the GPU based CNN already takes the majority of time (due to pixel-wise normalization).|
|||Also, in final tests (after submitting to evaluation portals) we were able to improve our CNN architecture (see supplementary material) so that it only needs 2.5s with only a marginal change in quality on our validation set.|
||24 instances in total. (in cvpr2017)|
|70|Masi_Pose-Aware_Face_Recognition_CVPR_2016_paper|Regarding methods that trained a CNN to  recognize faces, one approach to obtain pose-invariance is to train a single CNN with a large enough dataset covering a diverse set of poses so that the CNN in principle could learn some degree of pose invariance automatically: FaceNet [25] shows that it is possible to learn a compact embedding for faces with an end-to-end learning system trained on 260 million images.|
|||This work has been extended in [28] to show how the CNN is learning sparse features that implicitly encode attribute informations such as the gender.|
|||For instance, DeepFace [29] learns a CNN on 4 million face images using frontalization technique to reduce pose variability.|
|||in [5] showed that is possible to get compelling results on IJB-A by using a single CNN trained from scratch on a frontal view, fine-tuning it and learning the metric on the target dataset.|
|||[37] showed that a CNN can be used, not only for classification, but to recover and normalize a near-frontal face to a frontal view.|
|||Pose-Aware Face Models for Recognition  Our method assumes that in general the face pose distribution p(p|I), given one image I, is not dominated by nearfrontal faces and thus we propose to learn multiple posespecific CNN models as opposed to a single CNN.|
|||In  4839  Template A  Profile  +75 PAM  +40 PAM Profile PAM  Template B  Pose   Classification  Template  Score   Template  Score   Template  Score   Pooling  Pooling  Pooling  Template  Score   Pooling  Template  Score   Pooling  n o i s u F    e s o P  Same/Not Same  Frontal  0 PAM  Frontal PAM  Figure 2: Given a template pair to verify, pose classification is used to forward each image to the corresponding PoseAware CNN Model.|
|||Considering these two types of alignment, we trained an ensemble of five CNN models, each of which is aware of the face viewpoint by learning specific features for each view.|
|||Our models are called Pose-Aware CNN Models (PAMs) and are learned using the CASIA WebFace dataset [32], which is currently the largest publicly available dataset, containing roughly 500K face images.|
|||Learning Pose-Aware CNN Models (PAMs)  Differently from approaches that use just a single, frontal face reference to train a CNN [32, 29, 5], our idea is to learn Pose-Aware CNN Models (PAMs).|
|||In this way, we are able to partition the CASIA dataset in two different new datasets, that are used to learn two CNN models with in plane alignment namely PAMin-f and PAMin-p.|
|||Another issue is that is very hard to find publicly available datasets containing a large amount of full profile faces in order to learn a discriminative CNN model for a full-profile view.|
|||Since training from scratch a CNN could require millions of annotated images, we learn our Pose-Aware CNNs by fine-tuning state-of-theart CNN models trained on ImageNet.|
|||experiment with a CNN with 8 layers (AlexNet) [15] and one with 19 layers (VGGNet) [4].|
|||We experiment with different network types since we can show that our method is agnostic to the CNN model used and by fusing across pose, we can get improvement, irrespective of the architecture used.|
|||All these CNN models end with fully connected layers fc7 and fc8.|
|||Differently from [32, 29], that use a single, trained-at-once model, we have different CNN models to address a specific view point of the face and a specific alignment.|
|||Pose-Aware Face Recognition  The Pose-Aware CNN models learned in Sect.|
|||We can interpret each CNN as a discriminative classifier explicitly trained at a certain mode of the pose distribution.|
|||1 we report the improvement for each component of PAMs for two types of CNN models that we used (AlexNet and VGGNet).|
|||[30]  0.7330.034 0.5140.060  0.8200.024 0.9290.013            0.5810.054  0.4670.066    0.4110.081    0.37  0.25    0.25    0.72  0.5510.03  0.6940.017  0.7410.017  0.4130.022 0.5710.017  0.6240.018        0.3810.018 0.5590.021  0.6370.025        0.8380.012 0.9240.009  0.9490.006  e t a R   e c n a t p e c c A   e u r T  1  0.8  0.6  0.4 0.2  5     0 10  IJBA Verification (1:1)     Single Model (VGGNet) PAMs (VGGNet) 4 10 False Acceptance Rate  10  10  2  1  3  0 10  10  e t a R   e c n a t p e c c A   e u r T  1  0.8  0.6  0.4 0.2  5     0 10  JANUS CS2 Verification (1:1)  IJBA Identification (1:N)     100  JANUS CS2 Identification (1:N)        100  Single Model (VGGNet) PAMs (VGGNet) 4 10 False Acceptance Rate  10  10  2  3  1  0 10  10     e t a R n o i t i n g o c e R  90  80  70     60 10  0  e t a R   n o i t i n g o c e R  90  80  70     60 10  0  Single Model (VGGNet) PAMs (VGGNet)  1  10 Rank  2  10  Single Model (VGGNet) PAMs (VGGNet)  1  10 Rank  2  10  (a) ROC  (b) CMC  Figure 7: Improvement in the ROC (a) and in the CMC (b) comparing a single CNN and the proposed PAMs on the IJB-A challenge and JANUS CS2.|
|||Our approach is agnostic to the underlying CNN used.|
|||Additional future work consists in learning a better pose fusion and developing a single, multi-pose CNN in a unified framework.|
|||Unconstrained face verification using deep CNN features.|
||24 instances in total. (in cvpr2016)|
|71|Yao_Describing_Videos_by_ICCV_2015_paper|The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior.|
|||Furthermore, 3) we observe that the improvements brought by exploiting global and local temporal information are complimentary, with the best performance achieved when both the temporal attention mechanism and the 3-D CNN are used together.|
|||This has opened a door to a flood of computer vision systems that exploit representations from upper or intermediate layers of a CNN as generic high-level features for vision.|
|||In the case where the input is a video clip, an imagetrained CNN can be used for each frame separately, resulting in a single vector representation vi of the i-th frame.|
|||In our work here, we will also consider using the CNN from [31], which has demonstrated higher performance for object recognition.|
|||However, in their work the feature transformation function t consisted in a simple averaging, i.e.,  Our 3-D CNN architecture is composed of three 3-D convolutional layer, each followed by rectified linear activations (ReLU) and local max-pooling.|
|||Similarly to the object recognition trained CNN (see Sec.|
|||2.2), the 3-D CNN is pre-train on activity recognition datasets.|
|||Exploiting Global Structure:  A Temporal Attention Mechanism  where the vis are the elements of the set V returned by the CNN encoder from Sec.|
|||We use a 3-D CNN to build the higher-level representations that preserve and summarize the local motion descriptors of short frame sequences.|
|||The 3-D CNN features of the previous section allows us to better represent short-duration actions in a subset of consecutive frames.|
|||In contrast to other 3-D CNN formulations, the input to our 3-D CNN consists of features derived from a number of state of the art image descriptors.|
|||In this paper, we use a state-of-the-art static convolutional neural network (CNN) and a novel spatio-temporal 3-D CNN to model input video clips.|
|||In contrast to other approaches such as [11], which have explored CNNRNN coupled models for video description, here we use an attention mechanism, use a 3-D CNN and focus on opendomain video description.|
|||Video Preprocessing To reduce the computational and memory requirement, we only consider the first 240 frames of each video 3 For appearance features, (trained) 2D GoogLeNet [31] CNN is used to extract fixed-length representation (with the help of the popular implementation in Caffe [16]).|
|||We also apply the spatio-temporal 3-D CNN (trained as described in Sec.|
|||When using 3-D CNN without temporal attention, we simply use the 2500-dimensional activation of the last fully-connection layer.|
|||Those vector are contatenated with the 2D CNN features resulting in 26 feature vectors with 1376 elements.|
|||Is it based on an encoder using the 2-D GoogLeNet CNN [31] as discussed in Section 2.2 and the LSTM-based decoder outlined in Section 2.3.|
|||Enc-Dec + Local incorporates local temporal structure via the integration of our proposed 3-D CNN features (as outlined in Section 3.1) with the 2-D GoogLeNet CNN features as described above.|
|||Finally, EncDec + Local + Global incorporates both the 3-D CNN and the temporal attention mechanism into the model.|
|||3.1, the 3D CNN was trained on activity recognition datasets.|
|||Due to space limitation, details regarding the training and evaluation of the 3-D CNN on activity recognition datasets are provided in the Supplementary Material.|
|||As for the 3-D CNN local temporal features, we see that they allowed to correctly identify the action as frying, as opposed to simply cooking.|
||24 instances in total. (in iccv2015)|
|72|cvpr18-Kernelized Subspace Pooling for Deep Local Descriptors|To further increase the discriminative power of our descriptor, we propose a simple distance kernel integrated to the marginal triplet loss that helps to focus on hard examples in CNN training.|
|||Finally, we show that by combining SP with the projection distance metric [13], the generated feature descriptor is equivalent to that of the Bilinear CNN model [22], but outperforms the latter with much lower memory and computation consumptions.|
|||Together with the great success of deep CNN in various vision related tasks, it has recently been demonstrated that patch matching using CNNs can significantly improve the matching performance accuracies [42, 14, 30, 3, 20, 41, 34, 9].|
|||In this paper, we propose a novel CNN pooling method, namely Subspace Pooling (SP) to learn highly invariant and discriminative feature descriptors.|
|||The proposed SP method is differentiable and thus can be readily applied to a given CNN using the standard Back Propagation (BP) training method.|
|||In summary, our contributions include: (1) we propose a novel CNN subspace pooling method that can remarkably improve the patch matching accuracy; (2) we show that the SP method is invariant to all geometric changes that can be expressed as the column permutations of the matrix F ; (3) to increase its discriminative power, we further proposed a distance kernel integrated to the marginal triplet loss which is helpful to focus on hard examples in CNN training; (4) we finally show that our SP method combined with the projection distance metric [13] is equivalent to the Bilinear CNN, but outperforms the latter with much lower memory and computation consumptions.|
|||Preliminaries  A typical CNN consists of mainly three building blocks: convolution, active function, and pooling fuction.|
|||The conventional convolution operation in CNN uses a fixed size kernel to extract features in a sliding window fashion.|
|||Formally, we write the CNN features of an input patch as a matrix F  Rmp.|
|||The pooled CNN features obtained in this manner are r-dimensional linear subspaces of the m-dimensional Euclidean space, which lie on the (m, r) Grassmann manifold [13, 37], denoted by Gr m. Limiting r to be smaller than p and m has two reasons.|
|||F is  U(cid:1)2(cid:1) with (cid:0) l U(cid:1)1  U(cid:1)1(cid:12)(cid:12)(cid:0) l  triplet loss which is helpful to focus more on the hard examples during CNN training.|
|||We will show later that the form Y1Y T 1 is also related to the bilinear CNN model.|
|||Connection to the Bilinear CNN model  Recently, the bilinear CNN model [22] has yielded impressive performance on a range of visual tasks.|
|||The bilinear CNN model forms a descriptor as  b(F1, F2) = F1F T 2 .|
|||The bilinear CNN model incorporates two feature extractors, which can increase its discriminative power.|
|||Another drawback of the bilinear CNN is that the produced descriptor is of high dimensionality.|
|||We show that the performance of our method with a single CNN is comparable or superior to the bilinear CNN on several patch matching benchmarks, and a produced descriptor with much lower dimensionality.|
|||We also compare our methods to the original bilinear CNN model (BILINEAR).|
|||performance to the bilinear CNN but uses much lower dimensionality.|
|||The results of earlier CNN based methods DCS2S, D-DESC and TF-M are remarkably better than handcrafted descriptors SIFT and RSIFT in the presence of low numbers of distractors.|
|||the orthonormal bases is to reduce the effect of small useless variations in the CNN features.|
|||The proposed subspace pooling method is applicable to a given CNN architecture to increase its invariant ability.|
|||The proposed SP, as well as the max pooling, average pooling and bilinear CNN, do not use any spatial information of the CNN features which makes it convenient to handle complex transforms that do not have a parametric model, e.g., the nonrigid deformation.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||24 instances in total. (in cvpr2018)|
|73|Fernando_Discriminative_Hierarchical_Rank_CVPR_2016_paper|As richer features are used to describe the input frames (e.g., CNN features) and more complex video sequences are considered, a single level may no longer be sufficient for good performance.|
|||Our proposed method is useful for encoding dynamically evolving frame-based CNN features, and we are able to show significant improvements over other effective temporal encoding methods.|
|||Temporal max pooling and sum pooling are used with bag-of-features [32] as well as CNN features [23].|
|||Temporal fusion methods such as late fusion or early fusion are used in [13] in the context of CNN architectures.|
|||However, our method captures mid-level dynamics as well as dynamics of the entire video using hierarchical rank pooling principle which is suited for rich feature representations such as deep CNN features.|
|||As such, we propose a more powerful scheme for encoding the dynamics of CNN features from a video sequence.|
|||,   be the sequence of CNN feature vectors describing each frame in the input video.|
|||Typically the rectification applied  in CNN architectures keeps only the positive activations, i.e., () = max{0, }.|
|||Therefore, we propose to use the following non-linear function on the activations of fully connected layers of the CNN architecture.|
|||SER has the same property but specifically tailored for CNN activation to exploit negative activations as well.|
|||,  for each rank pooling layer, l = 1 :   1 do  extract CNN features,  (1) = generate transformed sub-sequences   () = (  (l) get video representation as (  ())  rank pool sub-sequences,  construct  (+1) as .|
|||Experimental details: Our primary focus is to evaluate activity recognition performance using CNN features and hierarchical rank pooling.|
|||Comparing temporal pooling methods  In this section we compare several temporal pooling methods using VGG-16 CNN features.|
|||To obtain a representation for average pooling, the average CNN feature activation over all frames of a video was computed.|
|||The max-pooled vector is obtain by applying the max operation over each dimension of the CNN feature vectors from all frames of a given video.|
|||Interestingly, on deep CNN features, Chi-square Kernel is more effective than SSR.|
|||[34]  59.4 65.9 Methods without CNN features 65.4 63.7 60.8 66.8 56.4 57.2  68.0 73.7 73.6  Lan et al.|
|||In this experiment we combine CNN features which are encoded using hierarchical rank pooling with trajectory features [33] which are encoded with Fisher vectors [22] and rank pooling [4].|
|||To obtain state-of-the-art results, we encode trajectory features [33] using rank pooling and combine them with CNN features encoded with hierarchical rank pooling.|
|||We use average kernel method to combine CNN features with trajectory features.|
|||The combination of rank pooled trajectory features (HOG+HOF+MBH) with hierarchically rank pooled CNN features gives a significant improvement.|
|||We combine rank pooled trajectory features  (HOG+HOF+MBH) with CNN features encoded with our method.|
|||Exploiting image-trained CNN architectures for unconstrained video classification.|
||23 instances in total. (in cvpr2016)|
|74|Fu_Relaxing_From_Vocabulary_ICCV_2015_paper|Traditional CNN Model  The first several layers of the traditional CNN model are convolutional and the remaining layers are fully-connected.|
|||Therefore, we combine the softmax regression of a traditional CNN with the trace optimization.|
|||1988  Algorithm 1 Weakly-Supervised CNN model  Input: Noisy Web Training Images: X = [x1, ..., xN ]  Initial Parameters W = [W(1), ..., W(M )] Rectified Linear Activation Function: f ()  Procedure: Repeat: Forward Propagation:  Implementing as the traditional CNN  Backward Propagation: 1.|
|||The complete algorithm of our weakly-supervised CNN model is in Algorithm 1.|
|||Baselines: We denote the proposed method as noiserobust CNN (NRCNN).|
|||The common four baselines are:   CNN: the state-of-the-art CNN network with convolutional layers and fully-connected layers.|
||| RPCA+CNN: before the CNN training, we reconstruct each training sample by RPCA [3] and remove those samples with large reconstruction error.|
||| NL+CNN: we reproduce the additional bottom-up noise-adaption layer in [21], and combine this layer with CNN network.|
|||Results: First of all, we adjusted the weight decay value of the basic CNN model, i.e.,  in Eqn.|
|||We found that the above parameters can make the basic CNN model achieve the best result on both datasets.|
|||We found that the traditional CNN dropped by nearly 20% in CIFAR-10 with 30% noises.|
|||First, we kept the positive/negative  3We  used  the  cifar10 quick train test  network  in Caffe  (caffe.berkeleyvision.org) as the baseline CNN model in this task.|
|||samples with the same number of that in VOC2007 and denoted methods on this training set as CNN (Web) and NRCNN (Web).|
|||Second, we increased the positive samples to 4 times of that in VOC2007 for each category, and denoted methods on this training set as CNN (Web4) and NRCNN (Web4).|
|||We can draw the following conclusions:   CNN (Web) can surpass over Web HOG with a significant gain, which demonstrates a stronger noise-robust ability of deep learning methods than the method using human-crafted features on noisy training data.|
|||However, the traditional CNN model cannot achiever the comparable result with ours.|
|||Besides, we found that the proposed model took 1.34 times time-cost of the standard CNN model with the 128 batch size.|
|||For the all 200 categories, we can only use the ImageNet dataset to train a CNN model on the 150 existing categories, with 1, 000 clean ImageNet training images for each category.|
|||We denote this method as CNN (ImageNet).|
|||Besides, CNN (ImageNet) is inferior to our method, because of its limited vocabulary.|
|||CNN (Web) RPCA+CNN (Web) CAE+CNN (Web) NL+CNN (Web) CNN (ImageNet) DeViSE (ImageNet)[7] NRCNN (Web)  NDCG@1  NDCG@3  NDCG@5  0.08 0.18 0.26  0.23 0.32 0.39  0.11 0.25 0.34  0.24 0.33 0.41  0.20 0.29 0.39  0.28 0.36 0.43  0.32 0.41 0.46  Table 4: The tagging performance in terms of Similarity@K trained by different models and different vocabulary sets.|
|||We observe that our model can achieve better results from Similarity@5 to Similarity@20, than the traditional CNN model, which is implemented by the released Alexs network on the 1,000 vocabulary set.|
|||Our model can predict a wide range of tags, and achieve a significant improvement with the gain of 14.0% in terms of Similarity@20, against the CNN model on the 1,000 vocabulary set.|
||23 instances in total. (in iccv2015)|
|75|Wang_RAID-G_Robust_Estimation_CVPR_2016_paper|Based on deep CNN features [9, 43], Cimpoi et al.|
|||[12] proposed a state-of-the-art texture descriptor called FV-CNN, which modeled outputs of convolutional layer from pre-trained deep CNN with Fisher vector (FV) coding [40].|
|||The reason may be that, for an input image, usually only a very small number of deep CNN features are available, the dimensions (512 or higher) of which are inherently much higher than those of the traditional ones, making robust estimation of covariances difficult.|
|||Different from them, we extend Gaussian descriptors to RKHS and employ the deep CNN descriptors as original features.|
|||Note that the Gaussian descriptors with deep CNN features in the original space is our baseline.|
|||Given a set X of features (deep CNN features in our case) in the original feature space R, we map x into a RKHS, denoted by H, by some mapping function  : R  H, x (cid:11) (x), where () is a function of much higher dimension or even infinite dimension.|
|||To our best knowledge,  we are among the first who apply the Hellingers and  2  kernel mappings to CNN features.|
|||We adopt deep CNN features as the original features.|
|||Note that we have not seen previous attempts which use very high-dimensional CNN features in the context of covariance or Gaussian descriptors.|
|||Given an input image, we extract CNN features in multiscale setting by rescaling the image by some factors (e.g.|
|||For the original CNN features, we first map them into RKHS through Eq.|
|||Computational complexity of RAID-G  Given CNN features extracted by CPU or GPU, RAIDG mainly includes three steps, namely, feature mappings (Eq.|
|||We use a single GeForce GTX Titan Black GPU to extract deep CNN features which can process about 20 images per second.|
|||The following methods are evaluated in our experiments: COV-CNN, Gau-CNN and RoG-CNN which respectively indicates covariance descriptor, Gaussian descriptor with LW estimator, Gaussian descriptor with the proposed vNMLE method, all using CNN features in the original space R; RAID-G-CNN-Hel and RAID-G-CNN-Chi which indicate Gaussian descriptor with our vN-MLE method, both  4437  Methods COV-CNN Gau-CNN RoG-CNN RAID-G-CNN-Hel RAID-G-CNN-Chi  FC [12] FV-CNN [12] FC + FV-CNN [12]  State-of-the-art I State-of-the-art II  FMD  UIUC Material  KTH-TIPS 2b  DTD  Open Surfaces  80.2  1.1 81.3  1.4 83.6  1.6 84.4  1.3 84.9  1.4 77.4  1.8 79.8  1.8 82.4  1.5 60.6 [42]  80.5  3.6 81.7  2.9 84.5  1.8 85.7  2.1 86.3  2.9 75.9  2.3 80.5  2.7 82.6  2.1 60.1 [18]  66.5  1.5 [4]  66.6  3.1 [22]  76.7  2.8 77.5  2.4 79.5  1.5 80.4  1.2 81.3  1.6 75.4  1.5 81.8  2.5 81.1  2.4  70.1  1.2 70.5  1.5 73.9  1.1 75.8  1.4 76.4  1.1 62.9  0.8 72.3  1.0 74.7  1.0  55.0 55.7 58.9 60.3 61.1 43.4 59.5 60.9  70.7  1.6 [16] 77.3  2.3 [11]  61.2  1.0 [40] 66.7  0.9 [11]  39.8 [40]   Table 2.|
|||using CNN features mapped to RKHS via the mapping functions  and h, respectively.|
|||Comparison with state-of-the-arts From Table 2, we see that RAID-G performs much better than deep CNN features based methods [16, 11, 4], and has clear advantages over FC, in which fully-connected (FC) layer outputs of deep networks are fed to SVM for classification.|
|||We adopt a linear SVM to RAID-G with both hand-crafted features and deep CNN features.|
|||Combination of the CNN features with the methods in [23, 20] will be computationally prohibitive (see complexity analysis therein).|
|||The methods in [23, 20] are slightly better than RAIDG. However, if employing high dimensional deep CNN features, RAID-G achieves more than 7% improvements over infinite dimensional covariance descriptors [23, 20], where CNN features cannot be used due to unaffordable cost (see complexity analysis therein).|
|||The gains over [17, 23, 20] in Table 4 and Table 5 demonstrate that RAID-G can flexibly handle high dimensional deep CNN features while bringing significant performance improvement.|
|||Note that we do not use any part detection methods, bounding box and fine-tuning of CNN models.|
|||[26] where segmentation, alignment and part models based on eight-layer CNNs are exploited, and [33] where an eight-layer CNN and sixteen-layer CNN are combined.|
|||Bilinear CNN mod [13] M. J. Daniels and R. E. Kass.|
||23 instances in total. (in cvpr2016)|
|76|Karianakis_An_Empirical_Evaluation_CVPR_2016_paper|Proposal mechanisms aim to remove nuisance variability due to position, scale and aspect ratio, leaving a Category CNN to classify the resulting bounding box as one of a number of classes it is trained with.|
|||Put differently, rather than computing the posterior distribution2 with nuisance transformations automatically marginalized, the CNN is used to compute the conditional distribution of classes given the data and a sample element that approximates the nuisance trans 1The region of the image the objects projects onto, often approximated  by a bounding box.|
|||When a CNN is tested on a proposal r  x determined by a refit computes p(c|x|r ) (x restricted to r), which is erence frame xr, an approximation of p(c|x, gr).|
|||Now, if a CNN was an effective way of computing the marginals with respect to nuisance variability, there would be no benefit in conditioning and averaging with respect to (inferred) nuisance samples.|
|||Should the converse be true, i.e., should averaging conditional distributions restricted to proposal regions outperform a CNN operating on the entire image, that would bring into question the ability of a CNN to marginalize nuisances such as translation and scaling or else go against the DPI.|
|||If we interpret the CNN restricted to a bounding box as a function that maps samples of the location-scale group to class-conditional distributions, where the proposal mechanism down-samples the group, then classical sampling theory [38] teaches that we should retain not the value of the function at the samples, but its local average, a process known as anti-aliasing.|
|||To test the effect of such anti-aliasing on a CNN absent the knowledge of ground truth object location, we follow the methodology and evaluation protocol of [16] to develop a domain-size pooled CNN and test it in their benchmark classification of wide-baseline correspondence of regions selected by a generic low-level detector (MSER [32]).|
|||Our third contribution is to show that this procedure improves the baseline CNN by 515% mean AP on standard benchmark datasets (Table 3 and Fig.|
|||Now, if averaging the conditional class posteriors obtained with various sampling schemes should improve overall performance, that would imply that the implicit marginalization performed by the CNN is inferior to that obtained by sampling the group, and averaging the resulting class conditionals.2 This is indeed our observation, e.g., for VGG16, as we achieve an overall performance of 8.02%, compared to 13.24% when using the whole image  4443  Method  Whole image  Ground-Truth Bounding Box (GT)  AlexNet  19.96 20.41  VGG16  13.24 12.44  GT padded with 10 px  Ave-GT, 4 domain sizes (padded with [0,30] px) Ave-GT, 8 domain sizes (padded with [0,70] px)  17.66 15.96 14.43  17.65 16.00 14.22  10.91 9.65 8.66  10.30 8.90 7.84  Isotropically Anisotropically  Isotropically Anisotropically  Table 1.|
|||Our investigation targets selecting a very small subset of the most discriminative candidates among generic object proposals, while building on popular CNN models.|
|||Finally we rescale the downsampled region to the CNN input.|
|||For a detected scale  at each MSER, the DSP-CNN samples D domain sizes within a neighborhood [1, 2] around it, computes the CNN responses on these samples and averages the posteriors.|
|||5 (center) and Table 3 we show comparisons between CNN and DSP-CNN for layer-3 and layer-4 representations and demonstrate 7.7% and 5.0% relative improvement.|
|||1 3 = P A m  (    N N C  P S D h     t i     w P A  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  DSPCNN vs. CNN on Oxford dataset  )  % 7 0  .|
|||3 5 = P A m  (    N N C  P S D h     t i     w P A  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  AP with CNN (mAP = 27.07%)  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  DSPCNN vs. CNN on Fischer dataset  DSPCNN vs. DSPSIFT on Fischer dataset  )  .|
|||% 4 7 3 5 = P A m     (    N N C  P S D h     t i     w P A  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  AP with CNN (mAP = 50.55%)  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  AP with DSPSIFT (mAP = 53.72%)  Figure 5.|
|||Head to head comparison between CNN and DSP-CNN on the Oxford [33] (left) and Fischers [16] (center) datasets.|
|||The DSP-CNN outperforms its CNN counterpart in terms of matching mAP by 15.1% and 5.0%, respectively.|
|||Given the inherent high-dimensionality of CNN layers, we perform dimensionality reduction with principal component analysis to investigate how this affects the matching performance.|
|||To test the hypothesis in the fairest possible setting, we have kept all these choices constant while comparing a CNN trained, in theory, to marginalize the nuisances thus described, with the same applied to bounding boxes provided by a proposal mechanism.|
|||Of course, like any universal approximator, a CNN can in principle capture the geometry of the discriminant surface by learning away nuisance variability, given sufficient resources in terms of layers, number of filters, and number of training samples.|
|||So in the abstract sense a CNN can indeed marginalize out nuisance variability.|
|||This leaves researchers the choice of investing more effort in the design of proposal mechanisms [18, 36], subtracting duties from the Category CNN downstream, or invest more effort in scaling up the size and efficiency of learning algorithms for general CNNs so as to render the need for a proposal scheme moot.|
||23 instances in total. (in cvpr2016)|
|77|Qin_Joint_Training_of_CVPR_2016_paper|Joint Training of Cascaded CNN for Face Detection  Hongwei Qin1  2  ,  Junjie Yan3  ,  4 Xiu Li1  ,  2 Xiaolin Hu3  1Grad.|
|||In this paper, we propose joint training to achieve end-to-end optimization for CNN cascade.|
|||We show that the back propagation algorithm used in training CNN can be naturally used in training CNN cascade.|
|||We present how jointly training can be conducted on naive CNN cascade and more sophisticated region proposal network (RPN) and fast R-CNN.|
|||For example, two other detection pipelines DPM [6] and CNN [16] can both use cascade for acceleration.|
|||In this paper, we show that in CNN based cascade detection, other than enjoying the advantages in efficiency as traditional cascade, different stages in the cascade can be jointly trained to achieve better performance.|
|||We show that the back propagation algorithm used in training CNN can be naturally used in training CNN cascade.|
|||Joint training can be conducted on naive CNN cascade and more sophisticated cascade such as region proposal network (RPN) and fast RCNN.|
|||We show that the jointly trained cascade CNN as well as the jointly trained RPN and fast R-CNN can achieve lead 3456  ing performance on face detection.|
|||Then we present how to jointly train naive CNN cascade in section 4 and how to jointly train RPN and Fast RCNN in section 5.|
|||Early CNN based methods  Face detection, as well as MNIST OCR recognition, are two tasks where CNN based approach achieve success in 1990s.|
|||In [26], CNN is used in a sliding window manner to traverse different locations and scales and classify faces from the background.|
|||In [22], CNN is used for frontal face detection and shows quite good performance.|
|||These methods are quite similar to modern CNN methods and get relatively good performance on easy datasets.|
|||Modern CNN based methods  In recent two years, CNN based methods show advantages in face detection.|
|||[32, 20] use boosting and DPM on top of CNN features.|
|||[5] fine-tune CNN model trained on 1000-way ImageNet classification task for face [34] uses fully convolubackground classification task.|
|||Cascaded CNNs  The cascaded CNN for face detection in [16] contains three stages.|
|||RPN + fast R(cid:173)CNN  In faster R-CNN, the authors use one CNN called Region Proposal Network (RPN) for generating proposals, the other CNN called fast R-CNN [9] for detection.|
|||We set  = 1 in our experiment, this is appropriate for  all three separate CNN networks.|
|||This is better than Cascaded CNN result (85.7%) reported in [16].|
|||Recent CNN algorithms are getting faster on high-end GPUs.|
|||We show that the back propagation algorithm used in training CNN can be naturally used in training CNN cascade.|
||23 instances in total. (in cvpr2016)|
|78|Using Ranking-CNN for Age Estimation|Basic CNNs are initialized with the weights of a pre-trained base CNN and fine-tuned with the ordinal age labels through supervised learning.|
|||Each basic CNN in ranking-CNN can be trained using all the labeled data, leading to better performance of feature learning and also preventing overfitting.|
|||[37] trained a deeper CNN for extracting features from different layers.|
|||Convolutional Neural Networks  There are numerous kinds of CNN models developed in deep learning.|
|||In text classification, CNN architectures have been widely adopted and achieved superior outcomes [18].|
|||To build more effective CNN models, several new components were introduced: activation unit such as rectified linear unit (ReLU) [28] helps to accelerate the convergence during training and has a positive influence on the performance [19]; regularizer like dropout prevents overfitting by setting some activation units to zero in a specific layer [33]; and batch normalization allows the use of much higher learning rates to make training faster and to improve performance [16].|
|||The fully connected layers in the binary CNN first flatten the features obtained in the previous layers and then relate them to a binary prediction.|
|||2, a basic CNN has three convolutional and sub-sampling layers, and three fully connected layers.|
|||A basic CNN in ranking-CNN differs from the softmax multi-class classification approach in the output layer.|
|||Since the VC dimension d of a softmax output CNN is greater than that of a basic CNN presented in Fig.|
|||(25) for a CNN with softmax output layer.|
|||2, it is derived from a simplified version of the ImageNet CNN [19] with fewer layers for higher efficiency [25].|
|||The training time for the base CNN with the selected 3 + 3 architecture is around 6 hours.|
|||Totally, it takes about 30 hours to pre-train the base CNN and fine-tune 50 basic CNNs.|
|||Multiclass CNN is commonly used for age estimation [25, 39],  5188  Table 2.|
|||Its structure is similar to a basic CNN (three convolutional and pooling layers and three fully connected layers) with the exception that the last fully-connected layer contains multiple outputs corresponding to the number of ages to be classified instead of the binary ones.|
|||Note the multi-class CNN represents the commonly used CNN-based age estimation methods [25, 39].|
|||The exactly same set of samples with multi-class labels are used to train multi-class CNN and SVM, respectively.|
|||Basically, we have three sets of features: engineered  In Table 3, we compare ranking-CNN with the most recent age estimation models, i.e., Ordinal Regression with CNN (OR-CNN), Metric Regression with CNN (MR-CNN) [29] and Deep EXpectation (DEX) [31].|
|||Numbers #1 to #8 correspond to eight compared models in the sequence of: RANKING-CNN, RANKING-CNN FEATURE+RANKING-SVM, ST+RANKING-SVM, BIF+OLPP+RANKINGSVM, MULTI-CLASS CNN, CNN FEATURE+SVM, ST+SVM and BIF+OLPP+SVM.|
|||#1 RANKING-CNN #2 RANKING-CNN FEATURE  +RANKING-SVM  #3 ST+RANKING-SVM #4 BIF+OLPP+RANKING-SVM #5 MULTI-CLASS CNN #6 CNN FEATURE+SVM #7 ST+SVM #8 BIF+OLPP+SVM  #1  NAN  #2 1  6.36e148  NAN  #3 1  1  0 0 0  0 0  0.14  4.12e276  8.90e184  NAN  1.79e135  1 1  #4 1  1  1  NAN  1 1  #5 1  0.85  0 0  NAN  5.43e24  0 0  0 0     1.94e121 4.56e90  2.00e4  0.18  0 0  #6 1  1  0 0 1  NAN  0 0  #7 1  1  1  #8 1  1  1  0.99  0.81  1 1  NAN 0.99  1 1  3.66e6  NAN  our results clearly illustrated the remarkable improvement of using ranking-CNN for age estimation.|
|||From the table, Ranking-CNN Feature+Ranking SVM and the Multi-Class CNN tied for the second place, followed by CNN Feature+SVM.|
|||Ordinal regression with multiple output cnn for age estimation.|
||23 instances in total. (in cvpr2017)|
|79|Self-Supervised Learning of Visual Features Through Embedding Images Into Text Topic Spaces|Illustrated texts are thus ubiquitous in  These authors contributed equally to this work  Figure 1: Our CNN learns to predict the semantic context in which images appear as illustration.|
|||Then we use this semantic level representation as the supervisory signal for CNN training.|
|||Surprisingly, the textual modality has been ignored until now in selfsupervised methods for CNN training.|
|||We claim that it is feasible to learn discriminative features by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration.|
|||By training a CNN to directly project images into a textual semantic space, our method is not only able to learn visual features from scratch without a large annotated dataset, but it can also perform multi-modal retrieval in a natural way without any extra annotation or learning efforts.|
|||[1] make use of egomotion information obtained by odometry sensors mounted on a vehicle to pre-train a CNN model.|
|||In this paper we explore a different modality, text, for self-supervision of CNN feature learning.|
|||Our CNN model, by learning to predict the semantic context in which images appear as illustrations, learns generic visual features that can be leveraged for other visual tasks.|
|||TextTopicNet  In order to train a CNN to predict semantic context from images (TextTopicNet) we propose a two-fold method: First, we learn a topic model on a text corpus of a dataset composed by pairs of correlated texts and images (i.e.|
|||Second, we train a deep CNN model to predict text representations (topic-probabilities) directly from the image pixels.|
|||Training a CNN to predict semantic topics  We train a CNN to predict text representations (topic probability distributions) from images.|
|||Our intuition is that we can learn useful visual features by training the CNN to predict the semantic context in which a particular image is more probable to appear as an illustration.|
|||One is the 8 layers CNN CaffeNet [18], a replication of the AlexNet [21] model with some differences (it does not train with the relighting data-augmentation, and the order of pooling and normalization layers is switched).|
|||The other architecture is a 6 layers CNN resulting from removing the 2 first convolutional layers from CaffeNet.|
|||Still we observe that the most representative images for each topic present some regularities and thus allow the CNN to learn discriminative features, despite the noise introduced by other images that appear in articles from the same topic.|
|||Furthermore, it can be easily extended to an image annotation or captioning system by leveraging the common topic space in which text and images can be projected by the LDA and CNN models.|
|||With this spirit we propose the use of TextTopicNet as a convolutional feature extractor and as a CNN pre-training method.|
|||Experiments  sification  In this experiment we evaluate how good are the learned visual features of the 6 layer CNN (CaffeNet) for image classification when trained with the self-supervised method explained in Section 3.|
|||Following [25] we extract features from top layers of the CNN and train one vs. rest linear SVMs for image classification in PASCAL VOC2007 dataset.|
|||The purpose of this experiment is to gain insight in what our CNN has learned to detect.|
|||Comparison to unsupervised pre(cid:173)training and  semi(cid:173)supervised methods  In this experiment we analyze the performance of TextTopicNet for image classification and object detection by fine-tuning the CNN weights to specific datasets (PASCAL and STL-10) and tasks.|
|||All of these methods use LDA for text representation and CNN features from pre-trained CaffeNet [18], which is trained on ImageNet dataset [5] in a supervised setting.|
|||By considering text found in illustrated articles as  noisy image annotations the proposed method learns visual features by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration.|
||23 instances in total. (in cvpr2017)|
|80|cvpr18-Fusing Crowd Density Maps and Visual Object Trackers for People Tracking in Crowd Scenes|(8)  After we obtain , the response map for a candidate region z can be computed in frequency domain from  f (z) = k  xz   ,  (9)  5355  S-KCF filters  of frame T-1  Current frame T  Location of Location of  frame T-1  Input  Image patch  Response map  Fusion neural network  Crop  Visual tracker  Target detection Target detection  F i Fusion map  Output  Density map  Fusion  CNN  Location of   frame T  Update  S-KCF filters   of frame T  The CNN for   density estimation  Crowd density estimation  Figure 2.|
|||The fusion CNN takes in the image patch, S-KCF response map, and crowd density map and produces a refined fused response map, whose maximum value indicates the location of the target.|
|||Crowd Density Map Estimation  The CNN structure for crowd density map estimation is shown in Fig.|
|||4, and loosely follows [30], except that we estimate a high-resolution density map for tracking by using a sliding window CNN to predict the density for each pixel in the tracking image patch.|
|||Similar to [30], we train the CNN using two tasks, density estimation and people counting.|
|||Both tasks share the same CNN feature extraction layers.|
|||The CNN contains 2 convolutional and 5 fully connected layers.|
|||Fusion CNN  The fusion CNN combines the tracker response map, the crowd density map, and the image patch to produce a refined (fused) response map, where the maximum value indicates the target position.|
|||The structure of our fusion CNN is shown in Fig.|
|||In the first stage, we train the fusion CNN in batch mode, where each frame is treated independently.|
|||Two(cid:173)stage Training Strategy for Fusion CNN  Training the fusion CNN requires the response maps generated by KCF, but the KCF model is also updated ac 4.1.|
|||To train the density map CNN on the UCSD dataset, we use 800 frames that are specified for training crowd counting methods, which are distinct from the 1200 frames used for evaluating tracking.|
|||For the PETS2009 dataset, the density map CNN is only trained on the PETS2009 data, while the fusion CNN is fine-tuned from the network learned from the UCSD dataset.|
|||To show the general effectiveness of using density map fusion, we train a separate fusion CNN for each tracker, using the two-stage training procedure (denoted as FusionCNNv2), and evaluate the tracking performance of the fused response map.|
|||To show the effectiveness of two-stage training, we compare with a fusion CNN using only the first stage of training, denoted as FusionCNN-v1.|
|||We implement the fusion CNN using the Caffe [16] framework.|
|||We also compare our CNN fusion method with the  Tracking results P@10 on the PETS2009 dataset are  5358  Table 2.|
|||On UCSD, the running times of fusion CNN with KCF, S-KCF, LCT and DSST are 19, 18, 10, 23 fps.|
|||Comparison of fusion CNN architectures  In this subsection we compare different variations of the fusion CNN architecture.|
|||To evaluate the effectiveness of the three-layer CNN, we tested a two-layer CNN by removing Conv2, and a four-layer CNN by adding a new convolutional layer with 32 3364 filters after Conv1.|
|||Finally, a three-layer CNN is trained only using the realtracking procedure (stage-2 of the two-stage training) to test the effectiveness of our proposed two-stage training strategy.|
|||The three-layer CNN performs better than the two or four layer versions.|
|||We fuse the appearance-based tracker and crowd density map together with a three-layer fusion CNN to produce a refined response map.|
||23 instances in total. (in cvpr2018)|
|81|Vu_Context-Aware_CNNs_for_ICCV_2015_paper|First, we leverage person-scene relations and propose a Global CNN model trained to predict positions and scales of heads directly from the full image.|
|||Second, we explicitly model pairwise relations among objects and train a Pairwise CNN model using a structured-output surrogate loss.|
|||The Local, Global and Pairwise models are combined into a joint CNN framework.|
|||The output of our method (bottom) is obtained from the combination of Local, Global and Pairwise CNN models.|
|||In this work we build on the recent CNN model for object detection [12] and extend it to contextual reasoning.|
|||First, we propose a Global CNN model which we train to predict coarse locations and scales of objects given the full low-resolution image on the input.|
|||Second, we introduce a Pairwise CNN model that explicitly models relations among pairs of objects.|
|||Our final joint model combines Local, Global and Pairwise CNN models (see Figure 1).|
|||The results demonstrate improvements of the proposed contextual CNN model compared to other recent baselines including R-CNN [12] on all three datasets.|
|||Section 3 describes the parts of our contextual CNN model.|
|||[28] applied CNN as a sliding window detector at multiple scales.|
|||The R-CNN model [12] is a combination of a CNN and a support vector machine (SVM) operating on object proposals generated by the selective search [35].|
|||Our Pairwise CNN model incorporates the structuredoutput The idea of combining the structuredprediction objective with neural networks has been explored in [2, 19].|
|||[4] use the dual message passing formulation of the inference task to construct a joint objective of the CNN parameters and the message-passing variables.|
|||[14] shows how to directly combine the structured SVM (SSVM) [31, 34] objective with the procedure of training a CNN for text recognition.|
|||Context-aware CNN model  This section presents main components of our contextual CNN model.|
|||In Section 3.2, we introduce the Global CNN model trained to score object proposals using  2894  the context of the full image.|
|||Differently from R-CNN which deploys the second pass of training using SVM, we use the outputs of CNN to score candidates.|
|||The Global model is a CNN that takes the whole image as input and outputs a score for each cell of a multi-scale heat map.|
|||The input image is isotropically rescaled and zero-padded to fit the standard CNN input of 224  224 pixels.|
|||Except the output layer, the architecture of the Global CNN is identical to our Local model described in Section 3.1.|
|||The Global CNN is trained with SGD, minimizing the sum of C log-loss functions, one per each grid cell c   {1    C},  l(fc(x), yc) = X  y{0,1}  log(1 + exp ((1)yc+y+1 fc, y(x))) ,  (1) where fc(x)  R2 is the output of the network for grid cell c of input image x; yc  {0, 1} is the label indicating the class of the grid cell c: background or head.|
|||The CNN model was first fine-tuned on all region proposals used to train our Local model.|
||23 instances in total. (in iccv2015)|
|82|Fu_Occlusion_Boundary_Detection_CVPR_2016_paper|More specifically, in order to better explore type (i) and (ii) contextual information, our CNN model considers a relatively big image patch L as input, performs reasoning on it, and outputs the state of a relatively small patch S with the same center with L (referred to as L2S).|
|||Previous studies have suggested that  the brain encodes contextual information and biologically inspired deep CNNs have been shown to be powerful for feature extraction and description [19, 26], which have motivated us to learn the internal correlation of an occlusion boundary in local patches using the CNN framework.|
|||[13] combine local image patches and a holistic view in a CNN framework to learn contextual information for human pose estimation.|
|||Motivated by nearest neighbor relationships within a local patch, Ganin and Lempitsky [16] detect edges by learning a 4  4 label feature vector for each patch and matching against a sample CNN output dictionary corresponding to training patches with known annotation.|
|||The Proposed Occlusion Boundary Detector  Firstly, in order to better characterize local contextual correlations in pixel labeling and contextual correlations between regional pixel labeling and surrounding observations, we: (i) consider each individual pixel patch as the unit of interest, and (ii) adopt a CNN to learn and predict a patchs occlusion boundary map based on the observation of a larger patch of pixels with the same center.|
|||Below we first briefly describe the architecture of our structured CNN and then discuss the initial input features/cues used for occlusion boundary detection.|
|||2.1.1 CNN Architecture  We train a CNN using a cross entropy loss function to predict the probability distribution in a small 7  7 patch from a large 27  27 image patch (i.e., M = 7 and N = 27 in our experiments).|
|||The overall CNN architecture is shown in Fig.|
|||The input of our CNN consists of 3 static color channels and 2 temporal channels  243  Figure 3.|
|||Illustration of the CNN architecture and the output of several layers.|
|||The CNN structure can be described by the size of the feature map at each layer as follows: conv1 (32@25*25)  maxp1  (64@11*11)  maxp2  LRN 2  LRN 1  conv2 (2048)  conv3 dropout1  f c2 (2048)  dropout2  f c3 (49), which corresponds to a probabilistic labeling map of size 77.|
|||In our CNN architecture,  the rectified linear units (ReLU s) non-linear active function, f (x) = max(0, x), is followed by all conv and f c layers except f c3.|
|||To this end, we first convert an RGB image to Lab space and consider the gradient magnitude of the Lab maps as three feature maps for the CNN model.|
|||Finally, the input of our CNN model consists of 5 (3 static + 2 motion) feature maps.|
|||The CRF energy is defined as:  E(x) = XiV  i(xi) + X{i,j}E  ij(xij)  (7)  Unary potentials (i())iV are used to encode the data likelihood on individual pixels based on the patch-based probabilistic labeling maps provided by the CNN presented in Sec.|
|||The 3 image and 2 motion cue channels are the CNN input to learn internal correlations around occlusion boundaries and predict probabilistic labeling maps and extract deep contextual features.|
|||2.1 for the motion cues computation and structured CNN framework.|
|||Our structured CNN model is built based on Caffe [24], developed by the Berkeley Vision And Learning Center (BVLC) and community contributors.|
|||The CRF model is then constructed to globally estimate occlusion boundaries for each image using the probabilistic labeling maps and the deep contextual features provided by the learned CNN model.|
|||2 and also demonstrates the  4This operation also prevents CNN from paying too much attention to  the center of the label patch and assigning a high probability to it.|
|||Different CNN Frameworks The way contextual information is explored via local patches is a key factor of the method, since the edge and node potentials in the final CRF framework are related to the contextual information aggregated by CNNs.|
|||5, from which we observe that: (i) the F-measure by S2S is obviously lower than that of other methods since the small input image patches contain much less contextual information and the contextual information of the surrounding  area is ignored; (ii) compared to L2S, the CNN is slightly less effective at extracting discriminative spatial contextual information when learning the mapping from L to its center pixel, each individual pixel located within S, and L itself (L2P, L2SP, and L2L).|
|||We explain these differences as follows: (i) L2P and L2SP: the CNN concentrates more on learning the differences between input samples to binary classify large patches and ignores correlations around occlusion edges within local image patches; (ii) L2L: on the one hand, the fixed training set becomes over sparse when the size of labeling map is too large, in which case the CNN can only learn superficial structural features; on the other hand, contextual correlation between the labeling of pixels and the observations from the surrounding area is not considered; and (iii) L2S: it properly handles the aforementioned issues exhibited in the variants so as to achieve better discriminative structured features.|
||23 instances in total. (in cvpr2016)|
|83|Learning Barycentric Representations of 3D Shapes for Sketch-Based 3D Shape Retrieval|[18] proposed the deep panoramic representation for 3D shape retrieval, where the CNN is applied to the panoramic representation of 3D shapes for learning deep shape features.|
|||Based on the CNN features of 2D projections of 3D shapes, Bai et al.|
|||In [23], by performing a max view-pooling operation, multi-view CNN is proposed to learn a compact shape feature from multiple projections of 3D shapes.|
|||We employ two deep CNNs to extract the CNN features of sketches and 2D projections.|
|||The Wasserstein barycenters of CNN features of 2D projections can then be computed to characterize 3D shapes.|
|||In subsection 3.1, we propose barycentric representations of projections of 3D shapes in the CNN feature space.|
|||Once 2D projections of 3D shapes are obtained, we In can extract the deep CNN features of 2D projections.|
|||this work, we employ the AlexNet [11] for the CNN features, which consists of five convolutional layers followed by three fully connected layers.|
|||For the shape Si, we denote the deep CNN feature of the projection from view j by xi,j , j = 1, 2,    , V .|
|||Since the Wasserstein distance is defined in the space of probability distributions, we need to guarantee each element xi,j(l) in the CNN feature xi,j to be xi,j(l)  0 and Pl xi,j(l) = 1.|
|||Given the normalized CNN feature xi,j  RL1 + , j = 1, 2,    , V , the isotropic Wasserstein barycenters hi  RL1 of these features can be obtained as:  argminhi  V  X  j=1  1 V  D(hi, xi,j)  (7)  where D(hi, xi,j) is the entropy regularized Wasserstein distance between hi and xi,j .|
|||Cross(cid:173)domain matching with learned Wasser(cid:173)  stein barycenters  In this subsection, we present to learn the Wasserstein barycenters of the deep CNN features of 3D shapes for sketch-based 3D shape retrieval.|
|||As mentioned in Section 3.1, we employ two AlexNets to extract deep CNN features of 2D projections and sketches, respectively.|
|||By rendering 3D shapes at multiple views, we extract deep CNN features of 2D projections.|
|||The Wasserstein barycenters of the deep CNN features are computed to represent 3D shapes.|
|||In the view network, the Wasserstein barycenters hi (the input of the metric network) are computed from the deep CNN features xi,j (the final fully connected layer) of multiple views.|
|||Input: CNN features xi,j ; view number V ; kernel matrix K; gradient J1(1,2) Output: gradient J1(1,2) Initialize a1 For t = 1, 2,    , Q: 1.|
|||In the sketch and view networks, the deep CNN features are extracted from the fc7 layer of the AlexNet, whose feature size is 4096.|
|||Evaluation of the proposed method  In order to demonstrate the effectiveness of the proposed method, we compare the proposed method to the max viewpooling operation of CNN features from multiple views on the SHREC14 benchmark dataset.|
|||In [23], the authors proposed a multi-view CNN structure for 3D shape retrieval.|
|||We perform the same max view-pooling operation across views in our view network to obtain the deep CNN features to represent 3D shapes.|
|||We compare our learned Wasserstein barycentric representation method to the max view-pooling operation in  5073  the multi-view CNN on the SHREC14 benchmark dataset for sketch-based shape retrieval.|
|||The Wasserstein barycenters of the deep CNN features of multiple projections are computed to represent 3D shapes.|
||23 instances in total. (in cvpr2017)|
|84|Kripasindhu_Sarkar_Learning_3D_shapes_ECCV_2018_paper|Keywords: CNN on 3D Shapes, 3D Shape Representation, ModelNet, Shape Classification, Shape Generation  1  Introduction  Over the last few years, Convolutional Neural Networks (CNNs) have completely dominated in solving vision based problems in 2D images achieving the state-of the-art-results in various domains [10, 26, 7, 3, 4, 19, 12, 6, 18, 11, 15].|
|||3) As a consequence, it provides a highly memory efficient CNN architecture for 3D shapes which is comparable to that of OctNet [20], while being similar in performance.|
||| We present state-of-the-art result on ModelNet benchmark [32] for classifi cation using our multi-view CNN and MLH descriptors.|
|||Multi-Layered Height-maps for 3D shape processing  3  2 Related Work  Core 2D convolution networks AlexNet[10] was the first deep CNN model trained in GPU for the task of classification, and is still often used as the base model or feature extractors for performing other tasks.|
|||Other famous models which are often used as base CNN are VGG [26], GoogLeNet [29], ResNet [7], InceptionV3/V4 [30].|
|||We use the 16 layered VGG [26] as our base CNN model because of its simplicity.|
|||[32] uses deep 3D CNN for voxelized shapes of resolution 303 and provides the classification benchmark dataset of ModelNet40 and ModelNet10.|
|||Finegrained analysis specific to shape classification and 3D CNN have been performed in [17, 24] making them the top performers in shape classification.|
|||In contrast to voxel gird, we use our multi-layer descriptors and use 2D CNN and perform better both in terms of accuracy and computation overhead in the task of shape classification in ModelNet benchmark.|
|||View-dependent rendering methods Image-view based methods take some sort of virtual snapshots (rendering or depth image) of the shape and then design a 2D CNN architecture to solve the task of classification.|
|||A multi-view CNN architecture is then developed to feed 3 such slices (across the 3 canonical axes) for classification.|
|||(Right) Visualization of the corresponding descriptor with k = 3 from 3 different views of X, Y and Z.  excellent alternative of CNN on 3D data.|
|||We show that our one view descriptor of resolution 256 and a simple 2D CNN performs similar to OctNet in terms of classification accuracy and memory requirements.|
|||3.2 Classification networks  Due to the fact that MLH descriptors are multi channel 2D grids, we can directly apply any feed forward 2D CNN with a classification loss (eg.|
|||Each rendered image is passed through a CNN branch separately.|
|||i.e., the CNN branches ci() have different weights.|
|||The CNN branches cis are simple feed forward 2D convolutional networks.|
|||We use one of the popular 2D CNN architectures trained on ImageNet, as we can use the trained weights to initialize our model.|
|||View merging We consider the 3 canonical axes of X, Y and Z as the 3 orthogonal view directions, and perform experiments with the following branch merging operations a) MVCNN type [28, 17] network with shared CNN branch followed by elt max b) independent branch followed by elt max c) our design of independent branch with non-commutative merge concatenation followed by convolution.|
|||Image-view based methods Image-view based methods take virtual snapshotsof the shape and then design a 2D CNN architecture to solve the task of classification.|
|||and design an appropriate CNN architecture to solve the task of classification.|
|||T converge denotes the Time taken for the CNN to Converge in a single GPU.|
|||Gomez-Donoso, F., Garcia-Garcia, A., Garcia-Rodriguez, J., Orts-Escolano, S., Cazorla, M.: Lonchanet: A sliced-based cnn architecture for real-time 3d object recognition.|
||23 instances in total. (in eccv2018)|
|85|Simon_Neural_Activation_Constellations_ICCV_2015_paper|Our part hypotheses are outputs of an intermediate CNN layer for which we compute neural activation maps [29, 30].|
|||Deep neural activation maps are used to exploit the channels of a CNN as a part detector.|
|||As shown by [36], intermediate CNN outputs can often be linked to semantic parts of common objects and we are therefore using them as part proposals.|
|||We demonstrate in our experiments that it even yields a more discriminative CNN compared to a CNN fine-tuned with ground-truth bounding boxes of the object.|
|||They cluster the channels of the last convolutional layers of a CNN into groups.|
|||Therefore, the CNN automatically learned implicit part detectors relevant for the dataset it was trained from.|
|||Each channel of the CNN delivers one neural activation map per image and we therefore obtain one part proposal per channel p. RGB images are handled by adding the absolute activation maps of each input channel.|
|||In addition, determining the part proposals is nearly  1145  as fast as the classification with the CNN (only 110ms per image for 10 parts on a standard PC with GPU), which allows for real-time applications.|
|||The P part proposals correspond to the channels an intermediate output layer in a CNN and i,p is determined by calculating the activation map of channel p for input image i and locating the maximum response.|
|||CNNs and parameters Two different CNN architectures were used in our experiments: the widely used architecture of Krizhevsky et al.|
|||Hence, we remove the testing images of Stanford dogs from the training set of ILSVRC 2012 and learned a CNN from scratch on this modified dataset.|
|||Each proposal is classified by the CNN and the proposal with the highest classification confidence is used as estimated bounding box.|
|||Data augmentation using part proposals  Fine-tuning is the adaption of a pre-learned CNN to a domain specific dataset.|
|||Since the domain specific datasets are often small and thus the training of a CNN is prone to overfitting, the training set is artificially enlarged by using data augmentation.|
|||Test Method  Parts Parts Parts Parts Parts Parts  Parts Parts Parts  Bbox CNN features Berg et al.|
|||We present three different results for every CNN architecture.|
|||In contrast, our work is easier to adapt to other datasets as we only require a generic pretrained CNN and no domain specific outside training data.|
|||To the best of our knowledge, there is only one work showing results for a CNN trained from scratch excluding the testing images of Stanford dogs.|
|||The first challenge is already solved by using the part detectors of a CNN trained to distinguish a huge number of classes.|
|||achieves slightly higher performance, their approach is also much more expensive due to dense evaluation of the whole CNN over all possible crops at three different scales.|
|||Their best result of 86.2% is achieved by using a fusion of two CNN models, which is not done in our case and consequently not comparable.|
|||Given a CNN pre-trained for classification, we exploit the learned inherit part detectors for generic part detection.|
||22 instances in total. (in iccv2015)|
|86|Zheng_Conditional_Random_Fields_ICCV_2015_paper|This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs.|
|||In fact, a number of recent approaches including the particularly interesting works FCN [35] and DeepLab [9] have shown a significant accuracy boost by adapting stateof-the-art CNN based image classifiers to the semantic segmentation problem.|
|||One way to utilize CRFs to improve the semantic labelling results produced by a CNN is to apply CRF inference as a post-processing step disconnected from the training of the CNN [9].|
|||More specifically, we formulate mean-field inference of dense CRF with Gaussian pairwise potentials as a Recurrent Neural Network (RNN) which can refine coarse outputs from a traditional CNN in the forward pass, while passing error differentials back to the CNN during training.|
|||Importantly, with our formulation, the whole deep network, which comprises a traditional CNN and an RNN for CRF inference, can be trained end-to-end utilizing the usual back-propagation algorithm.|
|||One representative instance of this scheme is the application of a CNN for the extraction of meaningful features, and using superpixels to account for the structural pattern of the image.|
|||This, for example, was shown in [16], where the authors replaced the last fully connected layers of a CNN by convolutional layers to keep spatial information.|
|||In contrast to these works, which employed CRF inference as a standalone post-processing step disconnected from the CNN training, our approach is an end-to-end trainable network that jointly learns the parameters of the CNN and the CRF in one unified deep network.|
|||In [6], the authors combined a CNN with Hidden Markov Models for that purpose, whereas more recently, Peng et al.|
|||Related to this line of works, in [24] a joint CNN and CRF model was used for text recognition on natural images.|
|||[55] showed the use of joint training of a CNN and an MRF for human pose estimation, while Chen et al.|
|||The pairwise energies provide an image data-dependent smoothing term that encourages assigning similar labels to pixels with similar  Algorithm 1 Mean-field in dense CRFs [27], broken down to common CNN operations.|
|||To this end, we first consider individual steps of the mean-field algorithm summarized in Algorithm 1 [27], and describe them as CNN layers.|
|||While reformulating the steps of the inference algorithm as CNN layers, it is essential to be able to calculate error differentials in each layer w.r.t.|
|||Once the individual steps of the algorithm are broken down as CNN layers, the full algorithm can then be formulated as an RNN.|
|||The softmax function has been extensively used in CNN architectures before and is therefore well known in the deep learning community.|
|||CRF as RNN  In the previous section, it was shown that one iteration of the mean-field algorithm can be formulated as a stack of common CNN layers (see Fig.|
|||Schematic visualization of our full network which consists of a CNN and the CNN-CRF network.|
|||Neither the CNN that provides unary values nor the layers after the CRF-RNN (i.e., the loss layers) need to perform any computations during this time since the refinement happens only inside the RNNs loop.|
|||During the backward pass, once the error differentials reach the CRF-RNNs output Y , they similarly spend T iterations within the loop before reaching the RNN input U in order to propagate to the CNN which provides the unary input.|
|||This can be attributed to the fact that during the SGD training of the CRF-RNN, the CNN component and the CRF component learn how to co-operate with each other to produce the optimum output of the whole network.|
|||Deeplab [9] is a two-stage approach, where the CNN is trained first, and then CRF is applied on top of the CNN output.|
||22 instances in total. (in iccv2015)|
|87|Sochor_BoxCars_3D_Boxes_CVPR_2016_paper|BoxCars: 3D Boxes as CNN Input  for Improved Fine-Grained Vehicle Recognition  Jakub Sochor, Adam Herout, Jir  Havel  Graph@FIT, Brno University of Technology  Brno, Czech Republic  {isochor,herout,ihavel}@fit.vutbr.cz  Abstract  We are dealing with the problem of fine-grained vehicle make & model recognition and verification.|
|||They also propose a CNN architecture for fine-grained vehicle recognition and publish benchmarking results.|
|||The shape and location of the bounding box is also input to the CNN and helps it to reference the relevant information.|
|||Lastly, the view direction extracted for each vehicle sample is also encoded and input to the fully connected CNN layers, further boosting the performance.|
|||The contributions of this paper are the following: i) We show that additional information easily obtainable in real time for static surveillance cameras can boost the CNN verification performance greatly (by 208 %), ii) The vehicle fine-grained classification error was decreased by 26 %, iii) We collected a dataset of vehicle samples accompanied with the 3D bounding boxes (BoxCars, 21,250 samples, 63,750 images, 27 different makes, 148 make & model + submodel + model year classes).|
|||Authors of [17] propose to learn discriminative parts of vehicles with CNN and use the parts for fine-grained classification.|
|||Authors also used input normalization to improve performance of CNN [39] and adding additional training data to CNN [20].|
|||Parts of the CNN can be viewed as feature extractors and independently reused.|
|||The original CNN processing cropped images of vehicles without any modifications is referenced as baseline.|
|||All these datasets are relatively small for training the CNN for realworld surveillance tasks.|
|||Examples of rasterized bounding boxes for CNN (colors are R,G,B in the actual computation, but are changed here for aesthetic reasons).|
|||Final CNN Using Images + Auxiliary Input  All this information is finally passed to the CNN (Fig.|
|||Experimental Results  The evaluation of the improvement caused by our modifications of the CNN input can be only done on our BoxCars  1https://medusa.fit.vutbr.cz/traffic  Top-1 Top-5  [43] 0.767 0.917 Ours 0.848 0.954  Table 1.|
|||As Table 1 shows, this net significantly outperforms the CNN used by Yang et al.|
|||[43], we measure the improvement achieved by our modifications of the CNN input relatively to the performance of this baseline net on our BoxCars dataset.|
|||Also, Table 5 shows that the improvement achieved by the modified CNN in the medium dataset is 0.060 (0.772 to 0.832) and 0.071 (0.733 to 0.804) in the hard case.|
|||The experimental results indicate that the most important improvement is unpacking the image (Section 3.1), presumably because it leads to better alignment of the vehicle features on the input CNN level.|
|||The plots show that our CNN input modifications have a huge impact on the average precision in the verification task.|
|||For example, considering the medium set and the median cosine distance over the three samples, RastViewUnp improved AP of the baseline CNN by 208 %.|
|||The numbers gradually increase as we add more and more modifications of the CNN input.|
|||Schematic image of improvements in the verification AP for different CNN input modifications.|
|||Bilinear cnn models for fine-grained visual recognition.|
||22 instances in total. (in cvpr2016)|
|88|Hu_Learning_Structured_Inference_CVPR_2016_paper|Structured model with convolutional neural networks (CNNs): Structured deep models extend traditional CNNs to applications of structured label prediction, for which the CNN model is found insufficient to learn implicit constraints or structures between labels.|
|||[33] take the CNN predictions as unary po tentials for body parts and feed them to a MRF-like spatial model, which further learns pairwise potentials of part relations.|
|||Schwing and Urtasun [28] proposed a structured deep network by concatenating a densely connected MRF model to a CNN for semantic image segmentation, in which the CNN provides unary potentials as the MRF model imposes smoothness.|
|||Given an input image, we extract CNN features at the last fully connected layer as activation (in blue box) at different visual concept layers.|
|||The CNN framework of Krizhevsky et al.|
|||Learning Framework  It is straightforward to build an image classification model, by adding a loss layer on top of the CNN features for each concept layer.|
|||In the second stage, we set the learning rate as 0.0001 and fine-tune the CNN together with our SINN.|
|||In the computation of visual activations from the CNN, as different experiment datasets describe different semantic domains, we adopt different pretrained CNN models: ImageNet pretrained model [13] for experiments 4.1 and 4.2, placenet pretrained model [38] for experiment 4.3.|
|||For each experiment, we compare our full method (CNN + SINN) with the baseline method: CNN + logistic regression.|
|||With further specifications, we may have extra baseline methods, such as CNN + BINN, CNN + logistic regression + extra tags, etc.|
|||Table 1 shows that our method outperforms the baseline methods (CNN + Logistics and CNN + BINN variants) as well as the USE method, in terms of each concept layer and each performance metric.|
|||Note that for the results in Table 1, we did not finetune the first seven layers of CNN [18] for fairer comparison with Hwang and Sigal [12] (which only makes use of DECAF features [6]).|
|||We pick up 10 representative images from NUS-WIDE, and visualize the predicted labels of our method compared with CNN + Logistics.|
|||As our usual baseline, we extract features from a CNN pretrained on ImageNet [25] and train a logistic classifier on top of it.|
|||Next, a stronger baseline that uses both CNN output and metadata vector with logistic classifier was evaluated.|
|||[14], with difference in visual feature (CNN on image in our method versus CNN on image neighborhood) and tag feature (1k tag vector versus 5k tag vector).|
|||Note that we did not report our performance with fine-tuning the first seven layers of the CNN in this table, so as to make direct comparison of structured inference on SINN with our baseline method CNN + Logistics.|
|||Finetuned CNN with SINN improves mAPL to 70.01  0.40 and mAPI to 83.68  0.13.|
|||However, the results show that, by modeling la Method  Graphical Model [22]  CNN + WARP [7]  5k tags + Logistics [14]  Tag neighbors + 5k tags [14]  CNN + Logistics 1k tags + Logistics  1k tags + Groups + Logistics  1k tags + Groups + CNN + Logistics  mAPL  49.00   mAPI   RecL   35.60  P recL   31.65  RecI   60.49  P recI   48.59  43.88  0.32 61.88  0.36  77.06  0.14 80.27  0.08  47.52  2.59 57.30  0.44  46.83  0.89 54.74  0.63  71.34  0.16 75.10  0.20  51.18  0.16 53.46  0.09  46.94  0.47 50.33  0.37 52.81  0.40 54.67  0.57  72.25  0.19 66.57  0.12 68.04  0.12 77.81  0.22  45.03  0.44 23.97  0.23 25.54  0.24 50.83  0.53  45.60  0.35 47.40  0.07 49.26  0.15 49.36  0.30  70.77  0.21 64.95  0.18 65.99  0.15 75.38  0.16  51.32  0.14 47.40  0.07 48.13  0.05 54.61  0.09  1k tags + CNN + SINN  1k tags + Groups + CNN + SINN  67.20  0.60 69.24  0.47 69.24  0.47 69.24  0.47  81.99  0.14 82.53  0.15 82.53  0.15 82.53  0.15  59.82  0.12 60.63  0.67 60.63  0.67 60.63  0.67  57.02  0.57 58.30  0.33 58.30  0.33 58.30  0.33  78.78  0.13 79.12  0.18 79.12  0.18 79.12  0.18  56.84  0.07 57.05  0.09 57.05  0.09 57.05  0.09  Table 2.|
|||The first experiment is similar to the study on AwA: we applied our model to layered image classification with label relations, and compare our model with CNN + Logistics and CNN + BINN baselines, as well as a state-of-the-art approach [37, 36].|
|||We compare to a set of baselines, including CNN + Logistics + Partial Labels,  Method  M CAcc  mAPL  Image features + SVM [37, 36]  42.70   CNN + Logistics  CNN + BINN CNN + SINN  57.86  0.38 55.31  0.30 57.52  0.29 55.57  0.63 57.60  0.38 58.00  0.33  CNN + Logistics + Partial Labels 59.08  0.27 56.88  0.29 63.46  0.18 64.63  0.28 63.46  0.18 63.46  0.18 64.63  0.28 64.63  0.28  CNN + SINN + Partial Labels  Table 4.|
|||Results show that our method combined with partial labels (i.e., CNN + SINN + Partial Labels) improves over baselines, exceeding the second best by 4% M CAcc and 6% mAPL.|
||22 instances in total. (in cvpr2016)|
|89|Lin_Visualizing_and_Understanding_CVPR_2016_paper|First we show that the recently proposed bilinear CNN model [25] is an excellent generalpurpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks.|
|||[4] showed that Fisher vectors built on top of CNN activations  dotted  water  landromat  honeycombed  wood  bookstore  Figure 1.|
|||Visualizing various categories by inverting the bilinear CNN model [25] trained on DTD [3], FMD [34], and MIT Indoor dataset [32] (each column from left to right).|
|||These images were obtained by starting from a random image and adjusting it though gradient descent to obtain high log-likelihood for the given category label using a multi-layer bilinear CNN model (See Sect.|
|||Our starting point is the bilinear CNN model of our previous work [25].|
|||Using the Flickr Material Dataset (FMD) [34], Describable Texture Dataset (DTD) [3] and KTH-T2b [1] we show that it performs favorably to Fisher vector CNN model [4], which is the current state of the art.|
|||Our experiments reveal that bilinear CNN models can be trained from scratch, resulting in better accuracy without requiring spatial jittering of data than the corresponding CNN architectures that consist of standard fully-connected layers trained with jittering.|
|||The synthesized results are visually appealing, demonstrating that the convolutional layers of a CNN capture textural properties significantly better than the first and second order statistics of wavelet coefficients of Portilla and Simoncelli.|
|||However, the approach remains impractical since it requires  hundreds of CNN evaluations and is orders of magnitude slower than non-parametric patch-based methods such as image quilting [9].|
|||Current state-of-the-art results on texture and material recognition are obtained by hybrid approaches that build orderless representations on top of CNN activations.|
|||Our previous work [25] proposed a general orderless pooling architecture called the bilinear CNN that outperforms Fisher vector on many fine-grained datasets.|
|||showed that replacing the linear filterbanks by CNN filterbanks results in better reconstructions.|
|||Dosovitskiy and Brox [8] propose a deconvolutional network to invert a CNN in a feed-forward manner.|
|||For an image I one can compute the activations of the CNN at a given layer ri to obtain a set of features Fri = {fj} indexed by their location j.|
|||Texture recognition  In this section we evaluate the bilinear CNN (B-CNN) representation for texture recognition and scene recognition.|
|||The FV-CNN builds a Fisher Vector representation by extracting CNN filterbank responses from a particular layer of the CNN using 64 Gaussian mixture components, identical to setup of Cimpoi et al.|
|||These differences in results are likely due to the choice of the CNN 1 and the range of scales.|
|||As fully connected layers in a standard CNN network encode spatial information, the model loses performance without spatial jittering.|
|||Although the same approach can be used for texture synthesis, it is not practical since it requires several hundreds of CNN evaluations, which takes several minutes on a high-end GPU.|
|||The original paper proposed an approach  top1 error  relu2 2  + relu3 3  + relu4 3  + relu5 3  38.7    B-CNN f1 37.1    B-CNN f5 36.6    B-CNN f25 46.4       CNN f1 39.6       CNN f25  water  foliage  bowling  Figure 4.|
|||We learn linear classifiers to predict categories using bilinear features from relu2 2, relu3 3, relu4 3, relu5 3 layers of the CNN on various datasets and visualize images that produce high prediction scores for each class.|
|||Visualizing various categories by inverting the bilinear CNN model [25] trained on DTD [3], FMD [34], and MIT Indoor dataset [32] (two columns each from left to right).|
||22 instances in total. (in cvpr2016)|
|90|Roy_Monocular_Depth_Estimation_CVPR_2016_paper|At every tree node, the sample is filtered with a CNN associated with that node.|
|||At every node of CRT, x is filtered with a CNN associated with that node, and then passed to left and right children nodes (i.e., CNNs) with a Bernoulli probability for further convolutional processing, until x reaches leaves of the tree.|
|||(a) A CNN is associated with every node of a binary Convolutional Regression Tree (CRT) for performing the convolutional processing of data samples.|
|||(b) While our CNNs process data samples as they pass down the CRT, the related deep architecture of [11] uses a single deep CNN to fully process the data before passing them through a decision tree.|
|||Hence, all CNN computations along a path from the root to the leaf amount to processing the data sample with a relatively deep CNN.|
|||6 specifies details of our CNN architecture, and Sec.|
|||A single deep CNN has been combined with a random forest for image classification [11].|
|||Outputs of the top CNN layer are considered as nodes of the decision tree.|
|||Each v  VT represents a CNN with parameters wv.|
|||This function is computed by vth CNN with a single output node, which produces the value of the split function through a sigmoid function.|
|||Learning  Our learning of each T estimates the following parameters: 1)  the set of Gaussian parameters in the leaf nodes, and 2) W the set of CNN parameters of all the split nodes.|
|||Learning CCN Parameters for the Split Nodes  In this section we describe how to learn the CNN parameters for the split nodes v  V, wv.|
|||The gradient is first computed for the final output node of the CNN and then passed to the lower  layers of the CNN using the standard backpropagation [13].|
|||To address this observation, we adjust the complexity of the CNN architecture along the tree height.|
|||The CNN architecture is determined by the number of: 1) convolution + pooling layers, and 2) fully connected perceptron layers.|
|||The convolutional outputs of the CNN at a split node are used as the input window of the CNNs at the children split nodes.|
|||Our tree architecture effectively resembles a deep CNN framework which consists of a set of small CNNs.|
|||Thus, split functions in every node share the same CNN parameters through a single deep network.|
|||NRF outperforms this baseline which suggests that a set of small CNN performs better than a large network for the small datasets.|
|||NRF without input forwarding for CNNs (NRFw/oF): In this baseline, for each CNN in the split node, we use only RGB input window instead of convolutional outputs from the parent split node.|
|||A shallow CNN with maximum two convolutional layers is associated with every node in the regression trees of NRF.|
|||Results demonstrate that NRF is able to robustly address these challenges, and outperform the state of the art, due to: (a) its ensemble architecture consisting of the forest of trees, (b) shallow CNN architecture with significantly few parameters than seen in recent work, and (c) accounting for smoothness in depths.|
||22 instances in total. (in cvpr2016)|
|91|Xie_Task-Driven_Feature_Pooling_ICCV_2015_paper|TDP can be combined with the traditional BoW models (coding vectors) or the recent stateof-the-art CNN models (feature maps) to achieve a much better pooled representation.|
|||Both BoW and CNN models are considered as the milestones in the research community of object recognition.|
|||The beginning of CNN is actually the same as the BoW model, but it differs from the latter in the following two aspects: (1) CNN is a fully  Figure 1.|
|||supervised model in which all the parameters are trained in a discriminative manner; (2) CNN is a reduplicative convolution and pooling process until the last layer.|
|||The common part of BoW and CNN is that they both adopt the pooling operation in their architecture to increase the invariance of the representation.|
|||In the current CNN architecture, pooling also plays an important role.|
|||Here procedure in red dashed box is our models, and the inputs for TDP (mTDP) learning are CNN feature maps of different scales.|
|||The main components of CNN are multiple convolution, pooling and different activating functions.|
|||Actually, the first convolutional and pooling layer can be seen as the BoW model (by viewing filters of CNN as the dictionary elements in BoW, and the convolution between filters and input image as feature coding in BoW).|
|||This motivated many researchers to construct deep BoW models to explore the gap between BoW and CNN [1].|
|||As we know, CNN is a fully  1180  supervised model while BoW is a step-by-step model and most steps are unsupervised.|
|||Considering the large number of different CNN models, we can integrate different feature informations contained in CNNs to achieve better performance.|
|||The most useful feature extractors of the CNN are the feature maps in the middle layers and the fully connected representations in the last several layers [37].|
|||Here, we denote the CNN feature maps as V  RKhh, where K is the feature map number, and h is the size of all these maps.|
|||Here in this paper, we aim to process different scaled feature maps of one CNN in a unified framework.|
|||Pooling has been used in many visual recognition systems, e.g., average pooling used in the biologically-inspired visual systems [13], and max pooling [29] used in the CNN [18].|
|||Let V  RKN be the input data, where N = l  w, e.g., K = 512, N = 8  8 for some CNN feature maps.|
|||We take feature maps of different scales from the same CNN as different tasks, which are used as inputs of our models.|
|||Moreover, we use the caffe implementation [17] of Vd19 to extract the CNN feature maps, and the fully connected layer activates, which can get enhanced performance on both databases when combined with our TDP or mTDP representations.|
|||is much better than the traditional methods, such as mode seeking [7], and is competitive with some CNN based methods, e.g., the order-less pooling [15].|
|||Possible reasons for this phenomenon may be that Caltech101 is an object-oriented database, thus max pooled representation from the convolutional layer of CNN can obtain    uAsIsAsZYZAG;  'w wdW G DAdz       sIIGGYZwdsYAsZYZIwusuGAU  Method ScSPM [35] LLC-SPM [32] Multipath Deep BoW [1] Visualize CNN [38] DeCAF-fc6 [8] VGG-Vd16 [30] VGG-Vd19 [30] VGG-Vd16+Vd19 [30] SPP-CNN [16] mTDP+fc mTDP+fc+fc-2  Acc (%) 73.200.54 73.44 82.500.50 86.500.50 86.910.70 91.801.00 92.300.50 92.700.50 93.420.50 93.020.24 93.680.50  Figure 3.|
|||By considering the CNN feature maps of different scales as the inputs to TDP, we can learn a much better pooled representation for image classification.|
||22 instances in total. (in iccv2015)|
|92|Rahmani_3D_Action_Recognition_CVPR_2016_paper|While learning the CNN model, we do not use action labels but only the pose labels after clustering all training poses into k clusters.|
|||The former is a deep CNN which represents different human body shapes and poses observed from numerous viewpoints in a viewinvariant high-level space.|
|||Our motivation for using a frame based CNN model comes from the findings [21] that a single frame model performs equally well as the multiframe CNN model.|
|||Our representation is a group sparse Fourier Temporal Pyramid that extracts features from the view-invariant high-level representation layer of the proposed CNN model.|
|||We capitalize on the fact that the output of different neurons in the CNN representation layer contributes differently to each human pose and hence each action.|
|||New action classes can be efficiently added to our framework as it requires retraining the action classifier only while using the same learned CNN model.|
|||fully connected layer with n filters and Drq a dropout layer with dropout ratio r. The architecture of our CNN follows: C11, 96, 4q N RL N P 3, 2q N N N C5, 256, 1q N RL N P 3, 2q N N N C3, 384, 1q N RL N C3, 384, 1q N RL N C3, 256, 1q N RL N P 3, 2q N F C4096q N RL N D0.5q N F C4096q N RL N D0.5q N F C339q.|
|||We initialize the CNN with a model that was trained on approximately 1.2 million RGB images from the 2012 ImageNet challenge and then fine-tuned on depth images from NYUD2 [18].|
|||We train our CNN with back-propagation and use an initial learning rate of 0.01 for the convolution layers and 0.01 for the fully-connected layers.|
|||Inference  So far, we have learned a deep CNN model whose input is a human pose depth image and output is the corresponding pose class.|
|||The proposed CNN is able to classify only 339 pose classes which do not cover all possible human poses.|
|||Pre-processing: The synthetic depth dataset used for training the proposed CNN model contains depth images of only human body poses.|
|||human body, and converted to a form that is compatible with the learned CNN model.|
|||Temporal modeling and classification  To represent an action sequence with our CNN model, we feed forward the depth images sequentially through the network and temporally align the f c7 layer features.|
|||More importantly, we use the same CNN model, learned from synthetic data, for all five datasets to show the generalization strength of our model and to show that our model can be applied to any depth action video without the need for re-training or fine-tuning.|
|||Video frames are individually passed through the CNN model and the Fourier Temporal Pyramid features are extracted from the time series of each neuron output of the CNN representation layer.|
|||The learned CNN model and MATLAB code of our method are freely available 1.|
|||In addition to other compared methods, we report the accuracy of our defined baseline method which uses a similar approach to [15] but with the CNN model that was fine-tuned on depth images from NYUD2 [18].|
|||We report the recognition accuracy of our method in two different settings: (1) HPM where we apply average pooling on the CNN features of all frames of a video to obtain its representation, and (2) HPM+TM where we employ the proposed temporal modeling approach on the CNN features to capture the temporal structure of the videos.|
|||Conclusion  It is interesting to note that our technique outperforms the current state-of-the-art on both cross-view datasets while using the same CNN model learned from synthetic data.|
|||Method  On-line training  Testing  NKTM [41] LARP [54] HOPC [39] Ours  12 fps 0.1 fps 0.04 fps 22 fps  16 fps 10 fps 0.5 fps 25 fps  We proposed a deep CNN model that represents depth images of different human poses acquired from multiple views in a view-invariant high-level space.|
|||Our method performs equally well on single-view benchmark datasets (see supplementary material) and generalizes to hand gestures even though the CNN model was trained on full human body poses.|
||22 instances in total. (in cvpr2016)|
|93|Shin_Automating_Carotid_Intima-Media_CVPR_2016_paper|The reliable performance is attributable to our unified framework based on convolutional neural networks (CNNs) coupled with our informative image representation and effective post-processing of the CNN outputs, which are uniquely designed for each of the above three operations.|
|||The suggested method utilizes effective pre-processing of patches and post processing of CNN outputs, enabling a significant  increase in the performance of a baseline CNN.|
|||The suggested method combines the discriminative power of a CNN with a contextual constrain to accurately localize the ROIs in the selected frames.|
|||We demonstrate that the suggested contextually-constrained CNN outperforms the performance of a baseline CNN.|
|||Specifically, given a localized ROI, the CNN initializes two open snakes, which further deform to acquire the shapes of intima-media boundaries.|
|||To do so, we introduce accumulated difference images that carry sufficient information for CNN to learn and distinguish R-peaks from non-R-peaks.|
|||Once the patches are labeled, we form a stratified set with 96,000 patches to train a 2-way CNN for frame selection.|
|||The probability of each frame being the EUF is measured as the average probabilities assigned by the CNN to the corresponding patches.|
|||Once the patches are collected, we form a stratified training set with approximately 410,000 patches to train a 3-way CNN for constrained ROI localization.|
|||Testing Phase: Referring to Figure 4, during the test stage, the trained CNN is applied to all the pixels in the EUF, generating two confidence maps with the same size as the EUF.|
|||Training Phase: We incorporate this constraint in the suggested system by training a 3-way CNN that simultaneously localizes both ROI and carotid bulb, and then refines the estimated location of the ROI given the location of the carotid bulb.|
|||The trained 3-way CNN is applied in a sliding-window fashion to a given test ROI, generating two confidence maps (Figure 5(b)) with the same size as the ROI.|
|||Architecture: As shown in Table 1, we employ a CNN architecture with 2 convolutional layers, 2 subsampling layers, and 2 fully connected layers (see Section 5 for our justifications).|
|||Our CNN architecture has input patches of size 32x32, so we resize the collected patches to 32x32 prior to the training process.|
|||(b) The trained CNN generates a confidence map where the green and red colors indicate the likelihood of lumen-intima interface and media-adventitia interface, respectively.|
|||Table 1: The CNN architecture used in our experiments.|
|||This can manifest itself as a sudden change in the CNN output and as a result in the corresponding probability signal.|
|||Alternatively, we could train a regression CNN where each pixel in the image directly votes for the location of the ROI.|
|||We will explore a regression CNN for ROI localization as future work.|
|||We used a LeNet-like CNN architecture in our study, but it does not limit the suggested framework to this architecture.|
|||In fact, we have experimented with deeper CNN architectures such as AlexNet [9] in both training and finetuning modes; however, we did not observe any significant performance gain.|
|||We based each of the above components on a CNN with a LeNet-like architecture and then boosted the performance of the employed CNNs with effective preand post-processing techniques.|
||22 instances in total. (in cvpr2016)|
|94|Zhang_Picking_Deep_Filter_CVPR_2016_paper|The first is that traditional CNN representation requires fixed size rectangle as input, which inevitably includes background information.|
|||We regard deep filter responses of a CNN as localized descriptors, and encode them via Spatially Weighted Fisher Vector (SWFV-CNN).|
|||Experimental results demonstrate that SWFV-CNN performs consistently better than traditional CNN, and is complementary with traditional CNN to further boost the performance.|
|||The second step is to pick CNN filters via Spatially Weighted combination of Fisher Vector, which we refer to SWFV-CNN.|
|||[21] propose to localize parts with constellation model, which incorporates CNN into deformable part model [9].|
|||Feature Representation  For  the description of  image, CNN features have achieved breakthrough on a large number of benchmarks [11], [20], [31], etc.|
|||Different from traditional descriptors which explicitly encode local information and aggregate them for global representation, CNN features represent global information directly, and can alleviate the requirement of manually designing a feature extractor.|
|||Though not specifically designed to model sub-category level differences, CNN features capture such information well [7].|
|||Most works choose the output of a CNN as feature representation directly [2], [15], [27], [32].|
|||However, CNN features still preserve a great deal of global spatial information.|
|||Our approach regards responses from deep CNN filters as localized descriptors (similar with SIFT), and encodes these responses via Fisher Vector.|
|||Different from previous works which encode CNN descriptors globally [6], [12], we project each response back to the original image and encode each part separately.|
|||Different from previous works, we propose a picking strategy which elaborately selects distinctive and consistent patches based on the responses of CNN filter banks.|
|||The key insight is that different layers of a CNN are sensitive to specific patterns.|
|||One direct method for part representation is to extract CNN features directly from the detected parts, and concatenate them for final representation.|
|||The first is the background disturbance, as CNN requires a fixed rectangle as input, which includes cluttered background inevitably.|
|||To deal with these issues, instead of extracting FC-CNN within a tight rectangle, we propose to compute part saliency map and pool CNN features with Spatially Weighted Fisher Vector (SWFV-CNN).|
|||FV-CNN pools CNN features with Fisher Vector.|
|||When switching to CNN features, our approach is best among methods under the same setting [21], [27], and obtains a 18% error reduction comparing with the best performing result [21] (81.01%).|
|||Few works report results on this dataset, due to there are not off-the-shelf CNN models for feature extraction.|
|||SWFV-CNN packs local CNN descriptors via spatially weighted combination of Fisher Vectors.|
||21 instances in total. (in cvpr2016)|
|95|Xiao_The_Application_of_2015_CVPR_paper|The selected patches drive the training of another CNN into a domain classifier, called DomainNet.|
|||The next key step is to extract discriminative features from the regions/patches selected by these two attentions. Recently, there have been convincing evidence that features derived by CNN can deliver superior performance over hand-crafted ones [25, 16, 7, 26].|
|||At the part-level, activations in the CNN hidden layers driven by detected parts yield another prediction through a part-based classifier.|
|||We do this by converting a CNN trained on the 1K-class ILSVR2012 dataset into an object-level FilterNet.|
|||Training a DomainNet The patches selected by the FilterNet are used to train a new CNN from scratch after proper warping.|
|||We call this second CNN the DomainNet because it extracts features relevant to the categories belonging to a specific domain (e.g., dog, cat, bird).|
|||In this figure, mid-level CNN filters can be served as head detector, body detector and leg detector for birds.|
|||Implementation Details  Our CNN architecture is essentially the same as the popular AlexNet et al.|
|||When using CNN as feature extractor, the activations of the first fully-connected layer are outputted as features.|
|||The baselines are performance of CNN but trained with  two different strategies, including:  Object-level FilteringPart-level DetectionObject-level ClassifierPart-level ClassifierDomainNetPart 1Part 2OriginSelected Part Patches...... ...... Object-Filtered PatchesClassification ResultBottom-Up Region Proposals...... ......Table 1.|
|||Method CNN domain CNN 1K Object-level attention Part-level attenion Two-level attention   CNN domain: The network is trained only on images from dog categories.|
||| CNN 1K: The network is trained on all images of ILSVRC2012 1K categories, then the softmax neurons not belong to dog are removed.|
|||Using object-level attention only drops the error rate by 9.3%, comparing against CNN trained with randomly cropped patches.|
|||We compare against two baseline feature extractors, one is hand-crafted kernel descriptors [3] (KDES) which was widely used in fine-grained classification before using CNN feature, the other is the CNN feature extractor pre-trained from all the data in ILSVRC2012 [16].|
|||This further demonstrates that using object-level attention to filter relevant patches is an important condition for CNN to learn good features.|
|||Accuracy and Annotation used between methods Testing phase  Training phase  Method Object-level attention Part-level attention Two-level attention DomainNet without attention BBox + DomainNet DPD [27] + DomainNet Part Discovery [17] Symbiotic [5] Alignment [9] DeCAF6 [7] CNNaug-SVM [16] Part RCNN [26] Pose Normalized CNN [4] POOF [2] Part RCNN [26] POOF [2]  BBox Info  Part Info BBox Info  Part Info Accuracy (%)  (cid:88) (cid:88)  (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)  (cid:88) (cid:88)  (cid:88) (cid:88) (cid:88) (cid:88)  (cid:88) (cid:88) (cid:88)  (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)  (cid:88)  67.6 64.9 69.7 58.8 68.4 70.5 53.8 61.0 62.7 58.8 61.8 73.5 75.7 56.8 76.7 73.3  curacies are reported in Table 2, along with how much annotations are used.|
|||To further improve the performance, part level annotation is also used in training phase to learn strongly-supervised deformable part-based model [1, 27] or directly used to finetune pre-trained CNN [4].|
|||Our work is also closely related to recently proposed object detection method (R-CNN) based on CNN feature [10].|
|||further improved the performance of CNN feature extractor by finetuning on fine-grained dataset [26].|
|||More importantly, we opt for the weakest supervision throughout the model, relying solely on CNN features to implement attention, detect parts and extract features.|
|||This leads to better CNN feature for fine-grained classification, as the network is driven by domain-relevant patches that are also rich with shift/scale variances.|
||21 instances in total. (in cvpr2015)|
|96|Ghodrati_DeepProposal_Hunting_Objects_ICCV_2015_paper|In the first part of this work we present a performance analysis of different CNN layers for gener 2578  ating proposals.|
|||More specifically, similarly to BING [5], we select a reduced set of window sizes and aspect ratios and slide them on each possible location of the feature map generated by a certain CNN layer.|
|||In this case the approach does not need to compute any feature, because it reuses the same features already computed by the CNN network for detection.|
|||Next, in section 3, we analyze the quality of different CNN layers for window proposal generation.|
|||However, in contrast to them, we use a hierarchy of high-to-low level features extracted from a deep CNN which has proven to be effective for object detection [12, 25].|
|||CNN layers for object proposals  In this section we analyze the quality of the different lay ers of a CNN as features for window proposal generation.|
|||Hence, similarly to [5] we select a set of window sizes that best cover the training data in terms of size and aspect ratio and use them in a sliding window fashion over the selected CNN layer.|
|||Let f (x, y) be the specific channel of the feature map from a certain CNN layer and F (x, y) its integral image.|
|||This makes sense because the CNN features are specific for object classification and therefore can easily localize the object of interest.|
|||If we compare the performance of the CNN layers for high overlap (see Fig.|
|||This is due to the coarseness of the CNN feature maps that do not allow a precise bounding box alignment to the object.|
|||In this case, the first layers of the net can recall many more objects with  2581  Layer  5 4 3 2 1  Feature map size Recall(#1000,0.5) Max(0.5) Recall(#1000,0.8) Max(0.8) 36  52  256 36  52  256 36  52  256 73  105  396 146  210  96  36% 36% 38% 29% 18%  70% 79% 79% 86% 89%  88% 91% 92% 87% 73%  97% 97% 97% 98% 99%  Table 1: Characteristics and performance of the CNN layers.|
|||This shows that a problem of the higher layers of the CNN is the lack of a good spatial resolution.|
|||In our experiments for contour detection we observed that layer 1 of CNN did not provide as good performance as layer 2 (0.61 vs. 0.72 AP on BSDS dataset [2]) so we choose sec 5.|
|||CNN features which are used by state-of-the-art object detectors like RCNN [12] and SppNet [14] and does not need any extra cues and features, we can consider just running time of our algorithm without CNN extraction time1.|
|||All are based on CNN features and use object proposals for detecting the object of interest.|
|||The first uses the window proposals to crop the corresponding regions of the image, compute the CNN features and obtain a classification score for each region.|
|||SppNet and fast-RCNN instead compute the CNN features only once, on the entire image.|
|||With these approaches then, we can also reuse the CNN features needed for the generation of the proposal so that the complete detection pipeline can be executed without any pre-computed component roughly in 1 second on our GPU.|
|||We believe this is due to the simplicity of the classifier (average pooling on CNN features) that avoids overfitting specific classes.|
|||By employing an efficient coarse to fine cascade on multiple layers of CNN features, we have a framework of objectness measurement that acts strongly on objects locations and our method can find reasonable accurate proposals, fast.|
||21 instances in total. (in iccv2015)|
|97|Durand_WELDON_Weakly_Supervised_CVPR_2016_paper|Secondly, the deep CNN is trained to optimize Average Precision, and fine-tuned on the target dataset with efficient computations due to convolutional feature sharing.|
|||The WELDON model is a deep CNN trained in a weakly supervised manner.|
|||WELDON is trained to automatically select relevant regions from images annotated with a global label, and to perform end-toend learning of a deep CNN from the selected regions.|
|||Despite their excellent performances, current CNN architectures only carry limited invariance properties: although a small amount of shift invariance is built into the models through subsampling (pooling) layers, strong invariance is generally not dealt with [53].|
|||[34, 36], incorporating a max CNN layer accounting for the MIL hypothesis.|
|||A systematic evaluation of our modeling and training contributions highlights their importance for training deep CNN models from weak annotations.|
|||Note that each 77 area in L5 in thus mapped to a fixed-size ddimensional vector, so that this transfer layer is equivalent to applying the whole CNN on each of the 7  7 region.|
|||Indeed, using a single area for training the model necessarily increases the risk of selecting outliers, guiding the training of the deep CNN towards bad local minima.|
|||We exhibit here the following result for WELDON (proof in supplementary 2.2):  In a second training phase, starting with W7 initialized from the first phase, a fine-tuning of all other CNN parameters is achieved.|
|||In all cases, error gradient is back-propagated in the deep CNN through chain rule.|
|||Transfer learning & fine-tuning Similarly to other deep WSL models, our whole CNN contains a lot of parameters.|
|||Experiments  Our deep CNN architecture is based on VGG16 [45].|
|||Compared to [10], the improvement of 4.4 pt on VOC 2007 essentially shows the importance of using multiple instances, and the relevance of an end-to-end training of a deep CNN in the target dataset.|
|||15 Scene MIT67  CaffeNet ImageNet [20] CaffeNet Places [54] VGG16 (online code) [45] MOP CNN [18] MANTRA [10] Negative parts [38] WELDON (OB)  84.2 90.2 91.2  93.3  94.3  56.8 68.2 69.9 68.9 76.6 77.1 78.0  Table 2.|
|||The results shown in Table 2 for scene recognition also illustrate the big improvement of WELDON compared to deep features computed on the whole image [20, 54, 45] and MOP CNN [18], a BoW method pooling deep features with VLAD.|
|||This shows the improvement  brought out by the end-to-end deep WSL CNN training with WELDON.|
|||Finally, note that the very good results in COCO also illustrate the efficiency of the proposed WSL training of deep CNN with WELDON, which is able to deal with this large datasets (80 classes and  80000 training examples).|
|||Systematic evaluation of our WSL deep CNN contributions.|
|||Systematic evaluation of our WSL deep CNN contributions.|
|||Our method exploits to the full extend deep CNN strategy in multiple instance learning framework to efficiently deal with weak supervision.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||21 instances in total. (in cvpr2016)|
|98|Bilen_Dynamic_Image_Networks_CVPR_2016_paper|Our new approximate rank pooling CNN layer allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance.|
|||Since a CNN provides a whole hierarchy of image representations, one for each intermediate layer, the first question is where temporal pooling should take place.|
|||13034  rank pooling [6] to pool the output of the fully-connected layers of a standard CNN architecture pre-trained on still images and applied to individual frames.|
|||A downside of this solution is that the CNN itself is unaware of the lower level dynamics of the video.|
|||First, the new RGB image can be processed by a CNN architecture which is structurally nearly identical to architectures used for still images, while still capturing the long-term dynamics in a video that relate to the long term dynamics therein.|
|||It is then possible to use a standard CNN architecture to learn suitable dynamic features from the videos.|
|||The second advantage of this method is its remarkable efficiency: the extraction of the dynamic image is extremely simple and efficient and it allows to reduce video classification to classification of a single image by a standard CNN architecture.|
|||We also show that, in this manner, it is possible to apply the concept of dynamic image to the intermediate layers of a CNN representation by constructing an efficient rank pooling layer.|
|||The first choice is to provide as input to the CNN a sub-video of fixed length, packing a short sequence of video frames into an array of images.|
|||The advantage of this technique is that they allow using simple modifications of standard CNN architectures (e.g.|
|||Our static/dynamic CNN uses a multi-stream architecture.|
|||In designing a CNN for video data, in particular, it is necessary to think of how the video information should be presented to the CNN.|
|||Here we propose an alternative and more efficient approach in which the video content is summarized by a single still image which can then be processed by a standard CNN architecture such as AlexNet [15].|
|||First, it is interesting to note that the dynamic images tend to focus mainly on the acting objects, such as humans or other animals such as horses in the horse racing action, or objects such as  3036  training a CNN on top of such dynamic images, we implicitly capture the temporal patterns contained in the video.|
|||However, since the CNN is still applied to images, we can start from a CNN pre-trained for still image recognition, such as AlexNet pre-trained on the ImageNet ILSVRC data, and fine-tune it on a dataset of dynamic images.|
|||Fine-tuning allows the CNN to learn features that capture the video dynamics without the need to train the architecture from scratch.|
|||Later, this technique, which we call approximate rank pooling, will be critical in incorporating rank pooling in intermediate layers of a deep CNN and to allow back-prop training through it.|
|||Here, we notice that every layer of a CNN produces as output a feature map which, having a spatial structure similar to an image, can be used in place of video frames in this construction.|
|||In order to train a CNN with rank pooling as an intermediate layer, it is necessary to compute the derivatives of eq.|
|||Hence, using dynamic maps and rank pooling directly as a layer in a CNN is not straightforward.|
|||Exploiting Image-trained CNN Architectures for Unconstrained Video Classification.|
||21 instances in total. (in cvpr2016)|
|99|Gkioxari_Actions_and_Attributes_ICCV_2015_paper|[21] uses a holistic CNN classifier that outperforms partbased approaches [20, 30].|
|||They classify region proposals using a CNN fine-tuned on object boxes.|
|||In addition, the task-specific CNN that we use for action or attribute classification shares the architecture of [18, 23] and is initialized by pre-training on the large ImageNet-1k dataset prior to task-specific fine-tuning.|
|||[5] tackle the problem of bird species categorization by first detecting bird parts with a HOG-DPM and then extracting CNN features from the aligned parts.|
|||They gain from using parts and also from fine-tuning a CNN for the task starting from ImageNet weights.|
|||[24] and Chen and Yuille [6] train keypoint specific part detectors, in a CNN framework, for human body pose estimation and show significant improvement compared to [25].|
|||For feature extraction, we use a CNN and more precisely, we use a variant of the singlescale network proposed by Krizhevsky et al.|
|||One could view the whole pipeline shown in Figure 2 as a fully convolutional model and thus one could train it endto-end, optimizing the weights of the CNN for the pool5 feature extraction and the weights of the part models jointly.|
|||We use the publicly available ImageNet weights of the CNN [12] to extract pool5 feature pyramids.|
|||The CNN used for this system is fine-tuned from an ImageNet initialization, as in [12], on jittered instance bounding boxes.|
|||However, unlike the previous two methods we fine-tune the CNN jointly using instance and part boxes from each training sample.|
|||This design explicitly forces the CNN to see each part box during fine-tuning.|
|||We conduct experiments using two different network architectures: a 8-layer CNN as defined in [18], and a 16-layer as defined in [23].|
|||For the 8-layer network, we use the CNN trained on instances, while for the 16-layer network we use the CNN trained jointly on instances and their parts based on results on the val set (Table 3).|
|||Ours (joint fine-tuning) shows the results when using a CNN fine-tuned jointly on instances and parts, while Ours (instance fine-tuning) shows our approach when using a CNN fine-tuned on instances only.|
|||Similar to the task of action classification, we separately learn the parameters of the CNN and the linear SVM.|
|||Again, we fine-tune a CNN for the task in question with the difference that the softmax layer is replaced by a cross entropy layer (sum of logistic regressions).|
|||Also, a network jointly fine-tuned on instances and parts seems to work significantly better than a CNN trained solely on the instance boxes.|
|||[21], Hoai [16] and Simonyan & Zisserman [23] are CNN based approaches.|
|||Ours (joint fine-tuning) uses a CNN fine-tuned jointly on the instances and the parts, while Ours (instance fine-tuning) uses a single CNN fine-tuned just on the instance box.|
|||Ours (instance fine-tuning) uses a CNN fine-tuned on instance boxes, while Ours (joint fine-tuning) uses a CNN fine-tuned jointly on instances and parts.|
||21 instances in total. (in iccv2015)|
|100|Shen_DeepContour_A_Deep_2015_CVPR_paper|We emphasize the following two points: (1) Partitioning contour patches into compact clusters according to their inherent structures is necessary for training an effective CNN model.|
|||(2) How to define the loss function of the CNN is significant for learning a discriminative feature for contour detection.|
|||The other one is proposed by Ganin and Lempitsky in [19], in which feature for image patch is learned using a conventional CNN and then the feature is mapped to an annotation edge map using kd-tree.|
|||Different from these two methods, we learn deep features using shape labels and a CNN with a novel loss.|
|||Data Preparation  In this section we describe how to prepare the training and validation data set for training a CNN model.|
|||The output of FC1 in our CNN will be used as the deep features for contour detection, which is a 128-dimensional feature vector.|
|||In our case, assume that we have K = 50 shape classes, then the unit number of the final layer of the CNN should be 51.|
|||According to the parameter configuration of each layer, the architecture of the CNN can be described concisely by layer notations with layer sizes: COV1(45 (cid:2) 45 (cid:2) 32) !|
|||Note that, the number of the layers in our CNN architecture is less than the generic one used for ImageNet LSVRC [5], as the contour is always represented by a local image patch with smaller size than generic objects.|
|||Positive(cid:173)sharing Loss Function  {  x(i); y(i)  }m  The goal of training a standard CNN is to maximize the probability of the correct class, which is achieved by minimizing the softmax loss.|
|||We sample 2,000,000 image patches from the training set of the BSDS500 dataset, in which the numbers of the positive and negative patches are equal, to form the training data for learning our CNN model.|
|||Another 2,000,000 image patches with the same numbers of the positive and negative patches are sampled from the validation set of the BSDS500 dataset for cross-validation when training our CNN model.|
|||Learning Deep Contour Features by CNN  In this section, we describe how to learn the deep features for contour detection by our CNN model.|
|||First, we introduce the architecture of our CNN model.|
|||CNN Architecture  We train our CNN on a multi-class classification task, namely to classify an image patch to which shape class or the negative class.|
|||The input of our CNN is a 3-channels (RGB) image patch of size 45 (cid:2) 45, which is filtered by the first convolutional layer (COV1) with 32 kernels of size 5 (cid:2) 5 (cid:2) 3 with a padding of 2 pixels.|
|||One can show that  @ log p(i) 0 @a(i) 0  @ log p(i) 0 @a(i)  l  @ log (1 (cid:0) p(i) 0 )  = 1 (cid:0) p(i) 0 ; @a(i) 0 @ log (1 (cid:0) p(i) 0 )  =  @a(i)  l  = (cid:0)p(i) 0 ;  l p(i) p(i) 1 (cid:0) p(i)  0  0  :  (4)  Then the partial derivatives of the new loss are obtained by  ;  l  = (cid:0)p(i) [  @J @a(i) 0  =  1 m  ((cid:21) + 1)1(y(i) = 0)(p(i) 0  K  ] (cid:0) 1)  [(  and  @J @a(i)  l  =  1 m  +((cid:21) + 1)  j=1  1(y(i) = j)p(i) 0  ;  (5)  )  p(i) l  (cid:21)1(y(i) = 0) + 1  K  j=1  (cid:0)(cid:21)  (cid:0) 1(y(i) = l) (  )]  p(i) 0 p(i) 1 (cid:0) p(i)  l  0  1(y(i) = j)  :  (6)  One can verify the effectiveness of the CNN model by a per-patch classification accuracy on the validation set.|
|||To learn our CNN model, we take the publicly available modifiable implementation named Caffe [22] and modify the softmax loss layer to ours.|
|||Their CNN model is targeted on a local contour map, which implicitly performs shape class partition by CNN itself.|
|||However, they use nearest neighbor search in the CNN feature space to obtain the local contour map, which may perform poorly due to the noisy responses in the CNN feature maps, as shown in Fig.|
|||As we apply random forest to our CNN features, the feature selection mechanism embedded in this classifier improves the robustness against the noise.|
||21 instances in total. (in cvpr2015)|
|101|Zhang_Improving_Object_Detection_2015_CVPR_paper|Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box, and 2) training the CNN with a structured loss that explicitly penalizes the localization inaccuracy.|
|||One drawback of CNN features, however, is that they are expensive to compute.|
|||Second, we train a CNN classifier with a structured SVM objective that aims at classification and localization simultaneously.|
|||Overall, the contributions of this paper are as follows: 1) we develop a Bayesian optimization framework that can find more accurate object bounding boxes without significantly increasing the number of bounding box proposals, 2) we develop a structured SVM framework to train a CNN classifier for accurate localization, 3) the aforementioned methods are complementary and can be easily adopted to various CNN models, and finally, 4) we demonstrate significant improvement in detection performance over the RCNN on both PASCAL VOC 2007 and 2012 benchmarks.|
|||With the notable success of CNN on large scale object recognition [25], several detection methods based on CNNs have been proposed [38, 37, 41, 11, 18].|
|||[41] used CNNs to regress the bounding boxes of objects in the image and used another CNN classifier to verify whether the predicted boxes contain objects.|
|||Our method is built upon the R-CNN framework using the CNN proposed in [39], but with 1) a novel method to propose extra bounding boxes in the case of poor localization, and 2) a classifier with improved localization sensitivity.|
|||In contrast, we built the linear structured objective upon high-level features learned by deep CNN architectures.|
|||Localization refinement can be also taken as a CNN regression problem.|
|||[38] refined bounding boxes from a grid layout to flexible locations and sizes using the higher layers of the deep CNN architecture.|
|||Our method is different in that 1) it uses the information from multiple existing regions instead of a single bounding box for predicting a new candidate region, and 2) it focuses only on maximizing the localization ability of the CNN classifier instead of doing any regression from one bounding box to another.|
|||However, evaluating the score function at all regions determined by the sliding window approach is prohibitively expensive when the CNN features are used as the image region descriptor.|
|||Alternatively, the recognition using regions [19, 18] method has been proposed, which requires to evaluate significantly fewer number of regions (e.g., few hundreds or thousands) with different scales and aspect ratios, and it can use the state-of-the-art image features with high computational complexity, such as the CNN features [10].|
|||In our case, (x, y) denotes the top-layer representations of the CNN (excluding the classification layer) at location specified by y,7 which are fed into the classification layer.|
|||7Following [18], we crop and warp the image patch of x at location given by y to a fixed size (e.g., 224224) to compute the CNN features.|
|||Then, we optionally use stochastic gradient descent to finetune the whole CNN classifiers (Sec.|
|||Following [18], we used the CNN models pretrained on ImageNet database [9] with 1, 000 object categories [25, 39], and finetuned the whole network using the target database by replacing the existing softmax classification layer to a new one with a different number of classes (e.g., 20 classes for VOC 2007 and 2012).|
|||Similarly to the training pipeline of R-CNN [18], we finetuned the CNN models (with softmax classification layer) pretrained on ImageNet database using images from both train and validation sets of VOC 2007 and further trained the network with linear SVM (baseline) or the proposed structured SVM objective.|
|||We report the performance with the AlexNet [25] and the VGGNet (16 layers) [39], a deeper  CNN model than AlexNet that showed a significantly better recognition performance and achieved the best performance on object localization task in ILSVRC 2014.9 First of all, we observed the significant performance improvement by simply having a better CNN model.|
|||By finetuning the whole CNN classifiers (StructObj-FT), we observed extra improvement for most cases; for example, we obtained 43.7% mAP in IOU criteria of 0.7, which im 9The 16-layer VGGNet can be downloaded from: https://gist.|
|||Conclusion  In this work, we proposed two complementary methods to improve the performance of object detection in RCNN framework with 1) fine-grained search algorithm in a Bayesian optimization framework to refine the region proposals and 2) a CNN classifier trained with structured SVM objective to improve localization.|
||21 instances in total. (in cvpr2015)|
|102|cvpr18-“Zero-Shot” Super-Resolution Using Deep Internal Learning|We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself.|
|||We exploit the internal recurrence of information within a single image and train a small image-specific CNN at test time, on examples extracted solely from the LR input image itself (i.e., internal self-supervision).|
|||As such, the CNN can be adapted to different settings per image.|
|||The unknown image-specific kernel is estimated directly from the LR test image using [15], and fed into our image-specific CNN as the downscaling kernel (note that externally-trained networks cannot make use of such image-specific information at test-time).|
|||Our image-specific CNN leverages on the power of the cross-scale internal recurrence of image-specific information, without being restricted by the above-mentioned limitations of patch-based methods.|
|||We train a CNN to infer complex image-specific HR-LR relations from the LR image and its downscaled versions (self-supervision).|
|||Since the visual entropy inside a single image is much smaller than in a general external collection of images [24], a small and simple CNN suffices for this image-specific task.|
|||Interestingly, our image-specific CNN produces impressive results (although not SotA) on the ideal benchmark datasets used by the SotA supervised methods (even though our CNN is small and has not been pretrained), and surpasses SotA supervised SR by a large margin on non-ideal images.|
|||Nevertheless, when additional information is available and provided (e.g., the downscaling kernel can be estimated directly from the test image using [15]), our image-specific CNN can make good use of this at test time, to further improve the results.|
|||Image-Specific CNN  Our image-specific CNN combines the predictive power and low entropy of internal image-specific information, with the generalization capabilities of Deep-Learning.|
|||Given a test image I, with no external examples available to train on, we construct an Image-Specific CNN tailored to solve the SR task for this specific image.|
|||We then apply the resulting trained CNN to the test image I, now using I as the LR input to the network, in order to construct the desired HR output I  s (bottom of Fig.|
|||Note that the trained CNN is fully convolutional, hence can be applied to images of different sizes.|
|||As such, these networks tend to be ex 3121  Image-Specific CNN  Zero-Shot SR.|
|||(b) Our proposed method (ZSSR): a small image-specific CNN is trained on examples extracted internally, from the test image itself.|
|||The resulting self-supervised CNN is then applied to the LR image I to produce its HR output.|
|||3, it trains a CNN to recover the handrail in the LR test image from its lower-res versions, even if no other handrail appears in its receptive field.|
|||When this CNN is then applied to the test image itself, it can recover new handrails elsewhere, due to using the same image-specific filters.|
|||Moreover, a single supervised CNN is unlikely to perform well for all possible types of degradations/settings.|
|||Indeed, our experiments show that for low-quality LR images, and for a wide variety of degradation types, the image-specific CNN obtains significantly better SR results than SotA EDSR+ [13] (see Sec.|
|||not SotA) on the ideal benchmark datasets for which the SotA supervised methods train and specialize (even though our CNN is small, and has not been pretrained).|
||21 instances in total. (in cvpr2018)|
|103|Liang_Towards_Computational_Baby_ICCV_2015_paper|We then fine-tune the CNN with the instances of previously learned visual concepts for transferring object classification network into the detection network.|
|||In addition, the pre-trained CNN will be gradually fine-tuned if enough instances are collected, which leads to more informative features for training detectors.|
|||[33] achieved great progress in the classification task with large and deep supervised CNN training.|
|||Also textbased [8] and semantic relationships [4] were further used  1000  Prior Knowledge ... ... ... ... Learning with Video Contexts Exploring  Physical World Pre-trained CNN Knowledge  Updating Better Features Detector  Updating Mining variable Instances Our Computational Learning Framework Motivations Two Positive Instances Exemplar Learning to provide more constraints on selecting instances.|
|||First, we pre-train a general CNN on the ImageNet [7] with image-level annotations.|
|||Second, we fine-tune the previous pre-trained CNN with the previously learned concepts in ILSVRC2013 detection dataset for transferring the object classification network into detection network.|
|||We explore two CNN arthe 7-layer architecture by chitectures for pre-training: Krizhevsky et al.|
|||The CNN fine-tuning starts SGD with a learning rate of 0.001 for both two networks.|
|||During finetuning, we only replace the 1000-way classification layer of the pre-trained CNN with a randomly initialized (N+1)-way classification layer, where N is the number of learned concepts, plus one for background.|
|||Exemplar Learning  The initial concept detector can be learned based on these deep features from pre-trained CNN and very few positive instances of a new concept.|
|||Once enough instances of each new concept (about 10,000 instances) are obtained, the pretrained CNN can be further improved to generate more informative features by fine-tuning it with these new instances.|
|||During fine-tuning, we replace the (N+1)-way output layer of the pre-trained CNN in Section 3.1 with a randomly initialized (M+1)-way classification layer (including M new concepts and one for background).|
|||We implement two versions of R-CNN (i.e., RCNN 179 and R-CNN 179 BB with bounding-box regression), which firstly fine-tune the classification CNN with 179 extra classes and then fine-tune the CNN with VOC 20 classes following the settings in [14].|
|||When fine-tuning the CNN based on the Network in Network (NIN) [21], our method (B NIN FT I2 BB) can achieve 67.1% on VOC2007, 63.8% on VOC 2010, and 63.2% on VOC2012, which outperforms the R-CNN BB [14] by a large margin of more than 8% on all three test sets and significantly outperforms the R-CNN NIN BB by 1.7% on VOC2007.|
|||Then we fine-tune the CNN with mined instances (FT) and 2 iterations are further performed to improve the detectors (FT I2).|
|||Figure 4 also reports the performances of fine-tuning the pre-trained CNN when more diverse instances are mined.|
|||After further fine-tuning the pretrained CNN with these mined instances, 4.8% improvement is achieved by comparing B FT with B I15), as reported in Table 1.|
|||Using NIN as the CNN architecture, we achieve 57.6% after 15 iterations (B NIN I15) and obtain 60.9% after finetuning (B NIN FT).|
|||seed number  1  2  3  4  5  6  7  8  9  1 2 5  65.1 66.1 68.1 67.3 67.2 68.2 65.4 66.1 67.8 66.7 67.0 66.8 70.4 69.7 68.3 69.2 66.9 67.9 69.2 69.8 69.7 69.3 71.2 72.3 70.1 70.8 71.5 70.9 70.2 71.3  10 mean 66.8 68.5 70.7  better CNN architectures (e.g., googleLeNet [33]), it is predictable that our detectors can be further improved.|
|||The CNN fine-tuning only with the aeroplane class may lead to this slightly decrease.|
|||The detectors are trained over the deep features from the 7-layer CNN and two more iterations are performed to mine more instances from videos.|
||21 instances in total. (in iccv2015)|
|104|Deep Outdoor Illumination Estimation|We extract limited field-of-view images from the panoramas, and train a CNN with this large set of input imageoutput lighting parameter pairs.|
|||Our key idea is to train a CNN using input-output pairs of LDR images and HDR illumination parameters that are automatically extracted from a large database of 360 panoramas.|
|||To this end, our goal is to train a CNN to directly regress a single input low dynamic range image to its corresponding high dynamic range (HDR) outdoor lighting conditions.|
|||Given the success of deep networks at related tasks like intrinsic images [42] and reflectance map estimation [34], our hope is that an appropriately designed CNN can learn this relationship.|
|||However, training such a CNN requires a very large dataset of outdoor images with their corresponding HDR lighting conditions.|
|||However, instead of designing hand-crafted features to estimate illumination, we train a CNN to directly learn the highly complex mapping between image pixels and illumination parameters.|
|||Overview  We aim to train a CNN to predict illumination conditions from a single outdoor image.|
|||Then, we design and train a CNN that given an input image sampled from the panorama, outputs the fit illumination parameters (sec.|
|||The proposed CNN architecture.|
|||The resulting photos are bilinearly interpolated from the panorama to a resolution 320  240, and used directly to train the CNN described in the next section.|
|||typically converges in around 78 epochs, because our CNN is not as deep as most modern feed-forward CNN used in vision.|
|||Evaluation  We evaluate the performance of the CNN at predicting the HDR sky environment map from a single image in a variety of ways.|
|||Illumination parameters on SUN360  Sun position We begin by evaluating the performance of the CNN at predicting the sun position from a single input image.|
|||4-(c) shows that the CNN is not biased towards an azimuth position, and is robust across  7316  020406080100120140160180Degrees0100020003000400050006000Number of images011.2522.533.754556.2567.578.7590Elevation (degrees)80604020020406080Error (degrees)20015010050050100150200Azimuth (degrees)20015010050050100150200Error (degrees)Figure 6.|
|||[26] fails while the CNN reports robust performance, comparable to fig.|
|||The CNN tends to favor clear skies (low turbidity), and has higher errors when the exposure is high.|
|||10 shows that the camera elevation estimated from the CNN can be used within the rendering pipeline to automatically rotate the virtual camera  Figure 9.|
|||From a single image, the CNN predicted a full HDR sky map, which is used to render an object into the image.|
|||For each example, the top row shows (left) a bunny model relit by the ground truth HDR illumination conditions captured in situ; (right) the same bunny model, relit by the illumination conditions estimated by the CNN solely from the background image, completely automatically.|
|||The CNN is then applied to the input photos to predict their illumination conditions.|
|||Our key idea is to train a deep CNN on pairs of photos and panoramas in the SUN360 database, which we augment with HDR information via a physics-based model of the sky.|
||21 instances in total. (in cvpr2017)|
|105|Fine-Tuning Convolutional Neural Networks for Biomedical Image Analysis_ Actively and Incrementally|AIFT starts directly with a pre-trained CNN to seek worthy samples from the unannotated for annotation, and the (fine-tuned) CNN is further fine-tuned continuously by incorporating newly annotated samples in each iteration to enhance the CNNs performance incrementally.|
|||Our AIFT method starts directly with a pre-trained CNN to seek salient samples from the unannotated for annotation, and the (fine-tuned) CNN is continuously finetuned by incrementally enlarging the training dataset with newly annotated samples.|
|||This outstanding performance is attributed to a simple yet powerful observation: To boost the performance of CNNs in biomedical imaging, multiple patches are usually generated automatically for each candidate through data augmentation; these patches generated from the same candidate share the same label, and are naturally expected to have similar predictions by the current CNN before they are expanded into the training dataset.|
|||17340  Several researchers have demonstrated the utility of finetuning CNNs for biomedical image analysis, but they only performed one-time fine-tuning, that is, simply fine-tuning a pre-trained CNN once with available training samples involving no active selection processes (e.g., [4, 19, 5, 2, 21, 7, 18, 24]).|
|||[2] replaced the fully connected layers of a pre-trained CNN with a new logistic layer and trained only the appended layer with the labeled data while keeping the rest of the network the same, yielding promising results for classification of unregistered multiview mammograms.|
|||In [5], a fine-tuned pre-trained CNN was applied for localizing standard planes in ultrasound images.|
|||However, they all performed one-time fine-tuningsimply fine-tuning a pre-trained CNN just once with available training samples, involving neither active selection processes nor continuous fine-tuning.|
|||i }, j  [1, m] {Ci has m patches}  Input: U = {Ci}, i  [1, n] {U contains n candidates} Ci = {xj M0: pre-trained CNN b: batch size : patch selection ratio Output: L: labeled candidates Mt: fine-tuned CNN model at Iteration t Functions: p  P (C, M) {outputs of M given x  C} Mt  F (L, Mt1) {fine-tune Mt1 with L} a  mean(pi) {a = 1 Initialize: L  , t 1  mPm  j=1 pj i }  1 repeat  2  3  4  5  6  7  8  9  10  11  12  for each Ci  U do  pi  P (Ci, Mt1) if mean(pi) > 0.5 then  S   i  top  percent of the patches of Ci  else  S   i  bottom  percent of the patches of Ci  end Build matrix Ri using Eq.|
|||Continuous fine(cid:173)tuning  At the beginning, the labeled dataset L is empty; we take a pre-trained CNN (e.g., AlexNet) and run it on U to select b number of candidates for labeling.|
|||The newly labeled candidates will be incorporated into L to continuously fine-tune the CNN incrementally until the performance is satisfactory.|
|||We also found that continuously fine-tuning the CNN with only newly labeled data demands careful meta-parameter adjustments.|
|||Formally, assuming the prediction of patch xj i , we define its entropy as:  i by the current CNN is pj  |Y |  ej i =   pj,k i  log pj,k  i  (1)  Xk=1  and diversity between patches xj  i and xl  i of candidate Ci as:  di(j, l) =  |Y |  Xk=1  (pj,k  i  pl,k  i )log  pj,k i pl,k i  (2)  i denotes the information furnished by patch xj  Entropy ej i of candidate Ci in the unlabeled pool.|
|||An illustration of prediction patterns  i , x2  i , ..., xm  Given unlabeled candidates U = {C1, C2, ..., Cn} with i }, assuming the prediction of patch xj Ci = {x1 i by the current CNN is pj i for j  [1, m] the prediction pattern of candidate Ci.|
|||The annotation of these types of candidates at this stage should be postponed because the current CNN has most likely predicted them correctly; they would contribute very little to fine-tuning the current CNN.|
|||We assume that a candidate has 11 patches, and their probabilities predicted by the current CNN are listed in Column 2.|
|||Using 5% of the whole training dataset (800/16300), the CNN can predict almost perfectly on the remaining 95% dataset.|
|||We adopt the 2-channel representation because it consistently captures PEs in cross-sectional and longitudinal views of vessels, achieving greater classification accuracy and accelerating CNN training process.|
|||AIFT (Entropy+Diversity)1/4 and Diversity1/4 converge the fastest among the 8 methods and yields the best overall performance, attributed to  Figure 7: Five different PEs in the standard 3-channel representation, as well as in the 2-channel representation [23] , which was adopted in this work because it achieves greater classification accuracy and accelerates CNN training convergence.|
|||From this process, we have observed the following:   Patterns A and B are dominant in the earlier stages of AIFT as the CNN has not been fine-tuned properly to the target domain.|
||| Patterns C, D and E are dominant in the later stages of AIFT as the CNN has been largely fine-tuned on the target dataset.|
||20 instances in total. (in cvpr2017)|
|106|G2DeNet_ Global Gaussian Distribution Embedding Network and Its Application to Visual Recognition|Specifically, as in [28, 33, 40], we use global Gaussians as image representations and propose a global Gaussian embedding layer to combine them in deep CNN architectures.|
|||The core of G2DeNet is a novel layer of global Gaussians as image representations, inserted after the last convolutional layer in a deep CNN in an end-to-end manner.|
|||At the heart of DeepO2P is a trainable O2P layer plugged into the deep CNN architecture performing secondorder pooling of convolutional features.|
|||The bilinear CNN (BCNN) [24] model inserts a trainable bilinear pooling layer after the last convolutional layer in CNN architectures.|
|||This layer computes the outer products of features from two CNN models, and then performs sumpooling and normalization.|
|||When the two CNN models are different, BCNN captures correlations of different sources of features.|
|||If the two CNN models are identical, the outer products plus sum-pooling leads to second-order, noncentral moments, as in DeepO2P; differently, BCNN performs power normalization followed by l2-normalization for the resulting SPD matrices rather than matrix logarithm used in DeepO2P.|
|||They both concern insertion into the CNN architectures of a trainable layer consisting of features encoding and pooling to form an orderless image representation.|
|||To make Gaussian be integrated into CNN architectures, we first map a Gaussian to a square rooted SPD matrix.|
|||Our layer can be plugged into various CNN architectures (e.g., AlexNet [21] and VGG-VD-Net [34]) in an end-to-end manner.|
|||Note that in this case the bilinear pooling method shares the same CNN model, and leads to the second-order, non-central moment of convolutional features.|
|||The BCNN obtains state-of-the-art performance by pooling of outer products of the outputs from the last convolutional layer (with the ReLU operation) of two CNN models [24].|
|||2735  Methods  PG-Alignment [18] RAID-G [40] ST-CNN [16] PD+FC+SWFV-CNN [42] SPDA-CNN+ensemble [41] PN-CNN [3] FC-CNN [D] (w/ ft) FC-CNN [D] (w/ ft) FV-CNN [D] (w/ ft) [6] FV-CNN [D] (w/ ft) [6] BCNN [D,D] (w/ ft) [24] BCNN [D,D] (w/ ft) [24] BCNN [D,M] (w/ ft) [24] G2DeNet (Ours) G2DeNet (Ours)  Train  Test  BBox  Parts  BBox  Parts  Pre-trained CNN models  Accuracy (%)                                        VGG-VD19 VGG-VD19 Inception+BN VGG-VD16  VGG-VD16 + AlexNet  AlexNet  VGG-VD16 VGG-VD16 VGG-VD16 VGG-VD16 VGG-VD16 VGG-VD16  VGG-VD16 + VGG-M  VGG-VD16 VGG-VD16  82.8 82.1 84.1 84.5 85.1 85.4 70.4 76.4 74.7 77.5 84.0 84.8 85.1 87.1 87.6  Table 3.|
|||STCNN [16] introduced a trainable Spatial Transformer (ST) module for overcoming lack of spatial invariance of existing CNN architectures.|
|||The semantic part detection and abstraction CNN (SPDACNN) [41] developed an end-to-end architecture containing two sub-networks which performed semantic parts detection and recognition in a unified framework.|
|||[3] proposed a pose normalized deep convolutional neural network (PNCNN) to locate and normalize image patches, while employing a deep CNN to extract features for patch representation.|
|||As we employ the same CNN model (i.e., VGG-VD16) with FC-CNN, FV-CNN and BCNN, we attribute the improvements to the proposed global Gaussian embedding layer.|
|||It shows that plugging the global Gaussian embedding layer into the deep CNN trained endto-end is much better than those with no training and training separately, and it also demonstrates the effectiveness of our structural backpropagation method.|
|||The proposed global Gaussian embedding layer is modular and is of no parameter to learn, readily applicable to AlexNet or VGG-Net, and combining this layer with other CNN models (e.g., Inception [36] and ResNet [12]) is our future research.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||20 instances in total. (in cvpr2017)|
|107|Viresh_Ranjan_Iterative_Crowd_Counting_ECCV_2018_paper|Hence, we present a two branch CNN architecture for generating high resolution density maps, where the first branch generates a low resolution density map, and the second branch incorporates the low resolution prediction and feature maps from the first branch to generate a high resolution density map.|
|||Crowd counting can be posed as a CNN-based density estimation problem, but this problem can be challenging for a single CNN due to the huge variation of density values across pixels of different images.|
|||Earlier works [20, 15] have tackled this challenge by using a multi-column or a switching CNN architecture.|
|||These CNN architectures consist of three parallel CNN branches with different receptive field sizes.|
|||More recently, a five-branch CNN architecture was proposed [16] where three of the branches resembled the previous multi-column CNN [20], while the remaining two branches acted as global and local context estimators.|
|||Some of the key takeaways from these previous approaches are: (1) using a multi-column CNN model with varying kernel sizes improves the performance of crowd density estimation; and (2) augmenting the feature set with the ones learned from a task related to density estimation, such as count range classification, improves the performance of the density estimation task.|
|||On the top is the Low Resolution CNN branch (LR-CNN) and at the bottom is the High Resolution CNN branch (HR-CNN).|
|||ic-CNN has two branches: Low Resolution CNN (LR-CNN) and High Resolution CNN (HR-CNN).|
|||We propose ic-CNN, a two-stage CNN framework for crowd density estima tion and counting.|
|||[4] classified an image into five broad crowd density categories and used a cascade of two CNNs in a boosting like strategy where the second CNN was trained on the images misclassified by the first CNN.|
|||[20] proposed a multi-column CNN architecture (MCNN) with filters and receptive fields of various sizes.|
|||The CNN column with smaller receptive field and filter sizes were responsible for the denser crowd images, while the CNN columns with larger receptive fields and filter sizes were meant for the less dense crowd images.|
|||Given that the number of training samples in annotated crowd counting datasets is much smaller in comparison to the datasets pertaining to image classification and segmentation tasks, training a CNN from scratch on full images might lead to overfitting.|
|||One issue with MCNN was that it fused the features form three CNN columns for predicting the density map.|
|||For a given patch, it is expected that the counting performance can be made more accurate by choosing the right CNN column that specializes in analyzing images of similar density values.|
|||On the Shanghaitech Part B dataset, using the one-stage ic-CNN, which has a simpler architecture than CP-CNN [16], we improve on the previously reported state of the art results by 48.3% using the MAE metric and 46.8% using the RMSE metric  Crowd CNN [19] MCNN [20] Switching CNN [15] CP-CNN [16] ic-CNN (one stage) ic-CNN (two stages)  Part A  Part B  RMSE  MAE  RMSE  277.7 173.2 135.0 106.4 117.3 116.2  32.0 26.4 21.6 20.1 10.4 10.7  49.8 41.3 33.4 30.1 16.7 16.0  MAE  181.8 110.2 90.4 73.6 69.8 68.5  4 Experiments  We conduct experiments on three challenging datasets: Shanghaitech [20], WorldExpo10 [19], and UCF Crowd Counting Dataset [6].|
|||Model  Training Time  Number of Parameters  MCNN [20] Switching CNN [15] CP-CNN [16] ic-CNN (proposed)  unknown  22 hrs  unknown  10 hrs  1.27  105 1.2  107 6.3  107 7.9  106  MAE  110.2 90.4 73.6 69.8  Table 5.|
|||An ic-CNN takes 10 hours to train, while a Switching CNN takes around 22 hours.|
|||Method  Crowd CNN [19] MCNN [20] Switching CNN (sans perspective) [15] Switching CNN (with perspective) [15] CP-CNN[16] ic-CNN (proposed)  S1  9.8 3.4 4.4 4.2 2.9 17.0  S2  14.1 20.6 15.7 14.9 14.7 12.3  S3  14.3 12.9 10.0 14.2 10.5 9.2  S4  22.2 13.0 11.0 18.7 10.4 8.1  S5  3.7 8.1 5.9 4.3 5.8 4.7  Avg  12.9 11.6 9.4 11.2 8.8 10.3  Table 7.|
|||al [6] Crowd CNN [19] Crowdnet [1] MCNN [20] Hydra2s [13] Switch CNN [15] CP-CNN [16] ic-CNN (proposed)  MAE  493.4 419.5 467.0 452.5 377.6 333.7 318.1 295.8 260.9  RMSE  487.1 487.1 498.5 509.1 425.6 439.2 320.9 365.5  4.5 Qualitative Results  In Figure 4, we show some qualitative results on images from the Shanghaitech Part-A dataset obtained using ic-CNN.|
||20 instances in total. (in eccv2018)|
|108|Lin_Deep_Structured_Scene_CVPR_2016_paper|Once these scene configurations are determined, then the parameters of both the CNN and RNN are updated accordingly by back propagation.|
|||In our deep CNNRNN architecture, the CNN and RNN models are collaboratively integrated for accomplishing the scene parsing from complementary aspects.|
|||We utilize the CNN to layerwise extract features from the input scene image and generate the representations of semantic objects.|
|||Then, the RNN is sequentially stacked based on the CNN feature representations, generating the structured configuration of the scene.|
|||The integration of CNN and RNN models is general to be extended to other high-level computer vision tasks.|
|||The input image is directly fed into the CNN to produce score map of each semantic category and feature representation of each pixel.|
|||tions on the proposed CNN and RNN networks.|
|||In our CNN-RNN architecture, the CNN model is introduced to perform semantic segmentation by assigning an entity label to each pixel, and the RNN model is introduced to discover hierarchical structure and interaction relations among entities.|
|||Moreover, the CNN model also produces the feature representation for each entity, which will be fed to the RNN model for generating parsing tree.|
|||In the following, we provide the more detailed explana The CNN model is designed to accomplish two tasks: semantic labeling and generating feature representations for entities.|
|||Vi is the set of semantic entity features produced by CNN from the i-th image.|
|||Semantic Label Loss  Given intermediate label map C, the semantic label task performed by CNN can be optimized as a pixel-wise classfication problem.|
|||Finally, the CNN loss  is computed according to Eq.(9).|
|||(iii) Updating the CNN and RNN parameters.|
|||the CNN and RNN parameters.|
|||For the waterfall strategy, we first perform 8,000 iterations of strongly-supervised pre-training on the CNN, followed by 16,000 iterations of weakly-supervised training on the CNN and RNN.|
|||For the fusion strategy, we use a weighted sum of stronglysupervised and weakly-supervised loss functions to train the CNN and RNN, where we use 280 strong samples together with weak training samples, and the loss weight is set as 1:1.5 (strong:weak).|
|||PASCAL 2012 val result with semi-supervised learning  2282  initialized values, we update all parameters of the CNN; iii) We separate the learning of CNN and RNN, i.e.|
|||we first update the CNN for 16000 iterations with the fixed RNN, and then update RNN for 16000 iterations with the fixed CNN; iv) We update both CNN and RNN in the whole process with an end-to-end and joint learning manner.|
|||Learning CNN and RNN separately performs better than learning with either fixed, but is still worse than endto-end and joint learning.|
||20 instances in total. (in cvpr2016)|
|109|Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper|One key module of CNN model is the convolution layer, which extracts features from high-dimensional structural data efficiently by a set of convolution kernels.|
|||Illustration of the difference between CNN and QCNN on convolution layers.|
|||However, how to design a quaternion CNN is still an open problem.|
|||AlexNet [20] is the first deep CNN that greatly outperforms all past models in image classification task.|
|||Recently, the CNN models are also introduced for low-level vision tasks.|
|||3 Proposed Quaternion CNNs  3.1 Quaternion convolution layers  Focusing on color image representation, our quaternion CNN treats a color image  as a 2D pure quaternion matrix, denoted as bA = [ann ]  HNN , where N  Quaternion Convolutional Neural Networks  5  bA = 0 + Ri + Gj + Bk,  represents the size of the image.4 In particular, the quaternion matrix bA is  (3)  where R, G, B  RNN represent red, green and blue channels, respectively.|
|||Denote L as the realvalued loss function used to train our quaternion CNN model.|
|||Compared with real-valued CNN models in these two tasks, our QCNN models show improvements on learning results consistently.|
|||In the experiment, each layer of real-valued CNN and QCNN are of same number of filters, so actually QCNN has more parameters.|
|||The classification accuracy on the testing set obtained by our QCNN is also better than that of real-valued CNN even in the very beginning of training phase.|
|||Moreover, even if we reduce the number of QCNNs parameters, the proposed QCNN model is still superior to the real-valued CNN with the same size.|
|||Experiment results in denoising tasks  Model  Dataset Test PSNR (dB)  Dataset  Test PSNR (dB)  Real-valued CNN 102 flowers Quaternion CNN 102 flowers  30.9792 31.3176  subset of COCO subset of COCO  30.4900 30.7256  in which the objects have obvious color attributes (i.e., the flowers in 102 Oxford flower data set).|
|||A QCNN and a real-valued CNN with this structure are both built, and the QCNN has fewer filters each layer to ensure a similar number of parameters to the real-valued CNN.|
|||For both real-valued CNN and our QCNN model, the optimizer is Adam with 0.001 learning rate, and the batch size is 64 for the 102 Oxford flower data set and 32 for the COCO subset, respectively.|
|||In the denoising task, QCNN shows at least 0.5dB higher PSNR than CNN for images in (a).|
|||For images in (b), CNN offers better result.|
|||Table 2 shows quantitative comparisons for the real-valued CNN model and the proposed QCNN model.|
|||Similar to the experiments in color image classification task, the loss function of our QCNN converges more quickly to a smaller value and its PSNR on testing images becomes higher than that of the real-valued CNN after 100 epochs.|
|||We can find that the images on which our QCNN shows better performance are often colorful, while images where our QCNN is inferior to the real-valued CNN are close to grayscale images.|
|||We show the quantile-quantile plots of these two metrics with respect to the difference between PSNR value of real-valued CNN and that of our QCNN (denoted as D) in Fig.|
||20 instances in total. (in eccv2018)|
|110|cvpr18-Learning a Single Convolutional Super-Resolution Network for Multiple Degradations|Notably, recent years have witnessed a dramatic upsurge of using CNN for SISR.|
|||In this paper, we focus on discriminative CNN methods for SISR so as to exploit the merits of CNN, such as the fast speed by parallel computing, high accuracy by end-to-end training, and tremendous advances in training and designing networks [16, 18, 21, 28].|
|||While several SISR models based on discriminative CNN have reported impressive results, they suffer from a common drawback: their models are specialized for a single simplified degradation (e.g., bicubic degradation) and lack scalability to handle multiple degradations by using a single model.|
|||Because the practical degradation of SISR is much more complex [40, 51], the performance of learned CNN models may deteriorate seriously when the assumed degradation deviates from the true one, making them less effective in practical scenarios.|
|||However, little work has been done on how to design a CNN to address this crucial issue.|
|||Then we argue that one may tackle this issue by taking LR input, blur kernel and noise level as input to CNN but their dimensionality mismatch makes it difficult to design a single convolutional superresolution network.|
|||To the best of our knowledge, there is no attempt to consider both the blur kernel and noise for SISR via training a single CNN model.|
|||It should be noted that we make no effort to use specialized network architectures but use the plain CNN as in [9, 41].|
|||Related Work  The first work of using CNN to solve SISR can be traced back to [8] where a three-layer super-resolution network (SRCNN) was proposed.|
|||By analyzing the relation between CNN and MAP inference, Zhang et al.|
|||An interesting line of CNN-based methods which can go beyond bicubic degradation adopt a CNN denoiser to solve SISR via model-based optimization framework [4, 34, 57].|
|||Though blur kernel and noise have been recognized as key factors for the success of SISR and several methods have been proposed to consider those two factors, there has been little effort towards simultaneously considering blur kernel and noise in a single CNN framework.|
|||Consequently, more insights on CNN architecture design can be obtained.|
|||By treating CNN as a discriminative learning solution to  Eqn.|
|||By doing so, the degradation maps then can be concatenated with the LR image, making CNN possible to handle the three inputs.|
|||(3)) can perform generic image super-resolution with the same image prior, it is intuitive to jointly perform denoising and SISR in a unified CNN framework.|
|||Moreover, the work [56] indicates that the parameters of the MAP inference mainly model the prior; therefore, CNN has the capacity to deal with multiple degradations via a single model.|
|||To show the effectiveness of the dimensionality stretching strategy, we resort to plain CNN without complex architectural engineering.|
|||First, with a moderate network depth and advanced CNN training and design such as ReLU [26], BN [20] and Adam [25], it is easy to train the network without the residual learning strategy.|
|||To enhance the practicability of CNN for SISR, it seems the most straightforward way is to learn a blind model with synthesized training data by different degradations.|
||20 instances in total. (in cvpr2018)|
|111|Li_SBGAR_Semantics_Based_ICCV_2017_paper|Symbol  indicates the operation of computing the dense optical flow image using two continuous frames, while symbol  indicates the operation of concatenating two CNN feature vectors into one single vector.|
|||prove that CNN features contain more representative information of an image than other manually designed features, e.g.|
|||In addition, CNN features perform well in the task of scene classification [16], which pro 2.|
|||We extract CNN features from both original frames (CNN2 in Figure 1) and optical flow images (CNN1 in Figure 1).|
|||A LSTM model can generate good captions using the CNN feature vector as its input [19].|
|||During the training process: The inputs of the Caption Generation Model consist of (i) concatenated CNN Features, (ii) Input Captions, and (iii) Target Captions (Ground Truth).|
|||We then feed the CNN Feature as well as the word2vec vector into a LSTM model (LSTM1) to generate the probability distribution of the next word in the sequence.|
|||During the testing process: The inputs of our model only consist of (i) CNN Features and (ii) Input Captions (initialized with a single starting symbol, <SOS>).|
|||A CNN model can generate vectors with the same dimension even if the lengths of input captions vary.|
|||[24] show that a simple CNN model achieves excellent results in the task of sentence classification.|
|||Given time steps captions, our model first extracts CNN features from these captions and then feed the CNN features into the prediction layer (LSTM2) to generate a probability distribution for all potential labels.|
|||CNN1 and CNN2: To extract CNN features from images, we use an Inception-v3 model [26] pre-trained on ImageNet [27] as a feature extractor.|
|||Specifically, we use the output of the final pooling layer (pool 3) in Inception-v3 model as the CNN feature of an image.|
|||Thus, the dimension of the extracted CNN feature is 2048 and the dimension of concatenated CNN1 and CNN2 features is 4096.|
|||Because the size of all cells in a LSTM model are the same, so we use a transformation matrix (4096  1024) and a bias vector (1  1024) to transform a 1  4096 CNN feature into 1  1024 (1024 is the dimension of embedded captions).|
|||To do so, we only need to multiply the CNN feature (1  4096) with the transformation matrix (4096  1024) and add the bias vector (1  1024).|
|||Then, we concatenate the transformed CNN feature with the embedded caption and feed them((nw + 1)  1024) into LSTM1.|
|||CNN3: We first embed the generated captions into a dense representation using word2vec model before feeding them into the CNN model (CNN3).|
|||Instead of using a pretrained CNN model, we implement a simple CNN model which only performs convolution and max-pooling operations with a generated caption as its input.|
|||Thus, there is a total 20 filters in this CNN model.|
||20 instances in total. (in iccv2017)|
|112|Koller_Deep_Hand_How_CVPR_2016_paper|Deep Hand: How to Train a CNN on 1 Million Hand Images  When Your Data Is Continuous and Weakly Labelled  Oscar Koller, Hermann Ney  Richard Bowden  Human Language Technology & Pattern Recog.|
|||Centre for Vision Speech & Signal Processing  RWTH Aachen University, Germany  {koller,ney}@cs.rwth-aachen.de  University of Surrey, UK r.bowden@surrey.ac.uk  Abstract  This work presents a new approach to learning a framebased classifier on weakly labelled sequence data by embedding a CNN within an iterative EM algorithm.|
|||This allows the CNN to be trained on a vast number of example images when only loose sequence level information is available for the source videos.|
|||The iterative EM algorithm leverages the discriminative ability of the CNN to iteratively refine the frame level annotation and subsequent training of the CNN.|
|||By embedding the classifier within an EM framework the CNN can easily be trained on 1 million hand images.|
|||As such, this manuscript provides the following contributions:   formulation of an EM-based algorithm integrating CNNs with Hidden-Markov-Models (HMMs) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of CNN architectures   robust fine grained single frame hand shape recognition based on a CNN-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining   making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1   and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline  Convolutional Neural Networks (CNNs) have been demonstrated to provide superior performance in many tasks.|
|||Weakly Supervised CNN Training  The proposed algorithm constitutes a successful solution to the problem of weakly supervised learning from noisy sequence labels to correct frame labels.|
|||The algorithm iteratively refines the temporal class  3794  boundaries and trains a CNN that performs single image hand shape recognition.|
|||The iterative process is similar to a forced alignment procedure, however, rather than using Gaussian mixtures as the probabilistic component we use the outputs of the CNN directly.|
|||Using Bayes decision rule, we maximise the posterior probability over all possible true labels l, corresponding to casting the class symbol model P r(xt|kt) given by the CNN as the marginal over all possible HMM temporal state sequences sT 1 = s1, .|
|||Pre-trained CNN models constitute such a source of knowledge, which seems reasonable as the pretrained convolutional filters in the lower layers may capture simple edges and corners, applicable to a wide range of image recognition tasks.|
|||We base our CNN implementation on [15], which is an efficient C++ implementation using the NVIDIA CUDA Deep Neural Network GPU-accelerated library.|
|||For evaluating the 1Million-Hands CNN classifier, we manually labelled 3361 images from the RWTH-PHOENIX-Weather 2014 Development set 2.|
|||To speed up CNN training time we randomly sample from the observation sequences of the garbage class.|
|||Showing the top-1 and top-5 CNN accuracies for every 16th training epoch measured on the manual annotations (top-1 top-5) and on a development split of the automatically labelled training data (auto-top-1 auto-top-5).|
|||In Figure 5 we show the evolving accuracy during one epoch of CNN training measured 16 times per iteration.|
|||Frame(cid:173)Level Evaluation  In terms of run time, the CNN requires 8.24ms in the forward-pass to classify a single image (when supplied in batches of 32 images) on a single GeForce GTX 980 GPU with 4095 MiB.|
|||Note, in the first two cases the CNN successfully classifies handshapes of an unseen data set and is thus independent of the data set (no samples of the evaluation corpus are used for training), as we are measuring the evaluation on the RWTH-PHOENIXWeather hand shape annotations.|
|||Conclusion  In the course of this work we presented a new approach to learning a frame-based classifier using weakly labelled sequence data by embedding a CNN within an iterative EM algorithm.|
|||  iterative EM algorithm leverages the discriminative ability of the CNN to iteratively refine the frame level annotation and subsequent training of the CNN.|
||20 instances in total. (in cvpr2016)|
|113|Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation|Inspired by recent works on multiscale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs.|
|||By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end.|
|||Depth map obtained by considering a pre-trained CNN (e.g.|
|||We also show that, by introducing a common CNN implementation for mean-fields updates in continuous CRFs, both models are equivalent to sequential deep networks and an end-to-end approach can be devised for training.|
|||First, we propose a novel approach for predicting depth maps from RGB inputs which exploits multi-scale estimations derived from CNN inner layers by fusing them within a CRF framework.|
|||We also show that our approach outperforms state of the art depth estimation methods on public benchmarks and that the proposed CRF-based models can be employed in combination with different pre-trained CNN architectures, consistently enhancing their performance.|
|||The most similar work to ours is [20], where the representational power of deep CNN and continuous CRF is jointly exploited for depth prediction.|
|||Our model is composed of two main components: a front-end CNN and a fusion module.|
|||Finally, we show how our whole deep network can be trained endto-end, introducing a novel CNN implementation for meanfield iterations in continuous CRFs.|
|||As shown in previous works [22, 3, 33], features generated from different CNN layers capture complementary information.|
|||The main idea behind the proposed fusion block is to use CRFs to effectively integrate the side output maps of our front-end CNN for robust depth prediction.|
|||Multi(cid:173)scale models as sequential deep networks  In this section, we describe how the two proposed CRFsbased models can be implemented as sequential deep networks, enabling end-to-end training of our whole network model (front-end CNN and fusion module).|
|||C-MF: a common CNN implementation for two models.|
|||Front-end CNN Architectures.|
|||In the first phase, we train the frontend CNN with parameters initialized with the corresponding ImageNet pretrained models.|
|||When the pretraining is finished, we connect all the side outputs of the front-end CNN to our CRFs-based multiscale deep models for end-to-end training of the whole net 5359  Error (lower is better) Accuracy (higher is better)  Network  Method  rel  log10   < 1.25  < 1.252  < 1.253  rms 0.185 0.077 0.723 HED [33] 0.980 Hypercolumn [10] 0.189 0.080 0.730 0.978 0.193 0.082 0.742 0.976 CRF 0.187 0.079 0.727 Ours (single-scale) 0.980 Ours Cascade (3-s) 0.176 0.074 0.695 0.980 0.981 0.169 0.071 0.673 Ours Cascade (5-s) 0.981 Ours Multi-scale (3-s) 0.172 0.072 0.683 Ours Multi-scale (5-s) 0.163 0.069 0.655 0.981 Table 1.|
|||We evaluate the proposed CRF-based models and compare them with other methods for fusing multiscale CNN representations.|
|||The core of the method is a novel framework based on continuous CRFs for fusing multi-scale representations derived from CNN side outputs.|
|||We demonstrated that this framework can be used in combination with several common CNN architectures and is suitable for end-to-end training.|
|||While this paper specifically addresses the problem of depth prediction, we believe that other tasks in computer vision involving pixel-level predictions of continuous variables, can also benefit from our implementation of mean-fields updates within the CNN framework.|
||20 instances in total. (in cvpr2017)|
|114|SCA-CNN_ Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning|Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image.|
|||However, we argue that such spatial attention does not necessarily conform to the attention mechanism  a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer.|
|||By stacking the layers, a CNN extracts image features through a hierarchy of visual abstractions.|
|||In this paper, we will take full advantage of the three characteristics of CNN features for visual attention-based image captioning.|
|||In summary, we propose a unified SCA-CNN framework to effectively integrate spatial, channel-wise, and multi-layer visual attention in CNN features for image captioning.|
|||This model is generic and thus can be applied to any layer in any CNN architecture such as popular VGG [25] and ResNet [8].|
|||SCA-CNN helps us gain a better understanding of how CNN features evolve in the process of the sentence generation.|
|||Overview  We adopt the popular encoder-decoder framework for image caption generation, where a CNN first encodes an input image into a vector and then an LSTM decodes the vector into a sequence of words.|
|||As illustrated in Figure 2, SCA-CNN makes the original CNN multi-layer feature maps adaptive to the sentence context through channelwise attention and spatial attention at multiple layers.|
|||At the l-th layer, the spatial and channel-wise attention weights  l are a function of ht1 and the current CNN features Vl.|
|||is of the same size as Vl or Xl, Note that  l i.e., W l  H l  C l. It will require O(W lH lC lk) space for attention computation, where k is the common mapping space dimension of CNN feature Vl and hidden state ht1.|
|||Therefore, we propose an approximation that learns spatial attention weights l and channelwise attention weights  l separately:  Vl = CNN (cid:0)Xl1(cid:1) ,  l =  (cid:0)ht1, Vl(cid:1) , Xl = f (cid:0)Vl,  l(cid:1) .|
|||where Xl is the modulated feature, () is the spatial and channel-wise attention function that will be detailed in Section 3.2 and 3.3, Vl is the feature map output from previous conv-layer, e.g., convolution followed by pooling, down-sampling or convolution [25, 8], and f () is a linear weighting function that modulates CNN features and atten (1)  l = s (cid:0)ht1, Vl(cid:1) ,  l = c (cid:0)ht1, Vl(cid:1) .|
|||Hence, we introduce a channel-wise attention mechanism to attend the features V. It is worth noting that each CNN filter performs as a pattern detector, and each channel of a feature map in CNN is a response activation of the corresponding convolutional filter.|
|||Setup  In our captioning system, for image encoding part, we adopted two widely-used CNN architectures: VGG-19 [25] and ResNet-152 [8] as the basic CNNs for SCA-CNN.|
|||2) More advanced CNN architectures are used; as Google NIC adopts Inception-v3 [27] which has a better classification perfor 5664  Dataset  Network Method  B@4 MT  RG  CD  Dataset  Network Method  B@4 MT  RG  CD  Flickr8k  Flickr30k  MS COCO  VGG  ResNet  VGG  ResNet  VGG  ResNet  S  SAT  C  S-C  C-S  S  SAT  C  S-C  C-S  S  SAT  C  S-C  C-S  S  SAT  C  S-C  C-S  S  SAT  C  S-C  C-S  S  SAT  C  S-C  C-S  23.0  21.3  22.6  22.6  23.5  20.5  21.7  24.4  24.8  25.7  21.1  19.9  20.1  20.8  21.0  20.5  20.1  21.5  21.9  22.1  28.2  25.0  27.3  28.0  28.1  28.3  28.4  29.5  29.8  30.4  21.0  49.1  60.6  20.3   20.3  20.9  21.1  19.6  20.1  21.5  22.2  22.1  48.7  48.7  49.2  47.4  48.4  50.0  50.5  50.9    58.7  60.6  60.3  49.9  55.5  65.5  65.1  66.5  18.4  43.1  39.5  18.5   18.0  17.8  18.0  17.4  17.8  18.4  18.5  19.0  42.7  42.9  43.3  42.8  42.9  43.8  44.0  44.6    38.0  38.2  38.5  35.3  36.3  42.2  43.1  42.5  23.3  51.0  85.7  23.0   22.7  23.0  23.5  23.1  23.2  23.7  23.9  24.5  50.1  50.6  50.9  51.2  51.2  51.8  52.0  52.5    83.4  84.9  84.7  84.0  84.9  91.0  91.2  91.7  Flickr8k  Flickr30k  MS COCO  VGG  ResNet  VGG  ResNet  VGG  ResNet  1-layer  2-layer  3-layer  1-layer  2-layer  3-layer  1-layer  2-layer  3-layer  1-layer  2-layer  3-layer  1-layer  2-layer  3-layer  1-layer  2-layer  3-layer  23.0  22.8  21.6  20.5  22.9  23.9  21.1  21.9  20.8  20.5  20.6  21.0  28.2  29.0  27.4  28.3  29.7  29.6  21.0  21.2  20.9  19.6  21.2  21.3  18.4  18.5  18.0  17.4  18.6  19.2  23.3  23.6  22.9  23.1  24.1  24.2  49.1  60.6  49.0  48.4  47.4  48.8  49.7  43.1  44.3  43.0  42.8  43.2  43.4  51.0  51.4  50.4  51.2  52.2  60.4  54.5  49.9  58.8  61.7  39.5  39.5  38.5  35.3  39.7  43.5  85.7  87.4  80.8  84.0  91.1  52.1  90.3  Table 2.|
|||To show the semantic information of the corresponding CNN filter, we used the same methods in [40].|
|||SCA-CNN takes full advantage of characteristics of CNN to yield attentive image features: spatial, channel-wise, and multi-layer, thus  5665  Layer-1  Layer-2  385  43  47  207  Layer-1  Layer-2  12  259  198  29  Ours:  a woman walking down a street holding an umbrella  Ours:  a clock tower in the middle of a city  SAT:  a group of people standing next to each other  SAT:  a clock tower on the side of a building  GT: two females walking in the rain with umbrellas  GT: there is an old clock on top of a bell tower  Layer-1  Layer-2  52  423  15  28  Layer-1  Layer-2  486  184  461  27  Ours:a street sign on a pole in front of a building  Ours:  a traffic light in the middle of a city street  SAT:a street sign in front of a building  SAT: a group of people walking down a street  GT: a stop sign is covered with stickers and graffiti  GT: a street light at an intersection in a small town  Layer-1  Layer-2  237  498  496  378  Layer-1  Layer-2  369  416  432  74  Ours:  a plane flying in the sky over a cloudy sky   SAT:  a plane flying through the sky in the sky   Ours: a man riding skis down a snow covered slope SAT: a man riding a snowboard down a snowy hill  GT: a couple of helicopters are in the sky  GT: a person riding skis goes down a snowy path  Figure 3.|
|||The contribution of SCA-CNN is not only the more powerful attention model, but also a better understanding of where (i.e., spatial) and what (i.e., channel-wise) the attention looks like in a CNN that evolves during sentence generation.|
|||Hcp: A flexible cnn framework for multi-label image classification.|
||20 instances in total. (in cvpr2017)|
|115|Niu_Ordinal_Regression_With_CVPR_2016_paper|Ordinal Regression with Multiple Output CNN for Age Estimation  Zhenxing Niu1 Gang Hua3 1Xidian University 2Xian Jiaotong University 3Microsoft Research Asia  Xinbo Gao1  Mo Zhou1  Le Wang2  {zhenxingniu,cdluminate}@gmail.com, lewang@mail.xjtu.edu.cn, xinbogao@mail.xidian.edu.cn  ganghua@gmail.com  Abstract  To address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem.|
|||Moreover, our CNN has multiple output layers where each output layer corresponds to a binary classification task, called Multiple Output CNN in this paper.|
|||On the contrary, in our approach we can conduct an End-to-End learning with CNN for age estimation, which could simultaneously optimize feature learning and regression modeling.|
|||However, the proposed CNN is very shallow, which only contains 4 layers (i.e., 1 convolution + 1 pooling + 1 local layer + 1 full connection), and only a subset of MORPH II (about 10K facial images) is used to train the shallow CNN.|
|||Currently, in [31] a relatively deeper CNN has been proposed for age estimation.|
|||However, in their work, CNN is only used to extract features, which is then fed to another regressor for the final age estimation.|
|||It is noticed that we adopt one CNN to collectively implement these binary classifiers in our approach.|
|||Thus, these binary classifiers are jointly trained in such a CNN (refers to Sec.3.4); (c) the  Input: training data D = {xi, yi}N {x  j}M  j=1    i=1 and testing images D =   Loop for k = 1, 2,    , K  1:   Build  a  distinct i=1 for  = training the k-th binary classifi data Dk  i , wk  {xi, yk cation task according to Eq.1 and Eq.2.|
|||i }N   The learning of the multiple output CNN:   The proposed multiple output CNN is trained with Dk, (k = 1, 2,    , K  1) according to the proposed learning algorithm (refers to Sec.3.4).|
|||Learning the Multiple Output CNN  For a CNN with single output, we have N samples {xi, yi}N i=1, where xi denotes the i-th image and yi denotes the corresponding class label.|
|||For binary class label yi  {0, 1}, it is reasonable to employ cross-entropy as the loss function,  Es =   1 N  N  Xi=1  1{oi = yi}wilog(p(oi|xi, W )),  (4)  where oi indicates the output of the CNN for the i-th image, wi indicates the weight of the i-th image, and W indicates the parameters of the entire CNN.|
|||For a CNN with K  1 outputs, each output corresponds to a distinct task.|
|||Moreover, before being fed to the CNN the images patches with the  In this section, the performance of age estimation is compared among several methods including metric regression based and ordinal regression based methods.|
|||For each kind of methods, they can be further categorized into two subcategories based on whether they used CNN learning algorithm.|
|||The third method [31] (denoted as CNN + LSVR) is the first work to develop a relatively deeper CNN (i.e., 3 convolution + 2 pooling + full connection) to address the problem of age estimation.|
|||However, the proposed CNN is only used to extract features, which is then fed to a regressor (i.e., a linear SVR regressor) for final age prediction.|
|||The architecture of network used in the method of Metric Regression with CNN (MR-CNN).|
|||Comparing Metric and Ordinal Regression  Our approach presents an End-to-End CNN learning method, which transforms ordinal regression into a series of binary classification sub-problems.|
|||To take apart their distinctive contributions, we propose another baseline method, which only keeps the End-to-End CNN learning part and drops the part of transforming framework, i.e., it casts age estimation as a metric regression problem instead of an ordinal regression problem, and addresses the metric regression problem with an End-to-End CNN learning algorithm.|
|||For clarity, the baseline method is called Metric Regression with CNN (MR-CNN), while the proposed approach is called Ordinal Regression with CNN (OR-CNN).|
||20 instances in total. (in cvpr2016)|
|116|Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper|NetVLAD: CNN architecture for weakly supervised place recognition  Relja Arandjelovi c  Petr Gronat  INRIA   INRIA  Akihiko Torii Tokyo Tech   Tomas Pajdla CTU in Prague   Josef Sivic  INRIA  Abstract  We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph.|
|||The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation.|
|||Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current stateof-the-art compact image representations on standard image retrieval benchmarks.|
|||While it has been shown that the trained representations are, to some extent, transferable between recognition tasks [20, 22, 49, 68, 87], a direct application of CNN representations trained  15297  for object classification [37] as black-box descriptor extractors has so far yielded limited improvements in performance on instance-level recognition tasks [7, 8, 23, 60, 61].|
|||In this work we investigate whether this gap in performance can be bridged by CNN representations developed and trained directly for place recognition.|
|||The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation.|
|||We show that the proposed architecture significantly outperforms non-learnt image representations and off-theshelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.|
|||Deep architecture for place recognition  This section describes the proposed CNN architecture f, guided by the best practises from the image retrieval community.|
|||In order to learn the representation end-to-end, we design a CNN architecture that mimics this standard retrieval pipeline in an unified and principled manner with differentiable modules.|
|||For step (i), we crop the CNN at the last convolutional layer and view it as a dense descriptor extractor.|
|||The layer can be implemented using standard CNN layers (convolutions, softmax, L2-normalization) and one easy-to-implement aggregation layer to perform aggregation in equation (4) (VLAD core), joined up in a directed acyclic graph.|
|||As illustrated in figure 2 the NetVLAD layer can be visualized as a meta-layer that is further decomposed into basic CNN layers connected up in a directed acyclic graph.|
|||Other works have proposed to pool CNN activations using VLAD or Fisher Vectors (FV) [14, 23], but do not learn the VLAD/FV parameters nor the input descriptors.|
|||However, in their work it is not possible to learn the input descriptors as they are hand-engineered (SIFT), while our VLAD layer is easily pluggable into any CNN architecture as it is amenable to backpropagation.|
|||Learning from Time Machine data  In the previous section we have designed a new CNN architecture as an image representation for place recognition.|
|||The base CNN architecture is denoted in brackets: (A)lexNet and (V)GG-16.|
|||Furthermore we compare our CNN representations trained for place recognition against the state-of-the-art local feature based compact descriptor, which consists of VLAD pooling [29] with intra-normalization [4] on top of densely extracted RootSIFTs [3, 43].|
|||Note that these are strong baselines that outperform most off-the-shelf CNN descriptors on the place recognition task.|
|||Our trained representation significantly outperforms off-the-shelf CNN models and significantly improves over the state-of-the-art on the challenging 24/7 Tokyo dataset, as well as on the Oxford and Paris image retrieval benchmarks.|
|||The two main components of our architecture  (i) the NetVLAD pooling layer and (ii) weakly supervised ranking loss  are generic CNN building blocks applicable beyond the place recognition task.|
||20 instances in total. (in cvpr2016)|
|117|Xu_Augmenting_Strong_Supervision_ICCV_2015_paper|Although both schemes have been widely used to boost recognition performance, due to the difficulty in acquiring detailed part annotations, strongly supervised fine-grained datasets are usually too small to keep pace with the rapid evolution of CNN architectures.|
|||The proposed method improves classification accuracy in two ways: more discriminative CNN feature representations are generated using a training set augmented by collecting a large number of part patches from weakly supervised web images; and more robust object classifiers are learned using a multi-instance learning algorithm jointly on the strong and weak datasets.|
|||By employing deep feature CNN extractors pre-trained on large datasets (such as ImageNet [9]) and domain-specific finetuning approaches, considerable improvements in a wide  12524  HEAD CNN BODY CNN BBOX CNN Weakly-supervised  WEB IMAGES Strongly-supervised Dataset range of image classification and detection tasks can be achieved, including in fine-grained categorization [25].|
|||For example, part-based R-CNN [33] achieved state-of-theart performance on the CUB200-2011 dataset [29] with the help of strong part annotations and CNN feature extractors, exemplifying the paradigm; nevertheless, it has been argued that further improving results this way may be problematic.|
|||In this paper, we propose a new method for fine-grained categorization that learns robust CNN feature representations and employs detailed object part annotations in a unified framework, and overcome the lack of training data with the help of weakly supervised web images.|
|||Part-based R-CNN  Whilst our method is agnostic to the specific form of part annotations and CNN architectures, we exploit part-based R-CNN [33] as the basic method for detecting object parts and for training fine-grained classifiers on strongly supervised datasets.|
|||However, the rapid evolution of CNN architectures involving an increasing number of model parameters has meant that current fine-grained datasets, especially datasets with strong supervision, are too small for training robust CNN representations.|
|||Each part (or root) pi is associated with an R-CNN detector di based on the respective CNN feature extractor (i).|
|||We use the same CNN architecture as discussed in Section 2.2, and once again randomly initialize the (K + 1)-way fc8 layer with the filter weights of previous layers kept fixed.|
|||All region proposals that have  0.5 IoU over the detected part bounding boxes are cropped, dilated, warped and then fed into the CNN architecture as input.|
|||in the CNN architecture to train R-CNN detectors and in image representation for classification.|
|||However, since the parameter-rich CNN architectures can easily overfit the training data, it is critical to find a balance between adding more training data and ensuring clean labels.|
|||Directly exploiting an ImageNet pre-trained CNN as the feature extractor achieved an accuracy of 68%.|
|||These results show that the larger amount of training data does indeed improve the discriminative power of the learned CNN representation.|
|||Results showed that when the feature representations were fixed (as in traditional features such as SIFT), the performance improvement was trivial ( 1%) compared to re-fine-tuning CNN features.|
|||Method  DPD [34] DeCAF6 [11] Symbiotic [8] CNNaug [25] Alignment [14] POOF [4] Part R-CNN [33] PoseNorm CNN [7] Our method  Train BBox  Train Part  Test BBox  Acc(%)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  51.0 58.8 61.0 61.8 67.0 56.8 73.9 75.7 84.6  Table 3.|
|||Meanwhile, with additional training samples, our method is likely to achieve better performance with more complicated CNN architectures such as the VGGNet [27].|
|||Visualization  Beyond the quantitative results presented above, here we present a more intuitive description of how our method  2530  GT 80  PD 81   GT 80  PD 80   (g)   NNs   (a)   BBOX CNN   HEAD CNN   BODY CNN   (b)   (c)   (d)   (e)   (f)   (h)   Figure 5.|
|||Conclusion  In this paper, we present a new fine-grained recognition method that trains robust CNN feature extractors with effective part-based models by employing the availablity of vast numbers of online images to augment manually-labeled strongly supervised datasets.|
|||We believe the proposed method is likely to be useful in practice, especially considering that the forms of part annotations are varied and CNN architectures are becoming more complicated over time.|
||20 instances in total. (in iccv2015)|
|118|Noisy Softmax_ Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation|chenbinghui@bupt.edu.cn, whdeng@bupt.edu.cn, junpingd@bupt.edu.cn  Abstract  Over the past few years, softmax and SGD have become a commonly used component and the default training strategy in CNN frameworks, respectively.|
|||This paper empirically verifies the superiority of the early softmax desaturation, and our method indeed improves the generalization ability of CNN model by regularization.|
|||The main differences are that we apply noise injection on CNN and impose noise on loss layer instead of previous layers.|
|||Of course, making its softmax output close to 1 is our ultimate goal of CNN training.|
|||Since, when optimizing CNN with gradients-based methods such as SGD, the prematurely saturated individual early stops contributing gradients to back-propagation due to negligible gradients, i.e.|
|||This demonstrates that our Noisy Softmax does improve the generalization ability of CNN by encouraging the SGD solver to be more exploratory and to converge at a global-minima.|
|||In summary, injecting non-negative noise n = || in softmax does prevent the early individual saturation and further improve the generalization ability of CNN when optimized by standard SGD.|
|||Regularization Ability  We find experimentally that Noisy Softmax can regularize the CNN model by preventing over-fitting.|
|||Our CNN configuration is shown in Table 2.|
|||This demonstrates that Noisy Softmax improves the generalization ability of CNN with implicit data augmentation.|
|||Architecture Settings and Implementation  As VGG [39] becomes a commonly used CNN architecture, the cascaded layers with small size filters gradually take the place of single layer with large size filters.|
|||Our CNN network architecture is shown in Table 2  and we use a 0.001 weight decay.|
|||We use different CNN architectures in CIFAR10 and CIFAR100 experiments, and these network configurations are shown in Table 2.|
|||Method  FaceNet [36]  DeepID2+ [44] DeepID2+ [44]  Sparse [45] VGG [33]  Images  200M* 300k* 300k* 300k* 2.6M  WebFace [52] Robust FR [5]  WebFace WebFace WebFace WebFace+ Noisy Softmax(2 = 0.1) WebFace+ Noisy Softmax(2 = 0.05) WebFace+  Lightened CNN [50]  Softmax  Models  LFW Rank-1 DIR@FAR=1% FGLFW YTF  1 1 25 1 1  1 1 1  1 1 1  99.65 98.7 99.47 99.30 97.27  97.73 98.43 98.13  98.83 99.18 99.02   95.00   74.10   89.21  91.68 92.68 92.24   80.70   52.01   69.46  69.51 78.43 75.67   88.13   91.22  92.95 94.50 94.02  95.18 91.90 93.20 92.70 92.80  90.60   91.60  94.22 94.88 94.51  Table 5.|
|||Our CNN configuration is shown in Table 2, here we add element-wise maxout layer [50] after the 3,000-dimensional fully connected layer, yielding a 1,500dimensional output, and contrastive loss is applied on this output as in DeepID2 [43].|
|||It significantly improves the performances of CNN models, since the early desaturation operation indeed exerts much effect on parameter update during back-propagation and furthermore improves the generalization ability of DCNNs.|
|||Part-stacked cnn for fine-grained visual categorization.|
|||Bilinear cnn models for fine-grained visual recognition.|
|||A lightened cnn for deep face  representation.|
|||Disturblabel:  Regularizing cnn on the loss layer.|
||20 instances in total. (in cvpr2017)|
|119|cvpr18-Regularizing Deep Networks by Modeling and Predicting Label Structure|Suppose we want to train a CNN to recognize and segment cats, but our limited training set consists only of tigers.|
|||It is conceivable that the CNN will learn an equivalence between black and orange striped texture and the cat category, as such association suffices to classify every pixel on a tiger.|
|||How can we force the CNN to notice this wealth of information during training?|
|||Adding the cluster identities as another semantic label, and requiring the CNN to predict them, would force the CNN to differentiate between the tail and ear by developing a representation of shape.|
|||Baseline CNN Architectures  3.3.|
|||Alternatively, the basic CNN architecture can be reformulated in a multigrid setting [19].|
|||Hence, we choose hypercolumn [14, 29] CNN architectures as a primary basis for experimentation, as they are are minimally separated from the established classification networks in design space.|
|||As shown in Figure 1, this can equivalently be viewed as associating with each spatial location a feature formed by concatenating a local slice of every CNN layer.|
|||The 67-layer  As shown by the large gray arrow in Figure 1, we impose our regularizer by connecting a CNN (e.g.|
|||The CNN now has two tasks, each with an associated loss, to perform during training.|
|||Backpropagation from losses along both paths influences CNN parameter updates.|
|||We connect VGG-16 or DenseNet to the decoder by predicting input for the decoder from the output of the penultimate CNN layer prior to global pooling.|
|||If the label autoencoder learns useful abstractions, requiring the CNN to work through the decoder ensures that it learns to work with those abstractions.|
|||The hypercolumn pathway allows the CNN to make direct predictions, while the decoder pathway ensures that the CNN has good reasons or a high-level abstract justification for its predictions.|
|||Here, we ask the CNN to directly predict the feature representation built by the label encoder.|
|||An auxiliary layer attempts to predict the encoder hypercolumn from the CNN hypercolumn at the corresponding spatial location.|
|||The CNN must  5632  b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b also still solve the original semantic segmentation task.|
|||Moreover, the decoder output is visually interpretable; we can see the semantic segmentation predicted by the CNN via the decoder.|
|||When applying the encoder as a regularizer, we task the CNN with predicting the concatenation of the encoders activations in its conv1 and conv3 layers.|
|||For examples on gray background, our DenseNet-67-hypercolumn CNN is used to predict the label space search representations from images.|
||20 instances in total. (in cvpr2018)|
|120|cvpr18-Learning Deep Models for Face Anti-Spoofing  Binary or Auxiliary Supervision|A CNN with softmax loss might discover arbitrary cues that are able to separate the two classes, such as screen bezel, but not the faithful spoof patterns.|
|||The main contributions of this work include:   We propose to leverage novel auxiliary information (i.e., depth map and rPPG) to supervise the CNN learning for improved generalization.|
|||For example, [30, 37] use CNN as feature extractor and fine-tune from ImageNet-pretrained CaffeNet and VGG-face.|
|||feed the optical flow map and Shearlet image feature to CNN [21].|
|||This representation is more informative than binary labels since it indicates one of the fundamental differences between live faces, and print and replay PA. We utilize the depth maps in the depth loss function to supervise the CNN part.|
|||The pixel-based depth loss guides the CNN to learn a mapping from the face area within a receptive field to a labeled depth value  a scale within [0, 1] for live faces and 0 for spoof faces.|
|||First, a CNN part evaluates each frame separately and estimates the depth map and feature map of each frame.|
|||3.3.1 CNN Network  We design a Fully Convolutional Network (FCN) as our CNN part, as shown in Fig.|
|||The CNN part contains multiple blocks of three convolutional layers, pooling and resizing layers where each convolutional layer is followed by one exponential linear layer and batch normalization layer.|
|||The first output of the CNN is the estimated depth map of the input frame I 2 R256256, which is supervised by the estimated ground truth depth D,  D = arg min  D  Nd X  i=1  ||CNND(Ii; D)  Di||2 1,  (4)  where D is the CNN parameters and Nd is the number of training images.|
|||The second output of the CNN is the feature map, which is fed into the non-rigid registration layer.|
|||The desired training data for the CNN part should be from diverse subjects, so as to make the training procedure more stable and increase the generalizability of the learnt model.|
|||Note that even though the first stream only updates the weights of the CNN part, the back propagation of the second stream updates the weights of both CNN and RNN parts in an end-to-end manner.|
|||This layer utilizes the estimated dense 3D shape to align the activations or feature maps from the CNN part.|
|||Hence the background area would not participate in RNN learning, although the background information is already utilized in the layers of the CNN part.|
|||also encourages the CNN part to generate zero depth maps for either all frames, or one pixel location in majority of the frames within an input sequence.|
|||Model 1 has an architecture similar to the CNN part in our method (Fig.|
|||Model 2 is the CNN part in our method with a depth map loss function.|
|||Model 3 contains the CNN and RNN parts without the non-rigid registration layer.|
|||The proposed network combines CNN and RNN architectures to jointly estimate the depth of face images and rPPG signal of face video.|
||20 instances in total. (in cvpr2018)|
|121|Deep Structured Learning for Facial Action Unit Intensity Estimation|To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables.|
|||These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm.|
|||To efficiently model these two types of structure within our deep CNN model, we borrow the modeling approach of conditional graph models (Conditional Random Fields  CRFs) to define (ordinal) unary and (copula) binary cliques in the output graph (i.e., the output layer of the deep net), which are then learned jointly with the CNN layers.|
||| Joint learning of the deep CNN and target dependency structure (CRF) in our model is challenging and can easily lead to overfitting if standard learning is applied.|
|||Another example is [14], where a CNN is jointly trained for detection and intensity estimation of multiple AUs.|
|||The CNN features are jointly learned for estimation of intensities of all AUs and the parameters of the unary potentials are shared.|
|||For instance, DeepLab-CRF [35] combines a CNN and fully connected CRF, where the binary cliques are used to model relations between image color and location.|
|||In the first setting, given an input face image, we first apply a pre-defined CNN network layer to the (normalized) input image, in order to generate a feature map.|
|||yQ  Deep Facial Features: In our experiments, we first use a CNN to extract the feature map fd(x, W ) from an input image x, where the network parameters are defined by W .|
|||First, the wights of the CNN are optimized in step 1 (S1).|
|||This imbalance is highly pronounced in the number of images per training subject, average number of examples per intensity level, as well as number of different label combinations, adversely affecting the learning of the CNN weights (W ) and the unary () and pairwise () potential parameters, respectively.|
|||Input: Training data: D = {xi, yi}N  i=1  Model parameters:  = {W, , , U, V }  while Eq.11 not converged do  Step 1: train W with balanced batches: W  argmax  P (yi|xi, W ), i  bbn  W PN (bbn)  i  Step 2: train  with balanced batches: q  U : q  argmax  Pr(yq  q PN (bbm)  i  i |xi, q) + q||q||2 2  , i  bbm Step 3: train  with balanced batches: (rs)  V : rs  rs PN (bbe) argmax  Pr(yr  i , ys  i  i |xi, rs) , i  bbe  end Output: Model parameters: opt = {W, , }  Algorithm 1: Structured CNN Learning with balanced batches  Augmented Learning from Multiple Datasets.|
|||We fist conducted experiments using standard CNN architectures employed in previous works ([14, 17]).|
|||The CNN [14] model is a standard CNN  2We use this dataset only to improve learning of our deep model; how ever, the evaluation results are in the supplementary materials.|
|||For both methods, the weights are jointly learned and the predictions are computed independently.We conducted our experiments on a relative shallow CNN (see Fig.|
|||It combines a basic CNN with a customized conditional layer that has region specific weights.|
|||SCNN [22] is a structured CNN introduced for object detection.|
|||While both models have a significant improvement over the standard CNN [14], they fail to accurately predict ordinal intensities.|
|||Conclusion  We proposed a novel Copula CNN deep learning approach for joint estimation of facial intensity for multiple, dependent AUs.|
|||Ordinal In  regression with multiple output cnn for age estimation.|
||20 instances in total. (in cvpr2017)|
|122|Jourabloo_Pose-Invariant_Face_Alignment_ICCV_2017_paper|To address these issues, we propose a new layer, named visualization layer, which can be integrated into the CNN architecture and enables joint optimization with different loss functions.|
|||For the purpose of learning an end-to-end face alignment model, our novel visualization layer reconstructs the 3D face shape (a) from the estimated parameters inside the CNN and synthesizes a 2D image (b) via the surface normal vectors of visible vertexes.|
|||Lack of end-to-end training: It is a consensus that endto-end training is desired for CNN [6, 14].|
|||Hand-crafted feature extraction: Since the CNNs are trained independently, feature extraction is required to utilize the result of a previous CNN and provide input to the current CNN.|
|||Our CNN architecture consists of several blocks, which are called visualization blocks.|
|||Benefiting from the design of the visualization layer, our  method has the following advantages and contributions:   The proposed method allows a block in the CNN to utilize the extracted features from previous blocks and extract deeper features.|
|||Among methods with cascade of regressors, CNN is a popular choice of regressor due to its strong learning ability.|
|||The main differences between the proposed method and CRNNs are: 1) existing CRNN methods are designed for near-frontal face alignment, while ours is for LPFA; 2) the CRNN methods share the same CNN at all stages, while our CNN of each block is different which might be more suitable for estimating the coarse-tofine mappings during the course of alignment; 3) due to our new differentiable visualization layer, our method has one additional flow of the gradient back-propagation (note the two blue arrows between consectutive blocks in Figure 2).|
|||The proposed CNN architecture.|
|||sults of a previous CNN to the next one [50].|
|||It guides the CNN to focus on the face area that incorporates both the pose and expression information.|
|||Towards this end, we propose a CNN architecture with end-to-end training for model fitting, as shown in Figure 2.|
|||In this section, we will first describe the underlying 3D face model used in this work, followed by our CNN architecture and the visualization layer.|
|||Proposed CNN Architecture  Our CNN architecture resembles the cascade of CNNs, while each shallow CNN is defined as a visualization block.|
|||We now describe the visualization block and CNN architecture, and dive into the details of the visualization layer in Section 3.3.|
|||Loss Functions Two types of loss functions are employed in our CNN architecture.|
|||Furthermore, we conduct experiments on different CNN architectures to validate our visualization layer design.|
|||This strategy helps the CNN to pay more attention to the landmark loss used in later blocks.|
|||Advantage of deeper features We train three CNN architectures shown in Figure 6 on AFLW.|
|||Time complexity  Compared to the cascade of CNNs, one of the main advantages of end-to-end training a single CNN is the reduced training time.|
||20 instances in total. (in iccv2017)|
|123|Kuzborskij_When_Naive_Bayes_CVPR_2016_paper|This is because (1) such algorithms cannot use CNN activations as input features; (2) they cannot be used as final layer of CNN architectures for end-to-end training , and (3) they are generally not scalable and hence cannot handle big data.|
|||We solve the first by extracting CNN activations from local patches at multiple scale levels, similarly to [13].|
|||To begin with, they require local feature representations without any vector quantization, as opposed to the global feature representation derived from the CNN activation layers [5, 3].|
|||[13] proposed a multiscale orderless pooling of CNN features extracted from densely sampled patches.|
|||In this work, we revisit NBNN considering its power in  conjunction with CNN features, in both categorization and domain adaptation scenarios.|
|||Given a query image, we first compute CNN activations for local patches at different scales, from a pre-trained architecture.|
|||Computing Local CNN Activations  As mentioned before, a key requirement for any NBNNbased framework is to deal with features that capture local information about the image.|
|||Following [13], we decide here to create orderless image representations from pre-trained CNN by extracting deep activation features from patches obtained at increasingly finer scales.|
|||The effectiveness of such features will depend on several designer choices, from the pre-trained CNN chosen, to the sampling rate for the patches, the patch size, and the computed CNN activations.|
|||Note also that, for any given CNN ar chitecture within this framework, fine tuning on a validation set might further improve results.|
|||Our experiments aim to verify two claims: first, that such methods coupled with local CNN activations at multiple scales are able to achieve results competitive with, or even  better than, end-to-end, fine tuned CNN architectures.|
|||For all scene experiments, we concatenated the CNN activations with the absolute position of every patch.|
|||Baselines For every scenario, for every setting, we always used the following three variants of our framework: (1) CNN-NBNN: this consists of using the NBNN classifier as originally proposed [2] , combined with the local CNN activations.|
|||These were obtained by training a linear SVM on Hybrid and Places-205 CNN features respectively.|
|||We speculate that at this patch size there is not enough visual information for CNN to provide meaningful representation.|
|||Finally, we note that CNN features extracted before ReLU generally perform better.|
|||We trained GPU-optimized implementation of STOML3 SVM  Figure 4: Results obtained by NBNN on CNN features computed with different patch sizes, sampling rates, on three datasets.|
|||Note that we could not run DA-NBNN, the only existing NBNN-based domain adaptation method on our local CNN multi scale activations because of its computational limitations.|
|||Conclusions  This paper provides a method for using CNN activation features combined with NBNN-based classifiers.|
|||The two key ingredients are: (1) extraction of CNN activations from local patches at different scales, and (2) a scalable NBNNbased algorithm that exploits the learning power of locally linear SVMs.|
||20 instances in total. (in cvpr2016)|
|124|cvpr18-DeLS-3D  Deep Localization and Segmentation With a 3D Semantic Map|Finally, based on the pose from RNN, we render a new label map, which is fed together with the RGB image into a segment CNN which produces perpixel semantic label.|
|||To incorporate the temporal correlations, the corrected poses from pose CNN are fed into a pose RNN to further improves the estimation accuracy in the stream.|
|||LSTMPoseNet [20] further captures a global spatial context after CNN features.|
|||1, our pose networks contain a pose CNN and a pose GRU-RNN.|
|||Particularly, the CNN of our pose network takes as inputs an image I and the rendered label map L from corresponding coarse camera pose pc i .|
|||Here, we set stronger weights for point cloud belong to certain classes like traffic light, and find it helps pose CNN to  5864  Figure 3: The GRU RNN network architecture for modeling a sequence of camera poses.|
|||Later, we insert such a rendered label map for the pose CNN and segment CNN, which guides the network to localize the camera and parse the image.|
|||Then, at each pixel [x, y]  2 in the 2D map, an offset value f (x, y) is pre-calculated indicating its 2D offset to the closest pixel  FC-32 GRU FC-7 GRU FC-32 GRU FC-7 GRU FC-32 GRU FC-7 GRU ConcatConcatConcatFigure 4: Architecture of the segment CNN with rendered label map as a segmentation priori.|
|||Thus, we propose an additional segment CNN to tackle these issues, while taking the rendered label map as segmentation guidance.|
|||4, our segment CNN is a light-weight network containing an encoder-decoder network and a refinement network, and both have similar architecture with the corresponding ones used in DeMoN [41] including 1D filters and mirror connections.|
|||A 512  608 image can be generated in 70ms with a single Titan Z GPU, which is also the input size for both pose CNN and segment CNN.|
|||Specifically, for pose CNN and segment CNN, we stops at 150 epochs when there is no performance gain, and for pose RNN, we stops at 200 epochs.|
|||2, we show the performance of estimated translation t and rotation r from different  3https://github.com/aleju/imgaug  Data  k r a p Z  e k a l D  Method Noisy pose Pose CNN w/o semantic Pose CNN w semantic Pose RNN w/o CNN Pose CNN w KF Pose CNN-RNN  Pose CNN w semantic Pose RNN w/o CNN Pose CNN-RNN  Trans (m)  3.45  0.176 1.355  0.052 1.331  0.057 1.282  0.061 1.281  0.06 1.005  0.044  1.667  0.05 1.385  0.057 0.890  0.037  Rot () 7.87  1.10 0.982  0.023 0.727  0.018 1.731  0.06 0.833  0.03 0.719  0.035  0.702  0.015 1.222  0.054 0.557 0.021  Pix.|
|||Another baseline we adopt is performing Kalman filter [23] to the output from Pose CNN by assuming a constant speed which we set as the averaged speed from training sequences.|
|||Finally when combining pose CNN and RNN, it achieves the best pose estimation both for t and r. We visualize some results at Fig.|
|||A cc  sky  car-lane  m IO  Data  k r a p Z  Method 64.66 95.87 93.6 98.5 ResNet38 [46] 32.61 91.7 Render PoseRNN 68.35 95.61 94.2 98.6 SegCNN w/o Pose 97.1 SegCNN w pose GT 79.37 96.1 99.4 SegCNN w Pose CNN 68.6 95.67 94.5 98.7 SegCNN w Pose RNN 69.93 95.98 94.9 98.8  73.1   ped-lane  bike-lane  82.9 50.4 83.8 92.5 84.3 85.3  87.2 62.1 89.5 93.9 89.3 90.2  t-cone  curb  61.8 46.1 16.9 6.6 69.3 47.5 81.4 68.8 69.0 46.8 71.9 45.7  t-stack  41.7 5.8 52.9 71.4 52.9 57.0  light-pole  37.5 8.9 52.2 71.7 53.7 58.5  t-light  26.7 6.7 43.5 64.2 39.5 41.8  tele-pole  45.9 10.1 46.3 69.1 48.8 51.0  t-sign  49.5 16.3 52.9 72.2 50.4 52.2  billboard  te m p-build  60.0 22.2 66.9 83.7 67.9 69.4  85.1 70.6 87.0 91.3 87.5 88.5  building  67.3 29.4 69.2 76.2 69.9 70.9  sec.-stand  38.0 20.2 40.0 58.9 42.8 48.0  plants  89.2 73.5 88.6 91.6 88.5 89.3  object  66.3   63.8 56.7 60.9 59.5  t-fence  82.0 30.5 83.9 90.8 84.9 85.9  o le  U  U  y  Method  Data e SegCNN w/o Pose k a l D  m I O 62.36 SegCNN w pose GT 73.10 SegCNN w pose RNN 67.00  u s 78.4 82.1 80.0 85.7 76.8 86.6 Table 3: Compare the accuracy of different segment networks setting over Zpark (top) and Dlake (bottom) dataset.|
|||at the 1st row, it achieve reasonable accuracy compare to our segment CNN (2nd row) when there is no pose priori.|
|||At 5th and 6th row, we show the results trained with rendered label map with pose after pose CNN and pose RNN respectively.|
|||In the figure, we can see the noisy pose (a), is progressively rectified by pose CNN (b) and pose RNN (c) from view of camera.|
||19 instances in total. (in cvpr2018)|
|125|cvpr18-Normalized Cut Loss for Weakly-Supervised CNN Segmentation|Normalized Cut Loss for Weakly-supervised CNN Segmentation  Meng Tang2,1*  Abdelaziz Djelouah1  Federico Perazzi1  Yuri Boykov2  Christopher Schroers1  1Disney Research, Z urich, Switzerland  2Computer Science, University of Waterloo, Canada  Abstract  Most recent semantic segmentation methods train deep convolutional neural networks with fully annotated masks requiring pixel-accuracy for good quality training.|
|||Weak supervision using partial cross entropy (pCE) on scribbles with our NC loss trains CNN (f) nearly as good as full mask supervision (c).|
|||Our approach to weakly-supervised CNN training is motivated both by common ideas in semi-supervised learning and by standard criteria in shallow image segmentation.|
|||In contrast to existing proposal generating technqiues, we advocate a principled yet simple and general approach: regularized semi-supervised loss directly integrating shallow segmentation criteria into CNN training, see Fig.1(d,f).|
|||The use of shallow learning regularizers in semi-supervised losses on DNN outputs [50] motivates direct integration of standard image segmentation regularizers into losses for weakly-supervised CNN segmentation.|
|||Shallow semi-supervised segmentation & clustering: Motivated by the general ideas above, we propose to incorporate regularizers from shallow segmentation/clustering into CNN segmentation loss.|
|||Figure 2  2This is only a minor conflict with earlier notation Sk  p  [0, 1] for realvalued network output; in the context of CNN segmentation such relaxed output variables Sk  p also correspond to (soft-max) class assignments.|
|||Connecting shallow and deep segmentation: Following the general idea of integrating (shallow) regularizers into semi-supervised loss for (deep) learning [12, 50, 7], we advocate this principled approach in the context of weaklysupervised CNN segmentation.|
|||That is, we propose semisupervised training loss on CNN output to combine empirical risk (e.g.|
|||In order to integrate common segmentation energies like (2) into CNN loss functions, they should have a relaxed formulation extendable to real-valued segmentation variables Sp  [0, 1]K (on probability simplex) typically produced by soft-max output of the network.|
|||Equation (3) is a basic example of segmentation energy that could be used as regularized semi-supervised loss directly over real-valued CNN output.|
|||In general, regularized semi-supervised loss functions for CNN segmentation can use differentiable relaxed versions of many standard shallow segmentation regularizers such as Potts [9], dense CRF [28], or their combinations  3Extreme Wpq = const gives a negative sum of squared cardinalities.|
|||While our future work plans include an empirical comparison for all these models in the context of regularized weakly-supervised losses for CNN segmentation, this paper focuses specifically on the normalized cut loss thoroughly discussed in Sec.|
|||However, we are the first to utilize ideas in such principled framework for weaklysupervised CNN segmentation.|
|||However, we take advantage of deep CNN rather than SVM used in [51].|
|||Regularization techniques for CNN segmentation include post-processing (e.g.|
|||Its beyond the scope of this work to compare all schemes for regularization in fullyor weakly-supervised CNN segmentation.|
|||Our Method  We propose a joint loss of (partial) cross entropy and normalized cut for weakly-supervised CNN segmentation.|
|||On Regularized Losses for Weaklysupervised CNN Segmentation.|
||19 instances in total. (in cvpr2018)|
|126|Chu_Online_Multi-Object_Tracking_ICCV_2017_paper|An intuitive thought is that applying the CNN based single object tracker to MOT will make sense.|
|||It solves the problem in computational complexity when simply applying CNN based single object tracker for MOT by sharing computation among multiple objects.|
|||All methods mentioned above do not make use of CNN based single object trackers, so they can not update features during tracking.|
|||Different from these methods, our work adopts online learned CNN based single object trackers into online multi-object tracking and focuses on handling drift caused by occlusion and interactions among targets.|
|||The target-specific CNN branch of each target is updated according to the loss of training samples in current and historical frames weighted by temporal attention.|
|||Dynamic CNN(cid:173)based MOT Framework  We propose a dynamic CNN-based framework for online MOT, which consists of both shared CNN layers and targetspecific CNN branches.|
|||All target-specific CNN branches share the same structure, but are separately trained to capture the appearance of different targets.|
|||The number of target-specific CNN branches varies with the number of existing targets.|
|||(a) The framework of the proposed CNN model.|
|||It contains shared CNN layers and multiple target-specific CNN branches.|
|||Each target has its own corresponding target-specific CNN branch which is learned online.|
|||The target-specific CNN branch acts as a single object tracker and can be added to or removed from the whole model according to the entrance of new target or exit of existing target.|
|||(b) The details of the target-specific CNN branch.|
|||Each target-specific CNN branch consists of feature extraction using visibility map and spatial attention as described in Sec.|
|||4839  Shared CNNFeature MapEstimated Target StatesInput FrameShared CNN LayersTarget-specific CNN Branch 1ROI-Pooled Featuresof Candidates StatesTarget-specific CNN Branch nVisibility MapSpatial AttentionFC LayerROI-PooledFeaturesRefinedFeaturesConv Layer3732FC layerReshapeLocalConnectedLayerConv Layer375Cross-entropy LossLclsClassification ScoreFeature ExtractionBinary Classification(a)(b)v i s i b e  l  o c c l u d e d  Figure 4.|
|||Model Initialization and Online Updating  Each target-specific CNN branch comprises of visibility map, attention map and binary classifier.|
|||In our implementation, we use the first ten convolutional layers of the VGG-16 network [42] trained on Imagenet Classification task [11] as the shared CNN layers. The threshold o0 is set to 0.5, which determines whether the location found by single object tracker is covered by a object detection.|
|||Conclusion  In this paper, we have proposed a dynamic CNN-based online MOT algorithm that efficiently utilizes the merits of single object trackers using shared CNN features and ROIPooling.|
|||Learning In  by tracking: Siamese cnn for robust target association.|
||19 instances in total. (in iccv2017)|
|127|Semantically Consistent Regularization for Zero-Shot Recognition|This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification.|
|||The main limitation of this approach is the effort required to 1) collect and annotate millions of images necessary to train these models, and 2) the complexity of training a CNN from scratch.|
|||We show that, in this context, the two methods reduce to a set of constraints on the CNN architecture: RIS learns a bank of independent CNNs, and RULE uses a single CNN with fixed weights in the final layer.|
|||SCoRe addresses these problems by exploiting the view of a CNN as an optimal classifier with respect to a multidimensional classification code, implemented at the top CNN layer.|
|||Semantics and deep learning  We now discuss the CNN implementation of RIS and RULE.|
|||Deep(cid:173)RIS  Under the independence assumption that underlies RIS, the CNN implementation reduces to learning Q independent attribute predictors.|
|||Inspired by the success of multitask learning, it is advantageous to share CNN parameters across attributes, and rely on a common feature extractor (x; ) of parameters , which can be implemented with one of the popular CNNs in the literature.|
|||To implement (1) in a CNN, it suffices to use one of the popular models to compute (x; ), add a fully-connected layer of Q units and parameters T, so that a(x) = TT (x; ) is a vector of attribute scores, and define the CNN class outputs  h(x; T, ) = T a(x) = T TT (x; ),  (5)  Symbol Meaning  Table 1.|
|||h = arg min  h  RE[h] + [h]  (8)  where the columns of W contain the weight vectors wc of the last CNN layer.|
|||In the Lagrangian of (8), the first summation becomes the weight decay regularizer already implemented by most CNN learning packages.|
|||Feature extraction based on common CNN architectures.|
|||Deep-SCoRe  Deep-SCoRe implements (10) using a CNN to compute (x; ).|
|||As shown in Figure 3, a CNN is used to compute the predictor f (x) = (f1, .|
|||Similarly to Deep-RIS and Deep-RULE, this is implemented through a linear transformation T of a feature vector (x) computed with one of the popular CNN models.|
|||Three CNN architectures were used to implement (x): AlexNet [28] (layer fc7), GoogLeNet [54] (layer pool5) and VGG19 [51] (layer fc7).|
|||Comparisons to state-of-the-art methods: A comparison to the literature is not trivial since methods differ in 1) CNN implementation, 2) train/ZS class partitioning, and 3) semantic space representation.|
|||Table 3 compares our ZS-MCA to previous approaches  1The unrestricted CNN is initialized with semantic codes.|
|||using three CNN architectures: AlexNet, GoogLeNet and VGG19.|
|||The complementarity found between class and semantic supervision lead to the introduction of a new ZSL procedure, denoted SCoRe, where a CNN is learned together with a semantic codeword set and two forms of semantic constraints: loss-based and codeword regularization.|
||19 instances in total. (in cvpr2017)|
|128|Cheng_Person_Re-Identification_by_CVPR_2016_paper|Person Re-Identification by Multi-Channel Parts-Based CNN with Improved  Triplet Loss Function  De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, Nanning Zheng  Institute of Artificial Intelligence and Robotics  Xian Jiaotong University,Xian, Shaanxi, P.R.|
|||The CNN model is trained by an improved triplet loss function that serves to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space.|
|||Given a persons image, the proposed CNN model outputs an 800 dimension feature representation of the input image.|
|||The proposed CNN model together with the improved triplet loss function can be considered as learning a mapping function that maps each raw image into a feature space where the difference between images of the same person is less than that of different persons.|
|||The main contributions of this paper are twofold: 1) a novel, multi-channel CNN model that learns both the global full-body and the local parts features, and integrates them together to produce the final feature representation of the input person; 2) an improved triplet loss function that requires the intra-class feature distances to be less than not only the inter-class ones, but also a predefined threshold.|
|||Our CNN model differs from the above deep network based approaches in both the network architecture and the loss function.|
|||We first describe the overall framework of our person re-id method, then elaborate the network architecture of the proposed multi-channel CNN model.|
|||Finally, we present the improved triplet loss function used to train the proposed CNN model.|
|||Each CNN in the figure is a proposed multi-channel CNN model that is able to extract both the global full-body and local bodyparts features.|
|||When the proposed CNN model is trained using the improved triplet loss function, the learned feature space will have the property that the distance between w(I o i ) is less than not only the distance between w(I o i ), but also a predefined margin.|
|||Multi(cid:173)Channel Parts(cid:173)based CNN Model  The proposed multi-channel CNN model mainly consists of the following distinct layers: one global convolution layer, one full-body convolution layer, four body-part convolution layers, five channel-wise full connection layers, and one network-wise full connection layer.|
|||As shown in Figure 3, the global convolution layer is the first layer of the proposed CNN model.|
|||Note that all the convolution layers in our CNN model contain a relu layer to produce their outputs.|
|||The Training Algorithm  We use the stochastic gradient decent algorithm to train the proposed CNN achitecture model with the improved triplet loss function.|
|||Experimental Evaluations  Our proposed person re-id method contains two novel ingredients: 1) the multi-channel CNN model that is able to learn both the global full-body and the local body-parts features, 2) the improved triplet loss function that serves to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space.|
|||Method KISSME [21] EIML[18] LMNN[41] LMNN-R[41] ITML[5] LDML[15] Maha[33] Euclidean[33] Descr[17] DeepM[45] Sakrapee[31] OursT OursTC OursTP OursTCP  52.0 51.0 42.0 43.0 47.0 11.0 51.0 14.0 37.0 55.4  39.0 39.0 30.0 32.0 36.0 6.0 41.0 10.0 24.0 45.9  Top1 Top10 Top20 Top50 Top100 68.0 80.0 15.0 68.0 81.0 16.0 59.0 73.0 10.0 60.0 76.0 9.0 64.0 79.0 12.0 19.0 32.0 2.0 64.0 76.0 16.0 28.0 45.0 3.0 56.0 70.0 4.0 17.9 71.4  17.9     55.0 17.0 15.0 58.0 78.0 22.0 22.0 83.0  49.0 53.0 67.0 76.0  39.0 41.0 43.0 47.0  46.0 47.0 55.0 57.0  of the proposed person re-id method, and compared them with a dozen of representative methods in the literature:  Variant 1 (denoted as OursT): We remove the four bodypart channels from the proposed CNN model and use the original triplet loss function to train the network.|
|||Variant 3 (denoted as OursTP): We use the full version of the proposed multi-channel CNN model and train it with the original triplet loss function.|
|||Among these works, DeepM also used body parts to train their CNN models.|
|||constructed a CNN architecture including both global body convolution layer and local parts convolution layers.|
||19 instances in total. (in cvpr2016)|
|129|Huang_Unsupervised_Learning_of_CVPR_2016_paper|Specifically, we first train a CNN coupled with unsupervised discriminative clustering, and then use the cluster membership as a soft supervision to discover shared attributes from the clusters while maximizing their separability.|
|||We follow the hashing idea to generate meaningful attributes1 in the form of binary codes, and train a CNN to simultaneously learn the deep features and hashing functions in an unsupervised manner.|
|||We start with pre-training CNN coupled with a modified clustering method [44] to find representative visual concepts.|
|||Considering the clusters are probably noisy, we use a triplet ranking loss to fine-tune our CNN for attribute prediction, treating the cluster membership as a soft supervision instead of forcing the same attributes for all cluster members.|
|||[29] further seek integration with CNN features which is desirable.|
|||Our method alternates between a modified clustering [44] and CNN training, which results in both robust visual concept clusters and deep feature representations.|
|||The first stage performs alternating unsupervised discriminative clustering and representation learning with CNN to discover representative visual concepts.|
|||Then we train a CNN to learn robust feature representations through classification of these clusters with a softmax loss.|
|||Considering the initial clusters might not have been very good to begin with, we alternate between the clustering process and CNN feature learning to ensure robustness.|
|||Figure 3 compares our CNN clustering results with those by k-means+initialized features.|
|||Via the back-propagation algorithm we  5178  fine-tune our CNN to update both the feature representations f () and hashing functions h().|
|||During CNN tuning, data xi is randomly sampled from all the clusters, x+ is randomly drawn from the same i cluster, while x is chosen as the between-cluster one that i most violates the triplet ranking order (loss is maximum) in one random mini-batch.|
|||Image classification on STL-10 and Caltech-101 dataset: To classify the high-resolution images from STL-10 (96  96) and Caltech-101 (roughly 300  200), we follow [13] to unsupervisedly train CNN on the 32  32 patches extracted from STL-10 images.|
|||We first train an initial CNN model to distinguish between the one million 45  45 color patches corresponding to contours and one million non-contour 4545 patches from BSDS500 dataset.|
|||Then we leverage the contour attributes to refine the pre-trained CNN in a multi-label prediction task.|
|||The attribute codes for non-edge patches are treated as missing labels, of which the gradients are not back-propagated to update CNN weights.|
|||For contour detection, We use the 6-layer CNN architecture of [43].|
|||Our CNN clustering attains the value of 0.34, much higher than 0.16 by k-means clustering with hand-crafted features [36].|
|||We observe a further performance drop when replacing the alternating CNN feature learning with only HOG features.|
||19 instances in total. (in cvpr2016)|
|130|Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper|More specifically, it consists of one image CNN encoding the image content and one matching CNN modeling the joint representation of image and sentence.|
|||The matching CNN composes different semantic fragments from words and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence.|
|||However, the ability of CNN on multimodal matching, specifically the image and sentence matching problem, has not been studied.|
|||The matching CNN composes different fragments from the words of the sentence and learns the joint representation of image and sentence fragments.|
||| Image CNN The image CNN is used to generate the image representation for matching the fragments composed from words of the sentence, which is computed as follows:  im = (wim(CN Nim(I)) + bim),  (1)  where () is a nonlinear activation function (e.g., Sigmoid or ReLU [4]).|
||| Matching CNN The matching CNN takes the encoded image representation im and word representations  i wd as input and produces the joint representation J R. As illustrated in Figure 1, the image content may correspond to the sentence fragments with varying scales, which will be adequately captured in the learnt joint representation of image and sentence.|
|||Firstly, the image CNN can be tuned to generate a better image representation for matching.|
|||Thirdly, the matching CNN (as detailed in the following) composes different fragments from word representations and lets the image representation interact with the fragments at different levels, which can fully exploit the inter-modal matching correspondences between image and sentence.|
|||(a) The word-level matching CNN architecture.|
|||The phrase-level matching CNN and composed phrases.|
|||We name the matching CNN for short phrases and image as MatchCNNphs.|
|||The matching CNN for long phrases and image is named as MatchCNNphl.|
|||More specifically, one image CNN encodes the image into a feature vector.|
|||The sentence CNN with layers of convolution and pooling is used to encode the whole sentence as a feature vector representing its semantic meaning.|
|||The joint representation obtained from the matching CNN is fed into the MLP with one hidden layer with the size as 400.|
|||One possible reason is that NIC uses a better image CNN [33], compared with VGG.|
|||As discussed in Section 5.3.3, the performance of image CNN greatly affects the performance of the bidirectional image and sentence retrieval.|
|||5.3.3  Influence of Image CNN  We use OverFeat and VGG to initialize the image CNN in m-CNN for the retrieval tasks.|
|||Moreover, the so-called region with CNN features in [7] can be used to encode image regions, which are adopted as the image fragments in Deep Fragment and DVSA.|
||19 instances in total. (in iccv2015)|
|131|Reflectance Adaptive Filtering Improves Intrinsic Image Estimation|Several works have included CNN methods in systems that recover reflectance and shading layers [25, 26, 37, 38].|
|||We first design a CNN method that, in contrast to previous work, does not include prior information on shading smoothness [23], reflectance [27, 15, 31, 7], or combinations [2].|
|||The authors used the relative judgment information in a multi-class setup and fine-tune an AlexNet CNN trained on ImageNet.|
|||The works of [37] and [38] are similar, both use a CNN to obtain pairwise judgment predictions, then followed by a step to turn the sparse information into a dense decomposition.|
|||A CNN is used to directly predict reflectance and shading with the objective function being the difference to ground truth decompositions.|
|||They also propose to use an adversary in order to remove typical generative CNN artifacts by discriminating between generated and ground truth decompositions.|
|||For every method we note whether or not it uses a CNN trained on IIW and whether the CNN decomposes densely into intrinsic layers without an additional globalization step.|
|||To our knowledge, there is no CNN based method that predicts a dense intrinsic decomposition and works well for images from IIW.|
|||The work of [12] also trains a CNN from relative judgments with a ranking loss to predict pixel-wise labels, but for the application of recovering dense depth estimates.|
|||This weak label information has been used in [37, 38] for CNN training already, treating it however as a multi-class classification problem.|
|||Remember that the CNN predicts a dense reflectance map on the test set, unaware of the point pairs performance will be evaluated on.|
|||The CNN implements no explicit prior knowledge, we will show next that encouraging piecewise constant reflectance improves the result.|
|||Using the CNN predictions as input to  6794  0.000.300.900.55a25303540455055WHDRTable 3.|
|||We note that using the L1 flattened result directly as reflectance image has 20.9 WHDR, which shows that there is complementary information in the CNN output and the guidance image.|
|||Patches around those centers are extracted and a CNN is used to provide an ordinal relationship via the three-way classification into darker, equal, and lighter.|
|||We applied this to the Direct CNN predictions and searched for the hyper-parameters (see supplementary) to find s = 22, r = 20.|
|||We presented the first end-to-end CNN method, trained on the WHDR 6795  020406080100WHDR0.1(in%)3xGF(Zoranetal.2015,flat)*BF(Zoranetal.2015,flat)*BF(Bietal.2015,flat)Bietal.2015GF(CNN,flat)Zoranetal.2015*BF(CNN,CNN)DirectCNNpredictionZhouetal.2015Belletal.2014L1flatteningZhaoetal.2012Garcesetal.2012Rescalingto[0.55,1]Retinex(gray)Retinex(color)Shenetal.2011Baseline(constR)Baseline(constS)medianmean14.5515.5116.1216.4216.7116.5518.1218.9119.1319.6319.5822.2324.4025.4425.8026.3531.3636.7151.0415.7816.3817.4617.6717.6917.8518.8919.4919.9520.6420.9423.2025.4625.7026.8426.8931.9036.5451.37(a) input image  (d) flat guidance  (g) input image  (j) flat guidance  (b) reflectance of [38]  (e) filtered reflectance  (h) reflectance of [38]  (k) filtered reflectance  (c) shading of [38]  (f) respective shading  (i) shading of [38]  (l) respective shading  Figure 6.|
|||We find that a filtered CNN output is on par with the best published learning based methods and further improve the initial result of [38] to 15.78% on its testset, which is the lowest WHDR performance for a dense decomposition.|
|||A future direction is to replace the expensive optimization of [7] with CNN inference, which would enable fast algorithms for high quality intrinsic video decompositions.|
||19 instances in total. (in cvpr2017)|
|132|Aubry_Understanding_Deep_Features_ICCV_2015_paper|The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors.|
|||In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis.|
|||While these visualizations reveal the nature of learned filters, they largely ignore the question of the dependence of the CNN representation on continuous factors that may influence the depicted scene, such as 3D viewpoint, scene lighting configuration, and object style.|
|||2875  Given a set of rendered images generated by varying one or more factors, we analyze the responses of a layer for a trained CNN (e.g.|
||| Relative to object style, color is more important for the Places CNN than for AlexNet and VGG.|
|||Also related are recent work to understand the quantitative tradeoffs across different CNN layers for networks trained on large image databases [1, 40], designing CNN layers manually [7], and measuring equivariance and equivalence in CNNs [24].|
|||Most related to us is the study of nonlinear CNN feature embeddings with the NORB dataset [14].|
|||We present the stimuli images to a trained CNN as input and record the feature responses for a desired layer.|
|||We seek to study how the higher levels of the CNN encodes the diversity present in a set of images.|
|||Image collection analysis  We seek to characterize how a CNN encodes a collection of related images, , e.g.|
|||Given a trained CNN, let  F L(r) be a column vector of CNN responses for layer L (e.g.|
|||The CNN responses  F L are high-dimensional feature vectors that represent the image information.|
|||(1)  (2)  Finally, we define a residual feature L(), which is the difference of the centered CNN features F L() and the sum of all the marginal features F L k (k).|
|||CNN features  We study three trained CNN models: AlexNet [18], winner of the 2012 ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) [30], Places [43], which has the same architecture as AlexNet but trained on a large image database depicting scenes, and Oxford VGG [8] CNN-S network.|
|||We use the publicly-available CNN implementation of Caffe [16] to extract features for the different layers and pre-trained models for AlexNet, Places, and Oxford VGG from their model zoo.|
|||Results  In this section we highlight a few results from our experiments on CNN feature analysis.|
|||Also, the relative variance associated with the residual decreases for the higher CNN layers, which indicates the two factors are more easily linearly separated in the higher layers.|
|||We also compare against the CNN trained on Places and show that our analysis can apply to complete complex networks, such as GoogLeNet [33].|
|||We thus select an object category and, using rendered views of 3D models, we analyze how the CNN features are influenced by the style of the specific instances as well as different transformations and rendering parameters.|
||19 instances in total. (in iccv2015)|
|133|Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper|Nonetheless, most deep CNN models nowadays are flat N-way classifiers, which share a set of fully connected layers.|
|||Although hierarchical classification has been proven effective for conventional linear classifiers [38, 8, 37, 22], few attempts have been made to exploit category hierarchies [3, 29] in deep CNN models.|
|||Since deep CNN models are large models themselves, organizing them hierarchically imposes the following challenges.|
|||Second, a hierarchical CNN classifier consists of multiple CNN models at different levels.|
|||Third, it would also be slower and more memory consuming to run a hierarchical CNN classifier on a novel testing image.|
|||First, easy classes are separated by a coarse category CNN classifier.|
|||Related Work  Our work is inspired by rapid progress in CNN design and integration of category hierarchy with linear classifiers.|
|||Recently, there has been considerable interest in enhancing CNN components, including pooling layers [36], activation units [11, 28], and nonlinear layers [21].|
|||These enhancements either improve CNN training [36], or expand the network learning capacity.|
|||This work boosts CNN performance from an orthogonal angle and does not redesign a specific part within any existing CNN model.|
|||Instead, we design a novel generic hierarchical architecture that uses an existing CNN model as a building block.|
|||ij}C  On the top of Fig 1(b) are independent layers of coarse category component B, which reuses the configuration of rear layers from the building block CNN and produces an intermediate fine prediction {Bf j=1 for an image xi.|
|||The layer configurations are mostly copied from the building block CNN except that the number of filters in the final classification layer is set to be the size of a partial set instead of the full set of categories.|
|||This flexible modular design allows us to choose the stateof-the-art CNN as a building block, depending on the task at hand.|
|||5.1.1  Initializing the Coarse Category Component  We first pretrain a building block CNN F p using the training set.|
|||Subscript p denotes the CNN is pretrained.|
|||Each fine category CNN is fine-tuned for 40K iterations while the initial learning rate 0.01 is decreased by a factor of 10 every 15K iterations.|
|||Column (d)-(f): top 5 guesses made by the top 3 fine category CNN components.|
|||Conclusions and Future Work  We demonstrated that HD-CNN is a flexible deep CNN architecture to improve over existing deep CNN models.|
||19 instances in total. (in iccv2015)|
|134|Ryoo_Pooled_Motion_Features_2015_CVPR_paper|Our motivation was to design a general representation that best takes advantage of such high-dimensional descriptors and confirm that these CNN features are able to increase first-person activity recognition performance significantly together with other features.|
|||As a human in the scene moves (e.g., changes his/her posture) and the camera changes its viewpoint because of egomotion, certain CNN feature values will become activated/deactivated and our idea is to keep track of such changes to represent the activity video.|
|||When using CNN features as our base per-frame descriptors, we get a 4096-dimensional feature vector (i.e., n=4096) for each image frame by obtaining outputs of the last convolutional layer (e.g., stage 7 in [19]).|
|||Representation implementation: We implemented our PoT representations with four different types of per-frame feature descriptors: histogram of optical flows (HOF), motion boundary histogram (MBH) used in [21], Overfeat CNN image feature [19], and Caffe CNN image feature [7].|
|||PoT obtained the best result using all four feature descriptors and obtained particularly higher performances for high-dimensional CNN features.|
|||In addition, we implemented the approach of combining ITF with CNN descriptors [6], which won the ECCV 2014 classification challenge on UCF101 dataset.|
|||and Caffe were used, and mean of per-frame CNN vectors were computed and added to ITF.|
|||2014 [5] 0.618939 0.818613 0.383081 0.510749 0.397806 0.41918 0.544725 0.86418 0.698887 0.779148 0.605 STIP + Cuboid (with IFV) 0.685341 0.788416 0.471734 0.519026 0.395602 0.23537 0.540914 0.837125 0.74858 0.738611 0.6291759  0.008 ITF + STIP + Cuboid 0.714646 0.871098 0.533088 0.591699 0.47348 0.423435 0.650097 0.827838 0.813176 0.753727 0.6912039  0.006 ITF + CNN [6] 0.696641 0.928735 0.703593 0.651387 0.434422 0.415808 0.778585 0.726934 0.808843 0.608596 0.692315  0.004 PoT + STIP + Cuboid 0.804031 0.925272 0.712457 0.591944 0.460633 0.433242 0.742753 0.866876 0.836603 0.797134 0.73137  0.001 PoT + ITF 0.826126 0.933284 0.71304 0.597523 0.477482 0.515245 0.754544 0.87818 0.851537 0.795619 0.7447038  0.001 PoT + ITF + STIP + Cuboid 0.819623 0.92363 0.703833 0.594038 0.479739 0.50065 0.733664 0.873437 0.848916 0.809405 0.7406666  0.001 PoT (Combined).78.02.08.06.06.98.02.03.62.15.04.10.03.04.60.08.02.12.02.04.06.02.04.15.44.16.03.02.02.11.05.07.32.39.06.03.08.04.16.03.76.02.02.02.02.04.86.02.02.04.04.02.81.06.09.01.02.87INRIA ITF.75.13.10.96.01.02.04.39.28.03.14.10.02.01.69.06.03.09.08.02.03.07.16.47.03.07.02.15.04.09.08.04.48.04.04.11.09.02.04.22.06.57.07.02.11.04.04.02.78.04.07.04.01.83.01.04.15.03.02.02.10.62  Final accuracy STIP (with IFV) [10] 0.6913438  0.003 Cuboid (with IFV) [3] 0.7233332  0.002 BoW all 0.7649616  0.002 IFV all 0.7640002  0.002 Inria ITF (with IFV) [21] 0.7662412  0.002 ITF + CNN [6] 0.757359  0.002 PoT all (best) 0.793641 PoT + ITF 0.794897  0 Figure 6.|
|||Performance gain from combining CNN features with conventional motion features for both datasets.|
|||Evaluation of appearance-based features Taking advantage of CNN descriptors: We explicitly compared the recognition accuracies of our approach with and without CNN-based appearance descriptors.|
|||The idea was to confirm how much benefit our PoT representation is able to get from CNN descriptors when representing firstperson videos for activity recognition.|
|||For the DogCentric dataset, using CNN descriptors greatly benefited the overall recognition accuracy.|
|||Notice that CNN descriptors themselves showed superior performance compared to all other motion-based descriptors (e.g., HOF) with our PoT, as described in Figure 3.|
|||Appearance descriptors: CNN vs.|
|||HOG: We tested another appearance descriptor, histogram of oriented gradients (HOG), and compared it with the CNN descriptors we are using.|
|||Performance comparison of CNN features as a replacement for HOG, showing consistent gains on both datasets with and without motion features included.|
|||We not only compared our PoT representation only based on HOG with those only based on CNN descriptors, but also tested our final combined PoT representing using both appearance descriptors and motion descriptors (i.e., HOF and MBH) by replacing CNN with HOG.|
|||Our PoT was designed to capture entire scene dynamics as well as local motion in first-person videos by representing longterm/short-term changes in high-dimensional feature descriptors, and it was combined with four different types of per-frame descriptors including CNN features.|
|||0.600.620.640.660.680.700.720.740.76DogCentricWithout CNN featuresWith CNN featuresHOF+MBHHOF+MBH+ITF0.660.680.700.720.740.760.780.800.82UEC ParkDogCentricUEC Park0.40.50.60.70.80.9PerformanceHOGCNNHOG+HOF+MBHCNN+HOF+MBH[17] M. S. Ryoo and J. K. Aggarwal.|
||19 instances in total. (in cvpr2015)|
|135|cvpr18-Classification-Driven Dynamic Image Enhancement|To this end, we present a unified CNN architecture that uses a range of enhancement filters that can enhance image-specific details via end-to-end dynamic filter learning.|
|||In addition, our approach is capable of improving the performance of all generic CNN architectures.|
|||Several recent works have shown that convolutional neural networks (CNN) [2, 3, 23, 27, 39, 40] can successfully  RGB  [b]  WLS Filter  [a]  C  O  N  V  C  O  N  V  Enhanced  Classification	 prediction	 with	lowconfidence  Classification	 prediction	 with	highconfidence  Figure 1: Overview of the proposed unified CNN architecture using enhancement filters to improve classification tasks.|
|||Given an input RGB image, instead of directly applying the CNN on this image ([a]), we first enhance the image details by convolving the input image with a WLS filter (see Sec.|
|||Our contribution is a method that jointly optimizes a CNN for enhancement and image classification.|
|||We achieve this by adaptively enhancing the features on an image basis via dynamic convolutions, which enables the enhancement CNN to selectively enhance only those features that lead to improved image classification.|
|||3.1 and 3.3), one can consistently improve the classification performance of vanilla CNN architectures on all the datasets.|
|||Background and Related Work  Considerable progress has been seen in the development for removing the effects of blur [2], noise [27], and compression artifacts [38] using CNN architectures.|
|||The investigated CNN frameworks [2, 3, 15, 22, 23, 27, 39, 40] are typically built on simple strategies to train the networks by minimizing a global objective function using input-output image pairs.|
|||[39] learn a CNN architecture to approximate existing edge-aware filters from input-output image pairs.|
|||[3] learn a CNN that approximates end-to-end several image processing operations using a parameterization that is deeper and more context-aware.|
|||[40] learn a CNN to approximate image transformations for image adjustment.|
|||[15] learn a CNN architecture to remove rain streaks from an image.|
|||[22] propose a learning-based joint filter using three CNN architectures.|
|||propose a CNN architecture to predict the complex Fourier coefficients of a deconvolution filter which is applied to individual image patches for restoration.|
|||To this end, we propose three CNN architectures described in this section.|
|||Note that, the  purpose of this paper is to improve the baseline performance of generic CNN architectures using an add-on enhancement filters, and not to compete against the state-of-the-art methods.|
|||Concluding Remarks  In this paper, we propose a unified CNN architecture that can emulate a range of enhancement filters with the overall goal to improve image classification in an end-toend learning approach.|
|||In addition to improving the baseline performance of vanilla CNN architectures on all datasets, our method shows promising results in comparison to the state-of-theart using our static/dynamic enhancement filters.|
||19 instances in total. (in cvpr2018)|
|136|Liu_Multi-Objective_Convolutional_Learning_2015_CVPR_paper|We show that this nonparametric prior significantly reduces the size of CNN in terms of both parameters and connections.|
|||In other words, it provides strong regularizations for CNN training to facilitate lightweight architectures.|
|||This is a typical two-step approach that sequentially trains a CNN and a graphical model.|
|||In contrast, our method trains a single CNN with two distinct losses (one for unary and the other for pairwise) through multiobjective optimization.|
|||Proposed CNN classifier with sliding window based inputs.|
|||Learning CNN parameters u and b jointly with the CRF model is difficult as the process needs to explore not only the combinatorial labeling space but also the large parameter space.|
|||We denote the parameters of the shared CNN network by , and the feature response extracted from the topmost intermediate layer of CNN by hi = h(xi, ).|
|||(7)  Based on these two loss functions (4) and (6), we train  the unified CNN through multi-objective optimization,    min  {Ou(, u), Ob(, b)},  (cid:26) Ou(, u) = E((cid:80) Ob(, b) = E((cid:80)  iV Lu(yi, xi, , u)) + (, u) i,jE Lb(zij, xij, , b)) + (, b) (8) where Ou(, u) is the expected loss E() for the unary classifier and Ob(, b) is the expected loss for the binary classifier over all the training samples.|
|||This multi-objective CNN has two main advantages: First, the convolutional network generates expressive representations at lower levels (layers that are close to the input  end) that can be utilized for both unary and pairwise model regressions.|
|||Moreover, the prior input introduces regularization to the CNN so that the training could converge faster.|
|||In the testing stage, the labeling process involves evaluating the learned CNN for  4  Figure 3.|
|||The CNN trained on image patches without exemplars incorrectly labels the face on the upper left part according to its local content while the CNN trained on both prior and image patches is able to reject the false label assignments.|
|||Adapting the patch CNN to the full image.|
|||To generate pixel-wise label likelihood maps efficiently, we make some adjustments for the CNN trained on patches.|
|||Our CNN architecture consists of more layers but has a smaller filter size, compared to that in [11].|
|||The CNN training procedure is carried out using minibatch gradient descent with the momentum, weight decay, dropout ratio, and batch size are set to 0.9, 5  104, 0.5, and 50, respectively.|
|||Specifically, we demonstrate that both the multi-objective approach and the nonparametric prior improve the performance in all aspects compared to a per-pixel CNN classifier.|
|||Moreover, the proposed multi-objective approach (beginning with MO) generally outperform the CNN classifiers (S-CNNs and S-CNNs with prior).|
|||On one hand, it introduces noises to the supervised CNN training, on the other hand, the superpixel-wise evaluation in [10] does not reveal the real accuracy.|
||19 instances in total. (in cvpr2015)|
|137|Unite the People_ Closing the Loop Between 3D and 2D Human Representations|Following best practices for CNN training, we use a validation set to determine the optimal number of training iterations and the person size, which is around 500 pixels.|
|||This high resolution allows the CNN to reliably predict small body parts.|
|||We use a state-of-the-art DeeperCut CNN [19] for our pose-related experiments, but believe that using other models such as Convolutional Pose Machines [42] or Stacked Hourglass Networks [32] would lead to similar findings.|
|||PCK@0.2  UPI-P14h  UPI-P14  UPI-P91  DeeperCut CNN [19]  Ours (trained on UPI-P14h)  Ours (trained on UPI-P91)  93.45  89.11  91.15  92.16  87.36  93.24  NA  NA  93.54  Table 3: Pose estimation results.|
|||Even though the DeeperCut CNN has been trained on almost by factor ten more examples, our model remains competitive.|
|||This estimator outperforms the plain DeeperCut CNN with a small margin from 0.897 PCK@0.2 (DeeperCut) to 0.9028 PCK@0.2 (ours) on the full, human labeled FashionPose test set.|
|||0.9217, 0.8823  0.8882, 0.6703  DP from 91 landmarks  SMPLify on DeepCut CNN lms.|
|||[4]  0.9189, 0.8807  0.8771, 0.6398  SMPLify on our CNN lms., tr.|
|||UPI-P14h  0.8944, 0.8401  0.8537, 0.5762  SMPLify on our CNN lms., tr.|
|||UP-P14  0.8952, 0.8475  0.8588, 0.5798  SMPLify on our CNN lms., tr.|
|||UP-P91  0.9099, 0.8619  0.8732, 0.6164  SMPLify on DeepCut CNN lms.|
|||[4]  SMPLify on our CNN lms., tr.|
|||UPI-P14h  SMPLify on our CNN lms., tr.|
|||UP-P14  SMPLify on our CNN lms., tr.|
|||The landmarks for these experiments are always predictions from our CNN trained on UP-P91.|
|||We do not reach the performance of the fits performed on the DeepCut CNN [34] predictions, largely because of few extreme poses that our pose estimator misses with a large influence on the final average score.|
|||Together with our ResNet101-based CNN model, it is possible to predict a full 3D body model configuration from an image in 0.378s.|
|||The pose-predicting CNN is the computational bottleneck.|
|||Because our findings are not specific to a CNN model, we believe that by using a speed-optimized CNN, such as SqueezeNet [18], and further optimizations of the direct predictor, the proposed method could reach realtime speed.|
||19 instances in total. (in cvpr2017)|
|138|Prabhakar_DeepFuse_A_Deep_ICCV_2017_paper|To our knowledge this is the first work that uses deep CNN architecture for exposure fusion.|
|||The contributions of this work are as follows:  A CNN based unsupervised image fusion algorithm  for fusing exposure stacked static image pairs.|
|||Section 3, we present our CNN based exposure fusion algorithm and discuss the details of experiments.|
|||In this work we explore the effectiveness of CNN for the task of multi-exposure image fusion.|
|||For example, let us assume that input x is mapped to output y by some complex transformation f. The CNN can be trained to estimate the function f that minimizes the difference between the expected output y and obtained output y.|
|||Architecture of proposed image fusion CNN illustrated for input exposure stack with images of size h  w. The pre-fusion layers C1 and C2 that share same weights, extract low-level features from input images.|
|||The CNN is used to fuse the luminance channel of the input images.|
|||DeepFuse CNN  The learning ability of CNN is heavily influenced by right choice of architecture and loss function.|
|||Let {yk}={yk|k=1,2} denote the image patches extracted at a pixel location p from input image pairs and yf denote the patch extracted from CNN output fused image at same location p. The objective is to compute a score to define the fusion performance given yk input patches and yf fused image patch.|
|||DeepFuse Baseline  So far, we have discussed on training CNN model in unsupervised manner.|
|||One interesting variant of that would be to train the CNN model with results of other state-ofart methods as ground truth.|
|||This experiment can test the capability of CNN to learn complex fusion rules from data itself without the help of MEF SSIM loss function.|
|||The choice of loss function to calculate error between ground truth and estimated output is very crucial for training a CNN in supervised fashion.|
|||The fused image appear blurred when the CNN was trained with l2 loss function.|
|||Unlike l1 and l2, results by CNN trained with SSIM loss function are both sharp and  2Due to space limitations, we have shown results for only few se 3In a user survey conducted by Ma et al.|
|||: tested with NVIDIA Tesla K20c GPU, : tested with Intel R(cid:13)Xeon @ 3.50 GHz CPU  Image size Ma15  Li13 Mertens07 DF   512*384 1024*768 1280*1024 1920*1200  2.62 9.57 14.72 27.32  0.58 2.30 3.67 6.60  0.28 0.96 1.60 2.76  0.07 0.28 0.46 0.82  already trained DeepFuse CNN to fuse multi-focus images without any fine-tuning for MFF problem.|
|||9 shows that the DeepFuse results on publicly available multi-focus dataset show that the filters of CNN have learnt to identify proper regions in each input image and successfully fuse them together.|
|||It can also be seen that the learnt CNN filters are generic and could be applied for general image fusion.|
|||Application of DeepFuse CNN to multi-focus fusion.|
||19 instances in total. (in iccv2017)|
|139|Regressing Robust and Discriminative 3D Morphable Models With a Very Deep Neural Network|The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set.|
|||The very recent method of [31] also uses a CNN to regress 3DMM parameters for face photos.|
|||These include methods designed for videos (e.g., [19, 36]) and the CNN based approaches of [20, 47].|
|||These multi image 3DMM estimates are then used as ground truth 3D face shapes when training our CNN 3DMM regressor.|
|||(c) These pooled estimates are used in place of expensive ground truth face scans to train a very deep CNN to regress 3DMM parameters directly.|
|||fine-tuned on CASIA images using the pooled 3DMM estimates as target values; different images of the same subject presented to the CNN using the same target 3DMM shape.|
|||We note that we also tried using the VGG-Face CNN of [27] with 16 layers.|
|||It is important to note that by choosing to use a CNN to regress 3DMM parameters, we obtain a function that is render-free.|
|||Parameter based 3D(cid:173)3D recognition  The CNN we train in Sec.|
|||3.1, all images here have equal weights, as we do not run landmark detection prior to 3DMM fitting with our CNN (see below).|
|||This may be due to our use of such a large dataset to train the CNN and their known robustness to training label errors and noise [44].|
|||This includes 3DDFA [47] which fits 3DMM parameters by using a CNN to deal with large pose variations as well as [19] and [2].|
|||We compared our approach with iterative methods such as classic 3DMM implementations [2, 19, 33], the flow-based method of [13] and also with a recent CNN based method [47].|
|||Unsurprisingly, at 0.088s (11Hz), our CNN is several orders of magnitude faster predicting 3DMM parameters than most of the methods we tested.|
||| Denotes the same method used to produce 3DMM target values for our CNN training (Sec.|
|||It is outperformed by the Facebook CNN ensemble system [39], explicitly designed for face recognition, by an AUC gap of only 1%.|
|||Comparing our 3DMM regression with others, including baseline face recognition methods. Denotes the same method used to produce 3DMM target values for our CNN training.|
|||We propose instead to use a very deep CNN architecture to regress 3DMM parameters directly from input images.|
|||DisturbLabel: Regularizing CNN on the loss layer.|
||19 instances in total. (in cvpr2017)|
|140|Hayder_Learning_to_Co-Generate_CVPR_2016_paper|The CNN module of each candidate predicts (i) an objectness score; (ii) a bounding box location; and (iii) a low-dimensional feature vector employed in the pairwise term of the CRF.|
|||In particular, [18] leverages the representation power of deep networks to learn a discriminative CNN that generates boxes and segmentation proposals.|
|||By contrast, in our previous work [10], we performed feature selection using pre-trained CNN features.|
|||Our deep structured network consists of two components: One CNN for each object candidate, with weights shared across all candidates, which provides a general object representation, and one fully-connected CRF, which captures the similarity between every pair of object candidates.|
|||the output of the FC7 layer of the CNN for object candidate xi.|
|||Fully(cid:173)connected CRF for Candidate Similarity  On top of the deep CNN modules, we construct a fullyconnected CRF, which models inter-candidate similarity.|
|||Our model consists of one deep CNN module per object candidate, linked by a fullyconnected CRF.|
|||1, a CNN module corresponding to candidate xi produces features for the unary term of xi and features for the pairwise terms involving xi.|
|||Furthermore, the pairwise term connects multiple CNN modules to form a structured prediction model.|
|||Deep CNNs for Individual Object Candidates  The network architecture constituting the CNN module for each individual object candidate xi is depicted by Fig.|
|||As mentioned above, this CNN module produces (i) a unary term consisting of an objectness score and of a refined object location; and (ii) a feature vector for the pairwise term.|
|||kzkSL1 =Xk  Our deep structured network has four sets of parameters, including the network weights Wcnn before the FC7 layer of the CNN module, the unary term weights Wu, the regression weights Wr and the pairwise feature weights Wp.|
|||The overall training procedure consists of two steps: We first pre-train the CNN modules and then train the full structured model using mini-batches.|
|||Pre(cid:173)training the Deep CNN Module  4.|
|||To this end, let D = {X, Y, T} = {(xi, yi, ti)}N i=1 be a set of object candidates extracted from training im In a first stage, we train the CNN module corresponding to the individual object candidates.|
|||The CNN module has two outputs: the object label probability Pu(yi) and the bounding box location offset ti.|
|||the unary and pairwise weights Wu and Wp, as well as the network features f net in order to backpropagate the gradient to the CNN modules.|
|||the pairwise weight matrix Wp and  the CNN features f net  i  can be computed as  i  hp T Wp  bT q Wp  bT q f net  i  bT q  =Xi  hp i hp T f net  i  i  bT q  hp i  =  =Xi  = WT p  bT q  hp i  bT q  hp i  f net T i  (19)  .|
|||In practice, we use different step sizes for the unary, pairwise, regression and CNN weights, which helps the learning procedure focus on the pairwise and regression weights, while only fine-tuning the rest of the network.|
||19 instances in total. (in cvpr2016)|
|141|Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper|Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain.|
|||A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant.|
|||Over many benchmark datasets CNN has substantially advanced the state-of-the-art accuracies of object recognition [26, 50, 33, 5, 43].|
|||For example, after training on 1.2 million im ages from ImageNet [8], CNN [26] has achieved better performance than handcraft features by a significant margin in classifying objects into 1000 categories.|
|||Furthermore, the pretrained CNN features on this dataset have been transfered to other datasets to achieve remarkable results [5, 43].|
|||In the paper, we present a recurrent CNN for static object recognition.|
|||Since its birth, CNN has been characterized by local connections, weight sharing and local pooling.|
|||[41], the excellent performance of CNN can be largely attributed to these properties as certain structures with random weights could also achieve good results.|
|||The use of GPU has greatly facilitated the training and testing of CNN on large-scale datasets.|
|||The first successful GPU implementation of CNN refers to the AlexNet [26] which won the recognition competition in the ImageNet [8] Large Scale Visual Recognition Challenge (ILSVRC) 2012.|
|||Different from this layer-by-layer propagation idea, Pinheiro and Collobert [36] used extra connections from the top layer to the bottom layer of a CNN directly.|
|||But these techniques have not made the sparse coding models competitive with CNN for object recognition.|
|||[9] studied the factors that influence the performance of CNN by employing a recursive convolutional neural network, which is equivalent to the time-unfolded  version of RCNN but without feed-forward input to each unfolded layer.|
|||The first term in (1) is used in standard CNN and the second term is induced by the recurrent connections.|
|||Note that simply increasing the depth of CNN by sharing weights between layers can result in the same depth and the same number parameters as RCNN, but such a model may not compete with RCNN in performance, as verified in our experiments (see Section 4.2.1).|
|||This was called recursive CNN [9] or rCNN for short.|
|||This structure enabled the units to be modulated by other units in the same layer, which enhanced the capability of the CNN to capture statistical regularities in the context of the object.|
|||The recurrent connections increased the depth of the original CNN while kept the number of parameters constant by weight sharing between layers.|
|||Experimental results demonstrated the advantage of RCNN over CNN for object recognition.|
||19 instances in total. (in cvpr2015)|
|142|Meng_Tang_On_Regularized_Losses_ECCV_2018_paper|On Regularized Losses  for Weakly-supervised CNN Segmentation  Meng Tang1, Federico Perazzi2, Abdelaziz Djelouah3  Ismail Ben Ayed4, Christopher Schroers3, and Yuri Boykov1  1 Cheriton School of Computer Science, University of Waterloo, Canada  2 Adobe Research, United States  3 Disney Research, Z urich, Switzerland  4 ETS Montreal, Canada  Abstract.|
|||Keywords: Regularization  Semi-supervised Learning  CNN Segmentation  1  Introduction  We advocate regularized losses for weakly-supervised training of semantic CNN segmentation.|
|||Surprisingly, this general idea was largely overlooked in weakly-supervised CNN segmentation where current methods often introduce computationally expensive MRF/CRF pre-processing or layers generating fake full masks from partial input.|
|||Though common as shallow regularizers [6, 5, 27, 20] or jointly trained in CNN [35, 28, 2], CRF/MRF were never used directly as losses in segmentation.|
|||In fully supervised setting, integrating the unary scores of a CNN classifier and the pairwise potentials of dense CRF achieve competitive performances [35, 2].|
|||These schemes typically iterate between two steps: CNN training and proposal generation via regularization-based shallow interactive segmentation, e.g.|
|||On Regularized Losses for Weakly-supervised CNN Segmentation  3   Comprehensive experiments (Sec.4) with our regularized weakly supervised losses show (1) state-of-the-art performance for weakly supervised CNN segmentation reaching near full-supervision accuracy and (2) better quality and efficiency than proposal generating methods or normalized cut loss [30].|
|||We advocate this principle for semantic CNN segmentation, propose specific shallow regularizers for such losses, and discuss their properties.|
|||spectral relaxation or graph cuts), but the standard gradient descent approach for optimizing losses during CNN training allows significant flexibility in including different regularization terms, as long as there is a reasonable relaxation.|
|||On Regularized Losses for Weakly-supervised CNN Segmentation  5  3 Connecting Proposals Generation & Loss Optimization  The majority of weakly-supervised methods generate segmentation proposals and train with such fake ground truth [22, 33, 18, 19, 23, 13].|
|||Then alternation can happen either when CNN training converges or online for each batch.|
|||On Regularized Losses for Weakly-supervised CNN Segmentation  7  4 Experiments  Sec.|
|||On Regularized Losses for Weakly-supervised CNN Segmentation  9  image  network output  NC grad.|
|||We compare SEC with its simplification replacing their constrain-to-boundary loss by our  On Regularized Losses for Weakly-supervised CNN Segmentation  11  Fig.|
|||This shows the effectiveness of our CRF loss for training CNN segmentation.|
|||On Regularized Losses for Weakly-supervised CNN Segmentation  13  length 100%  length 50%  70  65  60  55  )  %  (    U O m  I  50  0  full supervison  w/ KC loss w/ CRF loss w/ NC loss pCE loss ScribbleSup  0.2  0.4  0.6  0.8  1  scribble length ratio  length 30%  length 0%(click)  Fig.|
|||Comprehensive experiments (Sec.4) with our regularized losses show (1) state-of-the-art performance for weakly supervised CNN segmentation reaching near full-supervision accuracy and (2) better quality and efficiency than proposal generating methods or normalized cut loss [30].|
|||On Regularized Losses for Weakly-supervised CNN Segmentation  15  References  1.|
|||Tang, M., Djelouah, A., Perazzi, F., Boykov, Y., Schroers, C.: Normalized Cut Loss for Weakly-supervised CNN Segmentation.|
||19 instances in total. (in eccv2018)|
|143|Pedersoli_Areas_of_Attention_ICCV_2017_paper|We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion.|
|||Besides implementing our model using attention areas defined over CNN activation grids or object proposals, as used in previous work, we also present a end-to-end trainable convolutional spatial transformer approach to compute image specific attention areas (bottom).|
|||Earlier attention-based image captioning models used the positions in the activation grid of a CNN layer as attention areas, see e.g.|
|||In its basic form a CNN processes the input image to encode it into a vectorial representation, which is used as the initial input for an RNN.|
|||In thier approach the positions in the activation grid of a convolutional CNN layer is the loci of attention.|
|||In Section 3.3 we show how we integrate regions based on CNN activation grids, object pro posals, and spatial transformers networks in our model.|
|||Baseline CNN(cid:173)RNN encoder(cid:173)decoder model  Our baseline encoder-decoder model uses a CNN to encode an image I into a vectorial representation (I)  IRdI , which is extracted from a fully connected layer of the CNN.|
|||During training we minimize the sum of losses induced  by pairs of images Im with corresponding captions w1:lm ,  X  L(Im, w1:lm , ) =  X  X  ln p(wt|ht, ),  (3)  m  m  t=1  lm  where  collectively denotes all parameters of the CNN and RNN component.|
|||In this case the regions of attention correspond to the z = x  y spatial positions in the activation grid of a CNN layer (I) with c channels.|
|||In particular we use edge-boxes [39], and maxpool the activations in a CNN layer (I) over each object  1245  which has the same number of dimensions as the activation tensor (I) has channels.|
|||In all cases, the dimension of the region descriptors is given by the number of channels in the corresponding CNN layer, i.e.|
|||To ensure a high-enough resolution of the CNN layer which allows to pool activations for small proposals, we use a separate CNN which processes the input image at a higher resolution than the one used for the global image representation (I).|
|||In this case the number of proposals is not limited by the number of positions in the activation tensor of the CNN layer that is accessed for the region descriptors.|
|||In the first stage, we use pre-trained CNN weights obtained from the ImageNet 2010 dataset [7].|
|||In the second stage, we also update the CNN parameters.|
|||Experimental results  In this section we assess the relative importance of different components of our model, the effectiveness of the different types of attention regions, and the effect of jointly fine-tuning the CNN and RNN components.|
|||We now consider the effect of jointly fine-tuning the CNN and RNN components.|
|||They use a region-based high-level attribute representation instead of a global CNN image descriptor to condition the RNN language model.|
|||(iii) We evaluated our model with three different region types based on CNN activation grids, object proposals, and our region proposal network.|
||19 instances in total. (in iccv2017)|
|144|Jackson_Large_Pose_3D_ICCV_2017_paper|Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model.|
|||We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a  single 2D image.|
|||Main contributions  We describe a very simple approach which bypasses many of the difficulties encountered in 3D face reconstruction by using a novel volumetric representation of the 3D facial geometry, and an appropriate CNN architecture that is trained to regress directly from a 2D facial image to the  corresponding 3D volume.|
|||In summary, our contributions are:   Given a dataset consisting of 2D images and 3D face scans, we investigate whether a CNN can learn directly, in an end-to-end fashion, the mapping from image pixels to the full 3D facial structure geometry (including the non-visible facial parts).|
||| We demonstrate that our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry bypassing the construction (during training) and fitting (during testing) of a 3DMM.|
||| We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image.|
|||Our method uses only 2D images as input to the proposed CNN architecture.|
|||More recent work has proposed to estimate the update for the 3DMM parameters using CNN regression, as opposed to non-linear optimization.|
|||It is based on a single CNN that is iteratively applied to estimate the model parameters using as input the 2D image and a 3D-based representation produced at the previous iteration.|
||| Because of this fundamental difference, our method is also radically different in terms of the CNN architecture used: we used one that is able to make spatial predictions at a voxel level, as opposed to the networks of [28, 9] which holistically predict the 3DMM parameters.|
|||When compared to the state-of-the-art CNN method for 3DMM fitting of [28], we report large performance improvement.|
|||Our work has been inspired by the work of [5, 6] who showed that a CNN can be directly trained to regress from pixels to depth values using as input a single image.|
|||In contrast, we process faces at fixed scale (assuming that this is provided by a face detector), but we build our CNN based on a state-of-the-art bottom-up top-down module [15] that allows analysing and combining CNN features at different resolutions for eventually making predictions at voxel level.|
|||Another recent work is [25] which uses a very deep CNN for 3DMM fitting.|
|||Note that similar learning problems are encountered when a CNN is used to regress model parameters like the 3DMM parameters rather than the actual vertices.|
|||We approach this problem using recent CNN architectures from semantic image segmentation [14] and their extensions [15], as described in the next subsection.|
|||Our CNN architecture for 3D segmentation is based on the hourglass network of [15] an extension of the fully convolutional network of [14] using skip connections and residual learning [7].|
|||Volumetric Regression Networks largely outperform 3DDFA and EOS on all datasets, verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning.|
|||Conclusions  We proposed a direct approach to 3D facial reconstruction from a single 2D image using volumetric CNN regression.|
||19 instances in total. (in iccv2017)|
|145|Massa_Deep_Exemplar_2D-3D_CVPR_2016_paper|Our approach can be naturally incorporated into a CNN detection pipeline and extends the accuracy and speed benefits from recent advances in deep learning to 2D-3D exemplar detection.|
|||Here, we visualize CNN features using the inversion network of [13] (outlined in red), which infers the original image given a CNN layers response.|
|||More recently there has been work to enrich the feature representation for matching and alignment using CNNs, which include CAD retrieval based on CNN responses (e.g., AlexNet [31] pool5 features) [4], learning a transformation from CNN features to light-field descriptors for 3D shapes [35], and training a Siamese network for style retrieval [6].|
|||We formulate a generic domain adaptation approach over image features, which can be applied to hand-crafted features, e.g., HOG [12] or CNN responses.|
|||We start by computing CNN features for an image corresponding to a selective search window, along with CNN features for rendered views of CAD models.|
|||Note that in the case where  is an affine transformation, our formulation is similar to the one of Lenc and Vedaldi [33] where a  mapping was learned given image pairs to analyze the equivariance of CNN features under geometric transformations.|
|||We focused on transformations that could be formulated as CNN layers, and in particular successions of convolutional and ReLU layers.|
|||This is easily done in a CNN by replacing a fully-connected layer by a convolutional layer with limited support, which implies translation invariance in the adaptation.|
|||We observed that applying the ReLU function consistently improved results, and is in agreement with state-of-the-art CNN architecture design choices for object recognition.|
|||This is expected, since cosine similarity is known to work better when comparing CNN features [4], but also because we later used the cosine distance to compare real and synthetic features (c.f.|
|||Figure 1 shows our CNN for 2D3D exemplar detection.|
|||Our network starts with layers corresponding to a CNN trained on a different task (e.g., CaffeNet [29] trained for ImageNet classification in our experiments) until an intermediate layer (e.g.,pool5).|
|||Several standard similarity functions, such as dot product and cosine similarity, can be implemented as CNN layers.|
|||Instance retrieval  To select CNN features and a similarity function for comparing natural images and CAD rendered views, we consider a retrieval task where, given a cropped image depicting a query object, we seek to return a model corresponding to the object.|
|||We extracted CNN features from CaffeNet [29] for our experiments.|
|||As discussed in Section 3, the adaptation  in Equation (2) can be implemented in a CNN as a fully-connected layer, followed by a ReLU nonlinearity.|
|||A more detailed analysis reveals the importance of the adaptation for all the methods based only on CNN features from CAD models.|
|||Conclusion  We demonstrated an end-to-end CNN for 2D-3D exemplar detection.|
|||Joint embeddings of shapes and images via CNN image purification.|
||19 instances in total. (in cvpr2016)|
|146|cvpr18-PWC-Net  CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume|PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume  Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz  NVIDIA  Abstract  We present a compact but effective CNN model for optical flow, called PWC-Net.|
|||Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image.|
|||It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow.|
|||[15] propose two CNN models for optical flow, i.e., FlowNetS and FlowNetC, and introduce a paradigm shift.|
|||Right: among existing end-to-end CNN models for flow, PWC-Net reaches the best balance between accuracy and size.|
|||Is it possible to both increase the accuracy and reduce the size of a CNN model for optical flow?|
|||It makes significant improvements in model size and accuracy over existing CNN models for optical flow (Figs.|
|||The next-best method, FlowFieldsCNN [3], learns CNN features for sparse matching and densifies the matches by EpicFlow.|
|||The third-best method, MRFlow [53] uses a CNN to classify a scene into rigid and non-rigid regions and estimates the geometry and camera motion for rigid regions using a plane + parallax formulation.|
|||[15] construct two CNN networks, FlowNetS and FlowNetC, for estimating optical flow based on the U-Net denoising autoencoder [40].|
|||[34] learn CNN models for optical flow by interpolating frames.|
|||While inferior to supervised approaches on datasets with labeled training data, existing unsupervised methods can be used to (pre-)train CNN models on unlabeled data [30].|
|||The DenseNet architecture [22, 27] directly connects each layer to every other layer in a feedforward fashion and has been shown to be more accurate and easier to train than traditional CNN layers in image classification tasks.|
|||Third, as the cost volume is a more discriminative representation of the optical flow than raw images, our network has a layer to construct the cost volume, which is then processed by CNN layers to estimate the flow.|
|||Compared with energy minimization, the warping, cost volume, and CNN layers are computationally light.|
|||We use bilinear interpolation to implement the warping operation and compute the gradients to the input CNN features and flow for backpropagation according to [24, 25].|
|||The context network is a feed-forward CNN and its deIt consists of sign is based on dilated convolutions [57].|
|||Table 7 summarizes the model size for different CNN models.|
|||Conclusions  We have developed a compact but effective CNN model for optical flow using simple and well-established principles: pyramidal processing, warping, and the use of a cost volume.|
||19 instances in total. (in cvpr2018)|
|147|cvpr18-Learning Less Is More - 6D Camera Localization via 3D Surface Regression|Red: In the spirit of PoseNet [11], we train a CNN to predict poses directly.|
|||4654  EstimatedCameraPositions(Competitors)Test Frames with3D Model Fitted(OurResults)EstimatedCameraPositions(OurResults)Because of scarce training data, learning the direct mapping of global image appearance to camera pose with a general purpose CNN (convolutional neural net) has seen only limited success.|
|||Specifically, in the case of DSAC [2], one CNN predicts scene coordinates, and then random subsets of scene coordinates are used to create a pool of camera pose hypotheses.|
|||Each hypothesis is scored by a second CNN (scoring CNN) according to its consensus with the global, i.e.|
|||For example, the CNN might focus on where errors occur in the image rather than learning to assess the quality of errors.|
|||[11] proposed PoseNet, a CNN which learns to map an image directly to a 6D camera pose.|
|||A CNN predicts for each pixel i with position pi the corresponding 3D point yi(w) in the local coordinate frame of the scene.|
|||This CNN is the only learnable component of our system.|
|||Scene Coordinate Regression  The DSAC pipeline [2] uses a CNN for scene coordinate regression which takes an image patch of 42  42 px as input and produces one scene coordinate prediction for the center pixel.|
|||DSAC [2] uses a separate CNN for this task.|
|||This scoring CNN takes a 4040 image of reprojection errors, and regresses a score value s(h) for each hypothesis.|
|||The scoring CNN of DSAC [2] however learns patterns in the global error image that do not generalize well to unseen views.|
|||As mentioned earlier,  4657  Training Set (2 Images Total)  Estimated Camera Poses  3D Model Overlay Estimation with Scoring CNN (DSAC) Estimation with Soft Inlier Count (Our)  3D Model Overlay  Test  Image  DSAC  Our  Figure 3.|
|||Exchanging the scoring CNN for a soft inlier count (see Sec.|
|||We retrain DSAC, substituting the scoring CNN for our soft inlier count.|
|||The scoring CNN easily overfits to the spatial constellation of reprojection errors.|
|||We expect the accuracy gap to vanish completely given enough training data, but at the moment we see no evidence that the scoring CNN could learn to make a more intelligent decision than the inlier counting schema.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||18 instances in total. (in cvpr2018)|
|148|Generalized Rank Pooling for Activity Recognition|Towards this end, we propose a novel pooling method, generalized rank pooling (GRP), that takes as input, features from the intermediate layers of a CNN that is trained on tiny sub-sequences, and produces as output the parameters of a subspace which (i) provides a low-rank approximation to the features and (ii) preserves their temporal order.|
|||The activity predictions from such short temporal receptive fields are then aggregated via a pooling step [25, 7, 38], such as computing the average or maximum of the generated CNN features.|
|||Our experimental results show that the proposed scheme is significantly better at capturing the temporal structure of CNN features in action sequences compared to conventional pooling schemes or the basic form of rankpooling [14], while also achieving state-of-the-art performances.|
||| We show that subspace representation on CNN fea tures is highly beneficial for action recognition.|
|||Typically, the independent action predictions by a CNN along the video sequence is averaged or fused via a linear SVM [38] without considering the temporal evolution of the CNN features.|
|||[13, 16, 41] in a CNN setting via end-to-end learning.|
|||These methods are developed mostly for hand-crafted features and thus their performances on CNN features are not thoroughly understood.|
|||In contrast, we learn subspaces over more general CNN features and constrain them to capture dynamics.|
|||In contrast, we are the first to propose subspace representations on CNN features for action recognition in a joint framework that includes nonlinear chronological ordering constraints to capture the temporal evolution of actions.|
|||Proposed Method  Let X = hx1, x2, ..., xni be a sequence of n consecutive data features, each xt  Rd, produced by some dynamic process at discrete time instances t. In case of action recognition in video sequences using a two-stream CNN model, X represents a sequence of features where each xt is the output of some CNN layer (for example, fully-connected FC6 of a VGG-net as used in our experiments) from a single RGB video frame or a small stack of consecutive optical flow images (similar to [38]).|
|||The CNN training was stopped as soon as the loss on the validation set started increasing, which happened in about 6K iterations for the RGB stream and 40K iterations for the optical flow.|
|||Results  This section provides a systematic evaluation of the influence of various hyper-parameters in our model, namely (i) influence of the number of subspaces used in our model, (ii) influence of the threshold used in enforcing the temporal order, (iii) comparison of the performance difference FC6 and FC7 CNN layer outputs in GRP, and (iv) an analysis of various Grassmannian kernels.|
|||We note that these constraints have a bigger influence on the FLOW stream than on the RGB stream, implying that the dynamics are mostly being captured in the FLOW, as is obvious, while the RGB stream CNN is perhaps learning mostly the background context.|
|||We use the CNN features from the FLOW stream alone for this evaluation, using 2 subspaces.|
|||Comparison of CNN Features  Next, we evaluate the usefulness of CNN features from the FC6 and FC7 layers.|
|||Further, surprisingly, we note that a low-rank reconstruction of the CNN features by itself provides a very good summarization of the actions useful for recognition.|
|||While, using subspaces for action recognition has been done several times in the past [21, 44], we are not aware of any work that shows these benefits on CNN features.|
|||Higher-order pooling of CNN features via kernel linearization for action recognition.|
||18 instances in total. (in cvpr2017)|
|149|Huang_Part-Stacked_CNN_for_CVPR_2016_paper|Part-Stacked CNN for Fine-Grained Visual Categorization  Shaoli Huang1, Zhe Xu1,2, Dacheng Tao1, and Ya Zhang2  1  Centre for Quantum Computation & Intelligent Systems and Faculty of Engineering and Information Technology, University of  2Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China  {shaoli.huang@student.,dacheng.tao@}uts.edu.au, {xz3030,ya zhang}@sjtu.edu.cn  Technology Sydney, Australia  Abstract  In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy.|
|||In this paper, we propose a novel PartStacked CNN architecture that explicitly explains the finegrained recognition process by modeling subtle differences from object parts.|
|||In this paper, we propose a new part-based CNN architecture for fine-grained visual categorization that models multiple object parts in a unified framework with high efficiency.|
|||Since the proposed architecture employs a sharing strategy that stacks the computation of multiple parts together, we call it Part-Stacked CNN (PS-CNN).|
|||The contributions of this paper include: 1) we present a novel and efficient part-based CNN architecture for finegrained recognition; 2) our architecture adopts an FCN to localize object parts, which has seldom been studied before in the context of object recognition; 3) our classification network follows a two-stream structure that captures both object-level and part-level information, in which a new share-and-divide strategy is presented on the computation of multiple object parts.|
|||[51] and Bilinear CNN by Lin et al.|
|||The main idea of the proposed PS-CNN is largely inherited from [51], who first detected the location of two object parts and then trained an individual CNN based on the unique properties of each part.|
|||Network architecture of the proposed Part-Stacked CNN model.|
|||Part-Stacked CNN  We present the model architecture of the proposed PartStacked CNN in this section.|
|||A fully convolutional network is achieved by replacing the parameter-rich fully connected layers in standard CNN architectures by convolutional layers with kernels in spatial size of 1  1.|
|||complete set of CNN features from multiple scales.|
|||To enable domain-specific fine-tuning from pre-trained CNN model weights, we train an auxiliary CNN to initialize the weights for the additional convolutional layer.|
|||The proposed Part-Stacked CNN architecture is implemented using the open-source package Caffe [16].|
|||Since the CNN architecture has a large impact on the recognition performance, for fair comparison, we only compare results reported on the standard seven-layer architecture.|
|||Model interpretation  One of the most prominent features of the proposed Part-Stacked CNN (PS-CNN) method is that it can produce human-understandable interpretation manuals for finegrained recognition.|
|||n/a n/a n/a n/a  Method Constellation [36] Attention [48] Bilinear-CNN [22] Weak FGVC [54] CNNaug [31] Alignment [13] No parts [18] Bilinear-CNN [22] BBox+Parts Part R-CNN [51] BBox+Parts PoseNorm CNN [6] BBox+Parts POOF [2] BBox+Parts DPD+DeCAF[11] BBox+Parts Deep LAC [21] BBox+Parts Multi-proposal [35] Part R-CNN [51] BBox+Parts PS-CNN (this paper) BBox+Parts  BBox BBox BBox BBox  n/a n/a n/a n/a  BBox BBox BBox BBox  Test Anno.|
|||We have discussed the application of the proposed PartStacked CNN on fine-grained visual categorization with strong supervision.|
|||Bilinear cnn models for fine-grained visual recognition.|
||18 instances in total. (in cvpr2016)|
|150|Shin_Interleaved_TextImage_Deep_2015_CVPR_paper|Our deep CNN models on medical image modalities (mostly CT, MRI) are initialized with the model parameters pre-trained from ImageNet [8] using Caffe [19] framework.|
|||If a topic has too few images to be divided into training/CV/test for deep CNN learning (normally rare imaging protocols), then that topic is neglected for the CNN training (e.g., topic #5 Abdominal ultrasound, #28, #49 DEXA scans of different usages).|
|||Surprisingly, we find that transfer learning from the ImageNet pre-trained CNN parameters on natural images to our medical image modalities (mostly CT, MRI) significantly helps the image classification performance3.|
|||Thus our CNN models are fined-tuned from the ImageNet CNN models by default.|
|||Implementation & Results: All our CNN network settings are similar4 or same as the ImageNet Challenge AlexNet [24] and VGG-19 [41] models.|
|||For all the CNN layers except the newly modified ones, the learning rate is set 0.001 for weights and biases, momentum 0.9, weight decay 0.0005 and a smaller batch size 50 (as opposed to 256 [41]).|
|||Validation and top-1, top-5 test scores in classification accuracy using AlexNet [24] and VGG-19 [41] deep CNN models.|
|||Table 3 provides the validation and top-1, top-5 testing in classification accuracies for each level of topic models using AlexNet [24] and VGG-19 [41] based deep CNN models.|
|||Multi-level semantic concepts show good image learnability by deep CNN models which sheds light on the feasibility of automatically parsing very large-scale radiology image databases.|
|||Generating Image-to-Text Description  The deep CNN image categorization on multi-level document topic labels in Section 4 demonstrates promising results.|
|||Bi-gram disease terms are extracted so that we can train a deep CNN (in Section 5.3) to predict the vector/wordlevel image representation (R2562).|
|||A deep regression CNN model is employed here to map an image to a continuous output word-vector space from an image.|
|||Image input vectors as {X  R256256}) are learned through a CNN by minimizing the cross-entropy loss between the target vector and output vector.|
|||Image-to-Words Deep CNN Regression  It has been shown [44] that deep recurrent neural networks (RNN7) can learn the language representation for machine translation.|
|||This can be formulated as a regression CNN, replacing the softmax cost in Section 4 with the crossentropy cost function for the last output layer of VGG-19 CNN model [41]:  N(cid:88)  n=1  E =  1 n  [g(z)ng( zn) + (1  g(zn)) log(1  g(zn))], (2) where zn or zn is any uni-element of the target word vectors Zn or optimized output vectors Zn, g(x) is the sigmoid 7While RNN [37, 46] is the popular choice for learning language mod els [3, 31], deep CNN [28, 24] is more suitable for image classification.|
|||We adopt the CNN model of [41] for the image-to-text representation since it works consistently better than the other relatively simpler model [24] in our image categorization task.|
|||Then, the image is mapped to a R2562 output vector using the bi-gram CNN model in Section 5.3.|
|||Open questions remain such as defining clinically relevant image labels, how to annotate the huge amount of medical images required by deep learning models, and to what extent and scale the deep CNN architecture is generalizable in medical image analysis.|
||18 instances in total. (in cvpr2015)|
|151|cvpr18-Guide Me  Interacting With Deep Networks|If the deployed CNN were adaptable to feedback or specifications provided by a human user online, this interaction would hold the potential to improve the models performance and benefit real-world applications.|
|||For example, in photo editing, when a CNN is used to segment the foreground of an image from the background, the user might notice that the network has made a mistake.|
|||We propose a novel idea to allow user-network feedbackbased interaction that aims at improving the performance of a pre-trained CNN at test time.|
|||We introduce a system that is able to refine predictions of a CNN by injecting a guiding block into the network.|
|||The CNN is already trained and only asks for the users directions for the purpose of conditioned (on-demand) adjustments of an initial estimate.|
|||Most works focus on the combination of CNN and RNN models, often building attention mechanisms [3, 42, 59, 60].|
|||The output of the interactive CNN is in the same domain as the initial one.|
|||Methods  In this section, we describe how our interaction module is integrated into a fixed CNN following two different approaches: guiding with user clicks and back-propagation (Section 3.2) or natural language inputs (Section 3.3).|
|||The module we insert into the CNN is called the guide.|
|||The guide interacts with the guided CNN through a guiding block, which is built to adjust activation maps of the CNN to improve the final prediction for the given input.|
|||The guided CNN is thus split into two parts: the head, which processes the input until it reaches the guiding block, and the tail, that is the rest of the guided network up to the final prediction layer.|
|||The first one  guiding by back 8553  propagation (Section 3.2)  can be directly applied on a pretrained CNN that is kept constant.|
|||This demonstrates the benefit of guiding by back-propagation: it can be directly incorporated into a pre-trained CNN and, without any further training, it boosts a comparably low performance to reach the state of the art.|
|||We evaluate mIoU performance when guiding different layers inside the CNN using find queries.|
|||The first one alters the CNNs activations directly, therefore the weight vector size depends on the CNN layer that is being guided.|
|||Exemplary CNN predictions before and after guidance are shown in Figure 3.|
|||The guiding module was trained with find queries and does not modify the original CNN permanently, but only conditioned on the hints.|
|||This space is the intersection between features learned from the CNN for segmentation and text representation learned by the RNN.|
||18 instances in total. (in cvpr2018)|
|152|Kang_Pairwise_Relational_Networks_ECCV_2018_paper|To capture relations, the PRN takes local appearance patches as input by ROI projection around landmark points on the feature map in a backbone CNN network.|
|||The rest of this paper is as follows: in Section 2 we describe the proposed face recognition method including the base CNN architecture, face alignment,  Pairwise Relational Networks for Face Recognition  3  Global Appearance Representation (Holistic)  Conv  Layers  Global app.|
|||2 Proposed Methods  In this section, we describe our methods in detail including the base CNN model as backbone network for the global appearance representation, the face alignment method, the pairwise relational network, the pairwise relational network with face identity states, and the loss functions.|
|||2.1 Base Convolutional Neural Network  We first describe the base CNN model.|
|||The base CNN model consists of several 3-layer residual bottleneck blocks similar to the ResNet-101 [13].|
|||In contrast, our base CNN model accepts a face image with 140  140 resolution as input, and has 55 convolution filters with a stride of 1 in the first layer (conv1 in Table 1).|
|||More details of the base CNN architecture are given in Table 1.|
|||The base CNN is similar to ResNet-101, but the dimensionality of input, the size of convolution filters, and the size of each output feature map are different from the original ResNet-101  Layer name Output size  conv1  140  140  conv2 x  70  70  conv3 x  35  35  conv4 x  18  18  conv5 x  9  9  101-layer 5  5, 64  3  3 max pool, stride 2   3   4   1  1, 64 3  3, 64  1  1, 256  1  1, 128 3  3, 128  1  1, 512  1  1, 256 3  3, 256  1  1, 1024  1  1, 512 3  3, 512  1  1, 2048            23   3  1  1  global average pool, 8630-d fc, softmax  To represent the global appearance representation f g, we use the 112048  feature which is the output of the GAP in the base CNN (Table 1).|
|||2.2 Face Alignment  In the base CNN model, the input layer accepts the RGB values of the face image pixels.|
|||Based on the feature maps which are the output of the conv5 3 layer in the base CNN model, the face is divided into 68 local regions by ROI projection around 68 landmark  Pairwise Relational Networks for Face Recognition  7  (cid:3039) (cid:3039) (cid:2188)(cid:2870) (cid:2188)(cid:2869)  ....|
|||, f l  Pairwise Relational Networks for Face Recognition  9  f l i  R112,048 from each local region (nearly 1  1 size of regions) around 68 landmark points by ROI projection on the 9  9  2, 048 feature maps (conv5 3 in Table 1) in the backbone CNN model.|
|||Detailed settings in the model We implemented the base CNN and the PRN models using the Keras framework [7] with TensorFlow [1] backend.|
|||For fair comparison in terms of the effects of each network module, we train three kinds of models (model A, model B, and model C) under the supervision of cross-entropy loss with softmax :   model A is the baseline model which is the base CNN (Table 1).|
||| model C is the combined model with the output of the base CNN model (model A) and the output of the P RN + (Eq.|
|||Comparison of the number of images, the number of networks, the dimensionality of feature, and the accuracy of the proposed method with the state-of-the-art methods on the LFW  Method  Images  Networks  Dimension  Accuracy (%)  DeepFace [34] DeepID [30] DeepID2+ [32] DeepID3 [41] FaceNet [28] Learning from Scratch [40] CenterFace [36] PIMNetTL-Joint Bayesian [17] PIMNetfusion [17] SphereFace [23] ArcFace [10]  model A (baseline, only f g) PRN PRN+ model B (f g + P RN ) model C (f g + P RN +)  4M  202, 599 300, 000 300, 000  200M  494, 414  0.7M  198, 018 198, 018 494, 414  3.1M  2.8M 2.8M 2.8M 2.8M 2.8M  9  120 25 50 1 2 1 4 4 1 1  1 1 1 1 1  4, 096  4 150  120 150  120 300  100  128  160  2  512 1, 024  6  1, 024 512  2, 048 1, 000 1, 000 1, 024 1, 024  97.25 97.45 99.47 99.52 99.63 97.73 99.28 98.33 99.08 99.42 99.78  99.6 99.61 99.69 99.65 99.76  (the base CNN model, just uses f g) and P RN + outperforms model B which is jointly combined both f g with P RN .|
|||First, P RN itself provides slightly better accuracy than the baseline model A (the base CNN model, just uses f g) and P RN + outperforms model B which is jointly combined both f g with P RN .|
|||First, compared to model A (base CNN model), model C (jointly combined f g with P RN +) achieved a consistently superior accuracy (TAR and TPIR) on both 1:1 face verification and 1:N face identification.|
|||First, compared to model A (base CNN model, just uses f g), model C (jointly combined f g with P RN + as the local appearance representation) achieved a consistently superior accuracy (TAR and TPIR) on both 1:1 face verification and 1:N face identification.|
||18 instances in total. (in eccv2018)|
|153|cvpr18-Weakly Supervised Learning of Single-Cell Feature Embeddings|An example of transfer learning is a method for diagnosing skin cancer, which reached expert-level classification performance using a CNN pretrained on ImageNet and finetuned to the specialized domain [8].|
|||The CNN is used as a feature extractor to inform the GRU about image contents.|
|||This CNN is trained as a multiclass classification function that takes a single cell as input and produces a categorical output.|
|||We expect the CNN to learn features that encode the high-level associations between treatments without explicitly giving it this information.|
|||In either case, forcing a CNN to find differences where there are none may result in overfitting.|
|||3) Sometimes different treatments yield the same cell phenotypes, forcing a CNN to find differences between them may, again, result in overfitting to undesired variation.|
|||Baseline CNN  We first focus on training a baseline CNN with the best potential configuration to evaluate the contribution of the proposed regularization strategies.|
|||In our experiments, to our surprise, we found that preserving pixel intensities is not the best choice for CNN training.|
|||The GRU demands the CNN to extract relevant features from each cell for solving the classification problem (Fig.|
|||We leverage the ability of the recurrent network to regularize training using multiple examples at once, but after the optimization is over, we discard the RNN and only use the CNN for feature extraction.|
|||Note that our experiments involve two objectives: 1) auxiliary task: training the CNN with weak labels using images of single cells.|
|||To this end, we use the CNN to extract features for single cells using layer conv4a (see Fig.|
|||The accuracy obtained with a baseline CNN in the auxiliary task, which is classifying single cells into 10 variants, is 70.7%.|
|||Our best CNN model yields higher replicate and gene accuracies than baseline methods, indicating that it can extract more discriminative features than previous methods (Table 1).|
|||This evaluation was done with a baseline CNN on the BBBC021 dataset.|
|||A baseline CNN can correctly classify single cells into one of the 103 treatments with 64.1% accuracy (auxiliary task), and obtains 86.9% accuracy in the main goal as measured using the NSCB procedure.|
|||When compared to previous reports, we obtain the best NSC result using a baseline CNN, and the second best NSCB using a CNN regularized with mixup (Table 2).|
|||A salient attribute of large scale microscopy experiments in biology is that while there is no scalable approach to annotating individual cells, there is well-structured information that can be used to effectively train modern CNN architectures using an auxiliary task.|
||18 instances in total. (in cvpr2018)|
|154|cvpr18-MiCT  Mixed 3D 2D Convolutional Tube for Human Action Recognition|Jointly modeling spatiotemporal information via a 3D CNN in an end-to-end deep network provides a natural and efficient approach for action recognition.|
|||In spite of the large progress made by incorporating 3D CNN deep networks [30, 31, 11], the performance of action recognition in videos is still far from satisfactory compared with what has been achieved by 2D CNNs for visual recognition in images.|
|||Reconsidering current 3D CNN networks for action recognition, we notice that most of these methods share the same architecture that stacks 3D convolutions layer by layer, as proposed in C3D [30].|
|||For example, an 11-layer 3D CNN requires nearly 1.5 times as much memory as a 152-layer Residual Network.|
|||Here we outline work involving deep features and classify the related work into two categories, 2D CNN and 3D CNN based approaches, according to the convolutions used in feature learning.|
|||2D CNN based.|
|||However, their success depends greatly on hand-crafted optical flow information, which is computationally expensive, and pre-trained 2D CNN models with huge datasets.|
|||3D CNN based.|
|||The 3D CNN for action recognition was first presented in [14] to learn discriminative features along both spatial and temporal dimensions.|
|||Later, the C3D feature along with the corresponding 3D CNN architectures are presented in [30].|
|||We address this problem by proposing a Mixed Convolution Tube (MiCT), which enables the 3D CNN to incorporate fewer 3D convolutions while also empowering feature learning by taking advantage of 2D CNNs to achieve better performance.|
|||We then evaluate the performance of our scheme by comparisons to both the baseline 3D CNN and state-of-theart approaches.|
|||Comparison with the Baseline 3D CNN  We first evaluate the performance of our MiCT-Net in comparison with that of the baseline 3D CNN approach called C3D [30].|
|||C3D is a typical and popular 3D CNN for action recognition which stacks the 3D convolutions layer by layer.|
|||It can be observed that the 3D CNN with either the concatenated connection (MiCTcon-Net) or the cross-domain residual connection (MiCTres-Net) outperforms the traditional 3D CNN.|
|||Method  UCF101 HMDB51  Method  UCF101 HMDB51  Slow fusion [15]  C3D [30] LTC [31]  Two-stream [25]  Two-stream fusion [11] Two-stream+LSTM [40]  Transformations [36]  TSN [35]  FST CN [28] ST-ResNet [9]  Key-volume mining CNN [41]  TLE(C3D CNN) [7]  TLE(BN-Inception) [7]  I3D [5]  P3D ResNet [22]  MiCT-Net  65.4% 44.0%1 59.9% 73.0% 82.6% 82.6% 81.9% 85.7% 71.3% 82.2% 84.5% 86.3% 86.9% 84.5% 88.6%  88.9%   43.92%   40.5% 47.1% 47.1% 44.1% 54.6%3 42.0% 43.4%   60.3% 63.2% 49.8%   63.8%  Table 3.|
|||of the 2D CNNs [7, 41, 40], or learn very deep spatiotemporal features by either decomposing a 3D convolution into a 2D convolution along the spatial dimension and a 1D convolution along the temporal dimension to model spatiotemporal information [22] or directly inflating the stateof-the-art 2D CNN architecture into 3D CNN to take advantage of well-trained 2D models [5], the MiCT-Net still performs the best.|
|||We further show that the MiCT can be applied to other 3D CNN architectures, e.g.|
||18 instances in total. (in cvpr2018)|
|155|Teaching Compositionality to CNNs|Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN.|
|||Recently, researchers have investigated more inductive biases from neuroscience to improve CNN architectures.|
|||Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth information [14], and using physical interaction with the en input  images  standard   CNN  compositional  CNN (ours)  Figure 1: For a standard CNN (VGG, [37]), the presence of a nearby object (cup) greatly affects the activations in the region of an object of interest (airplane).|
|||Note that a typical CNN does not exhibit this property (Fig.|
|||1 visualizes the difference in activations between a CNN trained without (VGG [37]) and with our compositionality objective1).|
|||15058  representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization  instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.|
|||Second, we implement that notion in the form of a modified CNN training objective, which we show to be straightforward to optimize yet effective in learning compositional representations.|
|||Recent work [28] constrains CNN activations to lie within object masks in the context of weakly-supervised localization.|
|||The original CNN (red) is enhanced by K additional masked CNNs (blue), all of them sharing weights.|
|||We refer to these K CNNs as masked CNNs, and we denote the mapping onto layer n of the kth masked CNN as mk,n.|
|||We denote the mapping onto layer n of this CNN as u,n.|
|||3.2 for compositionality by introducing an objective function that combines an application-specific discriminative CNN loss with additional terms that establish dependencies between the different masked and unmasked CNNs.|
|||Specifically, on all layers at which an object mask is applied, we take the l2 difference  5060  between the activations of the masked CNN and the activations of the unmasked CNN.|
|||Because the unmasked CNN sees all objects and will naturally have different activations from the kth masked CNN due to the presence of the objects other than the kth object, we apply a mask to the unmasked CNNs activations before computing the penalty term.|
|||Since Lc is of a standard form, we can optimize it like any CNN via SGD (specifically, using the ADAM optimizer [19] and Tensorflow [1]).|
|||We believe this to be an artifact of the CNN needing a certain minimum number of layers and corresponding representational power to successfully discriminate between relevant and irrelevant (background) pixels.|
|||Experiments  In this section, we give a detailed experimental evaluation of our approach for teaching compositionality to CNNs, highlighting its ability to improve performance over standard CNN training on both synthetic (Sect.|
|||Conclusion  We have introduced an enhanced CNN architecture and novel loss function based on the inductive bias of compositionality.|
||18 instances in total. (in cvpr2017)|
|156|Shao_Slicing_Convolutional_Neural_CVPR_2016_paper|In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatioand 2D temporal-slices representations.|
|||In existing approaches, a video is treated as a 3D volume and 2D CNN is simply extended to 3D CNN [5], mixing the appearance and dynamic feature representations in the learned 3D filters.|
|||Alternative solutions include sampling frames along the temporal direction and fusing their 2D CNN feature maps at different levels [7], or feeding motion maps obtained by existing tracking or optical flow methods [21, 27].|
|||In this study, we wish to show that with innovative model design, appearance and dynamic information can be effectively extracted at a deeper layer of CNN that conveys richer semantical notion (i.e.|
|||We name our model as Slicing CNN (S-CNN).|
|||It consists of three CNN branches each of which adopts different 2D spatioor temporal-filters.|
|||1, the feature map from a selected filter of a CNN hidden layer only shows high responses on the ice ballet dancers, while that from another filter shows high responses on the audience.|
|||We apply the proposed model to the task of crowd attribute recognition on the WWW Crowd dataset [18] and achieve significant improvements against state-of-the-art methods that either apply a 3D-CNN [5] or Two-stream CNN [21].|
|||Related Work  Compared to applying CNN to the static image analysis, there are relatively few works on the video analysis [3,5,7,18,21,26,27,30].|
|||A 3D-CNN extends appearance feature learning in a 2D CNN to its 3D counterpart to simultaneously learn appearance and motion features on the input 3D video volume [3, 5].|
|||The input of the motion branch CNN is either 2D motion maps (such as optical flow fields [21] and dynamic group motion channels [18]).|
|||The proposed Slicing CNN model overcomes the limitations listed above.|
|||Slicing CNN Model  In this paper, we propose a new end-to-end model named as Slicing CNN (S-CNN) consisting of three branches.|
|||We first learn appearance features by a 2D CNN model on each  5621  Crowd Attributes  SVM  Concatenation  g n  i  n r a e L   e r u t a e F    e r u t a e F  n o i t c a r t x e  ......  ......  ......  Temporal Pooling  Temporal Pooling  Temporal Pooling  Conv Layers  Conv Layers  Conv Layers  TemporalFeatureLearning  Conv Layers  Feature Cuboids  Input Video  ......  ......  Filters  ......  Filters  ......  Filters  Figure 3.|
|||Single Branch of S-CNN Model  Our S-CNN starts with designing a CNN for extracting convolutional feature cuboids from the input video volume.|
|||In principle, any kind of CNN architecture can be used for feature extraction.|
|||Comparison with State-of-the-Art Methods  We evaluate the combined Slicing CNN model (S-CNN) with recent state-of-the-art spatio-temporal deep feature learning models: 1) DLSF+DLMF [18].|
|||Conclusion  In this paper, we present a novel Slicing CNN (S-CNN) for crowd video understanding, with only 2D filters.|
||18 instances in total. (in cvpr2016)|
|157|Learning Multifunctional Binary Codes for Both Category and Attribute Oriented Retrieval Tasks|This motivates us to adopt CNN models to learn unified binary codes that can preserve both similarities simultaneously.|
|||Second, we propose a new training scheme for the CNN models that can take partially labelled data as training inputs to improve the performance and alleviate overfitting.|
|||[19, 30], have adopted CNN models to simultaneously deal with multiple different tasks, and have achieved some successes.|
|||Second, we elaborately design the loss functions to exploit the huge amount of partially labelled data, which has rarely been considered by previous CNN models.|
|||To simultaneously encode category and visual attributes of images into binary codes, we devise a CNN model that can take partially labelled images as training input (step 1), and train the model on classification and attribute prediction tasks (step 2).|
|||the hash functions using CNN models.|
|||To address the shortcomings of previous works, we propose to exploit the CNN models to hierarchically extract the correlation between these two semantic descriptions in an end-to-end manner.|
|||Joint Optimization  With the loss functions defined above, the CNN model can be trained with standard back propagation algorithm with mini-batches.|
|||Implementation details: Our datasets are still relatively small in terms of training a deep CNN model from scratch.|
|||For CFW-60K, we adopted the CNN structure of [38] (from conv1 to pool5).|
|||This justifies our motivation of using partially labelled data to train the CNN models to alleviate overfitting.|
|||For fair comparison,  the conventional methods were trained using L2-normalized CNN features extracted from the pre-trained models (described in Section 4.1).|
|||We can see that: First, when equipped with CNN features, the conventional non-linear method KSH can hardly improve over linear methods.|
|||Comparative methods: We compare with three baseline methods for the attribute prediction part of retrieval: 1) Similar to [13], we train linear SVMs to predict attributes (we found that the performance of linear and kernel SVMs are almost the same, thus we used linear SVMs for efficiency), using the same CNN features as described in Section 4.3.|
|||2) We replace the CNN features in SVMreal with the 256-bit binary codes produced by DLBHC in Section 4.3.|
|||Comparative methods: Since this is a relatively unexplored task, we compare our DPH with two baselines: 1) JLBC [15], which is trained on the fully annotated TrainBoth set with the same CNN features as described above.|
|||The performance of JLBC on CFW-60K is very unsatisfactory, even though CNN features was used to train this model, which confirms that our end-to-end framework is necessary for learning dual purpose hash codes.|
|||The promising performance of our method can be attributed to: a) The utilization of CNN models for hierarchically capturing correlation between category and attributes in an end-to-end manner.|
||18 instances in total. (in cvpr2017)|
|158|Abhimanyu_Dubey_Coreset-Based_Convolutional_Neural_ECCV_2018_paper|We exploit the redundancies extant in the space of CNN weights and neuronal activations (across samples) in order to obtain compression.|
|||Our method requires no retraining, is easy to implement, and obtains state-of-the-art compression performance across a wide variety of CNN architectures.|
|||Popular CNN models such as AlexNet [35] and VGG16 [50], for instance, have 61 and 138 million parameters and consume in excess of 200MB and 500MB of memory space respectively.|
|||Additionally, design choices for CNN architectures, such as network depth, filter sizes, and number of filters seem arbitrary and motivated purely by empirical performance at a particular task, permitting little room for interpretability.|
|||Work aimed at designing efficient CNN architectures, such as Residual Networks (ResNets) [25] and DenseNets [28] have shown promise at alleviating the challenge of model complexity.|
|||Coupled with Deep Compression, we are additionally able to compress other popular CNN models such as VGGNet-16 [50] and AlexNet [35] by 238 and 55 respectively.|
|||More recent approaches have sought to adapt CNN architectures so as to make them robust to common transformations (e.g.|
|||rotation) within the data, by modifying the filter banks of a CNN [61,10] or by enforcing sparsity while training [2].|
|||Meta-learning approaches attempt to decipher the optimum CNN architecture by searching over the space of a gigantic number of possible candidates.|
|||fixes this problem and employs a low-rank decomposition approach to the full CNN to construct more efficient representations but their techniques principal bottleneck is re-training, which we avoid [36].|
|||consider an efficient utilization of CNN filters by representing them as a linear combination of a bases set [48].|
|||Our method aims at handling the shortcomings in each of these individual themes of CNN compression.|
|||However, coresets have remained unexplored in the context of CNN compression, which constitutes a major novelty of our work.|
|||3 Method  We begin with a fully-trained CNN and compress it without retraining, first by pruning out unimportant filters, followed by extraction of efficient coreset representation of these filters.|
|||Some of the major advantages of our method include: (i) Lack of retraining, therefore a major reduction in processing time, (ii) Capacity of our algorithm to significantly compress both convolutional and fully connected layers, and (iii) Ability of the compressed CNN to generalize to newer tasks.|
|||on a wide array of CNN architectures, including the highly efficient SqueezeNet [29].|
|||We evaluate the performance of the compressed CNN model VGGNet-16 [50] on target domain adaptation datasets CUB-2011[57] and Stanford-Dogs [34], two popular datasets for fine-grained image classification.|
|||Empirical evaluation reveals that our algorithm outperforms all other competing methods at compressing a wide array of popular CNN architectures.|
||18 instances in total. (in eccv2018)|
|159|cvpr18-Geometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning| Our geometry guided CNN significantly outperforms other self-supervised approaches and is also complementary to the ImageNet pre-trained models on two publicly available action recognition datasets.|
|||Section 3 presents the framework of geometry guide CNN framework.|
|||Figure 2 illustrates the whole framework of the geometry guided CNN for video feature learning.|
|||However, it remains challenging to choose a correct geometry task and CNN architecture for using the geometry information contained in the 3D movies.|
|||To train a deep CNN model, we design it in the following way.|
|||In this paper, we use the FlowNet Simple architecture [9] as our base CNN network.|
|||To solve this problem, we borrow the cost function of the knowledge distillation network [14] and the learning without forgetting network [21]:  arg min  s,o,n  Lnew(Yn, Yn) + Lold(Yo, Yo),  (1)  where the parameters  = {s, o, n} are the weights of the CNN (coded by different colors in Figure 2).|
|||We first apply the learned CNN as an off-the-shelf feature extractor and report the results on the video dynamic scene recognition task.|
|||We compare against both the shallow feature representations [10, 6] and CNN based self-supervised feature leaning approaches [37, 24].|
|||We then feed them into the pre-trained CNN model.|
|||Discussions  Does CNN learn useful knowledge for video recognition from geometry?|
|||In this section, we first examine whether the geometry guided CNN learns useful knowledge for the seemingly distant task  semantic video recognition.|
|||We compare, in Table 3, the recognition results on UCF101 and HMDB51 obtained by fine-tuning the same CNN architecture but with different initialization of the weights.|
|||Fusion results with ImageNet pre-trained CNN model on UCF101 and HMDB51.|
|||It is clear that, from the table, when our geometry guided CNN is trained with the progressive strategy, it outperforms all the other alternative training schemes.|
|||As shown in Table 2, our geometry guided CNN can achieve significantly better results than the other self-supervised approaches.|
|||Visualization of class knowledge inside Geometry guided CNN model by using discriminative localization [39].|
|||We have seen that the unsupervised pretraining using geometry data gives significant boost over training from scratch or using other labeling-free signals, but it still has gap compared with the CNN model pretrained on ImageNet.|
||18 instances in total. (in cvpr2018)|
|160|Kong_HyperNet_Towards_Accurate_CVPR_2016_paper|By employing an even deeper CNN model (VGG16 [32]), it gives 30% relative improvement over the best previous result on PASCAL VOC 2012 [9].|
|||There are two major keys to the success of the R-CNN: (a) It replaces the hand-engineered features like HOG [6] or SIFT [25] with high level object representations obtained from CNN models.|
|||To make resolution of the Hyper Feature appropriate, we design different sampling strategies for multi-level CNN features.|
|||We demonstrate that proper fusion of coarse-tofine CNN features is more suitable for region proposal generation and detection.|
||| Our speeding up version can guarantee object proposal and detection accuracy almost in real-time, with 5 fps using very deep CNN models.|
|||Recently, some researchers are using CNN to generate region proposals.|
|||In DeepProposal [11], a coarse-to-fine cascade on multiple layers of CNN features is designed for generating region proposals.|
|||With the great success of the deep learning on large scale object recognition [21], several works based on CNN have been proposed[31][35][13].|
|||In MultiBox [8], region proposals are generated from a CNN model.|
|||This ConvNet includes a ROI pooling layer, a Conv layer and a  847  ......Conv 1Conv 3Conv 5...scoringbbox regscoringbbox regConv nFCFCDiscarded layersPre-trained CNN model5X5X425X5X425X5X423X3X4256-d3X3X634096-d21-dNetwork for Hyper Feature ExtractionROI Pooling13X13Hyper Feature mapsRegion Proposal GenerationObject DetectionDeconvMax poolingFully Connect (FC) layer, followed by two sibling output layers.|
|||Step 1: Pre-train a deep CNN model for initializing basic layers in Step 2 and Step 3.|
|||Our network uses in-net sampling to fuse multi-level CNN features.|
|||[12] propose a multi-region & semantic segmentation-aware CNN model for object detection.|
|||The Role of Hyper Feature  An important property of HyperNet is that it combines coarse-to-fine information across deep CNN models.|
|||This is the reason why most systems use the last CNN layer for region proposal generation or detection [11][14][13].|
|||This indicates that the combination of wider coarse-to-fine CNN features is more important.|
|||HyperNet provides an efficient combination framework for deep but semantic, intermediate but complementary, and shallow but high-resolution CNN features.|
|||Object detection via a multiregion & semantic segmentation-aware cnn model.|
||18 instances in total. (in cvpr2016)|
|161|cvpr18-DecideNet  Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation|(b) The density map on the same image from a CNN regression network [28].|
|||Figure 1(b) provides the crowd density map visualization on the same image in (a), outputted by a 5-layer CNN based regression network with similar structure employed in [28].|
|||Recent approaches seek the end-to-end crowd counting solution by CNN based object detectors [14, 25, 24, 7] and greatly improve the counting accuracy.|
|||An end-to-end CNN model adopted from AlexNet is constructed [36] recently for counting in extreme crowd scenes.|
|||Later, instead of direct regressing the count, the spatial information of crowds are taken into consideration by regressing the CNN feature maps as crowd density maps [41] .|
|||Observing that the densities and appearances of image patches are of large variations, a multicolumn CNN architecture is developed for density map regression [42].|
|||Three CNN columns with different receptive fields are explicitly constructed for counting crowds with robustness to density and appearance changes.|
|||Three CNN blocks are included in our framework: the RegNet, the DetNet and the QualityNet, parameterized by  = (det, reg, qua).|
|||The parameters for three CNN blocks could be jointly learned on the training set.|
|||Since the density estimation result could be viewed as a CNN feature map with only one channel, we add a conv5 layer with only one filter and a 1  1 filter size.|
|||i  To handle varying perspectives, crowd densities and appearances, existing density estimation methods [41, 42, 28, 17] consist of several CNN structures like the RegNet block.|
|||The motivation is intuitive and simple: sparse and non-crowded image patches are expected settings for present CNN based object detectors.|
|||We compare our DecideNet with both detection based approaches: SquareChn Detector [1], R-FCN [7], Faster R-CNN [25]; and regression based approaches: Count Forest [23], Exemplary Density [38], Boosting CNN [35], MoCNN [17], Weighted VLAD [30].|
|||Method  SquareChn Detector [1]  R-FCN [7]  Faster R-CNN [25] Count Forest [23]  Exemplary Density [38]  Boosting CNN [35]  MoCNN [17]  Weighted VLAD [30]  DecideNet  MAE MSE 439.1 20.55 5.46 6.02 5.91 6.60 2.40 4.40 2.74 1.82 N/A 2.01 13.40 2.75 9.12 2.41 1.52 1.90  Table 1.|
|||Even the most recent CNN based object detectors [7, 25] still have a large performance gap to the CNN based regression approaches [35, 17, 30].|
|||This is achieved without using the ensemble scheme employed by the MoCNN and Boosting CNN methods.|
|||People counting based on head detection combining adaboost and cnn in crowded surveillance environment.|
|||Learning to count with cnn boosting.|
||18 instances in total. (in cvpr2018)|
|162|Saihui_Hou_Progressive_Lifelong_Learning_ECCV_2018_paper|Instead of directly finetuning on new data, we first train an Expert CNN dedicated to the new task and then uses it as an advisor to guide the adaptation of target model, via knowledge distillation [10].|
|||While in our algorithm, the soft labels are obtained by fine-tuning an Expert CNN on new data ignoring the constraint of preserving the performance on old tasks.|
|||In each phase, the model evolves to a new task  Lifelong Learning via Progressive Distillation and Retrospection  5  Training Data  Retrospection  from old tasks  Lifelong  learning  ...  Adaptation by  distillation to  new tasks  ...  ...  ...  ...  ...  ...  ...  Expert CNN for Task 1  Expert CNN for Task 2  Fig.|
|||The input to our algorithm is an Original CNN that contains the feature extractor F and task-specific classifiers To for old tasks.|
|||It is worth noting that, since the data  the output of Original CNN for the old task denoted by bYo is first computed for the old task is not available [15], bYo is computed on new data.|
|||Then, LF  computed as follows in the training:  old is  LF  old(Xn, bYo) =   1  |Nn|  |Nn|X  i=1  KoX  k=1  y(ik) o   log(cid:16)p(ik)  o  (cid:17) ,  (2)  where Ko is the number of classes for the old task, y(i) are the modified versions of recorded soft labels by Original CNN and current network predictions for the old task:  and p(i)  o  o  y(ik) o  =  (cid:16)y(ik) o (cid:17)1/o Pj (cid:16)y(ij) o (cid:17)1/o  , p(ik)  o =  (cid:16)p(ik) o (cid:17)1/o Pj (cid:16)p(ij) o (cid:17)1/o  ,  (3)  where o is usually set to be greater than 1 which increases the weights of small values.|
|||First, for preserving the performance on old tasks, the target model adapts to a new task with the constraint of mimicking the output of Original CNN as much as possible.|
|||The responses of Expert CNN as well as Original CNN can be recorded before training, both of which do not bring additional GPU memory consumption.|
|||o , T   First, an Expert CNN is trained purely on the new task.|
|||The resulting Expert CNN is skilled at discriminating new data so that it theoretically provides an upper bound for the performance on the new task.|
|||Second, the responses of Expert CNN for new data denoted by bYn are computed  and recorded, which are used as the supervision for learning on the new task in the next step.|
|||Besides, Distillation is also beneficial for the performance preservation on old tasks since it is easier for Original CNN to match the output on new data to a soft distribution (i.e.soft labels by Expert CNN ) instead of a very peaked one (i.e.one-hot labels).|
|||Besides, y(i) and p(ik) are the modified versions of recorded responses by Original CNN and current network predictions for the old task, which are computed as in Eq.|
|||o  o  Note that, with Retrospection, the loss for the old task (LR  old) as well as the  responses of Original CNN (bYo) is computed on the reserved small subset of  data for the old task instead of new data.|
|||As for the adaptation to the new task without catastrophic forgetting, it is conducted with the loss to mimic the output of Original CNN and we follow the similar practice.|
|||As far as we can observe, one reason is due to the regularization caused by mimicking the output of Original CNN as suggested in [15].|
|||The computation cost introduced by Distillation compared to LwF [15] lies in two aspects: training Expert CNN on the new task and then recording its output, neither of which is cumbersome.|
|||Adaptation by Distillation from an intermediate Expert CNN can not only facilitate the learning on the new task but also is beneficial for preserving the performance on old tasks.|
||18 instances in total. (in eccv2018)|
|163|cvpr18-When Will You Do What  - Anticipating Temporal Occurrences of Activities|Both, a CNN and an RNN are trained to learn future video labels based on previously seen content.|
|||The CNN then predicts a matrix that encodes the length and the action labels of the anticipated activities.|
|||While the RNN and CNN perform similarly for a long time horizon of more than 40 seconds, the RNN performs better for shorter time horizons less than 20 seconds.|
|||The source code for both the RNN and CNN mod 5344  gated recurrent units and fully connected layers at the input and output.|
|||The matrix X that encodes the observed labels c  t 1 is forwarded through a CNN which consists of two convolutional layers and two fully connected layers.|
|||Since the CNN approach predicts all actions directly while the RNN uses a recursive strategy, we also have to prepare the training data slightly differently.|
|||For the CNN approach, we set the number of rows S of the matrix X to 128 for Breakfast.|
|||20%, or 30%, however, the RNN outperforms the CNN in most cases.|
|||10%, or 20%, while the CNN performs similarly or even sometimes better than the RNN for longer prediction.|
|||However, this slightly better performance of the CNN comes with the drawback of favouring long action segments over short ones.|
|||For example in the case of (c), the CNN tends to miss small action segments, whereas the RNN seems to be more reliable.|
|||Qualitative results for the future action prediction task for both, RNN and CNN with and without ground-truth observations.|
|||On the other hand, the inter-class dependencies on 50Salads are very strong, making it easier for both RNN and CNN to learn valid action sequences.|
|||Comparing different loss functions for the CNN model on Breakfast with ground-truth observations.|
|||Analysis of the CNN Model  Model Architecture Compared to commonly used CNN architectures such as VGG-16 or ResNet, our model is comparably small.|
|||Results of the CNN model using the cross-entropy loss vs. the squared-error loss.|
|||Results of the CNN model with and without post-processing.|
|||Table 5 shows the results of the CNN model when applied on features directly compared to the two-step approach.|
||18 instances in total. (in cvpr2018)|
|164|Yoo_AttentionNet_Aggregating_Weak_ICCV_2015_paper|Introduction  After the recent advance [16] of deep convolutional neural network (CNN) [7], CNN based object classification methods in computer vision has reached human-level performance on ILSVRC classification [18]: top-5 error of 4.94% [15], 6.67% [22], 6.8% [21], and 5.1% for human [18].|
|||We thought that there must be a room for modification and improvement for the use of CNN as a regressor.|
|||In recent years, CNN-based approach leads to the successful development of object detection with drastic advances of deep CNN [7].|
|||Then each proposal is represented by mid-level CNN activations (e.g.|
|||[24] trains a CNN which maps an image to a rectangular mask of an object.|
|||These methods are free from object proposals, but it is a still debatable to leave all to a CNN trained with a mean-square cost to produce an exact bounding box.|
|||Compared to the previous CNN methods, our method does not rely on object proposals since we actively explorer objects by iterative classifications.|
|||Also, the proposed network has a unified classification architecture, which is verified from many CNN applications and also does not need to tune up individual components.|
|||This is also similar to the CNN suggested by Zeiler and Fergus [28].|
|||[1], strong mid-level activations in a CNN come from object parts that is distinctive to other object classes.|
|||Because Region-CNN based detection relies on each region score coming from the CNN activations, it is prone to focus on discriminative object part (e.g.|
|||As an object proposal based method, we represent object proposals by activations from a finetuned CNN and score them by a SVM.|
|||Our detection framework requires two predictions for TL and BR, but we prefer a CNN for classification to be trained with soft-max loss.|
|||Note, the CNN here and that in Fig.|
|||Following a technique successfully used in [25, 27], we also utilize the property that a CNN does not require fixed size input only because a fully connected layer is a convolution layer composed of filters of 11 size.|
|||For example, if an image 2-times larger (321321) than a regular CNN input (227227) is fed to AttentionNet, a spatial activation map of 335 size is produced from an output layer as shown in Fig.|
|||Except R-CNN [12] equipped with very large CNN [21], our method yields the best scores in both datasets.|
||17 instances in total. (in iccv2015)|
|165|cvpr18-3D Pose Estimation and 3D Model Retrieval for Objects in the Wild|While this allows them to train a single CNN purely on synthetic data, there are two main disadvantages:  First, there is a significant domain gap between real and synthetic RGB images: Real images are affected by complex lighting, uncontrolled degradation and natural backgrounds.|
|||Therefore, using a single CNN for feature extraction from both domains is limited in performance, and even domain adaption [13] does not fully account for the different characteristics of real and synthetic images.|
|||More specifically, we use a CNN to predict the 2D projections of virtual 3D control points from which we recover the pose using a PnP algorithm.|
|||[2] uses a CNN pre-trained on ImageNet [21] as a feature extractor and matches features of real images against those of 3D models rendered under multiple viewpoints to predict both shape and viewpoint.|
|||[28] finetunes a pre-trained CNN using lifted structure embedding [16] and averages the distance of a real image to renderings from multiple viewpoints to be more invariant to object pose.|
|||[23] presents a CNN architecture that combines information of renderings from multiple viewpoints into a single object pose invariant descriptor.|
|||[11] explicitly constructs an embedding space using a 3D similarity measure evaluated on clean 3D models and trains a CNN to map renderings with arbitrary backgrounds to the corresponding points in the embedding space.|
|||More precisely, we train a CNN to predict the 2D image locations of the projections of the objects eight 3D bounding box corners.|
|||For this purpose, we introduce a CNN architecture which jointly predicts the 2D image locations of the projections of the eight 3D bounding box corners (16 values) as well as the 3D bounding box dimensions (3 values).|
|||Let Mi be the i-th 3D bounding box corner and ProjR,t(Mi) its projection using the ground truth rotation R and translation t, then the projection loss  Lproj = E" 8Xi=1  kProjR,t(Mi)  emikHuber#  (2)  is the expected value of the distances between the ground truth projections ProjR,t(Mi) and the predicted locations of  these projections emi computed by the CNN for the training  set.|
|||The dimension loss  Ldim = E Xi=x,y,z  kdi  edikHuber  (3)  is the expected value of the distances between the ground  truth 3D dimensions di and the 3D dimensions edi predicted  by the CNN for the training set.|
|||(1) adds weight decay for all CNN parameters.|
|||this mapping using a separate CNN for each domain.|
|||For real RGB images, we extract image descriptors from the hidden feature activations of the penultimate layer of our pose estimation CNN (see Fig.|
|||For the synthetic depth images, we extract image descriptors using a CNN with the same architecture as our pose estimation CNN, except for the output layer (see Fig.|
|||(4) adds weight decay for all CNN parameters.|
|||Joint Embeddings of Shapes and Images via CNN Image Purification.|
||17 instances in total. (in cvpr2018)|
|166|Growing a Brain_ Fine-Tuning by Increasing Model Capacity|By making an analogy to developmental learning, we demonstrate that growing a CNN with additional units, either by widening existing layers or deepening the overall network, significantly outperforms classic fine-tuning approaches.|
|||Fortunately, when trained on a large enough, diverse base set of data (e.g., ImageNet), CNN features appear to transfer across a broad range of tasks [32, 4, 56].|
|||However, an open question is how to best adapt a pre-trained CNN for novel categories/tasks.|
|||Approach Overview  Let us consider a CNN architecture pre-trained on a source domain with abundant data, e.g., the vanilla AlexNet pre-trained on ImageNet (ILSVRC) with 1,000 categories [20, 33].|
|||We note in Figure 1 that the CNN is composed of a feature representation module F (e.g., the five convolutional layers and two fully connected layers for AlexNet) and a classifier module C (e.g., the final fullyconnected layer with 1,000 units and the 1,000-way softmax for ImageNet classification).|
|||Transferring this CNN to a novel task with limited training data (e.g., scene classification of 397 categories from SUN-397 [52]) is typically done through fine-tuning [3, 1, 15].|
|||T = F  In classic fine-tuning, the target CNN is instantiated and initialized as follows: (1) the representation module FT is copied from FS of the source CNN with the parameters F S ; and (2) a new classifier model CT (e.g., a new final fully-connected layer with 397 units and the 397-way softmax for SUN-397 classification) is introduced with the parameters C T randomly initialized.|
|||To reconcile the learning pace of the new and preexisting units, we introduce an additional normalization and adaptive scaling scheme in width augmented networks, which is inspired by the recent work on combining multiscale pre-trained CNN features from different layers [23].|
|||Experimental Evaluation  Network  Type  Method  Acc (%)  New F C7New F C6New All  In this section, we explore the use of our developmental networks for transferring a pre-trained CNN to a number of supervised learning tasks with insufficient data, including scene classification, fine-grained recognition, and action recognition.|
|||Ours significantly outperform the vanilla fine-tuned CNN in all these scenarios.|
|||Both our DA-CNN and WA-CNN significantly outperform the vanilla fine-tuned CNN in all the different fine-tuning scenarios.|
|||We roughly divide the baselines into four types: (1) ImageNet CNNs, which post-process the off-the-shelf CNN or fine-tune it in a standard manner; (2) task customized CNNs, which modify a standard CNN for a particular target task (e.g., for MIT-67, Places-CNN trains a customized CNN on the Places dataset with 400 scene categories [59]); (3) data augmented CNNs, which concatenate features from the ImageNet AlexNet and an additional CNN trained on 100 million Flickr images in a weakly supervised manner [18]; (4) multi-task CNNs, which (approximately) train a CNN jointly from both the source and target tasks.|
|||These datasets are widely used for evaluating the CNN transferability [3], and we consider their diversity and coverage of novel categories.|
|||Importantly, our approach can be also combined with other CNN variations (e.g., VGG-CNN [37], multi-scale CNN [12, 53]) for further improvement.|
|||In contrast to task customized CNNs that are only  2477  Dataset CNN WA-CNN-scratch WA-CNN-grow (Ours)  ImageNet 56.9  57.6  57.8  Table 6: Performance comparisons of classification accuracy (%) on the source dataset between a standard AlexNet (CNN), a wide AlexNet trained from scratch (WA-CNNscratch), and a wide network trained progressively by finetuning on the source task itself (WA-CNN-grow).|
|||Our conclusions support a developmental view of CNN optimization, in which model capacity is progressively grown throughout a lifelong learning process when learning from continuously evolving data streams and tasks.|
|||arXiv preprint  Good practice in CNN feature transfer.|
||17 instances in total. (in cvpr2017)|
|167|cvpr18-Exploit the Unknown Gradually  One-Shot Video-Based Person Re-Identification by Stepwise Learning|We first initialize a CNN model using one labeled tracklet for each identity.|
|||Then we update the CNN model by the following two steps iteratively: 1. sample a few candidates with most reliable pseudo labels from unlabeled tracklets; 2. update the CNN model according to the selected data.|
|||Initially, a CNN model is trained on the one-shot labeled tracklet.|
|||EUG then iteratively updates the CNN by two steps, the label estimation step and the model update step.|
|||In the second step, EUG re-trains the CNN model on both the labeled data and the sampled pseudo-labeled subset.|
|||The typical architecture is to combine CNN and RNN to learn a video representation or the similarity score.|
|||To be specific, for our progressive strategy EUG, we adopt an end-to-end CNN model with temporal average pooling (ETAP-Net) as the feature embedding function .|
|||The CNN model is initially trained on the labeled one-shot data.|
|||For each iteration, we (1) select the unlabeled samples with reliable pseudo labels according to the distance in feature space and (2) update the CNN model by the labeled data and the selected candidates.|
|||For a tracklet, each frame feature is first extracted by the CNN model and then temporally averaged as the tracklet feature.|
|||(2) learning a CNN model on a not-yet-reliable training set may not improve the re-ID performance.|
|||5180  Algorithm 1 Exploit the Unknown Gradually Input: Labeled data L, unlabeled data U , enlarging factor  p 2 (0, 1), initialized CNN model 0.|
|||Output: The best CNN model .|
|||1: Initialize the selected pseudo-labeled data S0   ;, sampling size m1   p  nu, iteration step t   0, best validation performance V    0  2: while mt+1  |U| do 3:  t   t + 1 Update training set: Dt   L [ St1 Train the CNN model (t, wt) based on Dt.|
|||As discussed in Section 3.2, we take ETAPNet as our basic CNN model for training on video-based re-ID.|
|||In label estimation and evaluation steps, all the frames are processed by the CNN model to get the representations for each tracklet, which are further l2 normalized and used to calculate the Euclidean distance.|
|||This is because that the performance of the trained CNN model highly depends on the reliability of the training set.|
||17 instances in total. (in cvpr2018)|
|168|Learning a Deep Embedding Model for Zero-Shot Learning|Most state-of-the-arts ZSL models [11, 13, 2, 3, 37, 47, 22] use deep CNN features for visual feature representation; the features are extracted with pretrained CNN models.|
|||For example, if sentence descriptions are used as the input to a neural language model such as recurrent neural networks (RNNs) for computing a semantic space, both the neural language  2021  model and the CNN visual feature representation learning model can be jointly optimised in an end-to-end fashion.|
|||In this work, we propose a novel deep neural network based embedding model for ZSL which differs from existing models in that: (1) To alleviate the hubness problem, we use the output visual feature space of a CNN subnet as the embedding space.|
|||One branch is the visual encoding branch, which consists of a CNN subnet that takes an image Ii as input and outputs a D-dimensional feature vector (Ii)  RD1.|
|||In particular, if we use the matrix form and write the outputs of the semantic representation unit as A and the outputs of the CNN visual feature encoder as B, and ignore the ReLU unit for now, our training objective becomes  L(W) = ||B  WA||2  F + ||W||2 F ,  (6)  which is basically ridge regression.|
|||Model setting and training Unless otherwise specified, We use the Inception-V2 [44, 19] as the CNN subnet of our model in all our experiments, the top pooling units are used for visual feature space with dimension D = 1, 024.|
|||The CNN subnet is pre-trained on ILSVRC 2012 1K classification without fine-tuning, same as the recent deep ZSL works [24, 34].|
|||The second group are all neural network based with a CNN subnet.|
|||For fair comparison, we implement the models in [10, 43, 46, 24] on AwA and CUB with Inception-V2 as the CNN subnet as in our model and [34].|
|||For comparing with the other 6 methods, we follow their setting and pretrain our CNN subnet from scratch with Alexnet [21] architecture using the 800 training classes for fair comparison.|
|||For neural network based methods, all use Inception-V2 (GoogLeNet with batch normalisation) [44, 19] as the CNN subnet, indicated as NG.|
|||Some of them [30, 18] use existing CNN model  Table 3.|
|||Further analysis  Importance of embedding space selection We argued that the key for an effective deep embedding model is the use of the CNN output visual feature space rather than the semantic space as the embedding space.|
|||1 by moving the two FC layers from the semantic embedding branch to the CNN feature extraction branch so that the embedding space now becomes the semantic space (attributes are used).|
|||We also hypothesised that using the CNN visual feature space as the embedding layer would lead to less hub 2027  chimpanzee giant panda leopard persian cat pig hippopotamus humpback whale raccoon rat seal  (a) S  V  (b) V  S  Figure 4.|
|||To answer this, we consider a very simple model based on linear ridge regression which maps from the CNN feature space to the attribute semantic space or vice versa.|
|||The model differs from existing ZSL model in that it uses the CNN output feature space as the embedding space.|
||17 instances in total. (in cvpr2017)|
|169|Jourabloo_Large-Pose_Face_Alignment_CVPR_2016_paper|In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM.|
|||While prior work on CNN for face alignment estimate no more than 6 2D landmarks per image, our cascaded CNN can estimate a substantially larger number (34) of 2D and 3D landmarks.|
|||We extend [9] in a number of aspects, including fitting a dense 3D morphable model, employing the powerful CNN as the regressor, using 3D-enabled features, and estimating cheek landmarks.|
|||TCDCN [34] uses one-stage CNN to estimates positions  4189  3D Morphable Model  Cascade of CNN Regressors  Input Face Images  Data Augmentation  Update  Projection  Matrix  Update 3D Shape Parameter  Update  Projection  Matrix  Figure 2.|
|||Architecture of CNN used in each stage of the proposed method.|
|||Cascaded CNN Coupled(cid:173)Regressor  Given a set of Nd training face images and their augmented (a.k.a.|
|||Given the success of CNN in vision tasks such as pose estimation [17], face detection [12], and face alignment [34], we decide to marry the CNN with the cascade regressor framework by learning a series of CNN-based regressors to alternate the estimation of m and p. To the best of our knowledge, this is the first time CNN is used in 3D face alignment, with the estimation of over 10 landmarks.|
|||Thus, at the stage k of the cascaded CNN, we can learn a CNN to estimate the desired  4191  update of the projection parameter,  Nd  k  m = arg min  k m  i=1  ||mk  i  CNNk  m(Ii, Ui, vk1  i  ; k  m)||2,  (6) where the true projection update is the difference between the current projection parameter and the ground truth, i.e., mk , Ui is current estimated 2D landmarks, computed via Eqn.|
|||4 based on mk1 , and vk1  i is estimated landmark visibility at stage k  1.  i = mi  mk1  and dk1  i  i  i  Similarly another CNN regressor can be learned to esti mate the updates of the shape parameter,  k  p = arg min  k p  Nd  i=1  ||pk  i  CNNk  p(Ii, Ui, vk  i ; k  p)||2.|
|||As a result, the appearance features around these cheek landmarks are part of the input to CNN as well.|
|||We used rectified linear unit (ReLU) [6] as the activation function which enables CNN to achieve the best performance without unsupervised pre-training.|
|||only possible because of the 3D model, can be extracted and contribute to the cascaded CNN learning.|
|||i  During the CNN learning, for the nth landmark of ith image, we project the four neighborhood vertexes onto the ith image and obtain four neighborhood points, U(n) i = sRA(:, d(n) p ) + t, based on the current estimated projection parameter m. Across all 2D face images, U(n) correspond to the same face vertexes anatomically.|
|||Parameter setting For the proposed method, the learning rate of CNN is constant at 0.0001 during training.|
|||According to our empirical evaluation, six stages of CNN are sufficient for convergence of fitting process.|
|||For the feature +Cheek Landmarks, additional up to four 19  19 patches of the contour landmarks, which are invisible for nonfrontal faces, will be replaced with patches of the cheek landmarks, and used in the input layer of CNN learning.|
|||Conclusions  We proposed a method to fit a 3D dense shape to a face image with large poses by combining cascade CNN regressors and the 3D Morphable Model (3DMM).|
||17 instances in total. (in cvpr2016)|
|170|Cai_Higher-Order_Integration_of_ICCV_2017_paper|Therefore, leveraging local discriminative patterns in CNN is crucial to obtain more powerful representation, and recently has been intensively studied for FGVC.|
|||However, designing an appropriate CNN architechture that can be plugged with non-linear local kernels in an end-to-end manner is non-trivial.|
|||[24] propose a bilinear CNN (B-CNN) as codebook-free coding that allows end-to-end training for FGVC.|
|||The very recent work in [1] builds a weakly place recognition system by introducing a generalized VLAD layer that can be trained with off-the-shelf CNN models.|
|||Kernelized convolutional activations  Most part-based CNN methods for FGVC consist of two components: (i) feature extraction for semantic parts on the last convolutional layer, and (ii) spatial configuration modeling for those parts to produce discriminative image representation.|
|||Our attention is to model the higher-order relationships for discriminative representation of local patch and design suitable local mapping function  which can be stacked upon CNN for end-to-end training.|
|||Tensor learning for polynomial kernels  Before deriving the end-to-end CNN architecture for (4), we first reformulate  learning the parameters in Eqn.|
|||(8), the derivatives for x and each degree-r convolutional filter ur,d in back propagation process can be achieved by:  s  l x  l  ur,d  s  =  =  l yr  Dr X  r  X  (Y  d=1  s=1  t6=s  hur,d  t  , xi)ur,d  s  l yr (Y  t6=s  hur,d  t  , xi)x  (10)  (11)  where yr = g(Z r) = g({zr}) is the pooled feature representation for degree-r polynomial module, l is the loss associated with yr. On this basis, we can embrace those polynomial modules with the trainable CNN architectures and are able to model the higher-order part statistics of any degree.|
|||2 presents our CNN architecture for integrating multiple convolutional layers.|
|||The experimental comparisons with state-of-the-art methods indicate that effective feature integration from CNN is a promising solution for FGVC in contrast with the requirements of massive ex 515  ternal data or detailed part annotation.|
|||However, STN [17] uses a better baseline CNN (Inception [37]) than our VGG-16 network and PDFS [48] cannot be trained  517  by end-to-end manner.|
|||BoostCNN uses BCNN as the base CNN and adopts an ensemble learning method to incorporate boosting weights.|
|||Thus, a fair comparison is to use ours as the base CNN in BoostCNN.|
|||In this paper, by considering the weak parts in CNN itself, we present a novel higher-order integration framework of hierarchical convolutional layers to derive a rich representation for FGVC.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Bilinear cnn models for fine-grained visual recognition.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||17 instances in total. (in iccv2017)|
|171|Kang_Convolutional_Neural_Networks_2014_CVPR_paper|With a deep structure, the CNN can effectively learn complicated mappings while requiring minimal domain knowledge.|
|||To the best of our knowledge, CNN has not been applied to general-purpose NR-IQA.|
|||The primary reason is that the original CNN is not designed for capturing image quality features.|
|||The difference between NR-IQA and object recognition makes the application of CNN nonintuitive.|
|||Our approach integrates feature learning and regression into the general CNN framework.|
|||Second, in the CNN framework, training the network as a whole using a simple method like backpropagation enables the possibility of conveniently incorporating recent techniques designed to improve learning such as dropout [5] and rectified linear unit [7].|
|||CNN for NR-IQA  The proposed framework of using CNN for image quality estimation is as follows.|
|||We use a CNN to estimate the quality score for each patch and average the patch scores to obtain a quality estimation for the image.|
|||It is worth noting that when using a CNN for object recognition, a global contrast normalization is usually applied to the entire image.|
|||[7] demonstrated in a deep CNN that ReLUs enable the network to train several times faster compared to using tanh units.|
|||For the overall evaluation, our CNN outperformed all previous state of the art NR-IQA methods and approached the state of the art FR-IQA method FSIM.|
|||It is not surprising that the kernels learned by CNN tend to be noisy patterns instead of presenting strong structure related to certain distortions as shown in CORNIA[20].|
|||Effects of Parameters  Several parameters are involved in the CNN design.|
|||We can see that our CNN outperforms previous state of the art methods.|
|||Local Quality Estimation  Our CNN measures the quality on small image patches, so it can be used to detect low/high quality local regions as well as giving a global score for the entire image.|
|||Computational Cost  Our CNN is implemented using the Python library Theano [2].|
|||Conclusion  We have developed a CNN for no-reference image quality assessment.|
||17 instances in total. (in cvpr2014)|
|172|Harmonic Networks_ Deep Translation and Rotation Equivariance|We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation.|
|||However, until now, if one rotates the CNN input, then the feature vectors do not necessarily rotate in a meaningful or easy to predict manner.|
|||In essence, translational equivariance has been baked into the architecture of existing CNN models.|
|||weights in a CNN reveals that many of them are rotated, scaled, and translated copies of one another [34].|
|||[6] feed in multiple rotated copies of the CNN input and fuse the output predictions.|
|||Their CNN is similar to [3] in terms of what is being computed, but rotating feature maps instead of filters.|
|||H-Nets hard-bake 360-rotation equivariance into their feature representation, by constraining the convolutional filters of a CNN to be from the family of circular harmonics.|
|||Below, we outline the formal definition of equivariance (Section 3.1), how the circular harmonics exhibit rotational equivariance (Section 3.2) and some properties of the circular harmonics, which we must heed for successful integration into the CNN framework (Section 3.2).|
|||2) Point-wise nonlinearities h: C C, acting solely on the magnitudes maintain rotational equivariance, so we can interleave cross-correlations with typical CNN nonlinearities adapted to the complex domain.|
|||3) The summation of two responses of the same order m remains of order m. Thus to construct a CNN where the output is M-equivariant to the input rotation, we require that the sum of rotation orders along any path equals M, so  N  Xi=1  mi =M.|
|||Usually in our experiments, we use streams of orders 0 and 1, which we found to work well and is justified by the fact that CNN filters tend to contain very little high frequency information [12].|
|||We replaced regular CNN filters with radially reweighted and phase shifted circular harmonics.|
|||We can then use a regular CNN architecture without any problems.|
|||We compare against a collection of previous state-of-the-art papers and [3], who build a deep CNN with filter copies at 90-rotations.|
|||This model actually has 33k parameters, which is about 50% larger than the standard CNN and [3], which have 22k.|
|||Interestingly, it does not overfit on such a small dataset and it still outperforms the standard CNN trained with rotation augmentations, which we do not use.|
|||For example, if we want to build an H-Net with similar computational cost to a regular CNN with 64 channels per layer, then if we use 2 rotation orders m{0,1}, then the number of H-Net channels is 64/(22)=16.|
||17 instances in total. (in cvpr2017)|
|173|Hou_Tube_Convolutional_Neural_ICCV_2017_paper|Also, most of these methods employ two-stream CNN framework to handle spatial and temporal feature separately.|
|||Moreover, in order to capture both spatial and temporal information of an action, twostream networks (a spatial CNN and a motion CNN) are used.|
|||[31] use recurrent neural network employing the CNN feature.|
|||Since these approaches only use frame based CNN features, the temporal information is neglected.|
|||[22] propose the two-stream CNN approach for action recognition.|
|||Besides a classic CNN which takes images as an input, it has a separate network for optical flow.|
|||fuse the trajectories and CNN features.|
|||3D CNN is a logical solution to this issue.|
|||[9] propose a 3D CNN based human detector and head tracker to segment human subjects in videos.|
|||[27] leverage 3D CNN for large scale action recognition problem.|
|||[25] propose a factorization of 3D CNN and exploit multiple ways to decompose convolutional kernels.|
|||However, to the best of our knowledge, we are the first ones to exploit 3D CNN for action detection.|
|||propose Region CNN (R-CNN) [4].|
|||Then the candidate regions are warped to a fixed size and fed into ConvNet to extract CNN features.|
|||To better capture the spatio-temporal information in video, we exploit 3D CNN for action proposal generation and action recognition.|
|||One advantage of 3D CNN over 2D CNN is that it captures motion information by applying convolution in both time and space.|
|||Furthermore, our ToI pooling can be combined with other deep learning based pipelines, such as two-stream CNN [22].|
||17 instances in total. (in iccv2017)|
|174|Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper|Different from traditional CNN that adopts fixed convolution kernels, samples in c-CNN are processed with dynamically activated sets of kernels.|
|||Decision tree inherently embeds the concept of conditional computation via hierarchical partitions, and thus is incorporated into CNN to substantiate the proposed framework.|
|||Traditional CNN activates all the kernels for all the training samples.|
|||In this paper, the conditional computation of decision tree is incorporated into CNN as a specific instance of c-CNN.|
|||Each tree node computes the intermediate representation with CNN and the partition of samples in the projected latent space.|
|||Compared with the conventional CNN of the same structure as one CNB in Figure 2, the proposed network appears to contain more parameters.|
|||For fair comparisons, we increase the width of the single-model CNN so that it can have the same number of parameters as ours C the baseline CNN has 20 filters in the 1st layer, 40 in the 2nd and 160 in the 3rd.|
|||Joint Learning of MPT and CNN Branch  Different from prior works that learn features nodebynode in a decision tree [5, 1], the feature representation and the split function of all nodes are jointly learnt with regard to a unified objective as  L =Xn  J (xn, yn) + Xi Xj  L(i,j),  (8)  where the first term represents the softmax loss for the nclass classification problem, and the second term is the node-wise loss defined in Eqn.|
|||Multi(cid:173)View Face Identification  L  ,  X  (i,j) n  W  (i,j) n  L  and L b(i,j)  n  derived similarly as standard CNN with the chain rule.|
|||CNN 40 is a single CNN network with the same configuration of convolutional layers as FIP 40.|
|||Moreover, c-CNN outperforms both CNN 40 and FIP 40 while maintaining a much lower computation cost.|
|||Cluster CNN firstly clusters the samples based on LBP features and trains a separate CNN for each cluster.|
|||Tree CNN follows the c-CNN structure, but optimizes the branching parameters w.r.t.|
|||the node-wise loss first, and then learns the parameters of CNN while fixing the branching parameters.|
|||We also include the single CNN based methods with the same network structure as one neural branch, i.e., CNN 20.|
|||As shown in the table, c-CNN demonstrates consistent improvements over CNN 20 and CNN 40, up to 3.5%.|
|||The significant improvements over CNN 20 can better demonstrate the superiority of the proposed method, since the two methods are of comparable computation cost.|
||17 instances in total. (in iccv2015)|
|175|Zhu_Learning_a_Discriminative_ICCV_2015_paper|Furthermore, we apply our learned model to compute optimal parameters of a compositing method, to maximize the visual realism score predicted by our CNN model.|
|||Given a low-dimensional color mapping function, we directly optimize the visual realism score predicted by our CNN model.|
|||A high-capacity convolutional neural network (CNN) clas 3944  Figure 2: Example composite images for CNN training: (a) image composites generated by fully supervised foreground and background masks, (b) image composites generated by a hybrid ground truth mask and object proposal, (c) image composites generated by a fully unsupervised proposal system.|
|||Automatically Generating Composites  To generate training data for the CNN model, we use the LabelMe image dataset [26] because it contains many categories along with detailed annotation for object segmentation.|
|||In this case, the CNN model we trained mainly picked up artifacts of high-frequency edges that appear in image composites and performed sigIn our experiments, we used  11, 000 nificantly worse.|
|||Notice that some composite images are artifact-free and appear quite realistic, which forces the CNN model to pick up not only the artifacts of the segmentation and blending algorithms, but also the compatibility between the visual content of the inserted object and its surrounding scene.|
|||Improving Image Composites  Let f (I; ) be our trained CNN classifier model predicting the visual realism of an image I.|
|||Notice that  f (Ig ,) into E can be computed through backpropagation of CNN model from the loss layer to the image layer while the other parts have a simple close form of gradient.|
|||Although our color adjustment model is relatively simple, our learned CNN model provides guidance towards better color compatible composite.|
|||We optimize the CNN model using SGD.|
|||Experiments  We first evaluate our trained CNN model in terms of clas sifying realistic photos vs. unrealistic ones.|
|||We also compare our SVM model with other SVM models trained on convolutional activation features (f c7 layer) extracted from different CNN models including AlexNet [14] (0.75), PlaceCNN [34] (0.73) and original VGG Net [28] (0.76).|
|||Dataset Generation Procedure The CNN we reported so far was trained on the image composites generated by the FullySupervised procedure.|
|||To better model indoor scenes, we train our CNN model on 21, 000 natural images (both indoor and outdoor) that contain  42, 000 object instances from more than 200 categories of objects in the LabelMe dataset.|
|||To avoid this unsatisfactory property, we add newly generated color adjustment results as the negative data, and retrain the CNN with newly added data, similar to hard negative mining in object detection literature [7].|
|||Then we use this new CNN model to recolor the object again.|
|||We repeat this process three times, and obtain three CNN models named as CN N Iter1, CN N Iter2 and CN N Iter3.|
||17 instances in total. (in iccv2015)|
|176|cvpr18-Video Representation Learning Using Discriminative Pooling|Instead, the trend has been on converting the video data to short temporal segments consisting of one to a few frames, on which the existing imagebased CNN models are trained.|
|||We also provide a joint objective that learns both the SVMP descriptors and the action classifiers, which generalizes the applicability of our method to both CNN features and hand-crafted ones.|
|||In a pure CNN setup, our pooling method can be implemented alongside the rest of the CNN layers and trained in an endto-end manner.|
|||Although the architecture of these networks are different, the core idea is to embed the video data into a semantic feature space, and then recognize the actions either by aggregating the individual features per frame using some statistic (such as max or average) or directly training a CNN based end-to-end classifier [11].|
|||Further our scheme is featureagnostic, i.e., they may be from a CNN or are hand-crafted.|
|||While, there are efficient approximate solutions for this problem (such as [30]), the solution must be scalable to large number of both high-dimensional features generated by a CNN and  Each step of Algorithm (1) solves a standard SVM objective.|
|||End-to-End CNN Learning  In this section, we address the problem of training a CNN end-to-end with SVM pooling as an intermediate layer  the main challenge is to derive the gradients of SVMP for efficient backpropagation.|
|||Assume a CNN f taking a sequence S as input.|
|||Let fL denote the L-th CNN layer and let YL denote the feature maps generated by this layer for all frames in S. We assume these features go into an SVMP layer and produces as output a descriptor w (using a precomputed set of negative feature maps), which is then passed to subsequent CNN layers for classification.|
|||As is by now clear, with regard to a CNN learning setup, we are dealing with a bilevel optimization problem here  that is, optimizing for the CNN parameters via stochastic gradient descent in the outer optimization, which requires the gradient of an argmin inner optimization with respect to its optimum, i.e., we need to compute the gradient of g(w) with respect to the data z.|
|||For this dataset, we analyze different combinations of features on multiple CNN frameworks.|
|||We use the temporal CNN proposed in [20] to generate features, but uses SVMP instead of their global average pooling.|
|||Comparisons using various features on HMDB-51 split-1  Feature/ model pool5 (vgg-16) fc6 (vgg-16) fc7 (vgg-16) fc8 (vgg-16) softmax (vgg-16) pool5 (ResNet-152) fc1000 (ResNet-152)  Accuracy  independently  57.9% 63.3% 56.1% 52.4% 41.0% 69.5% 61.1%  Accuracy when combined with:  63.8% (fc6)   57.1% (fc6) 58.6% (fc6) 46.2% (fc6)   68.8% (pool5)  the SVMP descriptor, implying that the CNN features are already equipped with discriminative properties for action recognition.|
|||Experiments on HMDB(cid:173)51  Following the recent trends, for this experiment, we use a two-stream CNN model in popular architectures, the VGG16 and the ResNet-152 [13, 42].|
|||SVMP on Different CNN Features: We generate SVMP descriptors from different intermediate layers of the CNN models and compare their performance.|
|||This scheme applies a temporal CNN with residual connections on the 3D skeleton data.|
|||We also extended the framework to deal with nonlinear decision boundaries and endto-end CNN training.|
||17 instances in total. (in cvpr2018)|
|177|Gupta_Synthetic_Data_for_CVPR_2016_paper|YOLO is part of a broad line of work on using CNN features for object category detection dating back to Girshick et al.s Region-CNN (R-CNN) framework [12] combination of region proposals and CNN features.|
|||The R-CNN framework has three broad stages  (1) generating object proposals, (2) extracting CNN feature maps for each proposal, and (3) filtering the proposals through class specific SVMs.|
|||They obtain 100 speed-up over R-CNN by computing the CNN features once and pooling them locally for each proposal; they also streamline the last two stages of R-CNN into a single multi-task learning problem.|
|||They have been widely used to learn large CNN models  Wang et al.|
|||[8] train inverted CNN models to render images of chairs, while Yildirim et al.|
|||[46] use deep CNN features trained on synthetic face renderings to regress pose parameters from face images.|
|||Such datasets are not only insufficient to train large CNN models, but also inadequate to represent the space of possible text variations in natural scenes  fonts, colours, sizes, positions.|
|||After acquiring suitable text and image samples (section 2.1), the image is segmented into contiguous regions based on local colour and texture cues [2], and a dense pixel-wise depth map is obtained using the CNN of [30] (section 2.2).|
|||The normals are estimated automatically by first predicting a dense depth map using the CNN of [30] for the regions segmented above, and then fitting a planar facet to it using RANSAC [10].|
|||An alternative to using a CNN to estimate depth, which is an error prone process, is to use a dataset of RGBD images.|
|||A Fast Text Detection Network  In this section we introduce our CNN architecture for text detection in natural scenes.|
|||The most common approach for CNN-based detection is to propose a number of image regions R that may contain the target object (text in our case), crop the image, and use a CNN c = (cropR(x))  {0, 1} to score them as correct or not.|
|||This approach, which has been popularised by R-CNN [12], works well but is slow as it entails evaluating the CNN thousands of times per image.|
|||We use a squared loss term for each of the H   W  7 outputs of the CNN as in YOLO [36].|
|||For recognition we use the output of the intermediary recognition stage of the pipeline based on the lexiconencoding CNN of Jaderberg et al.|
|||Conclusion  We have developed a new CNN architecture for generating text proposals in images.|
|||It would not have been possible to train this architecture on the available annotated datasets, as they contain far too few samples, but we have shown that training images of sufficient verisimilitude can be generated synthetically, and that the CNN trained only on these images exceeds the state-of-the-art performance for both detection and end-to-end text spotting on real images.|
||17 instances in total. (in cvpr2016)|
|178|Learning Shape Abstractions by Assembling Volumetric Primitives|Towards this, we learn a CNN h parametrized by  which outputs a primitive based representation.|
|||The task of learning this CNN is an unsupervised one  we do not have any annotations for the primitive parameters that best describe the target objects.|
|||Given the input volume corresponding to an object O, we use a CNN to predict primitive shape and transformation parameters {(zm, qm, tm)} for each part (Section 3.1).|
|||Given a discretized representation of the target shape as input, we use a CNN to predict a primitive representation (described in Section 3.1).|
|||Loss Function for Assembled Shape  We want  to define a differentiable loss function L({(zm, qm, tm)}, O) the CNN prediction {(zm, qm, tm)} and the target object O.|
|||We want to penalize the CNN prediction if the target object  Pm.|
|||We want to penalize the CNN prediction if the predicted  Pm is not completely inside the target object O.|
|||We first discuss the modified representation predicted by the CNN and discuss how the loss function can incorporate this.|
|||The prediction of the CNN in this scenario is as below.|
|||m, ze  {(zs  m, qm, tm, pm)|m = 1    M } = h(I) m ze  m  Bern(pm); zm  (zs  m, ze  (8)  (9)  m)  Note that the CNN predicts pm  the parameter of the Bernoulli distribution from which the part existence variable ze m is sampled.|
|||Under the reformulated representation of primitives, the CNN output does not induce a unique assembled shape  it induces a distribution of possible shapes where the mth primitive stochastically exists with probability pm.|
|||Corresponding to Oi, the input to our CNN is a discretized representation as a volumetric occupancy grid Ii of size 32  32  32 (we later experiment with rendered images as input in Section 5.3).|
|||in the initial stages if a primitive is incorrectly placed, the CNN may learn to predict a very small pm instead of learning to align the primitive correctly.|
|||After the CNN has been trained, when computing the assembled representation for an object, we use MLE estimates instead of sampling i.e.|
|||The final shape predictions using the CNN may still have redundant parts used and we use a simple post-processing step to refine the prediction by removing the parts which significantly overlap with others.|
|||Interpretable Shape Similarity  The trained CNN of our shape assembler maps every 3D shape to corresponding primitive parameters {(zm, qm, tm)}.|
|||Joint embeddings of shapes and images via cnn image purification.|
||17 instances in total. (in cvpr2017)|
|179|cvpr18-Multi-View Consistency as Supervisory Signal for Learning Shape and Pose Prediction|We consider Figure 2 and first examine the shape prediction CNN fs.|
|||Similarly, the pose prediction CNN fp is required to infer a viewpoint under which the predicted geometry can explain the verification image V .|
|||As V is chosen to be from the same viewpoint as the image I2, the pose CNN should predict the correct viewpoint corresponding to its input image (I2).|
|||How 2899  ever, to explain the verification image V , the pose CNN is required to predict a pose w.r.t the inferred shape fs(I1).|
|||The resolution to this is that the shape prediction CNN fs automatically learns to predict shape in some (arbitrary) view-agnostic canonical frame (e.g.|
|||the mini-batch size for the pose prediction CNN is between 16 and 24.|
|||We use extremely simple CNN architectures (depicted in Figure 2) corresponding to fs and fp.|
|||Our shape prediction CNN has an encoder-decoder structure similar to the one used by Tulsiani et al.|
|||The input to the CNN is an RGB image of size 64  64 and the outputs are corresponding voxel occupancy probabilities for a 32  32  32 grid.|
|||Our pose prediction CNN fp has a similar encoder to fs, but outputs the predicted pose via fully connected layers.|
|||While in this work we assume known intrinsic parameters, the pose prediction CNN could in principle be extended to infer these.|
|||Overcoming Local Minima  We observed that our training is susceptible to local minima, in particular for the pose prediction CNN fp.|
|||We use our loss function but train the shape prediction CNN fs using the ground-truth pose instead of predicted poses.|
|||Since different CNNs can be calibrated differently, we search for the optimal threshold (per CNN on the validation set) to binarize the predictions.|
|||While the empirical results reported below correspond to using the correct pose prior, we first show that the primary benefit of this prior is that it encourages the CNN to predict diverse poses and avoid local minima, and that even an approximate prior is sufficient.|
|||Some results (on images of novel instances) using our learned CNN are visualized in Figure 7.|
|||Unsupervised cnn for single view depth  estimation: Geometry to the rescue.|
||17 instances in total. (in cvpr2018)|
|180|Su_Render_for_CNN_ICCV_2015_paper|We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity.|
|||The learned CNN is applied to estimate the viewpoints of objects in real images.|
|||To explore the idea of Render for CNN for 3D tasks, we focus on the viewpoint estimation problem  for an input RGB image and a bounding box from an off-the-shelf detector, our goal is to estimate the viewpoint.|
|||In summary, our contributions are as follows:  We show that training CNN by massive synthetic data is an effective approach for 3D viewpoint estimation.|
|||Render for CNN System  Since the space of viewpoint is discretized in a highly fine-grained manner, massive training data is required for the training of the network.|
|||We describe how we synthesize such large number of training images in Sec 4.1, and how we design the network architecture and loss function for training the CNN with the synthesized images in Sec 4.2.|
|||To teach CNN to recognize occluded or truncated images, we crop the image by a perturbed object bounding box.|
|||We found that the CNN trained for viewpoint estimation of one class do not perform well on another class, possibly due to the huge geometric variation between the classes.|
|||Our method uses joint real and rendered images and trains a CNN tailored for this task.|
|||The improvement can be understood by observing ambiguous cases in Figure 10, where CNN gives two or multiple high probability proposals and many times one of them is correct.|
|||Since our CNN can predict viewpoints well, we expect the structure of our CNN feature space to reflect this nature.|
|||In Figure 7, we visualize the feature space of our CNN (output of the last shared fully connected layer) in 2D by t-SNE [36], using car as an example.|
|||We visualize features of car images extracted by original R-CNN for detection (left) and our CNN for viewpoint estimation (right) by t-SNE.|
|||Synthesis Parameter Analysis  In this section we show results of our control experiments, which analyze the importance of different factors in our Render for CNN image synthesis pipeline.|
|||Conclusion and Future Work  We demonstrated that images rendered from 3D models can be used to train CNN for viewpoint estimation on real images.|
|||In general, we envision our Render for CNN pipeline can be extended to many tasks beyond viewpoint estimation, especially to those that annotations are hard to acquire such as segmentation, dense correspondence and depth estimation.|
|||Joint embeddings of shapes and images via cnn image purification.|
||17 instances in total. (in iccv2015)|
|181|A-Lamp_ Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural Network for Photo Aesthetic Assessment|Conventional CNN methods (a) transform images via cropping, warping and padding.|
|||The proposed A-Lamp CNN (b) takes multiple patches and attributes graphs as inputs to represent fine grained details and the overall layout.|
|||These deep CNN methods have indeed shown promising results.|
|||To resolve this technical issues, we present in this paper a dedicated CNN architecture named A-Lamp.|
|||Second, how to effectively describe specific image layout and incorporate it into the deep CNN is again very challenging.|
|||Like DMANet in [23], our proposed A-Lamp CNN also crops multiple patches from original images to preserve fine-grained details.|
|||Second, unlike the DMA-Net that just focus on the finegrained details, this A-Lamp CNN incorporates the holistic layout via the construction of attribute graph.|
|||At the same time, a trained CNN is adopted to detect salient objects in the image.|
|||Layout(cid:173)Aware Subnet  We first employ a trained CNN [46] to localize the salient objects.|
|||DMA-Net proposed in [23] is a very recent dedicated deep Multi-Patch-based CNN for aesthetic assessment.|
|||It is obvious that, all recently developed deep CNN schemes outperform these conventional feature-based approaches.|
|||To show the effectiveness of the proposed layoutaware subnet, we compare A-Lamp with several latest deep CNN networks that incorporate global information for learning.|
|||Specifically,  SCNNc and SCNNw denote the single-column CNN in [22] that takes center cropping and warping, respectively, as inputs.|
|||DCNN denotes the double-column CNN in [22].|
|||Extensive experiments on the large-scale AVA benchmark show that this A-Lamp CNN can significantly improve the state of the art in photo aesthetics assessment.|
|||Meanwhile, the proposed A-Lamp CNN can be directly applied to many other computer vision tasks, such as style classification, object category recognition, image retrieval, and scene classification, which we leave as our future work.|
||16 instances in total. (in cvpr2017)|
|182|Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper|Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.|
|||Our approach is a generalization of CAM [47] and is applicable to a significantly broader range of CNN model families: (1) CNNs with fully-connected layers (e.g.|
|||For captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models are often good at localizing discriminative image regions despite not being trained on grounded image-text pairs.|
|||Related Work  Our work draws on recent work in CNN visualizations, model trust assessment, and weakly-supervised localization.|
|||A number of previous works [40, 42, 45, 13] have visualized CNN predictions by highlighting important pixels (i.e.|
|||This approach modifies image classification CNN architectures replacing fully-connected layers with convolutional layers and global average pooling [25], thus achieving class-specific feature maps.|
|||A drawback of CAM is that it requires feature maps to directly precede softmax layers, so it is only applicable to a particular kind of CNN architectures performing global average pooling over convolutional maps immediately prior to prediction (i.e.|
|||Approach  A number of previous works have asserted that deeper representations in a CNN capture higher-level visual constructs [5, 31].|
|||These gradients flowing back are global Grad-CAM  620  Figure 2: Grad-CAM overview: Given an image and a class of interest (e.g., tiger cat or any other type of differentiable output) as input, we forward propagate the image through the CNN part of the model and then through task-specific computations to obtain a raw score for the category.|
|||Recall that CAM [47] produces a localization map for an image classification CNN with a specific kind of architecture where global average pooled convolutional feature maps are fed directly into softmax.|
|||One obvious choice for such a visualization is image occlusion [45], where we measure the difference in CNN scores when patches of the input image are masked.|
|||We use Guided Grad-CAM to analyze failure modes of the VGG-16 CNN on ImageNet classification [9].|
|||We build Grad-CAM on top of the publicly available neuraltalk22 implementation [23] that uses a finetuned VGG-16 CNN for images and an LSTM-based language model.|
|||units in the last convolutional layer of the CNN (conv5_3 for VGG16) and generate Grad-CAM visualizations as described in Section 3.|
|||This shows that despite not being trained on grounded image-text pairs, even non-attention based CNN + LSTM based VQA models are surprisingly good at localizing discriminative  624  (a) Image captioning explanations  (b) Comparison to DenseCap  Figure 5: Interpreting image captioning models: We use our class-discriminative localization technique, Grad-CAM to find spatial support regions for captions in images.|
|||Deeper LSTM and normalized CNN Visual Question Answering model.|
||16 instances in total. (in iccv2017)|
|183|Zhang_Monocular_Object_Instance_ICCV_2015_paper|Our approach uses a CNN to predict instance-level segmentation and depth ordering in an image patch.|
|||In the remainder of the paper, we review related work, explain our CNN architecture, present our approach and detail the obtained results.|
|||In contrast, our approach directly predicts instance labeling and depth ordering via a powerful CNN network, and a merging procedure phrased as an energy minimization task.|
|||Toward this goal, we take advantage of deep learning and train a CNN to predict both instance segmentations and depth ordering.|
|||To deal with the different scales of objects, we split a given image into a set of overlapping patches, extracted at multiple scales, and employ the CNN to estimate a pixel-level labeling and the depth ordering for each patch.|
|||MRF Patch Merging  We next need to combine the output of the CNN at different overlapping patches to produce a single coherent explanation of the image.|
|||Let us emphasize that the CNN produces a prediction of up to 5 cars for a patch in the image and the goal of the MRF is to create a single coherent explanation.|
|||To estimate the high level structure, we first run a connected component algorithm on the merged CNN prediction map.|
|||It is important to note that the patch predictions that we obtain from the CNN are downsampled by a factor of 8.|
|||In order to correct for this reduced resolution, we bi-linearly interpolate each CNN patch output back to its original size.|
|||This is valid since the CNN has only access to a subset of the image, illustrating a subset of cars.|
|||We thus include a unary potential per pixel that favors all states equal to or higher than the one predicted by the CNN in each local patch.|
|||Short-range Connections: This term encourages nearby pixels to be assigned the same labeling if this is also the case in the CNN predictions.|
|||We then apply the method to provide the labeling of all 6, 744 images minus the 301 samples that have accurate segmentations, and train the CNN on this subset augmented by the three annotated images used for learning the model of [2].|
|||In the ablation analysis we provide the performance of the CNN output (CNNRaw) with different MRF formulations containing only unary (Unary), containing only unary and short range (Unary+ShortRange) or containing all defined energy terms (Full).|
|||On the other hand we observe challenges for tiny cars, e.g., missed by the CNN prediction, and instances that are merged by the connected component algorithm.|
||16 instances in total. (in iccv2015)|
|184|cvpr18-Learning Spatial-Aware Regressions for Visual Tracking|In this paper, we exploit both the KRR and CNN models, and learn the complementary spatial-aware regressions for visual tracking (LSART).|
|||Finally, the heat maps obtained by the KRR and CNN models are combined to generate a final heat map for target location.|
|||We propose the complementary KRR and CNN models based on their inherent limitations.|
|||The spatial-aware KRR model focuses on the holistic object and the spatialaware CNN model focuses on small and localized regions, thereby complementing each other for better performance.|
|||In this method, they use the CF-based tracker defined on a single CNN layer as an expert and learn the adaptive weights for different experts.|
|||Besides, we effectively combine the proposed KRR and CNN models to develop a robust tracker.|
|||CNN with Spatially Regularized Kernels (C(cid:173)  NNSRK)  The proposed CNN framework with spatially regularized kernels is illustrated in Figure 3 (a), which consists of two convolution layers interleaved with an ReLU layer and several distance transform pooling layers.|
|||Then, we generate an ideal heat map of Gaussian distribution [17] and exploit the L2 loss function for finetuning both KRR and CNN models to fit the ideal heat map.|
|||Experimental Results  In this section, we describe how to exploit both spatial aware KRR and CNN models for robust visual tracking.|
|||Target Location Estimation  We conduct visual tracking by combining the responses of our KRR and CNN models.|
|||fKRR(Xt) and fCN N (Xt) denote the heat maps produced by KRR and CNN models,  In this section, we first introduce the experimental setups, and then report the experimental results of different trackers on the OTB-2015 and VOT-2017 public datasets. In addition, we verify the effectiveness of different components of the proposed method.|
|||All the networks are trained with the SGD method, and the learning rates for t and t in the KRR network are set as 8e-9 and 1.6, while the learning rate for each layer in the CNN network is fixed as 8e-7.|
|||CFCF, ECO and CCOT) exploit a combination of CNN and hand-crafted features, we extend our feature set with HOG and Color Naming like CCOT in this dataset.|
|||We use the shorthand Baseline to denote the method that directly combines the conventional KRR and CNN models, and adopt the abbreviations CPS, SRK to denote the cross-patch similarity kernel and spatial regularized filter kernel.|
|||In addition, we propose a complementary CNN model which focuses more on the localized region via a spatially regularized filter kernel.|
|||Deformable part models with cnn features.|
||16 instances in total. (in cvpr2018)|
|185|Hidden Layers in Perceptual Learning|We first replicated two representative sets of perceptual learning experiments by training a shallow CNN to perform the relevant tasks.|
|||This modeling choice is justified by the resemblance between the CNN architecture and the organization of low level visual areas in the brain.|
|||Current CNN models are based on models developed in the 1980s [15].|
|||This was made possible by the availability of big data a large number of images collected into publicly available databases, which were used to train deep CNN models more effectively than ever before.|
|||Methods  CNN network We trained a two layer CNN using vanilla SGD, with a fixed learning rate and batch size (50).|
|||a) CNN network: the difficulty level is controlled by the angle difference between the odd line segment and the background segments: 8o, 16o, 30o.|
|||Top: CNN network, transfer in the easier task with  = 30o (left), and the more difficult task with  = 8o (right).|
|||a) CNN network.|
|||Like before, we trained the CNN using the original stimulus, and then tested performance using the same 3 manipulations: change of stimuli absolute orientations while keeping the orientation difference fixed, change of location, and change of both.|
|||Dynamics of Weight Modifications  In the previous section we showed how our simulations, when training a shallow CNN to perform visual discrimination tasks, were able to replicate many of the phenomena observed repeatedly in perceptual learning experiments.|
|||Feature maps and filters  We first note that, in both experiments, the CNN learned simple edge-like features matching the displayed stimuli, as can be readily seen in Fig.|
|||8a shows typical patterns of activation in the channels of the second CNN layer in a pop-out experiment.|
|||The 6 convolution filters learned in the first conv-pool layer of the CNN when: a) the network was trained to discriminate orientation o1 only; b) the network was trained to discriminate orientation o2 only; c) the network was trained to discriminate orientation o2 after being trained with orientation o1.|
|||The 6 convolution filters (for each of the 6 channels) learned in the first conv-pool layer of the CNN in one simulation are shown in Fig.|
|||We note that in a few repetitions, the CNN failed to learn new appropriate filters, and it also failed to learn the new discrimination task with orientation o2.|
|||Recognition of transitional action for short-term action prediction using discriminative temporal cnn feature.|
||16 instances in total. (in cvpr2017)|
|186|Deep Unsupervised Similarity Learning Using Partially Ordered Sets|Adopting a strategy of self-supervision, a CNN is trained to optimally represent samples in a mutually consistent manner while updating the classes.|
|||However, to achieve this performance boost, these CNN architectures require millions of samples of supervised train Both authors contributed equally to this work.|
|||During train 17130  ing, these methods rely on the CNN to indirectly learn comparisons between samples that were processed in independent training batches, and generalize to unseen data.|
|||Instead of relying on the CNN to indirectly balance and learn sample comparisons unseen during training, a more natural approach is to explicitly encode richer relationships between samples as supervision.|
|||Then, each group receives a mutually exclusive label and a CNN is trained to solve the associated classification problem, thereby learning a representation that encodes similarity in the intermediate layers.|
|||Furthermore, tuple and triplet formulations advocate on the CNN to indirectly learn to conceal unrelated pairs of samples (i.e.|
|||Formally, given a deep feature representation  (e.g an arbitrary layer in a CNN with parameters ), and a surrogate class Cc, a poset of unlabeled samples Pc = {xj, .|
|||As a result, triplet formulations rely on the CNN to indirectly learn to compare and reconcile the vast number of unrelated sampled pairs that were processed on different, independent mini-batches during training.|
|||Therefore, using posets during training enforces the CNN to order all unlabeled sam: yi =  1 according to their similarity to the Z ples xi nearest class representatives rz : z  {1, .|
|||In particular, we require the CNN to pull samples from posets xi  Pc closer to their Z nearest class representatives, while pushing  them further from all other class representatives in a training mini-batch.|
|||(3), t  i = t(xi) are the logits of sample xi for a CNN with parameters .|
|||During training, CNN parameters  are updated by error-backpropagation with stochastic minibatch gradient descent.|
|||However, this would be inefficient in our setup, as the CNN representation  is learnt using SGD, and thus, requires to be optimized for a large number of iterations to be reliable, especially at the first unrolled steps.|
|||In addition, when compared to the state-of-the-art methods that leverage tuples [5] or triplets [20] for training a CNN from scratch, our approach shows 16% higher performance.|
|||This is explained by the more detailed similarity relationships encoded in each poset, which in tuple methods the CNN has to learn implicitly.|
|||The representation  used to compute similarities on the PASCAL datasets is for each CNN method that we now compare the fc6 layer.|
||16 instances in total. (in cvpr2017)|
|187|cvpr18-Categorizing Concepts With Basic Level for Vision-to-Language|A confusion degree is defined to measure the difficulty for CNN to distinguish class pairs, and the concepts with large confusion degrees are merged to minimize the CNN classification error.|
|||Second, the proposed BaC level is applied as an optimized semantic representation that connects CNN and LSTM for V2L.|
|||Currently, the combination of CNN and Recurrent Neural Network (RNN) is the predominant framework to achieve V2L.|
|||Generally, CNN is adopted as the encoder to extract visual features and RNN as the decoder to generate sentences [14, 47].|
|||Between the encoder and the decoder, the CNN features are projected into the same representation space as word embedding to realize mapping from vision to language.|
|||The human-categorization knowledge is introduced to CNN learning which benefits both classification and V2L tasks in [44].|
|||Specifically, a CNN model is firstly trained until its classification accuracy reaches a bottleneck, then the difficulty is measured according to the error rate of each class.|
|||In order to measure how difficult a class-pair  can be distinguished by a certain CNN model, the confusion degree  is defined as   =       ( + )( + )( + )( + )  .|
|||For most class pairs, their confusion degree range from 0.95 to 1, which demonstrates that CNN can easily distinguish them.|
|||However, for several class pairs, CNN can hardly perform well and the distribution of prone-confused classes is generally uniform.|
|||Consequently, it is not easy for CNN to infer the latent information for categorization.|
|||Experimental Results  Since the BaC level is supposed as a proper level of cognitive consensus, it is employed as an optimized representation between CNN and LSTM to verify its efficiency for V2L tasks.|
|||(2) NIC (Neural Image Captioning) method, which is a commonly used image captioning baseline model [47] with an image embedding layer between CNN and LSTM.|
|||First, the comparisons of different categorizing methods with the same CNN model are demonstrated in Table 1, Table 2, Table 3 and Table 4.|
|||References  Method  GUESS VGG+BoW [37] VIS+LSTM [37] 2VIS+BLSTM [37] multimodal CNN [26] ILSVRC-1K [40] Shuffle-4K [31] Shuffle-8K [31] BaC Category  Total  6.7 55.9 53.3 55.1 54.9 54.2 55.5 56.0 57.2  Obj  2.1 58.7 56.5 58.2    57.5 57.4 57.7 59.3  Num Col  35.8 44.1 46.1 44.8    47.0 48.3 48.7 48.3  13.9 52.0 45.9 49.5    50.3 52.7 52.7 51.2  Loc  8.9 49.4 45.5 47.3    48.5 47.6 48.9 51.5  ity reduction, and a Bag-of-Word (BoW) vector is obtained by summing all the learned word vectors of the question.|
|||On the other hand, the method of multimodal CNN [26] encodes both images and questions by CNN only.|
||16 instances in total. (in cvpr2018)|
|188|Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper|In other words, if we leverage the CNN for 360 video saliency detection, the convolution operation corresponding to different view angle/FOV should maintain the same kernels.|
|||2 Related Work  2.1 Convolution Neural Networks for Spherical Data  Though CNN has demonstrated their effectiveness for many computer vision tasks [10] [11], the data fed into traditional CNN are perspective images.|
|||To tackle the spherical data, methods in [12] [13] firstly project a spherical image with equirectangular projection, then standard CNN is applied.|
|||To solve this problem, [8] propose to stretch kernels in standard CNN to fit the shape of the patch on the equirectangular plane in convolution.|
|||[16] propose a new type of Spherical CNN on SO(3) manifold, and their solution is expressive and rotation-equivariant.|
|||However, the concept of SO(3) CNN is not so adhere to our intuition to process 2D spherical images and quite distincts from the concept of planner CNN.|
|||Though many CNN models have been proposed for spherical data, none are customized for 360 videos.|
|||propose to predict the salient objects by feeding projected panorama images into CNN directly.|
|||We evaluate our proposed spherical CNN model on both Salient360!|
|||Compared with our spherical U-Net method, the CNN and MSE  loss in such baseline is conventional CNN and standard MSE loss.|
|||6.3 Evaluation of Different Components in Spherical U-Net  We conduct ablation study by replacing the spherical CNN with standard CNN (Ours w. standard conv) and replacing spherical MSE with standard MSE (Ours w. standard MSE), respectively.|
|||We can see that both spherical CNN and Spherical MSE contributes the performance.|
|||To this end, we introduce a new type of spherical CNN where the kernels are shared across all image patches on the sphere.|
|||Considering that the 360 videos are stored with panorama, we extent spherical CNN to the panorama case, and we propose to re-sample kernel based on its location for spherical convolution on panorama.|
|||It is worth noting our spherical CNN is a general framework, it can also be applied to other tasks involving 360 video/image.|
|||The combination of spherical CNN with other types of deep neural network is beyond the study scope of this paper, and we will leave them for future work.|
||16 instances in total. (in eccv2018)|
|189|Matzen_BubbLeNet_Foveated_Imaging_ICCV_2015_paper|Our method analyzes the contribution of localized content in images by modifying the data shown to a CNN during training and at test time.|
|||However, in order to be useful for visual discovery, we need to be able to extract insight about what a CNN has learned, and communicate these insights to humans.|
|||Raw CNN features (without fine-tuning) have been shown to have useful properties for this task [14].|
|||But on the other hand, standard CNN architectures, such as  AlexNet [12], are opaquetheir features and inner workings are such that can be very challenging for humans to understand and interpret.|
|||Method  Our method has two main parts: (1) patch discovery, where we train a CNN that can localize discriminative visual content and (2) patch clustering, where we aggregate the discovered visual elements in a way suited for visualization and for detecting visual elements in new images.|
|||We found that in practice if you take a CNN that has been trained on normal images for a particular task and then test it with bubble images, it will perform no better than random  guessing.|
|||To recap, starting from a CNN trained on ILSVRC12 [17], we use a three stage fine-tuning regimen to train a network better suited for spatially localized predictions:  AlexNet/GoogLeNet: Fine-tune the network on 100%  normal images.|
|||Now that we have a CNN trained on bubble images, we can use it to discover salient patches.|
|||Each photo in our validation set now has a normal image and a bubble image as well as corresponding feature vectors extracted from the penultimate layer of our CNN (e.g., fc7 for AlexNet) for both normal and bubble images.|
|||We split this set of CNN features into two groups.|
|||However, our baseline CNN produced by fine-tuning outperforms all prior work.|
|||The results of running PCA or random projections on the penultimate CNN vectors are plotted for comparison (note that these representations are not sparse).|
|||This suggests that (a) scenes in Indoor67 can often be classified accurately with only a couple canonical elements per scene, and (b) the invariances baked into the CNN are what make the good performance possible.|
|||(1) We take the BubbleBank feature vector (a) or PCA applied to the penultimate layer of the CNN (b) and perserve the top N features of each descriptor while setting the remaining to zero.|
|||Figure 5 (bottom) shows the effect of (1) sparsifying BubbleBank feature vectors rather than using PCA on the penultimate CNN layer and (2) using our ranking to select which clusters are to be included in the BubbleBank feature vector to build more concise representations.|
|||Finally, the high invariance of the CNN features can still cause some clusters to be too invariant and appear noisy.|
||16 instances in total. (in iccv2015)|
|190|Lazarow_Introspective_Neural_Networks_ICCV_2017_paper|Specifically, our model learns a sequence of CNN classifiers using a synthesis-by-classification algorithm.|
|||There are a number of interesting properties about INNg worth highlighting:   CNN classifier as generator: No special conditions on the CNN architecture are needed in INNg and existing CNN classifiers can be directly made into generators, if trained properly.|
||| All backpropagation: Our synthesis-by-classification algorithm performs efficient training using backpropagation in both stages: the sampling stage for the input images and the classification training stage for the CNN parameters.|
|||3, namely INNg (consisting of a sequence of CNN classifiers composed to produce the process seen in Fig.|
|||Classification-step: Train CNN classifier C t on S+  St1  , resulting in qt(y = +1|x).|
|||A sequence of CNN classifiers is progressively learned.|
|||The left panel displays pseudo-negative samples drawn at each time stamp t. The right panel shows the classification by the CNN on the training samples and pseudo-negatives at each time stamp t.  to include the pseudo-negative samples (l of them) selfgenerated by the model after stage t. We then train the model at each stage t to obtain  qt(y = +1|x), qt(y = 1|x)  (5)  over S+  St  resulting in the classifier C t. Note that q is an approximation to the true p due to limited samples drawn from Rm.|
|||We use a CNN as our base classifier.|
|||Note that we maintain a single CNN classifier throughout the entire learning process in INNg-single.|
|||Since a sequence of CNN classifiers (10  60) are included in INNg, INNg has a much larger model complexity than GAN.|
|||Our alternative INNg-single model maintains a single CNN classifier but its generative power is worse than those of INNg and GAN.|
|||For INNg-single which consists of a single CNN classifier, SGD-based sampling is performed directly using this CNN classifier to transform the initial normal distribution to a desired texture.|
|||The first, the second, and the third column are respectively results by DCGAN [34] (using tensorflow implementation [25]), INNg-single (1 CNN classifier with 4 layers), and INNg (10 CNN classifiers each with 4 layers).|
|||Following the same settings in the face modeling experiments, we shown some examples generated by the DCGAN model [34], INNg-single (1 CNN classifier with 4 conv layers), and INNg (10 CNN classifiers each with 4 conv layers).|
|||The first, the second, and the third column are respectively results by DCGAN [34] (using tensorflow implementation [25]), INNg-single (1 CNN classifier with 4 layers), and INNg (12 CNN classifiers each with 4 layers).|
|||8, we show some face examples generated by the DCGAN model [34], our INNg-single model (1 CNN classifier with 4 conv layers), and our INNg model (12 CNN classifiers each with 4 conv layers).|
||16 instances in total. (in iccv2017)|
|191|Vemulapalli_Gaussian_Conditional_Random_CVPR_2016_paper|Instead of applying CNN to each region independently as in [14, 17], [9] applied the convolutional layers only once to the entire image, and generated region features by using pooling after the final convolutional layer.|
|||To capture the information present at multiple scales, CNN was applied to the input image multiple times at different resolutions, and the features from all the resolutions were concatenated to get the final pixel features.|
|||Building on top of these CNN features, [46, 47] introduced a recursive context propagation network that enriched the CNN features by adding image level contextual information.|
|||Instead of using a CNN multiple times, [7, 18, 31] proposed to use the features extracted by the intermediate layers of a deep CNN to capture the multi-scale information.|
|||Different from these methods, [30] directly produced dense segmentation maps by upsampling the predictions produced by a CNN using a trainable deconvolution layer.|
|||Instead of using CRF as a post-processing step, [55] trained a CNN along with a CRF in an end-to-end fashion by converting the mean field inference procedure of [24] into a recurrent neural network.|
|||The idea of jointly training a CNN and graphical model has also been used for other applications such as sequence labeling [10, 34], text recognition [21], human pose estimation [52], predicting words from images [6], handwritten word recognition [4].|
|||Recently, [29] trained a CNN along with a GCRF model for image-based depth prediction.|
|||In contrast, we define our GCRF model directly on top of the dense CNN output and connect each node to several neighbors.|
|||We compute the similarity measure sij using  F(zizj ),  sij = e(zizj )  (4) where zi is the feature vector extracted at ith pixel using a DeepLab CNN (with parameters CN N ), and the learned matrix F (cid:23) 0 defines a Mahalanobis distance function.|
|||Note that since our GCRF model is applied to the CNN output whose resolution is 1/8 times the input resolution, the effective neighborhood size in the input image is 184  184.|
|||Pretraining: Before training the full GCRF network, we pre-trained the similarity layer and CNN of the pairwise  3229  Method  bkg areo bike bird boat bottle bus  car  cat  chair cow table dog horse mbk person plant sheep sofa train  tv mean  MSRA-CFM [9]  87.7 75.7 26.7 69.5 48.8 65.6 81.0 69.2 73.3 30.0 68.7 51.5 69.1 68.1 71.7  67.5  50.4  66.5 44.4 58.9 53.5 61.8  FCN-8s [30]  91.2 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5  73.9  45.2  72.4 37.4 70.9 55.1 62.2  Hypercolumns [18]  89.3 68.7 33.5 69.8 51.3 70.2 81.1 71.9 74.9 23.9 60.6 46.9 72.1 68.3 74.5  72.9  52.6  64.4 45.4 64.9 57.4 62.6  DeepLab CNN [7]  91.6 78.7 51.5 75.8 59.5 61.9 82.5 76.6 79.4 26.9 67.7 54.7 74.3 70.0 79.8  77.3  52.6  75.2 46.6 66.9 57.3 67.0  ZoomOut [31]  91.1 85.6 37.3 83.2 62.5 66.0 85.1 80.7 84.9 27.2 73.2 57.5 78.1 79.2 81.1  77.1  53.6  74.0 49.2 71.7 63.3 69.6  Deep message passing [27]  93.9 90.1 38.6 77.8 61.3 74.3 89.0 83.4 83.3 36.2 80.2 56.4 81.2 81.4 83.1  82.9  59.2  83.4 54.3 80.6 70.8 73.4  Approaches that use CNNs and discrete CRFs  Deep structure models [28]  93.6 86.7 36.9 82.3 63.0 74.2 89.8 84.1 84.1 32.8 65.4 52.1 79.7 72.1 77.6  81.7  55.6  77.4 37.4 81.4 68.4 70.3  DeconvNet + CRF [32]  92.9 87.8 41.9 80.6 63.9 67.3 88.1 78.4 81.3 25.9 73.7 61.2 72.0 77.0 79.9  78.7  59.5  78.3 55.0 75.2 61.5 70.5  object clique potentials [37]  92.8 80.0 53.8 80.8 62.5 64.7 87.0 78.5 83.0 29.0 82.0 60.3 76.3 78.4 83.0  79.8  57.0  80.0 53.1 70.1 63.1 71.2  DeepLab CNN-CRF [7]  93.3 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2  80.8  59.7  82.2 50.4 73.1 63.7 71.6  CRF-RNN [55]  94.0 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1  80.6  59.5  82.8 47.8 78.3 67.1 72.0  DeconvNet + FCN + CRF [32] 93.1 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6  80.2  58.8  83.4 54.3 80.7 65.0 72.5  Proposed GCRF network  93.4 85.2 43.9 83.3 65.2 68.3 89.0 82.7 85.3 31.1 79.5 63.3 80.5 79.3 85.5  81.0  60.5  85.5 52.0 77.3 65.1 73.2  Table 1: Comparison with state-of-the-art on PASCALVOC 2012 test set (when trained using ImageNet and PASCALVOC data).|
|||For training, we used a mini-batch of 15 images and a starting learning rate of 103 for the similarity layer parameters {fm} and 104 for the CNN parameters CN N .|
|||This shows that  3Since the Unary DeepLab CNN was trained by [7] using PASCALVOC segmentation data, it was already close to a good local minima.|
|||Figure 4 provides a visual comparison of the proposed approach with DeepLab CNN (which is same as our unary network) and DeepLab CNN + discrete CRF.|
|||3230  Input  Ground truth  DeepLab CNN  DeepLab CNN-CRF  Proposed  Figure 4: Comparison of the proposed approach with DeepLab CNN [7] and DeepLab CNN + discrete CRF [7].|
||16 instances in total. (in cvpr2016)|
|192|Ali_Diba_Spatio-Temporal_Channel_Correlation_ECCV_2018_paper|We introduce a new block that models correlations between channels of a 3D CNN with respect to temporal and spatial features.|
|||Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization.|
|||We have added the STC block to the current state-of-the-art 3D CNN architectures such as 3D-ResNext and 3D-ResNet [4].|
|||Specifically, we show that a 2D CNN pre-trained on ImageNet can act as a teacher for supervision transfer to a randomly initialized 3D CNN for a stable weight initialization.|
|||3.1 Spatio-Temporal Channel Correlation (STC) Block  STC is a computational block which can be added to any 3D CNN architecture.|
|||Architecture for knowledge transfer from a pre-trained 2D CNN to a 3D CNN.|
|||The 2D CNN acts as a teacher for knowledge transfer to the 3D CNN, by teaching the 3D CNN to learn mid-level feature representation by solving an image-video correspondence task.|
|||The model parameters of the 2D CNN are frozen, while the task is to effectively learn the model parameters of the 3D CNN only.|
|||Lets I be a pre-trained 2D CNN which has learned a rich representation from labeled images dataset, while V being a 3D CNN which is randomly initialized using [33] and we want to transfer the knowledge of the representation from I to V for a stable weight initialization.|
|||We leverage this for learning mid-level feature representations by an image-video correspondence task between the 2D and 3D CNN architecture, as depicted in Figure 2.|
|||The 2D ResNet CNN has 4 convolution blocks and one fully connected layer at the end, while our 3D architecture has 4 3D-convolution blocks with an STC block and we add a fully-connected layer after the last block.|
|||Finally, we compare our transfer learning: 2D  3D CNN performance with generic state-of-the-art 3D CNN methods.|
|||Here, we describe the implementation details of our two schemes, 3D CNN architectures and knowledge transfer from 2D to 3D CNNs for stable weight initialization.|
|||We employ 2D ResNet architecture, pretrained on ImageNet [35], while the 3D CNN is our STC-ResNet network.|
|||We show that a stable weight initialization via transfer learning is possible for 3D CNN architectures, which can be used as a good starting model for training on small datasets like UCF101 or HMDB51.|
|||Note that inflation can only be used if the structure of the 2D and 3D network are the same, while our approach allows to transfer the knowledge from any 2D CNN to a 3D CNN, e.g., from 2D-ResNet to the 3D STC-ResNet as in Table 7, which is not possible by inflation.|
||16 instances in total. (in eccv2018)|
|193|Xie_DisturbLabel_Regularizing_CNN_CVPR_2016_paper|DisturbLabel: Regularizing CNN on the Loss Layer  Lingxi Xie1  Jingdong Wang2 Zhen Wei3 Meng Wang4 Qi Tian5  1Department of Statistics, University of California, Los Angeles, Los Angeles, CA, USA  2Microsoft Research, Beijing, China  3Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China  4School of Computer and Information, Hefei University of Technology, Hefei, Anhui, China  5Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA  1  198808xc@gmail.com  2  jingdw@microsoft.com  3  94hazelnut@gmail.com  4  wangmeng@hfut.edu.cn  5  qitian@cs.utsa.edu  Abstract  During a long period of time we are combating overfitting in the CNN training process with model regularization, including weight decay, model averaging, data augIn this paper, we present DisturbLabel, mentation, etc.|
|||To the best of our knowledge, this is the first attempt to regularize the CNN on the loss layer.|
|||Comparison with different CNN regularization techniques.|
|||Avoiding over-fitting is a major challenge against training robust CNN models.|
|||Data augmentation generates more training data as the input of the CNN by randomly cropping, rotating and flipping the input images [15][2], and adding noises to the image pixels [27].|
|||The DisturbLabel Algorithm  We start with the typical CNN training process.|
|||The goal is to train a CNN model M: f (x; )  RC , in which  represents the model parameters.|
|||This verifies that DisturbLabel does improve the generalization ability of the trained CNN model (Section 3.2 provides an empirical verification that the improvement comes from preventing over-fitting).|
|||Figures 4 and 5 show the results on the MNIST and CIFAR10 datasets using the normal training without regularization and the DisturbLabel algorithm over the same CNN structure.|
|||In addition, we also report the results when training the CNN with Dropout which is an alternative approach of CNN regularization.|
|||This reveals that training a CNN with regularization (either DisturbLabel or Dropout) yields stronger generalization ability.|
|||Then we can normally train the CNN over the same data x and the soft label y, using which the loss function is changed.|
|||Relationship to Other CNN Training Methods  We briefly discuss the relationship between DisturbLabel  and other network training algorithms.|
|||This is different from Dropout [8], which regularizes the CNN on hidden layers.|
|||DisturbLabel is an approximate ensemble of many CNN models with the same structure trained over different noisy datasets, while Dropout is an approximate ensemble of many CNN models with different structures trained over the same data.|
|||We empirically discuss the combination of DisturbLabel with Dropout, which leads to an ensemble of many CNN models with different structures trained over different noisy data.|
||16 instances in total. (in cvpr2016)|
|194|Wang_STCT_Sequentially_Training_CVPR_2016_paper|We regard a CNN as an ensemble with each channel of the output feature map as an individual base learner.|
|||To apply deep CNNs for tasks with a limited amount of training samples, previous approaches [24, 30] adopt a transfer learning method by first pre-training a deep CNN on a source task with a large scale training data set and then fine-tuning the learned feature on the target task.|
|||Due to the good generalization capability of CNN features across different data sets, this transfer learning approach is effective and has shown state-of-the-art performance.|
|||Thus, directly online fine-tuning a pre-trained deep CNN is prone to over-fitting, which will degrade the tracker and gradually leads to tracking drift (See Figure 1 as an example).|
|||Specifically, a CNN is regarded as an ensemble, while each channel of the convolutional feature map is treated as a base learner and is updated using a different loss criterion, such that they are not highly correlated with each other.|
|||Online fine-tuning of the CNN is then formulated as a sequential ensemble learning problem.|
|||In [21], a three-layer CNN is trained on-line.|
|||Without pre-training and with limited training samples obtained online, CNN fails to capture object semantics and is not robust to deformation.|
|||(6)  For online applications, one simple approach to transfer offline pre-trained CNN features is to add one or more randomly initialized CNN layers, named as adaptation layers, on top of the pre-trained CNN model.|
|||(a) Conventional method for training a two-layer CNN online to transfer pre-trained deep features.|
|||(b) The proposed method trains the CNN model via sequentially sampling optimal base learners into an ensemble.|
|||When all the base learner parameters are sampled from the candidate set to the ensemble set, the ensemble F (X; E) evolves into a complete CNN model (Figure 2 (b), t = T ).|
|||The parameters of this CNN model are trained using different loss criterions and thus demonstrate a moderate diversity, which is empirically shown in our experiments to improve performance and reduce over-fitting.|
|||However, we find in our initial experiments that randomly dropping-out all the activations in a subset of feature map channels sometimes leads to divergence when training the CNN online with limited amount of training samples.|
|||To gain more insights on the effectiveness of the proposed sequential CNN training method, we also compare the proposed STCT tracker with two baseline methods: STCT-um and OTCT, where STCT-um denotes a variant of the proposed method that does not exploit the proposed con 1379  00.10.20.30.40.50.60.70.8STCTMEEMKCFTGPRTable 1.|
|||The success plot and precision plot in Figure 6 demonstrate that the proposed sequential CNN training method can significantly improve the tracking performance and that the proposed convolution with mask layer can further reduce over-training.|
||16 instances in total. (in cvpr2016)|
|195|Zou_HARF_Hierarchy-Associated_Rich_ICCV_2015_paper|3.3.1 Using CNN features as rich elementary features  In our framework, different types of elementary features can be used to capture different visual characteristics of an image region.|
|||Typically, the outputs of the last layer of CNN are used as features in semantic-level tasks, which makes sense for these applications because the last layer carries the highest semantic information.|
|||Specifically, we use the CNN model of [11], which is implemented based the architecture of [21] and contains five convolutional layers and two fully connected layers.|
|||The CNN parameters are trained from a large dataset of the 2012 ImageNet large scale visual recognition challenge with image-level annotations.|
|||The CNN model requires in 409  C o n v 1  C o n v 2  C o n v 3  C o n v 4  C o n v 5  F c 6  F c 7  7  jf  jR  1  jf  2  jf  3  jf  4  jf  5  jf  6  jf  Figure 3.|
|||The outputs of each layer of CNN are used as features.|
|||As illustrated in Figure 3, each CNN hidden layer output vector is used as a separate elementary feature, thus leading to 7 different types of elementary features, {f 1 j }.|
|||The local regional contrasts evaluate dissimilarities between the target region Rj and its adjacent neighbors Nj in terms of each elementary feature (including CNN features, color features and texture features).|
|||(6) is used for CNN features.|
|||We also validate the saliency performance for the combination of CNN and traditional features.|
|||To explore if the last two fully-connected layers of CNN (which carry more semantic information than the lower ones) contribute to salient object detection or not, we generate saliency results by using only the lower CNN layers (layer 1 to 5) as well as all CNN layers (layer 1 to 7).|
|||From Figure 4 and Table 1, we can directly see that using all CNN layers achieves higher performance than using the lower ones only, which validates the proposed CNN feature description for saliency detection.|
|||411  i  i  n o s c e r P  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1    0  MSRAB     HARF2 over CNN and traditional features HARF1 over CNN and traditional features CNN and traditional features (without HARF) HARF2 over all CNN layers HARF2 over lower CNN layers All CNN layers (layer 1 to layer 7) Lower CNN layers (layer 1 to layer 5) HARF2 over traditional features HARF1 over traditional features Traditional features (without HARF)  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  Recall  i  i  n o s c e r P  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1    0  PASCAL1500     HARF2 over CNN and traditional features HARF1 over CNN and traditional features CNN and traditional features (without HARF) HARF2 over all CNN layers HARF2 over lower CNN layers All CNN layers (layer 1 to layer 7) Lower CNN layers (layer 1 to layer 5) HARF2 over traditional features HARF1 over traditional features Traditional features (without HARF)  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  Recall  i  i  n o s c e r P  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1    0  SOD     HARF2 over CNN and traditional features HARF1 over CNN and traditional features CNN and traditional features (without HARF) HARF2 over all CNN layers HARF2 over lower CNN layers All CNN layers (layer 1 to layer 7) Lower CNN layers (layer 1 to layer 5) HARF2 over traditional features HARF1 over traditional features Traditional features (without HARF)  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  Recall  Figure 4.|
|||Configuration  Traditional features (without HARF) HARF1 over traditional features HARF2 over traditional features Lower CNN layers (layer 1 to layer 5) All CNN layers (layer 1 to layer 7) HARF2 over lower CNN layers HARF2 over all CNN layers CNN and traditional features (without HARF) HARF1 over CNN and traditional features HARF2 over CNN and traditional features  MSRA-B  F  0.853 0.864 0.865 0.747 0.768 0.799 0.811 0.868 0.878 0.879  MAE  0.092 0.073 0.072 0.211 0.193 0.134 0.130 0.085 0.067 0.064  PASCAL-1500 MAE  F  0.718 0.732 0.730 0.573 0.612 0.653 0.672 0.745 0.760 0.759  0.141 0.121 0.121 0.295 0.279 0.212 0.209 0.131 0.112 0.109  SOD  F  0.699 0.706 0.714 0.531 0.569 0.611 0.634 0.735 0.759 0.737  MAE  0.180 0.162 0.159 0.321 0.308 0.255 0.246 0.167 0.149 0.146  Table 1.|
|||Saliency maps generated by the proposed approach using different configurations: (b) traditional features, (c) HARF1 over traditional features, (d) HARF2 over traditional features, (e) CNN and traditional features, (f) HARF1 over CNN and traditional features, (g) HARF2 over CNN and traditional features.|
|||Run-time: Our current unoptimized MATLAB code for the proposed model on a PC with a GTX Titan X GPU (for CNN feature computation) and an Intel i5 3.2GHz CPU (for all other components) takes around 30h for training and 22s for testing on an image of resolution 400  300.|
||16 instances in total. (in iccv2015)|
|196|cvpr18-NAG  Network for Adversary Generation|The target CNN is denoted as f , therefore the output of a given layer i is denoted as f i(x).|
|||The image-agnostic additive perturbation that fools the target CNN is denoted as .  denotes the limit on the perturbation () in terms of its l8 norm.|
|||The modifications we make are: (i) Discriminator (D) is replaced by the target CNN (f ) which is already trained and whose weights are frozen, and (ii) a novel loss (fooling and diversity objectives) instead of the adversarial loss to train the Generator (G).|
|||We denote the label predicted by the target CNN on a clean sample x as benign prediction (c) and that predicted on the corresponding perturbed sample (x + ) as adversarial prediction.|
|||f i is the output of the CNN at ith layer and d(., .)|
|||We performed all our experiments on a variety of CNN architectures trained to perform object recognition task on the ILSVRC-2014 [25] dataset.|
|||We kept the architecture of our generator (G) unchanged for different target CNN architectures and separately learned the corresponding adversarial distributions.|
|||We now feed these through the target CNN (f ) and compute the loss.|
|||Note that, in each row, entry where the target CNN matches with the network under attack represents white-box attack and the rest represent the black-box attacks.|
|||VGG-F  CaffeNet  GoogLeNet  VGG-16  VGG-19  ResNet-50  ResNet-152  VGG-F 94.10 + 93.7   1.84  CaffeNet 81.28+ 71.8   3.50  1.44  79.25+ 74.0  0.86  64.83+ 46.2  2.24  60.56+ 63.4  2.49  67.80+ 64.0  2.60  47.06+  4.37  57.66+ 46.3  1.56  96.44+ 93.3  2.12  70.46+ 43.8  6.95  65.55+ 55.8  5.59  67.58+ 57.2  1.70  63.35+  2.95  64.86+ 46.3  GoogLeNet 64.15+ 3.41 48.4  1.84  66.66+ 47.7  1.55  90.37+ 78.9  4.84  67.38+ 56.5  0.94  74.48+ 53.6  1.14  65.30+  1.39  62.33+ 50.5  VGG-16 52.93+ 42.1  8.50  5.61  50.40+ 39.9  4.13  56.40+ 39.2  2.77  77.57+ 78.3  3.26  80.56+ 73.5  2.61  55.16+  3.41  52.17+ 47.0  VGG-19 55.39+ 42.1  2.68  4.15  55.13+ 39.9  3.17  59.14+ 39.8  1.63  73.25+ 73.1  2.45  83.78+ 77.8  2.58  52.67+  4.16  53.18+ 45.5  ResNet-50 50.56+  4.50  3.96  52.38+  4.40  63.21+  3.47  61.28+  3.38  68.75+  2.73  86.64+  2.75  73.32+  Our UAP  Our UAP  Our UAP  Our UAP  Our UAP  Our UAP  Our UAP  ResNet-152 Mean FR 47.67+ 47.4  63.73 57.58  4.12  4.25  48.58+ 48.0  1.64  59.22+ 45.5  2.63  54.38+ 63.4  1.90  65.43+ 58.0  1.89  66.40+  2.72  87.24+ 84.0  64.12 56.71  66.23 48.9  65.71 65.08  72.62 64.01  62.37  64.39 53.27  target CNN for which we have modelled the distribution of perturbations and the columns represent the classifiers we attack.|
|||Note that in each row, when the target CNN (row) matches with the system under attack (column), the fooling indicates the white-box attack scenario and all other entries represent the black-box attack scenario.|
|||Since our G network models the perturbation space, we can now easily generate a perturbation by sampling a z and feeding it through the G. In Table 1, we report the mean fooling rates after generating multiple perturbations for a given CNN classifier.|
|||Note that the black-box fooling rates are obtained by averaging the fooling rates obtained on three (GoogLeNet, VGG-19 and ResNet-50) CNN models.|
|||In this subsection, we analyze the labels predicted by the target CNN after adding perturbations modelled by the corresponding generative models (G).|
|||The target CNN is mentioned below the perturbations.|
|||Conclusion  In this paper, we have presented the first ever generative approach to model the distribution of adversarial perturbations for a given CNN classifier.|
||16 instances in total. (in cvpr2018)|
|197|Kernel Pooling for Convolutional Neural Networks|Recently, efforts in combining CNNs with 2nd order feature interactions, either by replacing hand-crafted features with CNN features [7] or jointly trained in an end-to-end fashion, yielded impressive performance gains on a wide range of visual tasks.|
|||Secondly, the proposed kernel pooling is differentiable and can be combined with a CNN for joint optimization.|
|||The composition of the kernel can also be learned through the end-to-end training with a CNN (see Sec.|
|||1 summaries pooling strategies adopted in commonly used CNN architectures.|
|||A summary of pooling strategies adopted in commonly used CNN architectures.|
|||The coefficients of the kernel are jointly learned together with weights of other CNN layers via back-propagation of the loss (denoted by outgoing arrows from Loss).|
|||By jointly learning the kernel composition together with CNN weights in an end-to-end fashion, we argue the learned kernel is more adaptive and suitable to the data we are working on.|
|||4.2, we run a comprehensive study of kernel approximation quality on CNN features.|
|||We pass the original crop and its horizontal flip to the CNN independently.|
|||The intial weights of the convolutional layers are pretrained on ImageNet classification dataset, and the initial weights of the final linear classifier is obtained by training a logistic regression classifier on the compact kernel pooling of pre-trained CNN features.|
|||1 on CNN features.|
|||Relative approximation error for Gaussian RBF kernel applied on CNN features with variant kernel configurations.|
|||The approximated representation is fully differentiable, thus the kernel composition can be learned together with a CNN in an end-to-end manner.|
|||Discussion  In this subsection, we discuss the relative importance of higher order information for different CNN architectures.|
|||Bilinear cnn models for fine-grained visual recognition.|
||15 instances in total. (in cvpr2017)|
|198|Xing_Wei_Grassmann_Pooling_for_ECCV_2018_paper|The reason is that the bilinear feature matrix is sensitive to the magnitudes and correlations of local CNN feature elements which can be measured by its singular values.|
|||Similarity measurement of images reduces to comparing the principal angles between these homogeneous subspaces and thus is independent of the magnitudes and correlations of local CNN activations.|
|||We argue that such phenomenon makes the bilinear feature matrix much more sensitive to the magnitudes and correlations of local CNN activations, which may cause the burstiness problem [26].|
|||i=1 iuivT  i=1   iuivT  i , where   B = Pc  That is, the pooling method transforms the CNN feature matrix A to an orthonormal matrix consists of its k principal left singular vectors.|
|||In geometry, the pooled CNN features obtained in this manner are k-dimensional linear subspaces of the c-dimensional Euclidean space, which lie on the (c, k) Grassmann manifold [30], denoted by Gk c .|
|||When inserted into a CNN and trained in an end-to-end fasion, this pooling method leads the model to learn only structural features which are independent of the magnitudes and correlations of visual elements.|
|||(a) The singular values can be influenced by the magnitudes and correlations of local CNN feature elements, and they are wide-spread and mixed with different classes.|
|||Consider that two duplicated feature matrix A generated by a CNN are concatenated together, thus the singular values of B are multiplied by a factor of 2.|
|||Nevertheless, such examples show that how to pool CNN features in a more robust manner.|
|||Our GP mainly differs from LRBP in two respects: 1) pooling: GP transforms CNN features to the compact Grassmann manifold while LRBP uses the conventional (or does not explicitly compute) bilinear feature matrix.|
|||2) classifier: the classifier of GP is exactly derived from the projection distance of the Grassmann manifold; while for LRBP, it is derived from the bilinear SVM and is further approximated in the Euclidean space  3.4 Learning A Grassmann Projection for Model Compression  Typically, reducing the number of feature maps in CNN is performed by a 11 conv layer, setting a smaller number of the output feature maps.|
|||[42] have explored such matrix backpropagation in CNN which is also adopted in this work.|
|||We show that the bilinear feature matrix is sensitive to the magnitudes and correlations of local CNN feature elements which can be measured by its singular values.|
|||Huang, S., Xu, Z., Tao, D., Zhang, Y.: Part-stacked cnn for fine-grained visual  categorization.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||15 instances in total. (in eccv2018)|
|199|Girshick_Deformable_Part_Models_2015_CVPR_paper|Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer.|
|||Although the model is a single network, for pedagogical reasons we describe it in terms of two smaller networks, a feature pyramid frontend CNN and a DPM-CNNtheir function composition yields the full network.|
|||Constructing an equivalent CNN from a DPM In the DPM formalism, an object class is modeled as a mixture of components, each being responsible for modeling the appearance of an object sub-category (e.g., side views of cars, people doing handstands, bi-wing propeller planes).|
|||(2) Each pyramid level is forward propagated through a fully-convolutional CNN (e.g., a truncated SuperVision CNN [27] that ends at convolutional layer 5).|
|||Since the whole system is the composition of two CNNs, it can be viewed as a single, unified CNN that takes a color image pyramid as input and outputs a DPM score pyramid.|
|||A DPM component can be written as an equivalent CNN by unrolling the DPM detection algorithm into a network.|
|||The CNN construction for these more complex models is analogous to the DPM-CNN construction: take the exact inference algorithm used for detection and explicitly unroll it (this can be done given a fixed model instance), then express the resulting network in terms of convolutional layers (for appearance and geometry filters), distance transform pooling layers, subsampling layers (if parts are at different scales), and maxout layers (for mixture components).|
|||Their CNN is structured like a DPM and includes a deformation layer that takes a distance penalized global max within a detection window.|
|||We extend their work by: (1) developing object geometry filters, (2) showing that multi-component models are implemented with maxout, (3) describing how DPM inference over a whole image is efficiently computed in a CNN by distance transform pooling, and (4) using a more powerful CNN front-end to generate the feature pyramid.|
|||Unlike DetectorNet and OverFeat, R-CNN does not perform sliding-window detection; instead R-CNN begins by extracting a set of region proposals [39] and classifies them with a linear SVM applied to CNN features.|
|||The first treats the model as a single CNN and trains it end-to-end with SGD and backpropagation.|
|||The second trains the model in two stages: (1) fit the front-end CNN; (2) train a DPM on top of stage 1 using latent SVM [10] while keeping the front-end CNN fixed.|
|||In our DeepPyramid DPM implementation, we chose to use a truncated variant of the SuperVision CNN [27].|
|||The resulting modelwhich we call a DeepPyramid DPMis a single CNN that performs multi-scale object detection by mapping an image pyramid to a detection score pyramid.|
|||In 3rd Parts  Deformable part models with CNN features.|
||15 instances in total. (in cvpr2015)|
|200|cvpr18-Wasserstein Introspective Neural Networks|The generative modeling aspect was studied in [29] where a sequence of CNN classifiers (10  60) were trained, while the power within the classification setting was revealed in [21] in the form of introspective convolutional networks (ICN) that used only a single CNN classifier.|
|||The resulting model, Wasserstein introspective neural networks (WINN) shows greatly enhanced modeling capability over INN by having 20 reduction in the number of CNN classifiers.|
||| By adopting the Wasserstein distance into INN, we are able to generate images using a single CNN in WINN with even higher quality than those by INN that uses 20 CNNs (as seen in Figure 2, 4, 5, 6, and 7; the similar underlying CNN architectures are used in WINN and INN).|
|||Here, we typically train 4-5 cascades to boost the numbers but WINN with one CNN is already promising.|
|||A single CNN classifier is trained in an introspective manner to improve the standard supervised classification result [21], however, a sequence of CNNs (typically 10  60) is needed to be able to synthesize images of good quality [29].|
|||Figure 1 shows a brief illustration of a single introspective CNN classifier [21].|
|||Using the same CNN architecture (ResNet from [14]) that was used as GANs discriminator, we also train a WINN-single model, making GANs discriminator and WINN-single to have the identical CNN architecture.|
|||Generated images by WINN-single and WINN (4 CNN classifiers) as well as DCGAN and INN are shown in Figure 6.|
|||The advantage of WINN over a vanilla CNN is evident.|
|||Method MNIST  Baseline vanilla CNN (4 layers)  CNN + GDL [44]  CNN + DCGAN [38]  ICN [21]  WINN-single vanilla (ours)  Baseline ResNet-32  WINN-single ResNet-32 (ours)  SVHN  Baseline ResNet-32  WINN-single ResNet-32 (ours)  Error  0.89% 0.85% 0.84% 0.81% 0.67% 0.45% 0.48%  4.64% 4.50%  Training Methods.|
|||The comparisons are made in two groups: the first group builds on top a vanilla 4 layer CNN and the second group adopts ResNet-32.|
|||We use a single CNN for each experiment.|
|||In most of the images shown in the paper, we find a single CNN classifier in WINN being sufficient to produce visually appealing images as well as significant error reduction against adversarial examples.|
|||WINN is agnostic to the architecture design of CNN and we demonstrate results on three networks including a vanilla CNN, ResNet [16], and DenseNet [18] networks where popular CNN discriminative classifiers are turned into generative models under the WINN procedure.|
|||Inspired by [22], we design a CNN architecture for 64  64 image as in Table 5.|
||15 instances in total. (in cvpr2018)|
|201|cvpr18-Depth-Based 3D Hand Pose Estimation  From Current Achievements to Future Goals|It showed that training a standard CNN on a million-scale dataset achieves state-of-the-art results.|
|||2D CNN vs. 3D CNN.|
|||[13] project the depth image onto three orthogonal planes and train a 2D CNN for each projection, then fusing the results.|
|||In [12] they propose a 3D CNN by replacing 2D projections with a 3D volumetric representation (projective D-TSDF volumes [40]).|
|||[24] propose a 3D CNN to estimate per-voxel likelihoods for each hand joint.|
|||NAIST RV [56] proposes a 3D CNN with a hierarchical branch structure, where the input is a 503-voxel grid.|
|||NAIST RV [56]  3D CNN with 5 branches, one for each finger  Vanora [14] strawberryfg [55] ResNet-152 + [42]  shallow CNN trained end-to-end  1024 3D points  9696  505050 3D grid resized 2D 224224  rvhand [1]  ResNet [16] + REN [15] + Deep Prior [29]  192192  mmadadi [21]  Hierarchical tree-like structured CNN [21]  192192  LSL [20]  ScaleNet to estimate hand scale + DeepModel [61]  128128  [0.8, 1.2] [-40,40] [-8,8] [0.7, 1.1] [0, 360] [-8,8]  random scaling*  [0.9,1.1] [-45,45] [-5,5] [0.9,1.1] [-90, 90] [-15,15] random scaling* None [0.9,1.1] [-90, 90] [-15,15] [random] [-30, 30] [-10,10] [0.85,1.15] [0,360] [-20,20]                                                                                                                          Table 2: Methods evaluated in the hand pose estimation challenge.|
|||Structural constraints are included in the CNN model [20, 27, 29] or in the loss function [21, 55].|
|||DeepPrior [29] learns a prior model and integrates it into the network by introducing a bottleneck in the last CNN layer.|
|||LSL [20] uses prior knowledge in DeepModel [61] by embedding a kinematic model layer into the CNN and using a fixed hand model.|
|||Bottom-left: direct comparison with 2D CNN and 3D CNN, mmadadi is a 2D CNN, NAIST RV has the same structure but replaced 2D CNN with 3D CNN.|
|||tion has been more successful than including them within the CNN layers.|
|||The hand detector is built on U-net [34] to predict a binary hand-mask, which, after  2642  Method  Model  NAIST RV obj [56]  THU VCLab obj [3]  rvhand obj [1]  RCN-3D obj [23]  Hand-object segmentation CNN + pose estimation [56] Hand-object segmentation (intuitive) + pose estimation [3] Hand-object segmentation CNN + pose estimation [1] Two RCNs: Feature maps of first are used in the second RCNs stage 2.|
|||Robust 3D hand pose estimation in single depth images: from singleview cnn to multi-view cnns.|
|||End-toend global to local cnn learning for hand pose recovery in depth data.|
||15 instances in total. (in cvpr2018)|
|202|LCNN_ Lookup-Based Convolutional Neural Network|Our fastest LCNN offers 37.6 speed up over CNN while maintaining 44.3% top-1 accuracy.|
|||[31] shows how to reduce the redundancy in parameters of a CNN using a sparse decomposition.|
|||Only in [44] a sparse CNN is trained from scratch which makes it more similar to our approach.|
|||LCNN  A convolutional layer in a CNN consists of four parts: 1) the input tensor X  Rmwh; where m, w and h are the number of input channels, the width and the height, respectively, 2) a set of n weight filters, where each filter is a tensor W  Rmkw kh , where kw and kh are the width and the height of the filter, 3) a scalar bias term b  R for each filter, and 4) the output tensor Y  Rnwh ; where each channel Y[i,:,:]  Rwh is computed by W  X + b.|
||| is a hyperparameter that adjusts the trade-off between the CNN loss function and the l1 regularizer.|
|||For example, we can train our dictionaries on a shallow CNN and reuse in a deeper CNN with the same channel size.|
|||On the deeper CNN we only need to train the indices and coefficients tensors I and C.  4.|
|||given a set of novel categories with as small as 1 training example per category, our model is able to learn a classifier that is both faster and more accurate than the CNN baseline.|
|||Our proposed lookup based convolution is general and can be applied on any CNN architecture.|
|||Comparison between the performance of LCNN and CNN baseline on few-shot learning, for {1, 2, 4} examples per category.|
|||The accuracy is computed by top-1 measure and the speedup is relative to the original CNN model.|
|||7126  We compare the performance of CNN and LCNN on few-shot learning in Figure 4.|
|||We tried  = ,  =  10 ,  =  100 and  = 0 for both CNN and LCNN, then picked the best for each configuration.|
|||The test accuracy of LCNN is 16.2% higher than CNN at iteration 10K.|
|||The accuracy of LCNN is 16.2% higher than CNN at iteration 10K.|
||15 instances in total. (in cvpr2017)|
|203|Qiu_Learning_Spatio-Temporal_Representation_ICCV_2017_paper|Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation.|
|||However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand.|
|||Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%, respectively.|
|||One natural way to encode spatio-temporal information in videos is to extend the convolution kernels in CNN from 2D to 3D and train a brand new 3D CNN.|
|||As such, the model size is significantly reduced and the advantages of pre-learnt 2D CNN in image domain could also be fully leveraged by initializing the 1  3  3 convolutional filters with 3  3 convolutions in 2D CNN.|
|||To tackle this problem, 3D CNN proposed by Ji et al.|
|||devise a widely adopted 11layer 3D CNN (C3D) for learning video representation over 16-frame video clips in the context of large-scale supervised video datasets, and temporal convolutions across longer clips (100 frames) are further exploited in [33].|
|||However, the capacity of existing 3D CNN architectures is extremely limited with expensive computational cost and memory demand, making it hard to train a very deep 3D CNN.|
|||Hence, suppose we have 3D convolutional filters with size of 3 3 3, it can be naturally decoupled into 1 3 3 convolutional filters equivalent to 2D CNN on spatial domain and 3  1  1 convolutional filters like 1D CNN tailored to temporal domain.|
|||Such decoupled 3D convolutions can be regarded as a Pseudo 3D CNN, which not only reduces the model size significantly, but also enables the pre-training of 2D CNN from image data, endowing Pseudo 3D CNN more power of leveraging the knowledge of scenes and objects learnt from images.|
|||The former performs a CNN which is similar to the architecture in [14] on one single frame from each clip to predict a clip-level score and fuses multiple frames in each clip with different temporal extent throughout the network to achieve the clip-level prediction.|
|||We briefly group the approaches on UCF101 into three categories: end-to-end CNN architectures which are fine-tuned on UCF101 in the upper rows, CNN-based video representation extractors with linear SVM classifier in the middle rows and approaches fused with IDT in the bottom rows.|
|||It is worth noting that most recent end-to-end CNN architectures on UCF101 often employ and fuse two or multiple types of inputs, e.g., frame, optical flow or even audio.|
|||We group the approaches into three categories, i.e., end-to-end CNN architectures which are fine-tuned on UCF101 at the top, CNN-based video representation extractors with linear SVM classifier in the middle and approaches fused with IDT at the bottom.|
|||Accuracy  73.0% (88.0%) 71.3% (88.1%) 82.6% (88.6%) 82.6% (92.5%) 82.4% (91.7%) 84.5% (93.1%) 82.2% (93.4%) 85.7% (94.0%)  Method End-to-end CNN architecture with fine-tuning Two-stream ConvNet [25] Factorized ST-ConvNet [29] Two-stream + LSTM [37] Two-stream fusion [6] Long-term temporal ConvNet [33] Key-volume mining CNN [39] ST-ResNet [4] TSN [36] CNN-based representation extractor + linear SVM C3D [31] ResNet-152 P3D ResNet Method fusion with IDT IDT [34] C3D + IDT [31] TDD + IDT [35] ResNet-152 + IDT P3D ResNet + IDT  82.3% 83.5% 88.6%  85.9% 90.4% 91.5% 92.0% 93.7%  the accuracy of P3D ResNet can achieve 88.6%, making the absolute improvement over the best competitor TSN on the only frame input and ResNet-152 in the first and second category by 2.9% and 5.1%, respectively.|
||15 instances in total. (in iccv2017)|
|204|PoseAgent_ Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning|While [3, 17] use a simple pixel wise distance function, [12] propose a learned comparison: a CNN compares rendered and observed images and outputs an energy value representing a parameter of the posterior distribution in pose space.|
|||We use a similar CNN construction as Krull et al., feeding both rendered and observed image patches into to our CNN.|
|||[5] use Q-learning, in which the CNN predicts the quality of the available state-action pairs.|
|||The vector  of learnable parameters consists of CNN weights (described in Sec.|
|||The CNN architecture: a) The system takes a pose hypothesis H t a encoding the context and history of the pose as input.|
|||We use a CNN to predict both energies, Et a.|
|||In the next section, we discuss the CNN architecture and how it governs the behaviour of the agent.|
|||a; ) abbreviated by E t  a and E t  3.2.3 CNN Architecture  We give an overview of the CNN architecture used in this work in Fig.|
|||As in [12], the CNN compares rendered and observed images.|
|||Our CNN consists of the following layers: 128 kernels of size 633, 256 kernels of size 12833, a 22 maxpooling layer, 512 kernels of size 25633, a max-pooling operation over the remaining size of the image, finally 3 fully connected layers.|
|||The algorithm can process an arbitrary amount of sequences using only a single back propagation pass of the CNN for each possible hypothesis state st In a na ve implementation, a. the number of required forward-backward passes would increase linearly with the number of sampled sequences.|
|||Gradient Update Phase: We once more process each of the hypothesis states s a with the CNN and use standard .|
|||for k = 1 : M do sample path (s receive reward rk; for t = 1 : Tk, a = 1 : N do  1:Tk k  1:Tk k  , a  ) using E  a ;  D(a, a,k)D(a, a,k) +  E a  end  end  Gradient Update Phase:  ln  (cid:0)at  k|St  k; (cid:1) rk  for  = 0 : max , a = 1 : N do  calculate  via back propagation;  E a j  for all CNN parameters j do  G(j)  G(j) + E a j  1  m D(a,  );  end  end Output: G(j)   j  E [r];  Algorithm 1: Efficient Gradient Calculation  4.|
|||In all experiments with the baseline method [12], we use the identical CNN with the original weights trained by Krull et al.|
|||To estimate the variance of the gradient, we calculated the standard deviation of 1000 randomly selected elements from the resulting gradient vector of the CNN and averaged them.|
||15 instances in total. (in cvpr2017)|
|205|Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper|We train a CNN based architecture which can implicitly capture and aggregate local evidences for predicting the euler angles to obtain a viewpoint estimate.|
|||Local Appearance based Keypoint Activation : We propose a fully convolutional CNN based architecture to model local part appearance.|
|||We capture the appearance at multiple scales and combine the CNN responses across scales to obtain a resulting heatmap which corresponds to a spatial log-likelihood distribution for each keypoint.|
|||To recover an estimate of the global pose, we use a CNN based architecture to predict viewpoint.|
|||Whereas DPMs explicitly model part appearances and their deformations, the CNN architecture allows such relations to be captured implicitly using a hierarchical convolutional  structure.|
|||[12] argued that DPMs could also be thought as a specific instantiation of CNNs and therefore training an end-to-end CNN for the corresponding task should outperform a method which instead explicitly models part appearances and relations.|
|||[36] demonstrated that CNN based models can successfully be used for keypoint prediction  for humans and Tompson et al.|
|||However, we show that we can significantly improve their results by combining multiscale convolutional predictions from a trained CNN with a more principled, viewpoint estimation based global model.|
|||[11] and finetune a CNN model whose weights are initialized from a model pretrained on the Imagenet [5] classification task.|
|||Instead of training a separate CNN for each class, we implement a loss layer that selectively considers the Na  N outputs corresponding the class of the training instance and computes a logistic loss for each of the angle predictions.|
|||This allows us to train a CNN which can jointly predict viewpoint for all classes, thus enabling learning a shared feature representation across all categories.|
|||Multiscale Convolutional Response Maps  We use CNN based architectures to learn the appearance of keypoints across an object class.|
|||The CNN architecture we use has the convolutional layers from ONet followed by an additional convolution layer with the output size 12 12 Nkp such that each channel of the output corresponds to a specific keypoint of a particular class.|
|||To study the effect of endto-end training of the CNN architecture, we use a linear classifier on top of the fc7 layer of TNet as another baseline (denoted as fc7-TNet ).|
|||The performance of our CNN based approach for viewpoint prediction is shown in Table 2 and it can be seen that we significantly outperform the state-of-the-art methods across all categories.|
||15 instances in total. (in cvpr2015)|
|206|Song_Locally-Transferred_Fisher_Vectors_ICCV_2017_paper|However, by truncating the CNN model at the last convolutional layer, the CNN-based FV descriptors would not incorporate the full capability of neural networks in feature learning.|
|||Also, for texture classification, this FV-CNN descriptor shows higher classification performance than FC-CNN, which is the descriptor obtained from the penultimate fully connected layer of the CNN [7].|
|||These observations indicate that FV encoding is more ef 14912  fective than the encoding by the fully connected layers in the CNN pipeline.|
|||However, with FV-CNN, the benefit of CNN is not fully utilized since it is truncated at the last convolutional layer.|
|||To better incorporate the learning capability of a CNN model, there is a trend to create end-to-end learning by mimicking the handcrafted encoding in a CNN model.|
|||Therefore, instead of attempting to use a single CNN model to encompass the benefits of both FV encoding and neural network, it becomes a simpler problem to design the additional neural network model on top of FV-CNN descriptors.|
|||Related work  The current state-of-the-art approaches for texture classification include the one with FV-CNN descriptors [7] and the bilinear CNN (B-CNN) model [12].|
|||Another model, namely HistNet, is recently proposed to simulate the histogram / BOW encoding in the CNN model [33].|
|||These approaches are however less coupled with the CNN model and not designed for texture image classification.|
|||In a CNN sense, this input layer has a size of 1  1  (2KD)  N , with N as the batch size during training.|
|||The classification accuracies (%), comparing our LFV method with FV-CNN [7], FV-CNN computed with fine-tuned VGG-VD model (backpropagation to the last convolutional layer), FV descriptor generated with end-to-end CNN learning similar to the NetVLAD model (backpropagation to the FV layer), and B-CNN [12].|
|||The main computational expensive process is the application of the CNN model to compute the local features at multiple scales, requiring about 2 seconds per image.|
|||After the CNN local features are computed, the encoding of Fisher vectors need less than 1 minute for each dataset.|
|||Therefore, for a test image at run time, there is little additional cost to compute the FV-CNN descriptor compared to obtain a CNN feature at the last fully connected layer.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||15 instances in total. (in iccv2017)|
|207|Scale-Aware Face Detection|Prior to detection, an efficient CNN predicts the scale distribution histogram of the faces.|
|||The CNN based approaches have recently shown great successes [13, 39, 4].|
|||However, as for scale invariance, CNN meets the limitation that is similar to the limitation of translation invariance for fully-connected networks.|
|||The CNN does not inherently have scale invariance.|
|||A CNN can be trained to have certain extent of scale invariance, but it needs more  1x  1/2x  1/4x  1/8x  1/16x  FLOPS  5.6G  S i n g l e   s c a l e   d e t e c t o r  1.4G  350M  87.5M  21.9M  NMS  Figure 1.|
|||Another way to avoid this problem is to fit a CNN model to multiple scales.|
|||Related works  The CNN based face detection approaches emerged in 1990s [33].|
|||[31] shows that CNN achieves good performance for frontal face detection and [32] further extends it for rotation invariant face detection by training faces of different poses.|
|||The CNN based methods again become popular thank to their great performance advantages.|
|||Early works combine CNN based features with traditional features.|
|||[28] combines CNN with deformable part model and [37] combines CNN with channel feature [7].|
|||[19] proposes a CNN cascade for efficient face detection.|
|||[8] shows that simple fine-tuning the CNN model from ImageNet classification task for face/background classification leads to good performance.|
|||The SPN is a CNN with a global max pooling layer at its end so that it can produce a fixed-dimensional Scale Histogram Vector disregard the input size and face locations.|
|||Joint training of cascaded cnn for face detection.|
||15 instances in total. (in cvpr2017)|
|208|Peng_Tang_Weakly_Supervised_Region_ECCV_2018_paper|The first stage evaluates the objectness scores of sliding window boxes by exploiting the low-level information in CNN and the second stage refines the proposals from the first stage using a region-based CNN classifier.|
|||We train a region-based CNN classifier, which is a small WSOD network [38], using P 1, and adapt the network to distinguish whether P 1 are object or background regions instead of to detect objects.|
|||We do not use the region-based CNN classifier on the sliding window boxes directly, because this requires an enormous number of sliding window boxes to ensure high recall and it is hard for a region-based CNN classifier to handle such a large number of boxes efficiently.|
||| We propose a two-stage region proposal network for proposal generation in WSOD, where the first stage exploits the low-level information from the early convolutional layers to generate proposals and the second stage is a region-based CNN classifier to refine the proposals from the first stage.|
|||Unlike them, we use CNN to generate proposals, and refine proposals using region-based CNN classifiers.|
|||But unlike EB which adopts edge detectors trained on datasets with pixel-level edge annotations [13] to ensure high proposal recall, we exploit the low-level information in CNNs to generate edge-like responses, and use a region-based CNN classifier to refine the proposals.|
|||The works by [3, 33] also show that the different CNN layers contain different level visual information.|
|||The second stage Proposal Refinement uses a small region-based CNN classifier to re-evaluate the objectness scores of each proposal in P 1 to get refined proposals P 2.|
|||The third stage Weakly Supervised Object Detection uses a large region-based CNN classifier to classify each proposal in P 2 as different object classes or background, to produce the object detection results.|
|||Here we exploit the low-level information, more specifically the edgelike information from the CNN for this stage.|
|||Similar to the previous stage, we use a region-based CNN for classification, see Fig.|
|||Algorithm 1 Proposal network training Input: Training images with image-level annotations; an initial CNN network Minit.|
|||We choose the VGG16 network [36] pre-trained on ImageNet classification dataset [32] as our initial CNN network Mpre in Section 3.4.|
|||5 Conclusion  In this paper, we focus on the region proposal generation step for weakly supervised object detection and propose a weakly supervised region proposal network which generates proposals by CNN trained under weak supervisions.|
|||Our proposal network consists of two stages where the first stage exploits low-level information in CNN and the second stage is a region-based CNN classifier which distinguishes whether proposals are object or background regions.|
||15 instances in total. (in eccv2018)|
|209|cvpr18-Zoom and Learn  Generalizing Deep Stereo Matching to Novel Domains|At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts.|
|||The remarkable work, DispNet, proposed by Mayer et al [23], is the first end-to-end CNN approach for stereo matching, where an encoder-decoder architecture is employed for supervised learning.|
|||These works explore different CNN architectures tailormade for stereo matching.|
|||To mitigate this problem, some recent works proposed semi-/un-supervised approaches to train a CNN model for stereo matching (or its related problem, monocular depth estimation).|
|||In contrast, we do not rely on external models or setups: our self-supervised domain adaptation method lets the CNN discriminatively learn the useful details from its own finer-grain outputs.|
|||To utilize scale diversity while avoiding generalization glitches (as mentioned in Section 1), we embed iterative regularization into the CNN training process, making the model parameters improve gradually.|
|||We formulate the CNN training as an iterative optimization problem with graph Laplacian regularization.|
|||On one hand, we let the CNN learn its own finer-grain output; on the other hand, a graph Laplacian regularization is imposed to discriminatively retain the useful edges while smoothing out the undesired artifacts.|
|||For the same stereo pair, feeding its zoomed-in version to a stereo matching CNN leads to a disparity map with extra details.|
|||As a CNN becomes too short-sighted, it lacks non-local information to estimate a proper disparity map, and its performance start to decline.|
|||Specifically, it imposes that the value of sTLs, i.e., the graph Laplacian regularizer, should be small for the ground-truth patch s, where L  Rmm is the graph Laplacian matrix of graph G. Given a disparity map D produced by a deep stereo model, we compute the values of the graph Laplacian regularizers  for the patches on D. The obtained values are summed up as a graph Laplacian regularization loss for CNN training.|
|||However, it has appeared in the finer-grain patch ffine by virtue of scale diversity (Section 3.2), then A should also appear in fleft; otherwise the CNN cannot generate A on ffine.|
|||Hence, our graph Laplacian regularizer guides the CNN to only learn the meaningful details.|
|||We first study the following models:  (i) ZOLE: Generalize the pre-trained model for smart phone stereo pairs with our method;  (ii) ZOLE-S: Remove graph regularization and simply let the CNN iteratively learn its own finer-grain outputs;  (iii) DispNetC-80: Finetune the pre-trained model on the  FlyingThings3D-80 examples;  (iv) DispNetC: Released model pre-trained with Fly ingThings3D [23].|
|||To tackle this problem, we propose a self-adaption approach for CNN training without ground-truth disparity maps of the target domain.|
||15 instances in total. (in cvpr2018)|
|210|Weakly Supervised Affordance Detection|timate a depth map and surface normals for a scene and a single-label CNN for semantic segmentation.|
|||An approach based on a CNN has been proposed in [27].|
|||The second row shows the prediction of the CNN on an image of the test set.|
|||If the CNN is only trained on the keypoint annotations, the predictions are not very precise.|
|||The third row shows the estimated annotation for the training image after the prediction of the CNN was refined by Grabcut for each affordance class.|
|||The last row shows the prediction of the CNN trained on the refined annotations of the training set.|
|||, xn} and the corresponding labeling as Y = {yi,l} where yi,l  {0, 1} indicates if pixel xi is labeled by affordance class l. We denote the set of affordances as L.  In the fully supervised case, we train the CNN by opti mizing the log likelihood given by  J() = log P (Y |I; ) =  n  Xi=1XlL  log P (yi,l|I; ),  (1)  where  are the parameters of the CNN.|
|||This does not work in the multilabel case and we define the loss based on the sigmoid function:  P (yi,l|I; ) =  1  1 + exp (fi,l (yi,l|I; ))  ,  (2)  2798  where fi,l (yi,l|I; ) is the output of the CNN at pixel xi and affordance l without the softmax layer.|
|||In order to learn the parameters  of the CNN we maximize  n  log P (yi,l|I; ).|
|||(4)  max    Xi=1XlL  The predictions of the learned CNN are reasonable but not very precise as illustrated in the second row of Fig.|
|||(7)  Since we know from Zx if an affordance label l is present, we have P (l|I, Zx) = P (l|Zx) and  While this could be already considered as the final estimate Y to update the CNN as described in (4), we use Grabcut [31] to refine the labels for each affordance l independently.|
|||The final row shows the improved results of the CNN trained on the training images refined by Grabcut.|
|||UMD Part Affordance Dataset  (8)  5.1.1 Supervised Setting  In (7), P (Yl|I; ) denotes the probabilities which have been predicted by the CNN for the affordance class l. In order to obtain the final annotation Y for the training images we binarize the predictions by setting  yi,l =(1  0 otherwise.|
|||Since the Grabcut step is essential during training, we also evaluated if an additional refinement by Grabcut of the predictions of the CNN on the test images also improves the results.|
|||A multi-scale CNN for affordance In ECCV, pages 186201,  segmentation in RGB images.|
||15 instances in total. (in cvpr2017)|
|211|cvpr18-A Deeper Look at Power Normalizations|We also provide a probabilistic interpretation of these operators and derive their surrogates with well-behaved gradients for end-to-end CNN learning.|
|||Firstly, we propose a kernel formulation which combines feature vectors collected from the last convolutional layer of ResNet-50 together with so-called spatial location vectors, previously explored in [31, 35, 33] around 20112013, which contain spatial locations corresponding to feature vectors in the CNN feature maps.|
|||We pass an image (or patches) to CNN and extract feature vectors  from its last conv.|
|||Our approach differs in that we perform end-to-end learning in the CNN setting while RCD and dictionary learning constitute shallow architectures that perform worse than CNNs on the majority of classification tasks.|
|||We note that the Log-Euclidean distance and Power Normalization have been implemented in the CNN setting [25, 24, 39, 41] for the purpose of region classification.|
|||There has been also a revived interest in creating cooccurrence patterns in CNN setting similar in spirit to RCD.|
|||Approach [40] applies a fusion of two CNN streams via outer product in the context of the fine-grained image recognition.|
|||Another approach for face recognition [23] uses cooccurrences of CNN feature vectors and facial attribute vec 1We assume that the eigenvalue decomposition of large matrices (d =  4096) in CUDA BLAS is fast and efficientwhich is not the case.|
|||A recent approach [52] extracts feature vectors at two separate locations in feature maps and performs an outer product to form a CNN co-occurrence layer.|
|||We take a similar view on PN functions, however, we devise an end-to-end trainable CNN layer and derive new pooling functions with well-behaved derivatives.|
|||Let A  {n}nNA , B  {n}nNB be datapoints from two images A and B, and N =|NA| and N=|NB| be the numbers of data vectors e.g., obtained from the last convolutional feature map of CNN for images A and B.|
|||2, assume that datapoints A  {n}nNA and B  {n}nNB from two images A and B are given, N = |NA| and N = |NB| are the numbers of data vectors obtained from the last convolutional feature map of CNN for images A and B.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
|||Two-stream contextualized cnn for fine-grained image classification.|
||15 instances in total. (in cvpr2018)|
|212|BranchOut_ Regularization for Online Ensemble Tracking With Convolutional Neural Networks|Our algorithm employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers.|
|||For better regularization, a subset of branches in the CNN are selected randomly for online learning whenever target appearance models need to be updated.|
|||To deal with these challenges, we take an extremely simple strategy, BranchOut, which disregards a subset of the branches in the CNN chosen randomly for model update.|
||| Our network has a different number of fully connected layers in individual branches and maintains multi-level representations based on a CNN using the branches.|
|||Since the CNNs trained for image classification task may not be appropriate for visual tracking, MDNet [30] attempts to train a CNN using external tracking sequences in a multi-domain learning framework.|
|||Stochastic Learning for Regularization  Our main goal is to develop an ensemble tracking algorithm based on a CNN with multiple branches by a proper regularization.|
|||When we train a CNN with multiple branches, e.g., CNN in Figure 1, a subset of branches are selected randomly by a Bernoulli  ) is the binary label of x  distribution.|
|||After going through the forward pass of the multi-branch CNN parametrized by k (k = 1, .|
|||Tracking Algorithm  This section describes our tracking algorithm based on the CNN with multiple branches using BranchOut technique for stochastic ensemble.|
|||CNN Initialization  The CNN integrated in our tracking algorithm has three convolutional layers (CONV1-3), each of which is followed by a rectified linear unit (RELU) layer and a max pooling (MAXPOOL) layer as illustrated in Figure 1.|
|||Model Update Strategy  The CNN maintaining target appearance models needs to be adaptive to new training examples.|
|||In both cases, we timated target x train the CNN using the BranchOut technique with the training examples obtained from the recent  successful frames at which the estimated target scores are positive.|
|||3359  Algorithm 1 Stochastic ensemble tracking by BranchOut Require: CNN with K branches of FC layers parametrized  by  = {1, .|
|||We enlarge search space wildly if classification scores from CNN are below the predefined threshold for more than 10 frames in a row.|
|||Conclusion  We presented a novel visual tracking algorithm based on stochastic ensemble learning based on a CNN with multiple branches.|
||15 instances in total. (in cvpr2017)|
|213|Khan_Scene_Categorization_With_ICCV_2017_paper|Its integration within any CNN architecture is straightforward, and a unitary transformation could be achieved with insignificant additional computation load during the training and testing processes.|
|||Recent best performing methods on scene recognition either employ feature encoding approaches on CNN activations  [17, 22] or leverage from large-scale scene-centric datasets [67] and feature jittering [20].|
|||Note that the following analysis has particular relevance to CNN activations which have high dimensionality and somewhat low correlation beforehand.|
|||The asymptotic equivalence can then be established considering the HilbertSchmidt norm of the difference between Cn and its projec 35640    1  1  h  Input Feature Map  1  h  Spectral Transformation Layer  1  1  1  Output Feature Map  Figure 2: As shown, the spectral transformation is implemented as a convolutional layer in the CNN model.|
|||tion used in our CNN model.|
|||The spectral transform is implemented as a convolution layer which can be placed at any level in the CNN architecture.|
|||An illustration of the spectral transformation layer implementation in a CNN is given in Fig.|
|||Approach  Accuracy (%)  Approach  Accuracy (%)  OOM-semClusters [16]  CNN-MOP [17]  CNNaug-SVM [50] Hybrid-CNN [67] SSICA-CNN [20] Places-CNDS [60]  Deep Filter Banks [13]  Baseline CNN (Places-VGGnet [59]) Places VGGnet + Spectral Features  68.6 68.9 69.0 70.8 74.4 76.1 81.0  80.9 84.3  Table 1: Average accuracy on the MIT-67 Indoor Scene dataset.|
|||Places-205 Dataset [67] is a large-scale scene-centric  Places-AlexNet [67]  Places-GoogLeNet [53]  Places-CNDS [60]  Places-VGGnet-11 [59]  Baseline CNN (Places-VGGnet [59]) Places VGGnet + Spectral Features  50.0 55.5 55.7 59.0  60.3 61.2  Table 2: Average accuracy on the Places-205 Scene Dataset.|
|||(Best seen when enlarged)  Bedroom   Hotel room  Beach   Sandbar  Approach  Accuracy (%)  ImageNet-VGGnet-16 [51]  Hybrid-CNN [67]  Deep19-DAG CNN [65] MetaObject-CNN [63]  Places-CNDS [60]  Baseline CNN (Places-VGGnet [59]) Places-VGGnet + Spectral Features  51.7 53.8 56.2 58.1 60.7  66.9 67.6  Table 3: Average accuracy on the SUN397 Scene Dataset.|
|||Due to this trend and for the sake of a fair comparison with baseline and VGGnet based approaches that use a 4096dimensional feature dimension, we use an equidimensional spectral transform in the CNN model.|
|||Figure 8 illustrates portions from the data covariance matrix corresponding to the baseline CNN features and the spectral features.|
|||50  100  150  200  250  300  350  400  450  1000  1050  1100  1150  1200  1250  1300  1350  1400  1450  50  100  150  200  250  300  350  400  450  50  100  150  200  250  300  350  400  450  50  100  150  200  250  300  350  400  450  1000  1050  1100  1150  1200  1250  1300  1350  1400  1450  1000  1050  1100  1150  1200  1250  1300  1350  1400  1450  1000  1050  1100  1150  1200  1250  1300  1350  1400  1450  (a) Baseline CNN Features  (b) Spectral Features  Figure 8: Portions of covariance matrices corresponding to baseline CNN features and spectral features.|
|||We tested our approach on three large-scale scene-centric datasets and reported encouraging improvements on the baseline CNN features.|
|||Harvesting discriminative meta objects with deep cnn features for scene classification.|
||15 instances in total. (in iccv2017)|
|214|Bell_Material_Recognition_in_2015_CVPR_paper|We convert these trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field (CRF) to predict the material at every pixel in an image, achieving 73.1% mean class accuracy.|
|||(c) We transfer the weights to a fully convolutional CNN to efficiently generate a probability map across the image; we then use a fully connected CRF to predict the material at every pixel.|
|||We use this data for material recognition by training different CNN architectures on this new dataset.|
|||By replacing the fully connected layers of the CNN with convolutional layers [24], the computational burden is significantly lower than a naive sliding window approach.|
|||[3] developed a CNN and improved Fisher vector (IFV) classifier that achieves state-of-the-art results on FMD and KTH-TIPS2.|
|||Driven by the ILSVRC challenge [21], we have seen many successful CNN architectures [32, 24, 28, 27], led by the work of Krizhevsky et al.|
|||[6] use a multi-scale CNN to predict the class at every pixel in a segmentation.|
|||First, we train a CNN that produces a single prediction for a given input patch.|
|||Then, we convert the CNN into a sliding window and predict materials on a dense grid across the image.|
|||This example shows predictions from a single GoogLeNet converted into a sliding CNN (no average pooling).|
|||To detect near-duplicates, we compare AlexNet CNN features computed from each photo (see the supplemental for details).|
|||Patch material classification  In this section, we evaluate the effect of many different design decisions for training methods for material classification and segmentation, including various CNN architectures, patch sizes, and amounts of data.|
|||We then show that on MINC, a finetuned CNN is even better.|
|||Having found that SIFT IFV+fc7 is the new best on FMD, we compare it to a finetuned CNN on a subset of MINC (2500 patches per category, one patch per photo).|
|||This experiment shows that a finetuned CNN is a better method for MINC than SIFT IFV+fc7.|
||15 instances in total. (in cvpr2015)|
|215|Osherov_Increasing_CNN_Robustness_ICCV_2017_paper|Increasing CNN Robustness to Occlusions by Reducing Filter Support  Elad Osherov  Technion, Israel  Michael Lindenbaum  Technion, Israel  eladosherov@campus.technion.ac.il  mic@cs.technion.ac.il  Abstract  Convolutional neural networks (CNNs) provide the current state of the art in visual object classification, but they are far less accurate when classifying partially occluded objects.|
|||We start by studying the effect of partial occlusions on the trained CNN and show, empirically, that training on partially occluded examples reduces the spatial support of the filters.|
|||The availability of large amounts of annotated data [3], parallel computational resources such as GPUs, and regularization techniques [20, 9, 26] have contributed greatly to CNN performance.|
|||Our contribution in this paper is twofold: first we analyze the effect of training with occlusions on CNN visual classifiers, and in particular show the reduction of the filters spatial support, accompanied by an increase in its effective depth.|
|||Section 3 analyzes the CNN trained on occluded object examples.|
|||Measures of Spatial Support  We start by briefly describing the CNN structure we use, and the associated notations.|
|||A CNN is typically composed of several convolution layers, and then several fully connected layers topped by a classifier.|
|||The Effect of Occlusions on Spatial Support  We are interested in the change of effective spatial support caused by training the CNN on images of partially occluded objects.|
|||Training two CNN networks separately, one on occluded objects and one on unoccluded objects, would result in two sets of unrelated filters.|
|||This procedure produces two CNN networks with correspondence between the filters.|
|||In the context of CNN kernels, for a particular filter weight X in some kernel, we use the other filter weights in the same kernel to calculate the weight of X.|
|||In the first type, we trained 2 CNN networks as follows.|
|||We evaluated the recognition accuracy under partial occlusion by training a LeNet [14] CNN with different choices of the proposed regularization terms; see Table 3.|
|||We show that training with partially occluded objects reduces the spatial support of the CNN filters and increase their effective depth.|
|||Using substantial experimentation (on LeNet5), we came to the conclusion that latter layers in the CNN benefit from larger regularization strength values ().|
||15 instances in total. (in iccv2017)|
|216|Yang_Joint_Unsupervised_Learning_CVPR_2016_paper|Intuitively, clustering with representations from a CNN initialized with random weights are not  15147  reliable, but nearest neighbors and over-clusterings are often acceptable; 2) These over-clusterings can be merged as better representations are learned; 3) Agglomerative clustering is a recurrent process and can naturally be interpreted in a recurrent framework.|
|||We start with an intial over-clustering, update CNN parameters (2b) using image cluster labels as supervisory signals, then merge clusters (2a) and iterate until we reach a stopping criterion.|
|||To summarize, the major contributions of our work are:  1 We propose a simple but effective end-to-end learning framework to jointly learn deep representations and image clusters from an unlabeled image set;  2 We formulate the joint learning in a recurrent framework, where merging operations of agglomerative clustering are expressed as a forward pass, and representation learning of CNN as a backward pass;  3 We derive a single loss function to guide agglomerative clustering and deep representation learning, which makes optimization over the two tasks seamless;  4 Our experimental results show that  the proposed framework outperforms previous methods on image clustering and learns deep representations that can be transferred to other tasks and datasets.|
||| are the CNN parameters, based on which we obtain deep representations X = {x1, ..., xns} from I.|
|||2, at the timestep t, images I are first fed into the CNN to get representations X t and then used in conjunction with previous hidden state ht1 to predict current hidden state ht, i.e, the image cluster labels at timestep t. In our context, the output at timestep t is yt = ht.|
|||Hence, at timestep t  X t = fr(I|t)  ht = fm(X t, ht1) yt = fo(ht) = ht  (5a)  (5b)  (5c) where fr is a function to extract deep representations X t for input I using the CNN parameterized by t, and fm is a merging process for generating ht based on X t and ht1.|
|||The intuitive reason we unroll partially is that the representation of the CNN at the beginning is not reliable.|
|||We need to update CNN parameters to obtain more discriminative representations for the following merging processes.|
|||In each period, we merge a number of clusters and update CNN parameters for a fixed number of iterations at the end of the period.|
|||An extreme case would be one timestep per period, but it involves updating the CNN parameters too frequently and is thus timeconsuming.|
|||5150  Algorithm 1 Joint Optimization on y and   Input:  I: = collection of image data; n  c : = target number of clusters;  Output:  y, : = final image labels and CNN parameters;  1: t  0; p  0 2: Initialize  and y 3: repeat 4:  Update yt to yt+1 by merging two clusters if t = te  p then  Update p to p+1 by training CNN p  (p + 1)  5:  6:  7:  8:  end if t  t + 1  9: 10: until Cluster number reaches n c 11: y  yt;   p  The intuition behind reformulation of the loss is that agglomerative clustering starts with each datapoint as a cluster, and clusters at a higher level in the hierarchy are formed by merging lower level clusters.|
|||We train a CNN based on our approach on one dataset, and then cluster images from another (but related) dataset using the image features extracted via the CNN.|
|||We implement our approach to train CNN model and cluster images on the training set.|
|||Then, we remove the L2-normalization layer and append a softmax layer to finetune our unsupervised CNN model based on the predicted image cluster labels.|
|||Using the same training samples and CNN architecture, we also train a CNN model with a softmax loss supervised by the groundtruth labels of the training set.|
||15 instances in total. (in cvpr2016)|
|217|cvpr18-Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks|Another CNN is used to learn the weights for the RNN at every location.|
|||The first one is that weights of the CNN are spatially invariant.|
|||It is hard to use a CNN with a small model size to approximate the dynamic scene deblurring problem, which has the spatially variant property (see Figure 1).|
|||[33] propose a deep CNN model to estimate the motion blur of every patch.|
|||[7] propose a deeper CNN to estimate the motion flow without post-processing.|
|||In addition, it is difficult to use a single CNN model to deal with different blurs.|
|||Thus, if we use a CNN to approximate (3) (which means that the CNN actually learns the weights of y in (3)), where the basic operations of CNN are convolution and non-linear activation, a large receptive field should be considered to cover the positions that are used in (3).|
|||It shows that by adding a CNN after the RNN,  CNN   RNN   (a)   (b)   (c)   Figure 3.|
|||(b) is the receptive field by adding a 1  1 CNN after the RNN to fuse the information from the RNN.|
|||One CNN is used in image reconstruction to estimate the final deblurred image.|
|||See Table 1 for detailed CNN configurations.|
|||[19] develop a hybrid network including a CNN and RNN for image processing.|
|||[7] develop CNN algorithms to estimate motion blur, both of them need a conventional non-blind deblurring algorithm to generate the final clean image, which increases the computational cost.|
|||The proposed network is compared with the network without skip links, the network without CNN between RNNs, the network without RNNs as well as only using the weight generation network structure to deblur.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||15 instances in total. (in cvpr2018)|
|218|cvpr18-Optical Flow Guided Feature  A Fast and Robust Motion Representation for Video Action Recognition|It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously.|
|||Since CNNs have achieved great successes in image classification and other related tasks [20, 30, 34, 15, 49, 51, 25], lots of CNN based methods have     0  + Figure 1.|
|||Moreover, 3D convolutions on RGB input can also capture temporal information, but the RGB-based 3D CNN still does not perform on par with its two-stream version.|
|||Actually the OFF unit only consists of pixelwise operators on CNN features.|
|||This pattern of disappearing at one location and emerging at another location can be easily treated as a specific motion pattern and captured by later CNN layers.|
|||Taking only RGB from videos, experimental results show that the CNN with OFF is close in performance when compared with the state-of-the-art optical flow based algorithms.|
|||As a significant breakthrough in action recognition, TwoStream based frameworks used the deep CNN to learn from the hand-craft motion features like optical flow and iDT [29, 41, 50, 43, 9, 47, 5, 35, 11, 12].|
|||Many approaches learn to capture the motion information directly from input frames using 3D CNN [35, 37, 5, 36, 9, 38].|
|||Boosted by the temporal convolution and pooling operations, 3D CNN could distill the temporal information between consecutive frames without segmenting them into short snippets.|
|||Therefore, current state-of-the-art 3D CNN based algorithms still rely on traditional optical flow to help the networks to capture motion patterns.|
|||The feature generation sub-network generates basic features using common CNN structures.|
|||The OFF unit can be applied for CNN layers on different levels.|
|||The other methods we compared here includes TSN [43] with different inputs, motion vector based RGB+EMV-CNN [50], dynamic image based CNN [3] and current state-of-the-art 3D-CNN with two stream [5].|
|||By plugging the OFF into CNN framework, the result with only RGB as input on UCF-101 is even comparable to the result obtained by Two-Stream (RGB+Optical Flow) approaches, and at the same time, the OFF plugged network is still very efficient with the speed over 200 frames per second.|
|||Based on this representation, we proposed an new CNN architecture for video action recognition.|
||15 instances in total. (in cvpr2018)|
|219|Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper|In this work, we address these two issues by (i) using the features from hierarchical layers of CNNs rather than only the last layer to represent targets; (ii) learning adaptive correlation filters on each CNN layer without the need of sampling.|
|||This observation suggests that reasoning with multiple layers of CNN features for visual tracking is of great importance as semantics are robust to significant appearance variations and spatial details are effective for precise localization.|
|||Convolutional layers of a typical CNN model, e.g., AlexNet [20], provide multiple levels of abstraction in the feature hierarchies.|
|||The main differences lie in the use of learned CNN features rather than hand-crafted features (e.g., HOG [5] or color-attributes [7]) and we construct multiple correlation filters on hierarchical convolutional layers as opposed to only one single filter by existing approaches.|
|||[21] construct multiple CNN classifiers on different instances of target objects to rule out noisy samples during model update.|
|||Visualization of the CNN features of an image with a horizontal step edge.|
|||DeepTrack [21] learns two-layer CNN classifiers from binary samples and does not require a pre-training procedure.|
|||We extract CNN features using the VGG-Net [25], which is trained on the large-scale ImageNet dataset [8] with category-level label.|
|||We show in Figure 2 an image of a horizontal step edge and visualize the CNN features on the third, fourth, and fifth convolutional layers, where the fifth convolutional layer is less effective to locate the sharp boundary due to its low spatial resolution while the third layer is more useful to locate it precisely.|
|||Proposed Algorithm  In this section, we first present the CNN features used in this work, technical details on learning linear correlation filters, and the coarse-to-fine searching strategy.|
|||Along with the CNN forward propagation, the semantical discrimination between objects from different categories is strengthened, as well as a gradual reduction of spatial resolution for precise localization (See also Figure 1).|
|||However, this involves solving a D  D linear system of equations per location at (m, n), which is computationally expensive as the channel number is usually large with the CNN features (e.g., D = 512 in the conv5-4 and conv4-4 layers in the VGG-Net).|
|||This suggests CNN features (e.g., VGG-Net) learned with category-level supervision are more effective to discriminate targets from background.|
|||However, such concatenation breaks down the hierarchies over CNN layers and thus does not perform well for visual tracking.|
|||In addition, the tracking performance is improved with hierarchically inference on the translation     cues using multiple CNN layers.|
||15 instances in total. (in iccv2015)|
|220|Fractal Dimension Invariant Filtering and Its CNN-Based Implementation|The FDIF can be efficiently and approximately re-instantiated via a CNN in (c).|
|||This work provides us with a geometrical interpretation of CNN based on local fractal analysis of image.|
|||Our method connects traditional handcrafted filter-based curve detector with a CNN architecture.|
|||This connection also allows us to instantiate a new predefined CNN that can work in an unsupervised setting, different from most of its peers known for their ravenous appetite for labeled data.|
|||Currently, the physical meanings of different CNN modules are not fully comprehended.|
|||As a result, the architecture of the proposed CNN is shown in Figs.|
|||Similar to the iterative FDIF framework, we can also add a sigmoid layer to the end of the CNN and train the model via traditional backpropagation algorithm, or apply a thresholding layer for the final output.|
|||In contrast to many CNN models with a disadvantage of their ravenous  3495  Algorithm 2 FraCNN-based Curve Detector  1: Input: Image f (X), filter bank F, layer number N .|
|||appetite for labeled training, we believe the adaptability for unlabeled data of our method is perhaps due to the fact that we instantiate our tailored CNN from the fractal-based geometry perspective.|
|||FIDF: The proposed CNN model can be viewed as a fast implementation of FIDF.|
|||As a result, the computational complexity of original FIDF is O(|X||B|3 +|X|R3), where the first term corresponds to adaptive filtering and the second term corresponds to local fractal dimension estimation (and R is the number of scales in Algorithm 1), while the complexity of proposed CNN is at most O(|X||B|L), where L is the number of filters in the filter bank.|
|||Both of our fractalbased CNN and the SCN can apply predefined filters and are suitable for unsupervised learning when labels are not available.|
|||First, SCNs aim at extracting discriminative feature for image recognition and classification, while our Fractal-based CNN model focuses on low-and middle-level vision problems, i.e., curve detection.|
|||For further demonstrating the superiority of our method, we consider the following competitors: the curve and line segment detector (ELSD) in [28]; the traditional Frangi filtering-based curve detector [11] the simple logistic regression LR using patches as features directly; the classical CNN so-called LeNet [18]; the state-of-art holisticallynested edge detector (HED) [42].|
|||The model is also re-implemented from a CNN interpretation.|
||15 instances in total. (in cvpr2017)|
|221|Passalis_Learning_Bag-Of-Features_Pooling_ICCV_2017_paper|In contrast to other global pooling operators and CNN compression techniques the proposed method utilizes a trainable pooling layer that it is end-to-end differentiable, allowing the network to be trained using regular back-propagation and to achieve greater distribution shift invariance than competitive methods.|
|||Despite their great success, most of the proposed CNN formulations are unable to handle arbitrary sized images, as they operate on a fixed input size, restricting the CNN into accepting images of a specific size and leading to a constant cost for feed-forwarding the network (the number of floating point operations (FLOPs) needed to feed-forward the network depends on the size of the input image).|
|||Therefore, an already trained CNN cannot be directly deployed in a lower computational resource scenario, i.e., an embedded real-time system on a drone or a mobile device, or adapt to the available computational resources, without completely replacing the fully connected layer and retraining it.|
|||The feature maps extracted from the last convolutional layer of a CNN can be converted to feature vectors as follows.|
|||This can reduce the size of the fully connected layer and allows to operate the CNN using images of any size.|
|||The representation learned by the convolutional layers of a CNN is constantly altering during the training prohibiting standard BoF quantization techniques, such as k-means quantization, to be used effectively.|
|||This allows for directly training CNN models with less parameters (instead of compressing them after training) as well as natively handling differently sized images.|
|||Note that the proposed CBoF model can be readily combined with any CNN compression technique to further decrease the size of the model.|
|||The proposed method is also compared to a regular CNN as well as to the SPP technique.|
|||The CNN method requires O(NiNF NH + NH NC) weights after the last convolutional layer, the SPP method requires O(NSNF NH + NH NC) weights, while the proposed CBoF method requires O(NSNKNF +NSNKNH + NH NC) weights.|
|||The proposed CBoF outperforms both the CNN and the GMP/SPP techniques, while using significantly less parameters after the convolutional layers (about one order of magnitude less parameters than a similarly performing CNN).|
|||For the CNN, a 1000  15 fully connected layer is added after the last pooling layer, while the GMP/SPP layer is added between the last convolutional and the fully connected layers Note that the fully connected layer of the pretrained network is not used [31], leading to slightly different reported results for the CNN method.|
|||Again, the proposed CBoF method outperforms both the CNN and the GMP/SPP techniques while using significantly less parameters.|
|||In contrast to CNN compression techniques, that simply compress an already trained CNN, the proposed method provides an improved CNN architecture that is endto-end differentiable and it is invariant to mild distribution shifts.|
|||Nonetheless, the proposed technique can be combined with CNN compression techniques, such as [5, 6, 29], to further reduce the size of the resulting network.|
||15 instances in total. (in iccv2017)|
|222|Konda_Reddy_Mopuri_Ask_Acquire_and_ECCV_2018_paper|Stage-I, Ask and Acquire is about the class impression generation from the target CNN model and Stage-II, Attack is training the generative model that learns to craft UAPs using the class impressions obtained in the first stage.|
|||We experimented on a variety of CNN architectures trained to perform object recognition on the ILSVRC [23] dataset.|
|||The generator (G) architecture is unchanged for different target CNN architectures and separately learned with the corresponding class impressions.|
|||We typically consider the softmax layer of the target CNN for extracting the embeddings.|
|||It is the percentage of data samples (x) for which the target CNN predicts a different label upon adding the UAP (v).|
|||Note that, in each row, entry where the target CNN matches with the network under attack represents white-box attack and the rest represent the black-box attacks.|
|||Sample universal adversarial perturbations (UAP), learned by the proposed framework for different networks, the corresponding target CNN is mentioned below the UAP.|
|||Note that the right most column shows the mean success rates achieved by the individual generator networks (G) obtained across all the 6 CNN models.|
|||GC GV 16 GR152 GE  MBBSR 60.34 61.46 52.43 68.52  We replace the single target CNN with an ensemble of three models: CaffeNet, VGG-16 and ResNet-152 and learn GE using the fooling and diversity losses.|
|||In our case, since we generate UAPs, we investigate if the interpolation has smooth visual changes and the intermediate UAPs can also fool the target CNN coherently.|
|||4.7 Adversarial Training  We have performed adversarial training of target CNN with 50% mixture of clean and adversarial samples crafted using the learned generator (G).|
|||Note that the improvement is minor and the target CNN is still vulnerable.|
|||After repeating this for multiple iterations, we observe that adversarial training does not make the target CNN significantly robust.|
|||In this paper, we have extracted the class impressions as proxy data samples to train a generative model that can craft UAPs for the given target CNN classifier.|
|||: CNN fixations: An unraveling approach to visualize the discriminative image regions.|
||15 instances in total. (in eccv2018)|
|223|Kim_Learning_to_Select_CVPR_2016_paper|The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitkens delta-squared process, which accelerates convergence of fixed point update.|
|||Introduction  Image representations from deep CNN models trained for specific image classification tasks turn out to be powerful even for general purposes [2, 6, 7, 21, 23] and useful for transfer learning or domain adaptation.|
|||We address a problem to select the best CNN out of multiple candidates as shown in this figure.|
|||In addition, when multiple pre-trained deep CNN models are available, it is unclear which pre-trained models are appropriate for target tasks and which classifiers would maximize accuracy and efficiency.|
|||More importantly, Bayesian LS-SVM provides an effective solution to select the best CNN out of multiple  5318  candidates and identify a good ensemble of heterogeneous CNNs for performance improvement.|
|||Refining a pre-trained CNN is called fine-tuning, where the architecture of the network may be preserved while weights are updated based on new training data.|
|||Although several deep CNN models trained on large scale image repositories are publicly available, there is no principled way to select a CNN out of multiple candidates and find the best ensemble of multiple CNNs for performance optimization.|
|||Bayesian LS-SVM for Model Selection  This section discusses a Bayesian evidence framework to select the best CNN model(s) in the presence of transferable multiple candidates and identify a reasonable regularization parameter for LS-SVM classifier automatically.|
|||Our goal is to identify the best performing deep CNN model among the M networks for transfer learning.|
|||Another option is to replace some of fully connected layers in a CNN with an off-the-shelf classifier such as SVM and check the performance of target task through parameter tuning for each network, which would also be computationally expensive.|
|||(12)  We compute the overall evidence corresponding to each deep CNN model, and choose the model with the maximum evidence for transfer learning.|
|||Fast Bayesian LS-SVM  Bayesian evidence framework discussed in Section 3 is useful to identify a good CNN for transfer learning and a reasonable regularization parameter.|
|||The capability to select the appropriate CNN model and the corresponding regularization parameter is one of the most important properties of our algorithm.|
|||In addition, Bayesian LS-SVM selects the proper CNN for each task by using the evidence, which is denoted by bold-faced numbers.|
|||Conclusion  We described a simple and efficient technique to transfer deep CNN models pre-trained on specific image classification tasks to another tasks.|
||15 instances in total. (in cvpr2016)|
|224|Chung_A_Two_Stream_ICCV_2017_paper|The main contributions of this paper are:   We propose a two stream CNN architecture where each stream is a Siamese network.|
|||Until recently, CNN architectures [32] have not been used for the ReID due to the small size of public datasets.|
|||In [36], multi channels part-based CNN is proposed to jointly learn both global and local body features of the person.|
|||Instead of using a single network to learn both spatial and temporal features, we propose the use of a two stream CNN architecture where each stream is a separate Siamese network.|
|||Therefore, we propose a two stream Siamese CNN which processes spatial and temporal information separately.|
|||We refer to this CNN as the base CNN and describe its structure in Section 3.2.|
|||The Base CNN Architecture  Conv 1  5x5x16  Conv 2  5x5x32  Conv 3  5x5x32  ( pad 4, stride 2 )  ( pad 4, stride 2 )  ( pad4, stride 2 )  tanh  tanh  tanh  2x2 max pool  2x2 max pool  2x2 max pool  full  128  Dropout  f  Figure 3: The structure of the base CNN and hyperparameters  As shown in Figure 2, the input sequence Ic is processed using the base CNN.|
|||Figure 3 shows the base CNN structure and the hyper-parameters associated with it.|
|||The CNN takes one input sample (S(t) or T (t)) and produces the output feature vector f S or f T for SpatialNet and TemporalNet, respectively.|
|||Our base CNN is composed of three convolution layers where each layer has convolution, non-linear activation and max-pooling steps.|
|||For the SpatialNet, the base CNN processes a single RGB frame out of the sequence of frames in the multi-shot scenario, or optical flow content in the case of the TemporalNet.|
|||If we denote the base CNN by the function C(), then the temporally pooled feature vector,  fic , is computed as follows:   fic =  1 L  L  Xt=1  C(I (t) ic )  (3)  where i is the person ID, c  {a, b} is the camera view and I (t) ic , t  1, .|
|||Note that the softmax matrix W is the matrix representation of the fully connected layer in the base CNN architecture.|
|||This also can verify the improvement gained by the use of a two stream CNN architecture.|
|||[36] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, Person re-identification by multi-channel parts-based cnn with improved triplet loss function, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.|
||15 instances in total. (in iccv2017)|
|225|Xie_Hyper-Class_Augmented_and_2015_CVPR_paper|Most recent work exploiting deep CNN for image recognition with small training data adopts a simple strategy: pre-train a deep CNN on a large-scale external dataset (e.g., ImageNet) and fine-tune on the small-scale target data to fit the specific classification task.|
|||In this paper, beyond the fine-tuning strategy, we propose a systematic framework of learning a deep CNN that addresses the challenges from two new perspectives: (i) identifying easily annotated hyper-classes inherent in the fine-grained data and acquiring a large number of hyper-class-labeled images from readily available external sources (e.g., image search engines), and formulating the problem into multitask learning; (ii) a novel learning model by exploiting a regularization between the fine-grained recognition model and the hyper-class recognition model.|
|||However, because it is often expensive to obtain a large number of labeled images in fine-grained image classification tasks, it can be difficult to train a good deep CNN on a small dataset without suffering from significant overfitting.|
|||Several works have used the ImageNet dataset (containing 1.2 million images from 1,000 classes) to pre-train a deep CNN and then directly use the resulting CNN to extract features that are then directly used in a fine-grained image recognition task at hand 1.|
|||In this paper, we propose a principled framework to explicitly tackle the challenges of learning a deep CNN for FGIC.|
|||[23], the authors developed a large deep CNN with 5 convolutional layers and 3 fully connected layers.The similar deep learning architecture has achieved state-of-the-art performance on other visual tasks, including face recognition [34], object detection [30, 17], and human pose estimation [35].|
|||Our implementation is based on Krizhevskys widely used CNN model, but the proposed learning framework can apply to any deep learning architecture.|
|||Nevertheless, as far as we know, there has been no study on designing a deep CNN for FGIC to explicitly model large  intra-class variance.|
|||Therefore the available labeled training data is usually insufficiently large to train a deep CNN without overfitting.|
|||To address the second challenge, we propose a novel deep CNN model that can fully utilize the hyper-class labeled augmented data.|
|||Baselines  We compare our approach with three baselines:  (i) ImageNet-Feat-LR, which learns a multinomial logistic regression (LR) classifier on the activation features extracted using a deep CNN pre-trained on ImageNet data; (ii) the CNN baseline trained directly on the given fine-grained images.|
|||For our approach, we report two results when training from scratch on the target data: one for hyper-class augmented deep CNN (HACNN) and another for hyper-class augmented and regularized deep CNN (HAR-CNN) for examining the effect of the two components.|
|||The relative improvement (6% for HACNN and 7% for HAR-CNN) compared to a CNN baseline validates our idea.|
|||Note that when enough data is available, a vanilla CNN can perform surprisingly well on this difficult task without any engineering tricks.|
|||This is the largest dataset for car recognition up to date  Method  Accuracy (%)  ImageNet-Feat-LR  CNN  HA-CNN (ours) HAR-CNN (ours)  42.8 81.6 82.4 83.6  From the results on all datasets, we can observe that (i) the features exacted from a pre-trained CNN on the present ImageNet data may not be suited for fine-grained classification; (ii) the proposed HAR-CNN dramatically improves the performance of fine-grained classification on smallscale datasets; (iii) exploiting the regularization between the fined-grained classes and hyper-classes further helps improve the generalization.|
||15 instances in total. (in cvpr2015)|
|226|Jue_Wang_Learning_Discriminative_Video_ECCV_2018_paper|For every video sequence (as CNN features), our scheme generates a positive bag (with these features) and a negative bag by adding adversarial perturbations to the features.|
|||While, better CNN architectures, such as the recent I3D framework [8], is essential for pushing the state-of-the-art on video tasks, it is also important to have efficient representation learning schemes that can capture the long-term temporal video dynamics from predictions generated by a temporally local model.|
|||[35] that show the existence of quasi-imperceptible image perturbations that can fool a well-trained CNN model.|
|||Precisely, suppose X denotes our dataset, let h be a CNN trained on X such that h(x) for x  X is a class label predicted by h. Universal perturbations are noise vectors o found by solving the following objective:  min  o  kok s.t.|
|||Differently to their work, we aim to learn a UAP on high-level CNN features as detailed in Alg.|
|||While one could use a simple discriminative classifier, such as described in (1) to achieve this, such a linear classifier might not be sufficiently powerful to separate the potentially non-linear CNN features and their adversarial perturbations.|
|||In this case, CNN backpropogation would need gradients with respect to the solutions of an argmin problem defined in (4), which may seem difficult.|
|||However, there exist well-founded techniques [12], [15][Chapter 5] to address such problems, specifically in the CNN setting [23] and such techniques can be directly applied to our setup.|
|||Differently from [35], we generate the perturbation in the shape of the high level CNN feature instead of an RGB image.|
|||We review below our the datasets, their evaluation protocols, the CNN features next.|
|||10  J. Wang and A. Cherian  4.1 Datasets, CNN Architectures, and Feature Extraction  HMDB-51 [29]: is a popular video benchmark for human action recognition, consisting of 6766 Internet videos over 51 classes; each video is about 20  1000 frames.|
|||[39] as our baseline in which a temporal CNN (with residual units) is applied on the raw skeleton data.|
|||On these two datasets, we simply apply our pooling method on the CNN features extracted from the pre-trained model.|
|||Qualitative Results: In Figure 4, we visualize the hyperplanes that our scheme produces when applied to raw RGB frames from HMDB-51 videos  i.e., instead of CNN features, we directly feed the raw RGB frames into our DSP, with adversarial noise generated as suggested in [35].|
|||Assuming the video frames are encoded as CNN features, such perturbations are often seen to affect vulnerable parts of the features.|
||15 instances in total. (in eccv2018)|
|227|Liu_Deep_Convolutional_Neural_2015_CVPR_paper|Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF.|
|||Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework.|
|||Despite all the successes in classification problems, deep CNN has been less explored for structured learning problems, i.e., joint training of a deep CNN and a graphical model, which is a relatively new and not well addressed problem.|
|||We here bridge this gap by jointly exploring CNN and continuous CRF.|
|||To sum up, we highlight the main contributions of this  work as follows:   We propose a deep convolutional neural field model for depth estimations by exploring CNN and continuous CRF.|
||| We jointly learn the unary and pairwise potentials of the CRF in a unified deep CNN framework, which is trained using back propagation.|
|||By jointly exploring the capacity of CNN and continuous CRF, our method outperforms state-of-the-art methods on both indoor and outdoor scene depth estimations.|
|||However, our method differs critically from theirs: they directly regress the depth map from an input image through convolutions; in contrast we use a CRF to explicitly model the relations of neighboring superpixels, and learn the potentials (both unary and pairwise) in a unified CNN framework.|
|||present a hybrid architecture for jointly training a deep CNN and an MRF for human pose estimation.|
|||The unary part then takes all the image patches as input and feed each of them to a CNN and output an n-dimentional vector containing regressed depth values of the n superpixels.|
|||Potential functions Unary potential The unary potential is constructed from the output of a CNN by considering the least square loss:  U (yp, x; ) = (yp  zp())2, p = 1, ..., n.  (5)  Here zp is the regressed depth of parametrized by the CNN parameters .  the superpixel p  The network architecture for the unary part is depicted in Fig.|
|||Our CNN model in Fig.|
|||2 using a CNN model trained on the ImageNet from [22].|
|||The proposed method combines the strength of deep CNN and continuous CRF in a unified CNN framework.|
|||1  [13] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, CNN features off-the-shelf: An astounding baseline for recognition, in Proc.|
||15 instances in total. (in cvpr2015)|
|228|Ng_Beyond_Short_Snippets_2015_CVPR_paper|The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task.|
|||Like feature-pooling, LSTM networks operate on frame-level CNN activations, and can learn how to integrate information over time.|
|||We propose CNN architectures for obtaining global video-level descriptors and demonstrate that using increasing numbers of frames significantly improves classification performance.|
|||Instead of trying to learn spatio-temporal features over small time periods, we consider several different ways to aggregate strong CNN image features over long periods of a video (tens of seconds) including feature pooling and recurrent neural networks.|
|||Approach  Two CNN architectures are used to process individual video frames: AlexNet and GoogLeNet.|
|||AlexNet, is a Krizhevsky-style CNN [15] which takes a 220  220 sized frame as input.|
|||In order to make learning computationally feasible, in all methods CNN share parameters across frames.|
|||Figure 4: Deep Video LSTM takes input the output from the final CNN layer at each consecutive video frame.|
|||To reduce CNN training time, the parameters of AlexNet and GoogLeNet were initialized from a pre-trained ImageNet model and then fine-tuned on Sports-1M videos.|
|||Since pooling is performed after CNN towers that share weights, the parameters for a single-frame and multi-frame max-pooling network are very similar.|
|||Comparison of CNN Architectures: AlexNet and GoogLeNet single-frame CNNs (Section 3) were trained from scratch on single-frames selected at random from Sports-1M videos.|
|||The increased accuracy is likely due to advances in CNN architectures and sampling more frames per video when training (300 instead of 50).|
|||Conclusion  We presented two video-classification methods capable of aggregating frame-level CNN outputs into video-level predictions: Feature Pooling methods which max-pool local information through time and LSTM whose hidden state evolves with each subsequent frame.|
|||If speed is  Improved Dense Trajectories (IDTF)s [23] Slow Fusion CNN [14] Single Frame CNN Model (Images) [19] Single Frame CNN Model (Optical Flow) [19] Two-Stream CNN (Optical Flow + Image Frames, Averaging) [19] Two-Stream CNN (Optical Flow + Image Frames, SVM Fusion) [19] Our Single Frame Model Conv Pooling of Image Frames + Optical Flow (30 Frames) Conv Pooling of Image Frames + Optical Flow (120 Frames) LSTM with 30 Frame Unroll (Optical Flow + Image Frames)  3-fold Accuracy (%) 87.9 65.4 73.0 73.9 86.9  88.0  73.3 87.6  88.2  88.6  Table 7: UCF-101 results.|
||14 instances in total. (in cvpr2015)|
|229|Chen_ASP_Vision_Optically_CVPR_2016_paper|ASPs replace both image sensing and the first layer of a conventional CNN by directly performing optical edge filtering, saving sensing energy, data bandwidth, and CNN FLOPS to compute.|
|||The advent of GPU computing has allowed CNN training on large, public online data sets, and triggered an explosion of current research.|
|||In particular, we use existing Angle Sensitive Pixels (ASPs) [53], bio-inspired CMOS image sensors that have Gabor wavelet impulse responses similar to those in the human visual cortex, to perform optical convolution for the CNN first layer.|
|||We call this combination of ASP sensor with CNN backend ASP Vision.|
|||Note that we are neither introducing ASPs for the first time nor claiming a new CNN architecture.|
|||Since this paper does not improve CNN accuracy or propose new networks, we highlight recent work on real-time performance and resource efficiency.|
|||[29] propose a new analog-to-digital converter for image sensors that performs CNN image classification directly to avoid the I/O bottleneck of sending high resolution images to the processor.|
|||The custom image sensor is composed of Angle Sensitive Pixels which optically computes the first layer of the CNN used for visual recognition tasks.|
|||Hardcoding the First Layer of CNNs  In partitioning a deep learning pipeline, a central question is what layers of the CNN should be implemented in hardware versus software.|
|||In particular, the first layer learned by most CNN architectures consists of oriented edge filters, color blobs, and color edges (as visualized AlexNets [23] first layer in Figure 2(a)).|
|||For a baseline, we use LeNet [6] which is a five layer CNN with both 20 and 12 first-layer filters to achieve 99.12%and 99.14% percent respectively.|
|||Kernel  7  7  96  7  7  12  5  5  192  5  5  12  5  5  20  5  5  12  FLOPS of First Layer  708.0M  88.5 M  14.75M  921.6K  392 K  Total FLOPS  6.02G  3.83 G  200.3M  157 M  First Layer FLOPS Saving  11.76%  2.3%  7.36%  0.6%  10.4 M  3.77%  235 K  8.8 M  2.67%  Table 2: Network Structure and FLOPS: Common CNN architectures such as VGG-M-128 [4], NiN [31], LeNet [6] are compared for the FLOPS savings from optically computing the first layer of these networks.|
|||We first look at different CNN architectures and their potential savings from optically computing the first layer shown in Table 2.|
|||This might be partly because the CNN learns to work with the filters it is given in the first layer.|
||14 instances in total. (in cvpr2016)|
|230|Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs|In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features.|
|||We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework.|
|||It implies that a spectral CNN model learned on one graph cannot be trivially transferred to another graph with a different Fourier basis, as it would be expressed in a different language.|
|||The key advantage of spatial techniques is that they generalize across different domains, which is a crucial property in computer graphics applications (where a CNN model can be trained on one shape and applied to another one).|
|||Such a construction allows to formulate previously proposed Geodesic CNN (GCNN) [26] and Anisotropic CNN (ACNN) [7] on manifolds or GCN [21] and DCNN [3] on graphs as particular instances of our approach.|
|||Such a parametrization results in filters with a number of parameters constant in the input size n.  Chebyshev Spectral CNN (ChebNet).|
|||Geodesic CNN (GCNN).|
|||Anisotropic CNN (ACNN).|
|||[7] considered the anisotropic diffusion equation on the manifold  Diffusion CNN (DCNN).|
|||Table 3 shows that other deep learning methods (including the classical CNN on Euclidean domains, DCN and DCNN on graphs, and GCNN and ACNN on manifolds) can be obtained as particular settings of our framework with appropriate definition of u and w(u).|
|||Three methods were compared: classical CNN LeNet5 architecture [23] (containing two convolutional, two maxpooling, and one fully-connected layer, applied on regular grids only), spectral ChebNet[13] and the proposed MoNet.|
|||Classification accuracy of classical Euclidean CNN (LeNet5), spectral CNN (ChebNet) and the proposed approach (MoNet) on different versions of the MNIST dataset.|
|||For comparison, we show the performance of a Euclidean CNN with a comparable 3-layer architecture.|
|||For comparison, we show the performance of a standard Euclidean CNN in equivalent architecture (3 convolutional layers) applied on raw depth values and on SHOT descriptors.|
||14 instances in total. (in cvpr2017)|
|231|cvpr18-Learning Latent Super-Events to Detect Multiple Activities in Videos|For each frame, we combine the super-event representation with the per-frame or per-segment CNN representation for its binary classification per event.|
|||Two-stream CNN approaches take both RGB frames and optical flow as input [26] to capture motion and image features.|
|||Improving activity recognition by temporally aggregat 5305  ctTxDMxDxNDxNPer-segment CNNTemporal structure filtersSuper-event representationSoft-attentionVideo framesct+1Activity detectionContinuous videoing such per-frame CNN features has also been studied.|
|||pooling over per-frame CNN features.|
|||Fully-connected CRFs were applied as a post-processing of per-frame CNN features as well as object features.|
|||Per(cid:173)frame representation  The base component of our detection is a CNN providing per-frame (or per-local-segment) representation.|
|||This is obtained by learning standard video CNN models (e.g., [26, 3]).|
|||We train the model to learn binary per-frame classifiers by optimizing:  L(v) =Xt,c  zt,c log(p(c|vt)) + (1  zt,c) log(1  p(c|vt))  (1) where vt is the per-frame or per-segment CNN feature at frame t and zt,c is the ground truth label for class c and time t. This gives a sequence of probabilities for each class which can be used to find activity intervals.|
|||The learned CNN producing vt serves as our base component.|
|||We are using I3D as the base per-segment CNN mentioned in Section 3.1.|
|||I3D  5307  is a two-stream 3D CNN that achieved state-of-the-art performance on several action recognition tasks.|
|||Per-frame CNN representation dimensionality is D = 1024 for I3D features and D = 4096 for VGG features.|
|||In order to make the final per-frame decisions based on per-segment CNN features and our super-event representations, we trained a single fully-connected layer with input size D + D  N and output size of the number of classes.|
|||We also tested our approach with the standard two-stream CNN using VGG models.|
||14 instances in total. (in cvpr2018)|
|232|Dai_Metric_Imitation_by_2015_CVPR_paper|Three sophisticated, high-dimensional features were used as the SFs: objectbank [36], SIFT-llc [50], and the CNN feature [6], and three general, cheap features used as the TFs: GIST [40], LBP [39], and PHOG [4].|
|||The SFs are: SIFT-llc [50], objectbank(OB) [36], and the CNN feature [6].|
|||The CNN feature [6] is obtained by vectorizing the convolutional results of the deep convolutional neural network [29], which is trained on the ImageNet dataset.|
|||For the CNN feature, the MatConvNet package [49] was used with a pre-trained CNN model.|
|||The convolutional results at layer 16 were stacked as the CNN feature vector, with dimensionality 4096.|
|||TFs  SFs  GIST PHOG LBP CNN SIFT-llc 21504  4096  59  20  40  OB 44604  ing.|
|||For instance, on Scene-15 MI improves the purity of LBP from 0.36 to 0.48 if the CNN feature is used as the SFs and LapEigen is chosen.|
|||MI  TFs LBP MI LLE MI Lap 0.36 0.33 0.32 0.39  0.40 0.44 0.34 0.46  0.46 0.46 0.34 0.46  MI  SFs  SFs  SFs SIFT-llc MI LLE MI Lap CNN MI LLE MI Lap OB 0.54 0.44 0.52 0.46  0.42 0.31 0.37 0.48  0.49 0.39 0.51 0.57  0.47 0.33 0.37 0.47  0.48 0.37 0.35 0.48  0.69 0.60 0.68 0.82  0.48 0.41 0.36 0.47  MI  Scene-15 CUReT-61 Caltech-101  Event-8  Table 3.|
|||MI  TFs LBP MI LLE MI Lap 0.63 0.62 0.57 0.70  0.67 0.62 0.62 0.72  0.70 0.64 0.60 0.74  MI  SFs  SFs  SFs SIFT-llc MI LLE MI Lap CNN MI LLE MI Lap OB 0.74 0.68 0.70 0.80  0.61 0.51 0.64 0.75  0.65 0.66 0.59 0.70  0.85 0.65 0.73 0.80  0.90 0.77 0.77 0.89  0.66 0.69 0.57 0.72  0.59 0.58 0.63 0.73  MI  Scene-15 CUReT-61 Caltech-101  Event8  Table 4.|
|||The computational time (in seconds) of a full matrix of pairwise distance with different features, where MI(X) denotes that Metric Imitation for feature X.  Scene-15 CURet-61 Caltech-101  Event-8  GIST PHOG LBP MI(GIST) MI(PHOG) MI(LBP) CNN SIFT-llc 0.41 104.83 161.29 0.61 388.01 1.51 0.05 13.56  21.92 32.68 78.67 2.76  0.41 0.62 1.51 0.05  0.47 0.71 1.73 0.06  0.53 0.84 2.01 0.08  0.53 0.78 1.96 0.08  0.48 0.71 1.75 0.06  OB  215.56 325.97 796.86 26.69  Table 5.|
|||MI  TFs LBP MI LLE MI Lap 0.50 0.83 0.35 0.44  0.54 0.90 0.39 0.53  0.55 0.87 0.38 0.52  MI  SFs  SFs  SFs SIFT-llc MI LLE MI Lap CNN MI LLE MI Lap OB 0.65 0.90 0.60 0.60  0.58 0.90 0.41 0.53  0.60 0.90 0.57 0.70  0.56 0.86 0.40 0.51  0.72 0.95 0.79 0.89  0.58 0.90 0.41 0.55  0.57 0.84 0.40 0.50  MI  Scene-15 CUReT-61 Caltech-101  Event-8  Table 6.|
|||MI  TFs LGP MI LLE MI Lap 0.52 0.84 0.42 0.52  0.61 0.93 0.46 0.63  0.60 0.95 0.48 0.63  MI  SFs  SFs  SFs SIFT-llc MI LLE MI Lap CNN MI LLE MI Lap OB 0.65 0.91 0.59 0.58  0.64 0.94 0.51 0.65  0.62 0.92 0.48 0.60  0.63 0.90 0.48 0.56  0.72 0.95 0.79 0.88  0.60 0.90 0.57 0.70  0.64 0.96 0.51 0.64  MI  Scene-15 CUReT-61 Caltech-101  Event-8  Table 7.|
|||SFs SIFT-llc MI LLE MI Lap CNN MI LLE MI Lap OB 0.48 0.58  TFs LBP MI LLE MI Lap 0.38 0.33  Holiday Ukbench  0.48 0.36  0.48 0.38  0.50 0.44  0.50 0.39  0.66 0.63  0.49 0.39  0.46 0.38  0.72 0.86  SFs  SFs  MI  MI  MI  ance matrix of the high-dimensional SFs is needed and the regression itself is also computationally heavy.|
|||For instance, the MAP of LBP is improved from 0.38 to 0.50 on Holidays and from 0.33 to 0.44 on the UKbench dataset, when the CNN feature is used as the SFs.|
||14 instances in total. (in cvpr2015)|
|233|Herranz_Scene_Recognition_With_CVPR_2016_paper|As an alternative to Places-CNN holistic representation,  some recent works[4, 1, 21, 16] have shown that CNN features extracted locally in patches can be also aggregated into effective scene representations.|
|||Using the same fixed CNN model for all the scales inevitably leads to dataset bias[13], since the properties of the data vary at different scales, while the feature extractor remains fixed.|
|||In the next sections:  )  %  (   s t c e j b o   f o n o i t c a r F     4.5  4  3.5  3  2.5  2  1.5  1  0.5  0   0     1  )  %  (   s e g a m     i   f o n o i t c a r F  90  80  70  60  50  40  30  20  10  0   0     50  ILSVRC2012    SUN397  10 40 Objects in the scene  20  30  ILSVRC2012    SUN397  0.4  0.6  0.8  0.2  Normalized size   We show that using a single CNN as a generic feature extractor from patches is quite limited, due to the dataset bias induced by scale changes.|
||| Instead of using naively the same CNN model for all the scales, we select the most suitable one for each (ImageNet-CNN, Places-CNN or fine tuned).|
||| Optionally we fine tune each CNN model to further adapt it to the range of each scale.|
|||The main emphasis in these works is on the way multi-scale features are combined, implemented as either VLAD or FV encoding, while leaving the CNN model fixed.|
|||While adding a BOW encoding layer can help to alleviate somewhat the dataset bias, the main problem is still the rigid CNN model.|
|||with scale and achieves significantly better performance, by simply adapting the CNN model to the target scale, even without relying to sophisticated pooling methods.|
|||Again, the main  limitation is that the CNN model is fixed, not adapting to the scale-dependent distributions of patches.|
|||including more scales while keeping the same CNN model is marginally helpful and increases significantly the extraction cost and the noise in the representation.|
|||The performance of hybrid spliced is also significantly better than a 7 network architecture with a fixed CNN model.|
|||The combination same indicates that the 7 networks share the same CNN model (i.e.|
|||Conclusions  In contrast to previous works, in this paper we analyzed multi-scale CNN architectures focusing on the local CNN model, rather than on the pooling method.|
|||Harvesting discriminative meta objects with deep CNN features for scene classification.|
||14 instances in total. (in cvpr2016)|
|234|Ma_Going_Deeper_into_CVPR_2016_paper|More specifically, our proposed network has a two stream architecture composed of an appearancebased CNN that works on localized object of interest image frames and a motion-based CNN that uses stacked optical flow fields as input.|
|||The motion-based stream is a generalized CNN that takes as input a stack of optical-flow motion fields.|
|||Similar to third-person vision activity recognition research, there has also been a number of attempts to use CNN for understanding activities in first-person videos.|
|||[29] develops a new pooled feature representation and shows superior performance using CNN as a appearance feature extractor.|
|||Object CNN and motion CNN are then trained separately to recognize objects and actions.|
|||For instance, we can train another CNN or a regressor using features from the hand segmentation network.|
|||With the cropped image sequence of objects of interest {Oi}, we then train the object CNN using the model of CNN-M-2048 [3] to recognize the objects.|
|||In order to train a CNN network with motion input, we follow [30] to use optical flow images to represent motion information.|
|||With motion represented in optical flow images, we train the motion CNN using {(Xi, yaction)} pairs as training data and softmax as the loss function.|
|||To train the fused network, we transfer the weights of the trained motion CNN and object CNN and fine-tune it to recognize the activity.|
|||To further address the problem of limited data, we apply data augmentation [15] to improve generalization of CNN networks.|
|||We use a fixed learning rate of  = 1e8 for fine-tuning hand segmentation and object localization CNNs,  = 5e  4 for motion CNN and  = 1e  4 for object CNN.|
|||(b) Two-stream CNN [30] results with single streams, SVM-fusion and joint training.|
|||Conclusion  We have developed a twin stream CNN network architecture to integrate features that characterize ego-centric activities.|
||14 instances in total. (in cvpr2016)|
|235|Predicting Salient Face in Multiple-Face Videos|In particular, we first learn a CNN for each frame to locate salient face.|
|||However, to our best knowledge, the existing video saliency prediction methods rely on the handcrafted features, despite CNN being applied to automatically learn features for image saliency prediction in the most recent works of [14, 24, 25, 28, 32].|
|||(3) We propose a DL-based method to predict the salient face with transition across frames, which integrates a CNN and an LSTM-based RNN model.|
|||Built on the long short-term memory (LSTM) of recurrent neural network (RNN), we develop a multiplestream LSTM (M-LSTM) network for predicting the dynamic transitions of salient faces alongside video frames, taking the extracted CNN features as the input.|
|||Proposed Method  In this section, we introduce our DL-based method for saliency prediction in multiple-face videos, which integrates CNN and LSTM in a uniform framework.|
|||Second, we design a CNN to learn the features related to salient face at each static video frame, which is discussed in Section 3.2.|
|||Section 3.3 presents MLSTM that learns to predict salient face, by taking into consideration saliency-related features of CNN and the temporal transition of salient faces across video frames.|
|||Architecture of our CNN for the task of predicting salient face.|
|||CNN for Feature Extraction  We now design a CNN to automatically learn features from the detected faces, for the task of predicting whether the detected face is salient.|
|||M-LSTM for Salient Face Prediction  The CNN defined above mainly extract spatial information of each face at a single frame.|
|||(cid:3)N Our M-LSTM takes the CNN features {fn,t}N,T  n=1,t=1 as input, where fn,t stand for feature vector of the n-th face at frame t. We assume an upper limit of N faces per video.|
|||In this figure, the curves of CNN refer to the output of CNN (either 0 or 1), and the curves of M-LSTM are obtained upon  4426  Table 2.|
|||To predict the salient face in multiple-face videos, we proposed in this paper a DLbased method, in which both CNN and RNN are combined in a framework and then trained over MUFVET-II.|
|||Saliency weights of faces along with processed frames for the video in Figure 10, predicted by our CNN (green line), MLSTM (blue line) and GT (red line).|
||14 instances in total. (in cvpr2017)|
|236|Multi-View Supervision for Single-View Reconstruction via Differentiable Ray Consistency|[6] learned a CNN to predict a voxel representation using a single (or multiple) input image(s).|
|||While the progress demonstrated by these methods is encouraging and supports the claim for using CNN based learning techniques for reconstruction, the requirement of explicit 3D supervision for training is potentially restrictive.|
|||In contrast, we only need to perform a single gradient computation to obtain a learning signal for the CNN and can even work with sparse set of views (possibly even just one view) per instance.|
|||Using this, we can compute the derivatives of the loss function Lr(x) w.r.t the CNN predictions (see Appendix for derivation).|
|||The output of our singleview 3D prediction CNN is f(I)  (x, [p]) where x denotes voxel occupancy probabilities and [p] indicates optional per-voxel predictions (used if corresponding training observations e.g.|
|||To learn the parameters  of the single-view 3D prediction CNN, for each training image Ii we train the CNN to minimize the inconsistency between the prediction f(Ii) and the one or more observation(s) {(Oi k)} corresponding to Ii.|
|||Our CNN model is a simple encoder-decoder which predicts occupancies in a voxel grid from the input RGB image (see appendix [1] for details).|
|||We then train the CNN with a cross-entropy loss, restricted to voxels where the views provided any information.|
|||We only use the PASCAL3D instances with pose, object mask annotations to train the CNN with the proposed view consistency loss.|
|||We observe that using the PASCAL data via the view consistency loss in addition to the ShapeNet 3D training data allows us to improve across categories as using real images for training removes some error modes that the CNN trained on synthetic data exhibits on real images.|
|||We train a CNN to predict, from a single scene image, occupancies and per-voxel semantic labels for a coarse voxel grid.|
|||See appendix [1] for details, CNN architecture etc.|
|||In this scenario, our CNN predicts a per-voxel occupancy as well as a color value.|
|||Unsupervised cnn for single view depth  estimation: Geometry to the rescue.|
||14 instances in total. (in cvpr2017)|
|237|Nam_Learning_Multi-Domain_Convolutional_CVPR_2016_paper|Our algorithm pretrains a CNN using a large set of videos with tracking groundtruths to obtain a generic target representation.|
|||Motivated by this fact, we propose a novel CNN architecture, referred to as Multi-Domain Network (MDNet), to learn the shared representation of targets from multiple annotated video sequences for tracking, where each video is regarded as a separate domain.|
|||Another interesting aspect of our architecture is that we design the CNN with a small number of layers compared to the networks for classification tasks such as AlexNet [28] and VGG nets [4, 37].|
||| Our framework is successfully applied to visual tracking, where the CNN pretrained by multi-domain learning is updated online in the context of a new sequence to learn domain-specific information adaptively.|
|||[28] brought significant performance improvement in image classification by training a deep CNN with a largescale dataset and an efficient GPU implementation.|
|||RCNN [12] applies a CNN to an object detection task, where the training data are scarce, by pretraining on a large auxiliary dataset and fine-tuning on the target dataset.|
|||An early tracking algorithm based on a CNN can handle only predefined target object classes, e.g., human, since the CNN is trained offline before tracking and fixed afterwards [9].|
|||Contrary to the existing approaches, our algorithm takes advantage of large-scale visual tracking data for pretraining a CNN and obtain effective representations.|
|||Multi-Domain Network (MDNet)  This section describes our CNN architecture and multidomain learning approach to obtain domain-independent representations for visual tracking.|
|||Second, a deep CNN is less effective for precise target localization since the spatial information tends to be diluted as a network goes deeper [20].|
|||The goal of our learning algorithm is to train a multidomain CNN disambiguating target and background in an arbitrary domain, which is not straightforward since the training data from different domains have different notions of target and background.|
|||Our CNN is trained by the Stochastic Gradient Descent (SGD) method, where each domain is handled exclusively in each iteration.|
|||The filter weights in the jth layer of CNN are denoted by wj , where w1:5 are pretrained by mutli-domain learning and w6 is initialized randomly for a new sequence.|
|||4296  Algorithm 1 Online tracking algorithm  Input  : Pretrained CNN filters {w1, .|
||14 instances in total. (in cvpr2016)|
|238|Action Unit Detection With Region Adaptation, Multi-Labeling Learning and Optimal Temporal Fusing|3) An LSTM-based temporal  fusion recurrent net (LSTM Net) is proposed to fuse static CNN features, which makes the AU predictions more accurate than with static images only.|
|||We then explain our proposed region learning based CNN network in Section 3.|
|||Section 4 describes the way the temporal information of the CNN features is fused with the LSTM layers.|
|||[3] proposed a hybrid approach for combining CNN and LSTM to learn a better representation of an AU sequence.|
|||A shallow region and shape mask CNN is employed to learn the static feature while LSTM is used to extract a dynamic feature from the trained CNN model.|
|||In a classic CNN structure, a convolutional layer is composed of multiple filters and activation functions.|
|||As most traditional approaches tried to find local SIFT or Gabor features near facial landmark points, we would like to learn local CNN features in these regions of interest (ROIs).|
|||Comparison of the ROI learning and conventional CNN learning will be performed in the evaluation section (Section 4).|
|||Connection of CNN and LSTM (24 means number of frames in a sequence)  1844  LSTM can be easily connected to CNNs.|
|||Fully connected layers of a CNN can be directly fed into the input of LSTM blocks.|
|||As shown in Figure 4, the CNN can extract the image features as a 1-D vector.|
|||Instead of training everything from scratch, we choose to borrow parameters from an existing very deep CNN model.|
|||[23] designed a network by combining both CNN and LSTM.|
|||To obtain the spatiotemporal fusion features, the last layer features of the CNN and LSTM nets are concatenated.|
||14 instances in total. (in cvpr2017)|
|239|Shubham_Tulsiani_Layer-structured_3D_Scene_ECCV_2018_paper|We learn a CNN that can predict, from a single input image, a layered representation of the scene (an LDI).|
|||3.3 Network Architecture  We adopt the DispNet [18] architecture for our LDI prediction CNN shown in Figure 4.|
|||While we use a single CNN to predict disparities and textures for all LDI layers, we find it critical to have disjoint prediction branches to infer each LDI layer.|
|||3.4 Training Objective  To train our CNN f, we use view synthesis as a proxy task: given a source image Is, we predict a corresponding LDI and render it from a novel viewpoint.|
|||4: Overview of our CNN architecture.|
|||Our CNN architecture consists of a convolutional encoder and decoder with skip-connections.|
|||Our final learning objective, combining the various loss terms defined above (with  different weights) is:  Lfinal = Lvs + Lmvs + Lsc + Linc + Lsm  (8)  Using this learning objective, we can train our LDI prediction CNN f using a dataset comprised only of paired source and target images of the same scene.|
|||We use the corresponding images and objects to generate training samples to train our LDI prediction CNN f.|
|||We train our CNN for 600k iterations using the ADAM optimizer [16].|
|||We visualize the predictions of our learned LDI prediction CNN in Figure 6.|
|||We compare our 2 layer LDI prediction CNN against a single layer model that can only capture the visible aspects.|
|||We visualize sample predictions of our learned LDI prediction CNN in Figure 7.|
|||[33], since we use a similar CNN architecture facilitating a more apples-to-apples comparison.|
|||Garg, R., Reid, I.: Unsupervised CNN for single view depth estimation: Geometry to the  rescue.|
||14 instances in total. (in eccv2018)|
|240|Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper|Recent progress in this field can be attributed to two factors: 1) the availability of large-scale reID datasets [49, 51, 44, 19] and 2) the learned embedding of pedestrians using a CNN [8, 10].|
|||In an attempt to overcome the above-mentioned challenges, this paper 1) adopts GAN in unlabeled data generation, 2) proposes the label smoothing regularization for outliers (LSRO) for unlabeled data integration, and 3) reports improvements over a CNN baseline on three person re-ID datasets.|
|||To summarize, our contributions are:   the introduction of a semi-supervised pipeline that integrates GAN-generated images into the CNN learning machine in vitro;   an LSRO method for semi-supervised learning.|
|||[35] combine the CNN with some gate functions, aiming to adaptively focus on the salient parts of input image pairs, this method is limited by computational inefficiency because the input should be image pairs.|
|||A CNN can be very discriminative by itself without explicit part-matching.|
|||[43] combine the CNN embedding with hand-crafted features.|
|||combine an identification model with a verification model and improve the fine-tuned CNN performance.|
|||Note that, our system does not make major changes to the network structures of the GAN or the CNN with one exception the number of neurons in the last fully-connected layer in the CNN is modified according to the number of training classes.|
|||We adopt the CNN re-ID baseline used in [50, 51].|
|||During testing, we extract the 2,048-dim CNN embedding in the last convolutional layer for an 224  224 input image.|
|||The outputted image is resized to 256  256 and then used in CNN training (with LSRO).|
|||Through LSRO, they are added to the training sets of DukeMTMC-reID and CUB-200-2011 to regularize the CNN model.|
|||Therefore, a trade-off is recommended to avoid poor regularization and over-fitting  3759  Single Query rank-1 mAP 20.76 44.42 26.11 45.58 48.15 29.94 26.35 51.90  method   BoW+kissme [49] MR CNN [34] FisherNet [42] SL [6] S-LSTM [36] 55.43 DNS [47] 65.88 Gate Reid [35] SOMAnet [5]* 73.87 79.51 Verif.-Identif.|
|||Person reidentification by multi-channel parts-based cnn with improved triplet loss function.|
||14 instances in total. (in iccv2017)|
|241|Learning Non-Lambertian Object Intrinsics Across ShapeNet Categories|Our CNN delivers accurate and sharp results in this classical inverse problem of computer vision.|
|||We train and test our CNN across different object categories.|
|||Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories.|
|||We train an encoder-decoder CNN that delivers much sharper and more accurate results than the prior art of direct intrinsics (DI).|
|||The state-of-the-art DI CNN model [22] is adapted from a depth regression CNN with a coarse native spatial resolution.|
|||While benchmark scores for many CNN intrinsics models [23, 37, 38, 22, 24] are improving, the visual quality of these results remains poor, compared with those from traditional approaches based on hand-crafted features and multitudes of priors [7].|
|||A new CNN model with accurate and sharp results.|
|||Surprising from deep learning perspective on classification  and segmentation, our intrinsics CNN shows remarkable generalization across categories: networks trained only on chairs also obtain reasonable performance on other categories such as cars.|
|||[38] extend the IIW-CRF model with a CNN learning component.|
|||Direct Intrinsics [22] is the first entirely deep learning model that outputs intrinsics predictions, based on a depth regression CNN model [12] and trained on the synthetic Sintel intrinsics dataset.|
|||Traditional intrinsics models consider diffuse shading only, by decomposing the input image I as a product of  Figure 2: Our mirror-link CNN architecture has one shared encoder and three decoders for albedo, shading, specular components separately.|
|||Learning Intrinsics  We develop our CNN model and training procedure for  non-Lambertian intrinsics.|
|||2 illustrates our encoderdecoder CNN architecture.|
|||Our CNN approach consistently outperforms the state-ofthe-art both visually and numerically.|
||14 instances in total. (in cvpr2017)|
|242|cvpr18-Learning Deep Structured Active Contours End-to-End|DSAC uses a CNN to predict the energy function used by an Active Contour Model (ACM) to modify an initial instance polygon using learned geometric priors.|
|||In essence, we employ a CNN to learn the energy function that would allow an ACM to generate polygons close to a set of ground truth instances.|
|||Our proposed DSAC aims at making high-level geometric information available to CNN based methods as a step towards bridging this gap.|
|||For example, [8] employ a multi-task CNN to detect candidate objects and infer segmentation masks and class labels per detection.|
|||[10] train a CNN on pairs of locations and predicts the likelihood for the pair to belong to the same object.|
|||In [5], a recursive neural network is used to generate a segmentation polygon node by node, while in [24] a CNN predicts the direction of the nearest object boundary for each node in a polygon and uses it as a data term in an ACM.|
|||The CNN predicts the values of the energy terms to be used by the active contour model (ACM): a global  for the length penalization and maps for local D, the data term, , the curvature penalization and , the balloon term.|
|||At every iteration, the CNN forward pass is followed by ACM inference, which yields a contour that is used to compute the structured loss.|
|||N , and a task loss function (y, y), we would like to find the CNN parameters  such that, by optimizing Eq.|
|||using the chain rule and mod Finally, we can get L(Y,X ,) ifying each CNN parameter  applying:    t+1 = t    L(Y, X , )    ,  (22)  which will simultaneously decrease E(yi, xi; ) and increase E(yi, xi; ), thus making a better solution more likely when performing inference anew.|
|||CNN architecture and general setup  To learn the ACM energy terms, we use a CNN architecture similar to the Hypercolumn model in [14].|
|||We compare DSAC against a baseline where we train a CNN with the same architecture used by DSAC, but with a 3-class cross entropy loss with classes: building, building boundary, background.|
|||The proposed Deep Structured Active Contours (DSAC) uses a CNN to pre 8883  5  0  5  10  15  20  25  30  14  12  10  8  6  4  2  0  60  50  40  30  20  10  0  a) image x  b) data term D(x)  c) balloon term  (x)  d) thin plate term  (x)  Figure 6. a) Image from the Vaihingen test set.|
|||The model is trained end to-end by bringing the ACM inference into the CNN training schedule and using the ACMs output and the ground truth polygon to assess a structured loss that can be used to update the CNNs parameters using back-propagation.|
||14 instances in total. (in cvpr2018)|
|243|Yang_Grasp_Type_Revisited_2015_CVPR_paper|The goal of CNN is to learn a hierarchy of feature representations.|
|||The learning in CNN is based on Stochastic Gradient Descent (SGD), which includes two main operations: Forward and Back Propagation.|
|||We used a five layer CNN (including the input layer and one fully-connected perception layer for regression output).|
|||For each training image, we first pass the target hand patches (left hand and right hand, if present) of the main character in the image to the trained CNN model, and we obtain two belief distributions: PGraspT ype1 and PGraspT ype2.|
|||Experiments  The theoretical framework we presented suggests three hypotheses that deserve empirical tests: (a) the CNN based grasp type recognition module can robustly classify input hand patches into correct categories; (b) hand grasp type is a reliable cognitive feature to infer human action intention; (c) the evolution of hand grasp types is useful for fine-grain segmentation of human manipulation actions.|
|||We used a GPU based CNN implementation [9] to train the neural network, following the structure described above.|
|||Experimental results We achieved an average of 59% classification accuracy using the CNN based method.|
|||It can be seen that the CNN based approach has a decent advantage.|
|||To provide a full picture of our CNN based classification model, we also show the confusion matrix in Fig.|
|||The grasp type CNN model was used to extract a 14 dimension belief distribution as grasp type feature (which is due to data from both hands of the main character).|
|||We used the same CNN implementation [9] to train the neural network, following the same structures described above.|
|||The image patches were further resized to 64  64 and pipelined to the trained CNN model.|
|||Conclusion and Future Work  Our experiments produced three results: (i) we achieved in average 59% accuracy using the CNN based method for grasp type recognition from unconstrained image patches;  (ii) we achieved in average 65% prediction accuracy in inferring human intention using the grasp type only; (iii) using the grasp type temporal evolution, we achieved 78% recall and 80% precision in fine grain manipulation action segmentation tasks.|
|||We have proposed a CNN based learning framework to address these problems with decent success.|
||14 instances in total. (in cvpr2015)|
|244|Liao_Video_Super-Resolution_via_ICCV_2015_paper|This SR draft-ensemble CNN also integrates the function of deconvolution to form the final HR image with minimal visual artifacts.|
|||Our SR draft-ensemble CNN considers contextual information provided from external data for super-resolution.|
|||With these concerns, we resort to a CNN solution, which is found surprisingly capable to deal with these challenges.|
|||First, the three-dimensional filter of CNN plays a role of continuous weight combination of multiple local spatial regions, which is beneficial for artifact removal.|
|||Second, our CNN framework is novel on concatenating two modules for SR draft construction and final reconstruction.|
|||Finally, the shareweight nature of CNN makes it effective in terms of representativeness than many classical models, such as pairwise MRF and run quickly during testing as the computation is only on a few convolution operations.|
|||The architecture of our CNN is shown in Fig.|
|||The two-stage architecture of our CNN framework.|
|||CNN Training  For our CNN training, rather than adopting the l2 loss function as [4, 3], we exploit the l1 loss with total variation (TV) regularization, inspired by recent reconstruction methods [6, 12] to reduce visual artifacts.|
|||Moreover, to evaluate the generalization ability of our CNN model, we collect 40 real-world natural video sequences captured by cell phones and digital cameras with varying quality and containing a set of objects of flower, text, barcode, etc.|
|||We implement our CNN based on the Caffe platform [8].|
|||Smaller learning rates for the last layers are important for our CNN to converge.|
|||Conclusion  In this paper, we have proposed a SR draft-ensemble framework, which exploits CNN to solve the VideoSR problem.|
|||Based on this finding, we resort to CNN to integrate the reconstruction and deconvolution steps.|
||14 instances in total. (in iccv2015)|
|245|Shen_Shadow_Optimization_From_2015_CVPR_paper|A CNN learning framework is designed to capture the local structure information of shadow edges and automatically learn the most relevant features.|
|||They showed that the CNN with learned features outperforms the current state-of-the-art with hand-crafted features.|
|||Structured Deep Shadow Edge Detection  In this section, we present our structured CNN learning framework, then we explain how to apply it to shadow edge detection.|
|||One challenge for training CNN with structured labels is that structured output spaces are high dimensional, which causes long training duration.|
|||Structured Learning Shadow Edges  We employ the proposed structured CNN for feature learning for shadow edge detection.|
|||The structured CNN operates on the patches at image edges: only the patches that contain image edges at its 5 5 central area are used.|
|||Figure 1: CNN architecture used for learning the structure of shadow edges.|
|||Our CNN was implemented in unoptimized Matlab code.|
|||The overlapping edge patches are then fed to the proposed CNN for labelling.|
|||The trained structured CNN differentiates between shadow and reflectance edges and predicts shadow edge structure of the  (a)  (b)  Figure 2: (a) Shadow and (b) non-shadow patches learned input 28  28 by the proposed structure CNN.|
|||Our structured CNN achieves robust results by combining the neighbouring prediction results.|
|||As can be seen, the proposed structured CNN can recover better local edge structures (local consistency), and avoid assigning implausible label transitions.|
|||We can see that 5x5 structured CNN is able to learn fine shadow details.|
|||We show that the structured CNN Networks framework can capture the local structure information of shadow edge.|
||14 instances in total. (in cvpr2015)|
|246|Learning Deep Binary Descriptor With Multi-Quantization|Inspired by the fact that CNN features deliver strong discriminative power and binary features present low computational cost, DeepBit [26] learns deep compact binary descriptors in an unsupervised manner, which achieves the state-of-the-art in binary feature description.|
|||For each image patch from the training set, we first learn a real-valued feature vector with a pre-trained CNN by replacing the softmax layer with a fully connection layer.|
|||[26] proposed a DeepBit by designing a CNN to learn compact binary codes in an unsupervised manner.|
|||[50] proposed a CNN hashing (CNNH) method by learning deep hashing codes and image representation in a supervised manner.|
|||For each image from the input set, we first encode and decode its CNN feature with all KAEs.|
|||Learning Deep Binary Descriptor with Multi(cid:173)  Quantization  We initialize the CNN with the pre-trained 16 layers VGGNet [41] trained on the ImageNet dataset, which replaces the softmax layer with a fully connection layer.|
|||Let X = [x1, x2,    , xN ] be the CNN features of N images, where xn  Rd (1  n  N ) is the nth feature of the input images.|
|||The first term J1 may lead to similar features for all input patches, which harms the discriminativeness of the learned feature, while the third term J3 maximizes the variance of each element of the features, so that each element  As it is not convex to simultaneously optimize CNN and KAEs, we use an iterative approach to update one by fixing the others.|
|||In the training procedure, we simultaneously learn the parameters of CNN and the KAEs to obtain energy-saving and evenly-distributive binary descriptors.|
|||Output: Projection parameters of CNN W, and parame ters of KAEs Wk.|
|||1: Initialize pre-trained CNN features X and KAEs Wk.|
|||4:  5:  6:  end loop until convergence Update CNN with Wk fixed using (5).|
|||Discussion  The proposed DBD-MQ improves the conventional sign function based binary codes learning methods in the following two aspects:  1) Instead of employing a hand-crafted threshold, the proposed DBD-MQ simultaneous learns the parameters of CNN and KAEs to minimize the quantization loss.|
|||Unlike most existing binary codes learning methods which utilize the rigid sign function for binarization, our DBD-MQ simultaneously learns the parameters of CNN and KAEs, replacing the sign function with the fine-grained multiquantization to minimize the quantization loss.|
||14 instances in total. (in cvpr2017)|
|247|cvpr18-Pyramid Stereo Matching Network|The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision.|
|||Moreover, we design a stacked hourglass 3D CNN in conjunction with intermediate supervision to regularize the cost volume.|
||| We present a stacked hourglass 3D CNN to extend the regional support of context information in cost volume.|
|||Pyramid Stereo Matching Network  Name   Layer setting   concat[conv2_16, conv4_3, branch_1,   branch_2, branch_3, branch_4]   Cost volume   Concat left and shifted right   3D CNN (stacked hourglass)   3 output [output_1, outpu_t2, output_3]   upsampling   Bilinear interpolation   Disparity Regression   5412  We present PSMNet, which consists of an SPP [9, 32] module for effective incorporation of global context and a stacked hourglass module for cost volume regularization.|
|||pool  (cid:885)(cid:885),(cid:885)(cid:884)   bilinear interpolation (cid:885)(cid:885),(cid:883)(cid:884)8 (cid:883)(cid:883),(cid:885)(cid:884)   (cid:885)(cid:885)(cid:885),(cid:885)(cid:884) (cid:885)(cid:885)(cid:885),(cid:885)(cid:884)  (cid:885)(cid:885)(cid:885),(cid:885)(cid:884) (cid:885)(cid:885)(cid:885),(cid:885)(cid:884)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  deconv (cid:885)(cid:885)(cid:885),6(cid:886)  deconv (cid:885)(cid:885)(cid:885),(cid:885)(cid:884)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  deconv (cid:885)(cid:885)(cid:885),6(cid:886)  deconv (cid:885)(cid:885)(cid:885),(cid:885)(cid:884)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  (cid:885)(cid:885)(cid:885),6(cid:886)  deconv (cid:885)(cid:885)(cid:885),6(cid:886)  deconv (cid:885)(cid:885)(cid:885),(cid:885)(cid:884)  (cid:885)(cid:885)(cid:885),(cid:885)(cid:884) (cid:885)(cid:885)(cid:885),(cid:883)   (cid:885)(cid:885)(cid:885),(cid:885)(cid:884) (cid:885)(cid:885)(cid:885),(cid:883)   (cid:885)(cid:885)(cid:885),(cid:885)(cid:884) (cid:885)(cid:885)(cid:885),(cid:883)    add 3Dstack2_3   add 3Dstack1_3   add 3Dstack1_1   add 3Dstack1_1   add 3Dstack1_1   add 3Dconv1   add output_1   add output_2   add 3Dconv1   add 3Dconv1   Output dimension   (cid:1834)(cid:1849)(cid:885)  (cid:2869)(cid:2870)(cid:1834)(cid:2869)(cid:2870)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2870)(cid:1834)(cid:2869)(cid:2870)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2870)(cid:1834)(cid:2869)(cid:2870)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2870)(cid:1834)(cid:2869)(cid:2870)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)6(cid:886)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:883)(cid:884)8  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:883)(cid:884)8  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)(cid:882)  (cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)6(cid:886)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2876)(cid:1830)(cid:2869)(cid:2876)(cid:1834)(cid:2869)(cid:2876)(cid:1849)6(cid:886)  (cid:2869)(cid:2869)(cid:2874)(cid:1830) (cid:2869)(cid:2869)(cid:2874)(cid:1834) (cid:2869)(cid:2869)(cid:2874)(cid:1849)6(cid:886)  (cid:2869)(cid:2876)(cid:1830)(cid:2869)(cid:2876)(cid:1834)(cid:2869)(cid:2876)(cid:1849)6(cid:886)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2876)(cid:1830)(cid:2869)(cid:2876)(cid:1834)(cid:2869)(cid:2876)(cid:1849)6(cid:886)  (cid:2869)(cid:2869)(cid:2874)(cid:1830) (cid:2869)(cid:2869)(cid:2874)(cid:1834) (cid:2869)(cid:2869)(cid:2874)(cid:1849)6(cid:886)  (cid:2869)(cid:2876)(cid:1830)(cid:2869)(cid:2876)(cid:1834)(cid:2869)(cid:2876)(cid:1849)6(cid:886)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2876)(cid:1830)(cid:2869)(cid:2876)(cid:1834)(cid:2869)(cid:2876)(cid:1849)6(cid:886)  (cid:2869)(cid:2869)(cid:2874)(cid:1830) (cid:2869)(cid:2869)(cid:2874)(cid:1834) (cid:2869)(cid:2869)(cid:2874)(cid:1849)6(cid:886)  (cid:2869)(cid:2876)(cid:1830)(cid:2869)(cid:2876)(cid:1834)(cid:2869)(cid:2876)(cid:1849)6(cid:886)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:885)(cid:884)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:883)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:883)  (cid:2869)(cid:2872)(cid:1830)(cid:2869)(cid:2872)(cid:1834)(cid:2869)(cid:2872)(cid:1849)(cid:883)  (cid:1830)(cid:1834)(cid:1849)  (cid:1834)(cid:1849)   Left image   Right image   input   CNN   SPP module   conv   weight sharing   weight sharing   weight sharing   3D CNN   u p s a m p  l i  n g  r e g r e s s i o n     CNN   SPP module   conv      cost volume   final prediction   Spatial Pyramid Pooing Module   Basic   b  i l i  n e a r    r e g r e s s i o n     conv4_3   conv2_16   pooling   6(cid:886)6(cid:886)  (cid:885)(cid:884)(cid:885)(cid:884)  (cid:883)6(cid:883)6  88   conv   conv   conv   conv   u p s a m p  l i  n g     Stacked hourglass   c o n c a t    3D conv   3D conv, stride 2   3D deconv    bilinear   bilinear   bilinear   regression   regression   regression   Figure 1.|
|||The left and right input stereo images are fed to two weight-sharing pipelines consisting of a CNN for feature maps calculation, an SPP module for feature harvesting by concatenating representations from sub-regions with different sizes, and a convolution layer for feature fusion.|
|||The left and right image features are then used to form a 4D cost volume, which is fed into a 3D CNN for cost volume regularization and disparity regression.|
|||To aggregate the feature information along the disparity dimension as well as spatial dimensions, we propose two kinds of 3D CNN architectures for cost volume regularization: the basic and stacked hourglass architectures.|
|||KITTI 2015  Ablation study for PSMNet We conducted experiments with several settings to evaluate PSMNet, including the usage of dilated convolution, pooling at different levels, and 3D CNN architectures.|
|||The default 3D CNN design was the basic architecture.|
|||The stacked hourglass 3D CNN significantly outperformed the basic 3D CNN when combined with dilated convolution and the SPP module.|
|||Ablation study for Loss Weight The stacked hourglass 3D CNN has three outputs for training and can facilitate the learning process.|
|||In this work, we propose PSMNet, a novel endto-end CNN architecture for stereo vision which consists of two main modules to exploit context information: the SPP module and the 3D CNN.|
|||The 3D CNN further learns to regularize the cost volume via repeated top-down/bottom-up processes.|
||14 instances in total. (in cvpr2018)|
|248|DeepPermNet_ Visual Permutation Learning|To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations.|
|||Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task.|
|||Last, we introduce the Sinkhorn layer that transforms standard CNN predictions into doubly-stochastic matrices using Sinkhorn iterations; these matrices are continuous approximations to discrete permutation matrices, and thus allow efficient learning via backpropagation.|
|||We also develop a CNN based framework to efficiently solve such a problem, which can be applied in diverse applications, although in this paper we limit our scope to computer vision, and review below topics that are most similar to the applications considered in the sequel.|
|||However, different from us, they train a CNN to predict only a tiny subset of possible permutations generated from an image shuffling grid of size 3  3 (specifically, they use only 100 permutations from 362k possible permutations).|
|||Then, we describe our end-to-end learning algorithm, CNN model, and inference procedure.|
|||To the best of our knowledge, currently, there is no standard CNN layer that is able to explore such structure.|
|||3.3.1 Sinkhorn Normalization  A principled and efficient way to enforce a CNN to generate DSMs as outputs is to make use of the Sinkhorn normalization algorithm [40, 41].|
|||Here, we propose a CNN layer that performs such a normalization.|
|||We train a CNN model for each attribute in the Public Figures dataset by sampling 30K ordered image sequences from as training images.|
|||Note that, we outperform the recent method in [42], which is a VGG CNN model that has significantly more modeling capacity than the AlexNet [24] architecture we use.|
|||We also provide results by building our scheme on a VGG CNN model.|
|||Using a CNN to recover an image from its parts is a challenging task because it requires the network to learn semantic concepts, contextual information, and objects-parts relationships, in order to predict the right permutation.|
|||We proposed a novel CNN layer that can convert standard CNN predictions to doubly-stochastic approximations of permutation matrices using Sinkhorn normalizations; this CNN can be trained in an end-to-end manner.|
||14 instances in total. (in cvpr2017)|
|249|zechun_liu_Bi-Real_Net_Enhancing_ECCV_2018_paper|To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut.|
|||The first is to reduce the number of weights, such as Sparse CNN [15].|
|||In [17], the 1-bit CNN model is initialized using the real-valued CNN model with the ReLU function pre-trained on ImageNet.|
|||In Sparse CNN [15], a sparse matrix multiplication operation is employed to zero out more than 90% of parameters to accelerate the learning process.|
|||3 Methodology  3.1 Standard 1-bit CNNs and Its Representational Capability  1-bit convolutional neural networks (CNNs) refer to the CNN models with binary weight parameters and binary activations in intermediate convolution layers.|
|||Compared to the real-valued CNN model with the 32-bit weight parameter, the 1-bit CNNs obtains up to 32 memory saving.|
|||However, it has been demonstrated in [7] that the classification performance of the 1-bit CNNs is much worse than that of the real-valued CNN models on large-scale datasets like ImageNet.|
|||Consequently, the representational capability of each block in the 1-bit CNN with the above shortcut becomes (2892)6272.|
|||The representational capability of each block in the 1-bit CNN is significantly enhanced due to the simple identity shortcut.|
|||The only additional cost of computation is the addition operation of two real activations, as these real activations already exist in the standard 1-bit CNN (i.e., without shortcuts).|
|||In [14], the initial weights of the 1-bit CNNs are derived from the corresponding real-valued CNN model pre-trained on ImageNet.|
|||Instead, we propose to replace ReLU with clip(1, x, 1) to pre-train the real-valued CNN model, as the activation of the clip function is closer to the sign function than ReLU.|
|||5 Conclusion  In this work, we have proposed a novel 1-bit CNN model, dubbed Bi-Real net.|
|||Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||14 instances in total. (in eccv2018)|
|250|Noh_Image_Question_Answering_CVPR_2016_paper|The proposed networkjoint network with the CNN for ImageQA and the parameter prediction network is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU.|
|||Among these methods, simple deep learning based approaches that perform classification on a combination of features extracted from image and question currently demonstrate the state-of 1 30  the-art accuracy on public benchmarks [23, 16]; these approaches extract image features using a convolutional neural network (CNN), and use CNN or bag-of-words to obtain feature descriptors from question.|
|||To realize this idea, we propose a deep CNN with a dynamic parameter layer whose weights are determined adaptively based on questions.|
|||We claim that a single deep CNN architecture can take care of various tasks by allowing adaptive weight assignment in the dynamic parameter layer.|
|||The entire network including the CNN for ImageQA and the parameter prediction network is trained end-to-end through back-propagation, where its weights are initialized using pre-trained CNN and GRU.|
|||Our main contributions in this work are summarized below:   We successfully adopt a deep CNN with a dynamic parameter layer for ImageQA, which is a fully-connected layer whose parameters are determined dynamically based on a given question.|
|||In this work, we hope to solve the heterogeneous recognition tasks using a single CNN by adapting the weights in the dynamic parameter layer.|
|||One of the  fully-connected layers in the CNN is the dynamic parameter layer, and the weights in the layer are determined adaptively by the parameter prediction network.|
|||We observe that the gradients below the dynamic parameter layer in the CNN are noisy since its weights are predicted by the parameter prediction network.|
|||Hence, a na ve approach to fine-tune the CNN typically fails  34  to improve performance, and we employ a slightly different technique for CNN fine-tuning to sidestep the observed problem.|
|||We performed controlled experiments to analyze the contribution of individual components in the proposed algorithmdynamic parameter prediction, use of pre-trained GRU and CNN fine-tuning, and trained 3 additional models, CONCAT, RAND-GRU, and CNN-FIXED.|
|||Evaluation results on COCO-QA  IMG+BOW [23]  2VIS+BLSTM [23]  Ensemble [23] ConvQA [16]  DPPnet  Acc 55.92 55.09 57.84 54.95 61.19  WUPS 0.9  WUPS 0.0  66.78 65.34 67.90 65.36 70.84  88.99 88.64 89.52 88.58 90.61  CNN-FIXED is useful to see the impact of CNN fine-tuning since it is identical to DPPnet except that the weights in CNN are fixed.|
|||CONCAT is the most basic model, which predicts answers using the two fully-connected layers for a combination of CNN and GRU features.|
|||Obviously, it does not employ any of new components such as parameter prediction, pre-trained GRU and CNN fine-tuning.|
||14 instances in total. (in cvpr2016)|
|251|Di_Chen_Person_Search_via_ECCV_2018_paper|In order to extract more representative features for each identity, we propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams.|
||| We propose a Mask-Guided Two-Stream CNN Model (MGTS) for person re-id, which explicitly makes use of one stream from the foreground as the emphasized information and enriches the representation by incorporating another separate stream from the original image.|
|||Person Search via A Mask-Guided Two-Stream CNN Model  3  input  original  Detector  crop  O-Net  channel  channel  re-weighting  concatenate  (n)  GAP  & FC  (n)  OIMLoss  crop  N  F-Net  SEBlock  feature vector  segmentation mask  foreground  Stage One: Detection and Segmentation  Stage Two: Re-identification  Fig.|
|||Most of those CNN models can be categorized into two groups.|
|||In  Person Search via A Mask-Guided Two-Stream CNN Model  5  order to separate the foreground person from background, we apply an off-the-shelf instance segmentation method FCIS [10] on the whole image, and then designate the person to the right mask via majority vote.|
|||3.3 Person Re-ID via A Mask-Guided Two-Stream CNN Model  After RoIs for each person are obtained (either from a detector or ground truth), we aim to extract discriminative features.|
|||fGAP is the operation of GAP and  Person Search via A Mask-Guided Two-Stream CNN Model  7  Fig.|
|||The  Person Search via A Mask-Guided Two-Stream CNN Model  9  input image patches are re-scaled to an arbitrary size of 256  128 and the batch size is set to 128.|
|||We follow the notations defined in [3], where CNN denotes the Faster R-CNN detector based on ResNet50 and IDNet represents a reidentification net defined in [3].|
|||05001000150020002500300035004000Gallery Size5055606570758085mAP(%)OursNPSMIANOIMPerson Search via A Mask-Guided Two-Stream CNN Model  11  Table.|
|||910111213140.0k10.0k20.0k30.0k40.0k50.0k60.0k910111213140.0k5.0k10.0k15.0k20.0k25.0kPerson Search via A Mask-Guided Two-Stream CNN Model  13  Table.|
|||Person Search via A Mask-Guided Two-Stream CNN Model  15  References  1.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel  parts-based CNN with improved triplet loss function.|
|||(2016) 8  Person Search via A Mask-Guided Two-Stream CNN Model  17  51.|
||14 instances in total. (in eccv2018)|
|252|Mahendran_Understanding_Deep_Image_2015_CVPR_paper|We then use this technique to study the inverse of recent state-of-theart CNN image representations for the first time.|
|||We relate this to the depth of the representation, showing that the CNN gradually builds an  1  increasing amount of invariance, layer after layer.|
|||in most CNN implementations B = 128).|
|||Furthermore, it shows how to implement DSIFT and HOG in a standard CNN framework in order to compute their derivatives.|
|||Filtering is a standard CNN operation but these binning functions are not.|
|||The conclusion is that approximations to DSIFT and HOG can be implemented with conventional CNN components plus the non-conventional gradient norm offset.|
|||in the rest of the paper we will use exact CNN equivalents of DSIFT and HOG, using modified or additional CNN components as needed.|
|||1 These CNNs are numerically indistinguishable from the VLFeat reference implementations, but, true to their CNN nature, allow computing the feature derivatives as required by the algorithm of Sect.|
|||In terms of speed, an advantage of optimizing (1) is that it can be switched to use GPU code immediately given the underlying CNN framework; doing so results in a ten-fold speed-up.|
|||tained from each individual CNN layer for 100 ILSVRC validation images (these were not used to train the CNN-A model [15]).|
|||This figure includes also the reconstruction of an abstract pattern, which is not included in any of the ImageNet classes; still, all CNN codes capture distinctive visual features of the original pattern, clearly indicating that even very deep layers capture visual information.|
|||Note that all these and the original images are nearly indistinguishable from the viewpoint of the CNN model; it is therefore interesting to note the lack of detail in the deepest reconstructions, showing that the network captures just a sketch of the objects, which evidently suffices for classification.|
|||We now examine reconstructions obtained from subset of neural responses in different CNN layers.|
|||Diversity in the CNN model.|
||14 instances in total. (in cvpr2015)|
|253|Zhu_Face_Alignment_Across_CVPR_2016_paper|In this work, we adopt CNN to fit the 3D face model with a specifically designed feature, namely Projected Normalized Coordinate Code (PNCC).|
|||[39] firstly use CNN to regress landmark locations with the raw face image.|
|||[51] further combine face alignment with attribute analysis through multi-task CNN to boost the performance of both tasks.|
|||Although with considerable achievements, most CNN methods only detect a sparse set of landmarks (5 points in [39, 51, 28]) with limited descriptive power of face shape.|
|||At kth iteration, Netk takes a medium parameter p coordinate code (PNCC), stacks it with the input image and sends it into CNN to predict the parameter update p  k.  k as input, constructs the projected normalized  3.1.|
|||Unlike existing CNN methods [39, 28] which apply different networks for different fitting stages, 3DDFA employ a unified network structure across the casIn general, at iteration k (k = 0, 1, ..., K), given cade.|
|||Projected Normalized Coordinate Code  The special structure of the cascaded CNN has three requirements of its input feature: Firstly, the feedback property requires that the input feature should depend on the CNN output to enable the cascade manner.|
|||Cost Function  The performance of CNN can be greatly impacted by the cost function, which is difficult to design in 3DDFA since each dimension of the CNN output (model parameter) has different influence on the 3DDFA result (fitted 3D face).|
|||In the training process, CNN firstly concentrate on the parameters with larger kV (pd(i))  V (pg)k such as scale, rotation and translation.|
|||As pd(i) is closer to pg, the weights of these parameters begin to shrink and CNN will optimize less important parameters but at the same time keep the high-priority parameters sufficiently good.|
|||Face Profiling  All the discriminative models rely on the training data, especially for CNN which has thousands of parameters to train.|
|||Initialization Regeneration  With a huge number of parameters, CNN tends to overfit the training set and the networks at deeper cascade might receive training samples with almost zero errors.|
|||During testing, 3DDFA takes 25.24ms for each iteration, 17.49ms for PNCC construction on 3.40GHZ CPU and 7.75ms for CNN on GTX TITAN Black GPU.|
|||Different from traditional methods, 3DDFA skips the 2D landmark detection and starts from 3DMM fitting with cascaded CNN to handle the self-occlusion problem.|
||14 instances in total. (in cvpr2016)|
|254|Detangling People_ Individuating Multiple Close People and Their Body Parts via Region Assembly|Experiments on three datasets show our method strongly outperforms an array of existing approaches, including bounding box detectors, CNN region proposals, and human pose detectors.|
|||Hypercolumn [46] is a CNN approach that can be used for people parsing by classifying pixels in the initial person detection bounding boxes.|
|||Different from our approach, Deep(er)cuts body part candidates are body joint candidates from CNN and thus the method does not infer region assembly and it does not deal with region splitting and merging as our approach does.|
|||The map is obtained by first computing a stack of probability maps from a CNN (a modified AlexNet) for each part at different scales.|
|||In (a), note how the CNN output does not individuate parts into person instances (center), whereas our output does (right).|
|||Overview: In the following, we compare our approach to 1) simpler inference methods, to show the value added over the initial CNN body part maps; 2) bounding box detector methods; 3) CNN methods using region proposals; 4) human pose detection based methods.|
|||Nearly all test images  Are our initial CNN body part maps enough?|
|||Would a simpler inference method on top of the CNN maps be sufficient?|
|||First, we stress that the CNN body part maps are not enough by definition, as they do not individuate which body part blobs go to which person.|
|||For example, if their arms touch, that yields one connected component in the CNN output; see Fig 4, second column in each set.|
|||Our CNN semantic segmentation itself is reasonable.|
|||Comparison with CNN object detectors using region proposals: Another method for human instance segmentation is first generating many region proposals and then using a classifier to extract true human instances, e.g.|
|||The CNN pose detector [43] baseline (CNN-D) is designed to detect a single person stick figure.|
|||R-I: RCNN+OIP, R-II: RCNN+MCG, R-III: RCNN+SelectiveSearch, CNN-D: CNN pose detector [43].|
||14 instances in total. (in cvpr2017)|
|255|Wu_What_Value_Do_CVPR_2016_paper|Current state-of-the-art captioning methods use a CNN as an image encoder to produce a fixed-length vector representation [25, 29, 45, 48], which is then fed into the decoder RNN to generate a caption.|
|||Same as image captioning, the current state of the art in VQA [13, 35, 43] relies on passing CNN features to an RNN language model.|
|||In contrast to the aforementioned two-stage methods, the recent dominant trend in V2L is to use an architecture which connects a CNN to an RNN to learn the mapping from images to sentences directly.|
|||[36], for instance, proposed a multimodal RNN (m-RNN) to estimate the probability distribution of the next word given previous words and the deep CNN feature of an image at each time step.|
|||[24] constructed a joint multimodal embedding space using a powerful deep CNN model and an LSTM that encodes text.|
|||In the baseline model, as in [13, 43, 50] we use a pre-trained CNN to extract image features CNN(I) which are fed into the LSTM directly.|
|||Given a test image, a set of proposal regions are selected and passed to the shared CNN, and finally the CNN outputs from different proposals are aggregated with max pooling to produce the final multi-label prediction, which gives us the high-level image representation, Vatt(I)  To address the concern that some attributes may only apply to image sub-regions, we follow Wei et al.|
|||The shared CNN is then fine-tuned on the target multi-label dataset (our image-attribute training data).|
|||Moreover, different from [13, 35, 43] who use CNN features directly, we use our attributes representations Vatt(I) as the input for decoding LSTM (see Figure 3 (c)).|
|||The baseline framework is the same as that proposed in section 3.2, except that the attributes vector Vatt(I) is replaced by the last hidden layer of CNN directly (see the blue arrow in Figure 3).|
|||Various CNN architectures are applied in the baseline method to extract image features, such as VggNet[45] and GoogLeNet[48].|
|||We also evaluate an approach that combines CNN features and attributes vector together as the input of the LSTM, but we find this approach (B-1=0.71) is not as good as using attributes vector alone in the same setting.|
|||[2] provided a baseline for this dataset using a Q+I method, which encodes the image with CNN features and questions with LSTM representation.|
|||209  as it uses CNN features as the input to the LSTM, while LSTM Q only provides questions as the input.|
||14 instances in total. (in cvpr2016)|
|256|Deep TEN_ Texture Encoding Network|Current methods build from distinct components, using standard encoders with separate off-the-shelf features such as SIFT descriptors or pre-trained CNN features for material recognition.|
|||In this section, we discuss the properties of the Deep-TEN, that is the property of integrating Encoding Layer with an end-to-end CNN architecture.|
|||We fix the input image size to 352352 for SIFT, pre-trained CNNs feature extractions  712  MINC-2500  FMD GTOS KTH 4D-Light MIT-Indoor Caltech-101  FV-SIFT  FV-CNN (VGG-VD)  Deep-TEN (ours)  46.0  61.8 80.6  47.0  75.0  65.5  77.1  66.3  71.0  58.4  70.4  80.20.9 84.31.9 82.03.3  81.71.0  51.6  67.8 71.3  63.4  83.0 85.3  Table 3: The table compares the recognition results of Deep-TEN with off-the-shelf encoding approaches, including Fisher Vector encoding of dense SIFT features (FV-SIFT) and pre-trained CNN activations (FV-CNN) on different datasets using single-size training.|
|||For FV-CNN encoding, the CNN features of input images are extracted using pretrained 16-layer VGG-VD model [38].|
|||For comparison with multi-size training Deep-TEN, multi-size FV-CNN (VD) is used, the CNN features are extracted from two different sizes of input image, 352352 and 320320 (sizes determined empirically).|
|||Table 3 shows overall experimental results using single-size training,  Comparing with Baselines As shown in Table 3, DeepTEN and FV-CNN always outperform FV-SIFT, which shows that pre-trained CNN features are typically more discriminant than hand-engineered SIFT features.|
|||The CNN models VGGVD and ResNet are pre-trained on ImageNet, which is also an object classification dataset like Caltech-101.|
|||Therefore, learning CNN from scratch is not supposed to work well due to the limited training data.|
|||Note that the traditional CNN architecture is not applicable due to different image sizes from this two datasets.|
|||we developed an Encoding Layer which bridges the gap be tween classic computer vision approaches and the CNN architecture.|
|||The Encoding Layer shows superior performance in transferring pre-trained CNN features.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Bilinear cnn models for fine-grained visual recognition.|
|||A 4D light-field dataset and CNN architectures for material recognition.|
||14 instances in total. (in cvpr2017)|
|257|Mind the Class Weight Bias_ Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation|Besides the inspiring progress in model and learning, the achievement of CNN is undoubtedly attributed to the availability of massive labeled dataset Corresponding author.|
|||s. For a CNN trained on large scale datasets [8], while the lower layers of features are safely transferable, the learned features gradually moves from general to specific along the network [40].|
|||When the source and target tasks are significantly diverse, the CNN pretrained on the source task may not generalize well to the target task.|
|||Such scenario leads to an emerging topic to transfer the CNN from the source task to the target task with the enhanced and discriminative representation [2].|
|||Using unbiased estimate of multi-kernel MMD [15, 17], our proposed weighted MMD can be computed as mean embedding matching with linear time complexity and be incorporated into CNN for unsupervised domain adaptation.|
|||The superiority of weighted MMD over MMD has been verified on various CNN architectures and different datasets.|
|||In this paper, we focus on learning transferable CNN features for unsupervised domain adaptation (UDA), where the labels of all target samples are unknown during training.|
|||Therefore, to generalize CNN for domain adaptation, the weighted MMD-based regularizers are added to the higher layers of CNN.|
|||E-step: Fixed W, for each sample xt  j from target domain, the CNN output to the cth class is represented as gc(xt j; W).|
|||Following the common setting in UDA, we implement our WDAN model based on four widely used CNN architectures, i.e., LeNet [22], AlexNet [21], GoogLeNet [34] and VGGnet-16 [32].|
|||By AlexNet, GoogLeNet, and VGGnet-16 we indicate to fine-tune the pre-trained CNN models for special tasks.|
|||By embedding a single kernel MMD layer into CNN structure, DDC [36] develops a unified deep framework to jointly learn semantically meaningful feature and perform adaption cross domain.|
|||DAN achieves the best performance, independently of the employed CNN structure.|
|||To sum up, the promising performance of our weighted MMD model can be verified on various CNN architectures (i.e., AlexNet, GoogLeNet and LeNet) and various datasets with different number of classes.|
||14 instances in total. (in cvpr2017)|
|258|Jampani_Learning_Sparse_High_CVPR_2016_paper|These works combine structured prediction frameworks on top of CNNs, to model the relationship between the desired output variables thereby significantly improving upon the CNN result.|
|||Most CNN architectures use spatial convolution layers, which have fixed local receptive fields.|
|||By computing the gradients of the filter elements we enable the use of gradient based optimizers, e.g., backpropagation for CNN in the same way that spatial filters in a CNN are learned.|
|||The approach of [18] is a CNN model that produces a result at 1/4th of the input resolution due to down-sampling operations in maxpooling layers.|
|||Furthermore, the authors downsample the 640  480 images to 320  240 as a pre-processing step before CNN convolutions.|
|||The DeepLab architecture runs a CNN model on the input image to obtain a result that is down-sampled by a factor of 8.|
|||+ MF-1step  + MF-2 step  + loose MF-2 step  Semantic segmentation (IoU) CNN [15]: 72.08 / 66.95  Gauss CRF Learned CRF  +2.48 +2.93  +3.38 +3.71  +3.38 / +3.00 +3.85 / +3.37  Material segmentation (Pixel Accuracy) CNN [11]: 67.21 / 69.23  Gauss CRF +7.91 / +6.28 Learned CRF +9.48 / +6.23  +9.68 / +7.35 +11.89 / +6.93  +9.68 / +7.35 +11.91 / +6.93  Table 2.|
|||We also initialize the mean-field update equations with the CNN unaries.|
|||Their approach proposes the same architecture as in the previous section; a CNN to predict the material labels (e.g., wool, glass, sky, etc.)|
|||We re-use the pre-trained CNN and choose the CRF parameters and Lab color/position features as in [11].|
|||Having the gradients available allows for endto-end training with backpropagation, without the need for any change in CNN training protocols.|
|||Let us consider handwritten character recognition, one of the prime cases for CNN use.|
|||A CNN is a natural choice to approach this classification task.|
|||We experiment with two CNN architectures for this experiment that have been used for this task, LeNet-7 from [33] and DeepCNet [16, 22].|
||14 instances in total. (in cvpr2016)|
|259|Recurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization|Furthermore, although deep CNN has been proved to outperform hand-crafted features in almost all computer vision tasks, there is no direct, precise frame-level supervision for CNNs training in this problem.|
|||A complex end-toend model with CNN as visual feature extractor may lead to overfitting as well.|
|||[18] integrate CNN into a weakly supervised learning scheme.|
|||They use the weakly labelled sequence with hand shape as an initialization to iteratively tune CNN and refine the hand shape labels with Expectation Maximization (EM) algorithm.|
|||They develop a hybrid CNN-HMM approach, which treats the outputs of CNN as Bayesian posteriors and uses the frame-level hidden states predicted by HMM to tune CNN.|
|||In this work, our proposed approach employs CNN with temporal convolution and pooling for spatio-temporal representation learning from video clips, and RNN with long short-term memory (LSTM) module to learn the mapping of feature sequences to sequences of glosses.|
|||Network architecture  Our proposed architecture consists of a CNN with temporal convolution and pooling for spatial and local temporal feature extraction, a bidirectional LSTM [13] for global sequence learning, and a detection network for refining the sequence learning results.|
|||We set the stacked temporal convolution with zero-padding and temporal pooling operations as function P : Rlc  Rd with receptive field l, temporal stride  and output dimension d, and each segment with length l is transformed into a spatio-temporal representation:  t=1 = F({xt}T  {sn}N  n=1 = P({ft}T  t=1) = P(F({xt}T  t=1)),  (1)  where N = T / represents the number of segments, and sn  Rd denotes the representation of segment n. The structure of CNN stacked with temporal operations P  F is the spatio-temporal feature extraction architecture, which transforms video segments into approximate gloss-level representations.|
|||At this stage, we focus on pretraining the CNN for spatial representations before the stacked 1dimensional temporal convolution and pooling.|
|||We first initiate our CNN with VGG-S model pretrained on ILSVRC2012 dataset [27].|
|||We choose this relatively shallow network mainly in consideration of GPU memory constraints when jointly optimizing CNN and RNN.|
|||At the stage of tuning the representation learning architecture, we employ GoogLeNet [28] pretrained on ILSVRC-2014 [27] as the CNN model, which shows better performance on the problem of large-scale image classification.|
|||We substitute 3D-CNN [21, 29] for our proposed CNN with stacked temporal convolution and  7366  pooling, and we also assess the utility of pre-training the CNN with loss employed in PN-Net [1] from video frames.|
|||We think the reason for CNN with temporal convolutions performing better is that there are less parameters compared to 3D-CNN with the same number of layers, thus it is less prone to overfitting.|
||14 instances in total. (in cvpr2017)|
|260|Wang_Visual_Tracking_With_ICCV_2015_paper|In contrast, we conduct in-depth study on the properties of CNN features from the perspective of online visual tracking and in order to make better use of them in terms of both efficiency and accuracy.|
|||First, CNN features at different levels/depths have different properties that fit the tracking problem.|
|||Second, the CNN features pre-trained on ImageNet are for distinguishing generic objects.|
|||The contributions of this work are three folds:  i) We analyze CNN features learned from the large-scale image classification task and find important properties for online tracking.|
|||In [19], tracking was performed as foreground-background classification with CNN trained online without offline pretraining.|
|||In this section, we present some important properties of CNN features which can better facilitate visual tracking.|
|||Observation 1 Although the receptive field 1 of CNN feature maps is large, the activated feature maps are sparse and localized.|
|||We also use the approach in [26] to obtain the saliency maps of CNN features.|
|||Thus, these CNN features can be used for target localization.|
|||Observation 2 Many CNN feature maps are noisy or unrelated for the task of discriminating a particular target from its background.|
|||The CNN features pre-trained on ImageNet can describe a large variety of generic objects and therefore they are sup 1We use the term receptive field to denote the input image region that  are connected to a particular neuron in the feature maps  350  300  250  200  150  100  50     s p a M e r u t a e F   f o   r e b m u N  0  0  500  400  300  200  100     s p a M e r u t a e F   f o   r e b m u N  5,000  10,000  15,000  20,000  25,000  Activation Value  0  0  200  400  800  600 1200 Activation Value  1000  1400  1600  1800  Figure 3.|
|||This should be partially attributed to the robustness of CNN features.|
|||Note that the sel-CNN for selecting features and the SNet and GNet for localization are different in CNN structures.|
|||Conclusion  In this paper, we empirically present some important properties of CNN features under the viewpoint of visual tracking.|
||14 instances in total. (in iccv2015)|
|261|Wang_Shape_Inpainting_Using_ICCV_2017_paper|We propose a hybrid network structure based on 3D CNN that leverages the generalization power of a Generative Adversarial model and the memory efficiency of Recurrent Neural Network (RNN) to handle 3D data sequentially.|
|||3D CNN requires much more GPU memory than 2D CNN, which impedes volumetric network analysis of high-resolution 3D data.|
|||[10] applied 2D CNN and LSTM on 3D data (video) and developed a recurrent convolutional architecture for video recognition.|
|||[6] used a recurrent network and a CNN to reconstruct 3D models from a sequence of multi-view images.|
|||We introduce an 3D Encoder-Decoder CNN by extending a 3D Generative Adversarial Network [26], namely 3D Encoder-Decoder Generative Adversarial Network (3DED-GAN), to accomplish the 3D inpainting task.|
|||Long(cid:173)term Recurrent Convolutional Network  (LRCN) Model  3D CNN consumes much more GPU memory than 2D CNN.|
|||It works by passing each 2D slice with its neighboring slices through a 3D CNN to produce a fixed-length  3 to a volume with dimension dh  Figure 3: Framework for LRCN.|
|||The input of the 3D CNN is then v  1, I  , I  t  dh /dl  t  dh /dl  +1, I  t  dh /dl  +2}.|
|||At step t, the 3D CNN transforms c slices of 2D images v t into a 200D vector vt.|
|||The 3D CNN encoder has the same structure with the 3D encoder in 3D-ED-GAN with an fc layer at the end.|
|||Although the LRCN contains a 3D CNN encoder, the thin input slices makes the network sufficiently small compared to a regular volumetric CNN.|
|||We fine-tune this CNN classifier on ModelNet10 and ModelNet40.|
|||By comparing RandomInit and Ours-FT, we can see unsupervised 3D-ED-GAN pretraining is able to guide the CNN classifier to capture the rough geometric structure of 3D objects.|
|||Since our model is easy to fit into GPU memory compared with other 3D CNN methods [21, 22].|
||14 instances in total. (in iccv2017)|
|262|Pradeep_Kumar_Jayaraman_Quadtree_Convolutional_Neural_ECCV_2018_paper|Unfortunately, most of the traditional CNN architectures, particularly for images, are unable to exploit the sparsity of such data, and learning from such datasets is unnecessarily inefficient in both training time and memory consumption.|
|||To alleviate this, recent works such as OctNets [3], O-CNN [4], and OGN [5] decompose the 3D meshes hierarchically into octrees and adapt CNN operations to consider the special octree structure.|
|||Third, we adapt CNN operations in the quadtree by considering the special data representation.|
|||However, for representing data hierarchically for CNN training purposes, this is infeasible since CPUs and GPUs are efficient in processing contiguous array data.|
|||Graham proposed a sparse version of the CNN for 2D image [12] and 3D voxel [13] data that only performs convolutions on non-zero sites and their neighbors within the receptive field of the kernel.|
|||[4] only considered the surface voxels of the 3D data in the octree representation and drastically improved memory and computational costs in performing CNN operations.|
|||3.3 Quadtree CNN Operations  Data structure To facilitate CNN operations such as convolution and pooling on the quadtree, we employ a custom data structure that is different from  6  Pradeep Kumar Jayaraman, Jianhan Mei, Jianfei Cai and Jianmin Zheng  Quadtree (l = 3)  Index array '( & Offset array )(  Data array .|
|||3: Illustration of CNN operations on quadtrees.|
|||With these fundamental operations defined on the quadtree, it is straightfor ward to compose them to design commonly used CNN architectures.|
|||Note that this experiment is mainly to study the behaviour of QCNN compared to traditional CNN and is not tuned to obtain the best accuracy.|
|||4: Comparison of mean test accuracy (computed from 5 runs) of QCNN and traditional CNN classifiers progressively during training on various datasets.|
|||As before, we also train a traditional CNN similarly for  Quadtree Convolutional Neural Networks  13  Fig.|
|||5: Sketch simplification results obtained with traditional CNN and QCNN trained using the same network architecture.|
|||Resolution #MACC(108) Memory (MB)  CNN QCNN CNN QCNN  256  256 512  512  724.76 137.54 127.14 13.5 2899.05 349.28 508.56 35.35 1024  1024 11596.21 917.14 2034.24 97.9  with much lower computational complexity and memory cost, compared to traditional convolutional neural networks.|
||14 instances in total. (in eccv2018)|
|263|cvpr18-Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation|Focusing on the latter issue, recent works have shown that CRFs can be integrated into deep architectures [22, 31] and can be exploited to optimally fuse the multi-scale information derived from inner layers of a CNN [36].|
|||However, we significantly depart from previous methods and we argue that more accurate estimates can be obtained operating not only at the prediction level but exploiting directly the internal CNN feature representations.|
|||In summary, we make the following contributions: (i) We propose a novel deep learning model for calculating depth maps from still images which seamlessly integrates a front-end CNN and a multi-scale CRF.|
|||In [22] a continuous CRF is proposed for generating depth maps from CNN features computed on superpixels.|
|||Differently from previous research [22, 36], our CRF model does not simply act in order to refine the final prediction map of the CNN neither requires as input multiple score maps of the same size.|
|||In other words, by learning the attention maps we automatically discover which information derived from inner CNN representations is relevant for final depth estimation.|
|||(11) shows  1 A  that, in analogy with previous methods employing an attention model [4, 13], in our framework we also compute the attention variables by applying a sigmoid function to the features derived by our CNN model.|
|||The CNN architecture is made of two main components, i.e.|
|||For a fair comparison in the table we also report information about the adopted training set, as it represents an important factor for CNN performance.|
|||Method  Error  (lower is better) rel  log10  rms  Accuracy  (higher is better)   < 1.25  < 1.252  < 1.253  0.168 1.072 5.101 Front-end CNN (w/o multiple deep supervision) Front-end CNN (w/ multiple deep supervision) 0.152 0.973 4.902 0.143 0.949 4.825 Multi-scale feature fusion with naive concatenation 0.134 0.895 4.733 Multi-scale feature fusion with CRFs (w/o attention model) Multi-scale feature fusion with CRFs (w/ attention model) 0.127 0.869 4.636 Multi-scale feature fusion with CRFs (w/ structured attention model) 0.122 0.897 4.677  0.741 0.782 0.795 0.803 0.811 0.818  0.932 0.931 0.939 0.942 0.950 0.954  0.981 0.974 0.978 0.980 0.982 0.985  et al.|
|||In the table, multiple deep supervision refers to training the front-end CNN with the approach in [32]; w/ attention model refers to considering attention variables ai s in the optimization but discarding the structured potential; w/ structured attention model indicates the using of the structured attention model.|
|||The main contribution of this work is a CRF model which optimally combines multi-scale information derived from the inner layers of a CNN by learning a set of latent features representations and the associated attention model.|
|||Importantly, our framework can be used in combination with several CNN architectures and can be trained end-to-end.|
|||Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||14 instances in total. (in cvpr2018)|
|264|cvpr18-Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition|Learning a Discriminative Filter Bank  within a CNN for Fine-grained Recognition  Yaming Wang1, Vlad I. Morariu2, Larry S. Davis1  1University of Maryland, College Park  {wym, lsd}@umiacs.umd.edu  2Adobe Research morariu@adobe.com  Abstract  Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for finegrained recognition essentially enhance the mid-level learning capability of CNNs.|
|||These methods achieve better performance compared to two types of baselines: (i) they outperform their counterparts with hand-crafted features (e.g., SIFT) by a huge margin, which means that lowlevel CNN features are far more effective than previous hand-crafted ones; (ii) they significantly outperform their baselines which finetune the same CNN used for feature extraction.|
|||The second category, end-to-end feature encoding [30, 10, 23, 8, 5], enhances CNN mid-level learning by encoding higher order statistics of convolutional feature maps.|
|||The resulting framework enhances the mid-level learning capability of the classical CNN by introducing a bank of discriminative filters.|
|||Related Work  Fine-grained recognition Research in fine-grained recognition has shifted from multistage frameworks based on hand-crafted features [52, 3, 48, 6, 13] to multistage  framework with CNN features [24, 44, 35, 53, 42], and then to end-to-end approaches.|
|||Intermediate representations in CNN Layer visualization [49] has shown that the intermediate layers of a CNN learn human-interpretable patterns from edges and corners to parts and objects.|
|||As empirically observed by [1], a classical CNN learns a combination of grandmother cells and a distributed code.|
|||For example, in a 16-layer VGG network (VGG-16), the output of the 10th convolutional layer conv4 3 represents patches as small as 92  92 with stride 8, which is small and dense enough for our task given common CNN input size.|
|||In each table from top to bottom, the methods are separated into five groups, as discussed in Section 1, which are (1) fine-tuned baselines, (2) CNN features + multi-stage frameworks, (3) localization-classification subnets, (4) end-to-end feature encoding and (5) DFL-CNN.|
|||Earlier multi-stage frameworks built upon CNN features achieve comparable results, while they often require bounding box annotations and the multi-stage nature limits their potential.|
|||Conclusion  We have presented an approach to fine-grained recognition based on learning a discriminative filter bank within a CNN framework in an end-to-end fashion without extra annotation.|
|||Part-stacked cnn for  fine-grained visual categorization.|
|||Bilinear CNN models for fine-grained visual recognition.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||14 instances in total. (in cvpr2018)|
|265|McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper|The CNN involves many individual processing steps, therefore for notational simplicity we refer to the complete CNN as a function, f = C(x), that takes an image x as input and produces a vector f as output.|
|||In general, a CNN processes an image using a series of layers, where each individual layer is composed of convolution, pooling, and non-linear activationfunction steps.|
|||Let s = s(1)...s(T ) be a video sequence, of length T , consisting of whole-body images of a person, where s(t) is the image at time t. Each image, s(t), is passed through the CNN to produce a vector, f (t) = C(s(t)), where f (t) is the vectorised representation of the CNNs final layer acti Figure 2.|
|||The structure of our proposed CNN and recurrent layer, where r(t) is the RNNs state at time t and o(t) is the sequence vector output at time t. See Section 3.2 and Section 3.3 for details.|
|||Note that the parameters of the CNN are shared across all time-steps meaning that each input frame is processed by the same feature-extraction network.|
|||Dropout [37] is used between the CNN and the recurrent layer in order to reduce over-fitting.|
|||Complete details of the CNN architecture are given Fig.|
|||It is commonly accepted that the performance of deep networks is due to hierarchical feature extraction that takes place over many layers [11], therefore we use a CNN to pre-process each input image into a higher-level representation before the recurrent layer.|
|||By incorporating recurrent connections between the CNN and temporal pooling layers, we aim to better capture temporal information present in the video sequence.|
|||As described in Section 3.2, f (t) is the vectorized output of the CNNs final layer activation maps, for the image  1327  s(t) observed at time t. We can incorporate recurrent connections between the CNN and temporal-pooling layer as follows:  o(t) = Wif (t) + Wsr(t1) r(t) = T anh(o(t))  (1)  (2)  The output, o(t)  Re1, at each time-step is a linear combination of the vectors, f (t)  RN 1, containing information on the current input image, and, r(t1)  Re1, containing information on the RNNs state at the previous timestep.|
|||In the temporal pooling layer, after forward propagation of a sequence of images, the appearance features produced by the combined CNN and recurrent layer for all time-steps,  {o(1) .|
|||During training with back propagation through time, all recurrent connections are unrolled to create a deep feed-forward graph, where the weights of the recurrent layer and CNN are shared between all timesteps [31].|
|||We also consider a baseline method [30] for computing a similarity-score between  sequences that processes each frame individually using a single-frame CNN trained using a Siamese architecture and whose individual frame outputs are combined into a single decision without mean-pooling: The similarity between the sequences is then taken as the average Euclidean distance between corresponding frames.|
|||This single-shot CNN is exposed to all the data from the video sequences available in training, and trained using pairs of still-images, rather than sequence pairs, where a different single frame over the full sequence length was randomly selected at each epoc.|
||14 instances in total. (in cvpr2016)|
|266|CLKN_ Cascaded Lucas-Kanade Networks for Image Alignment|The CNN learns data-driven features and the Lucas-Kanade layer performs the inverse compositional Lucas-Kanade algorithm [3].|
|||Our network combines the strengths of both the CNN and the Lucas-Kanade algorithm.|
|||The CNN provides the ability to learn features that are both useful for alignment and robust to photometric variations.|
|||They trained a VGG-style CNN to directly regress the homography between two images.|
|||However, their CNN model cannot achieve sub-pixel accuracy.|
|||The CNN we employ here is fully convolutional [20] and hence can take input of arbitrary sizes.|
|||At each stage k = 1 ... K in the cascade, the CNN at level k extracts feature maps F (k) from I and T , respectively.|
|||In particular, the CNN produces the output feature map with a downsampling factor of 2Kk.|
|||More specifically, the CNN at the top pyramid level (k = 1) is trained on the original training set {(T, I, p0, p)}.|
|||For the CNN at level k = 1 ... K, all its convolutional layers have 64 filters except for the last layer, which has C filters to produce an output feature map with C channels.|
|||We used a shallower CNN (three layers) in the last level to reduce the memory usage and the training time.|
|||We found that such CNN is sufficient to bring roughly aligned images into sub-pixel alignment.|
|||DHN [10]: A deep learning based method that trains a VGG-style CNN to directly regress the homography.|
||13 instances in total. (in cvpr2017)|
|267|Lin_Efficient_Piecewise_Training_CVPR_2016_paper|For  Deep structured model: contextual deep CRF  Unary potential net: Multi-scale CNN   Pairwise potential net Multi-scale CNN  ...  ...|
|||Unlike these methods, our method focuses on improving the coarse (low-resolution) prediction by learning general CNN pairwise potentials to capture semantic relations between patches.|
|||Different from these methods, we explore efficient piecewise training of CRFs with CNN pairwise potentials.|
|||It is the confidence value for the node pair (p, q) when they are labeled with the class value (yp, yq), which measures the compatibility of the label pair (yp, yq) given the input image x. V is the corresponding set of CNN parameters for the potential V , which we need to learn.|
|||Therefore, our system takes advantage of both contextual CNN potentials and the traditional smoothness potentials to improve the final system.|
|||(5)  Adding regularization to the CNN parameter , the optimization problem for CRF learning is:  min     2  kk2  2 +  N  Xi=1  (cid:20)E(y(i), x(i); ) + log Z(x(i); )(cid:21).|
|||Here we develop this idea for the case of training the CRF with the CNN potentials.|
|||As previously discussed, CNN training usually involves a large number of gradient update iterations.|
|||Our contextual model with CNN pairwise potentials achieves the best performance, which sets a new state-ofthe-art result on the NYUDv2 dataset.|
|||The result shows that our CNN baseline implementation (FullyConvNet) achieves very similar performance (slightly better) than the FCN method.|
|||Finally, adding our contextual CNN pairwise potentials brings significant further improvement, for which we achieve the best performance in this dataset.|
|||Results on PASCAL VOC 2012  PASCAL VOC 2012 [12] is a well-known segmentation evaluation dataset which consists of 20 object categories  FCN-32s [32] FullyConvNet Baseline + sliding pyramid pooling + multi-scales + boundary refinement + CNN contextual pairwise  60.0 61.5 63.5 67.0 68.5 70.0  42.2 43.2 45.3 50.1 50.9 53.6  29.2 30.5 32.4 37.0 38.3 40.6  (a) Testing  (b) Truth  (c) Predict  (d) Testing  (e) Truth  (f) Predict  Figure 7.|
|||We formulate CNN based pairwise potentials for modeling semantic relations between image regions.|
||13 instances in total. (in cvpr2016)|
|268|Yeung_End-To-End_Learning_of_CVPR_2016_paper|[46] use fusions of dense trajectories, frame-level CNN features, and/or sound features in a sliding window framework to perform temporal action detection.|
|||All these methods compute dense trajectories [38] and/or CNN features over temporal windows, and use a sliding window approach with non-maximum suppression to obtain predictions.|
|||[13] uses dense trajectories only, [39] uses temporal windows of combined dense trajectories and CNN features, and [22] uses temporal windows of dense trajectories with videolevel CNN classification predictions.|
||| CNN with NMS removes direct prediction of temporal action bounds.|
|||The relative difference with the CNN decreases and then flips when we decrease , indicating that  Baseball Pitch Basket.|
|||Finally, the CNN with NMS achieves significantly lower performance than all ablation models except the Ours w/o loc model, quantifying the contribution of our end-to-end framework.|
|||The LSTM with NMS achieves lower performance than the CNN with NMS, despite adding greater temporal consistency.|
|||For reference, we also show frame-level CNN probabilities from the VGGNet used in our observation network; higher intensity indicates higher probability and provides insight into frame-level signal for the class.|
|||For reference, we also show framelevel CNN probabilities from the VGGNet used in our observation network, to provide insight into frame-level signal for the action.|
|||While the strength of the frame-level CNN probabilities over the sequence would be difficult for standard sliding-window approaches to handle, our model is able to discern the two separate instances.|
|||Our model outperforms existing work [3], which is based on a combination of dense trajectories, SIFT, and ImageNet-pretrained CNN features, by significant margins.|
|||6, this is evident in the weaker, more diffuse frame-level CNN probabilities for the action.|
|||Exploiting image-trained cnn architectures for unconstrained video classification.|
||13 instances in total. (in cvpr2016)|
|269|cvpr18-Weakly-Supervised Deep Convolutional Neural Network Learning for Facial Action Unit Intensity Estimation|[6] exploited a 7-layer CNN for both AU intensity estimation and AU detection.|
|||CRF is parameterized by using copula functions to allow non-linear AU intensity relations while CNN is used to learn deep representation.|
|||We present the structure of CNN in Sec.|
|||We train a CNN for each AU individually since the locations of peak and valley frames are different for different AUs.|
|||We encourage image representations extracted from the CNN to satisfy that the closer two frames are, the closer their representations are, namely,  d(f i  m, f j  m)  d(f i  m, f k  m), 1  i < j < k  Nm,  (1)  m, f j  m) = ||f i  where d(f i m||2.|
|||We encourage the CNN to provide head pose invariant representation for AU intensity estimation.|
|||We encourage the CNN to provide representations that can tell emotional faces apart from neutral faces.|
|||CNN structure  We use a CNN with 3 convolutional layers, 3 max pooling layers, and 1 fully connected layer.|
|||The CNN structure is shown is Fig.|
|||The dimension of the output is 1 since we train a CNN for each AU individually.|
|||Though the CNN is trained by using tuples, it can be used to predict AU intensity for a single image.|
|||Given a testing image, we feed it to the CNN and the predicted AU intensity is y = f (X; ).|
|||Ordinal In  regression with multiple output cnn for age estimation.|
||13 instances in total. (in cvpr2018)|
|270|DeepNav_ Learning to Navigate Large Cities|In deep RL the policy is encoded by a CNN that outputs the value of performing an action in the current state.|
|||A mini-batch for CNN training is formed by some transitions that are driven by the current policy and some that are random.|
|||For example, the navigation task discussed in this paper requires the CNN to predict the direction of the next step taken by the agent.|
|||Such tasks require the CNN to assess the future implications of the content of an image, and are relatively unexplored in the literature.|
|||This dataset is marked automatically with locations of five types of destinations (Bank of America, church, gas station, high school and McDonalds) using publicly available  mapping APIs [2, 3]   We develop and evaluate 3 different CNN architectures that allow an agent to pick a direction at each location to reach the nearest destination.|
|||The rest of the paper is organized as follows: 2 describes the related work in this area, 3 describes our dataset collection process, 4 describes the CNN architectures and training processes and 5 presents results from our algorithm.|
|||In this paper, we first use a CNN to predict the distance and show that data-driven convolutional features perform better than expensive hand crafted features for this task.|
|||Next, we propose 2 novel mechanisms to supervise a CNN for this task and show that they lead to better performance.|
|||The agent takes a step in the direction that is predicted by the CNN to have the least distance estimate.|
|||The third approach, DeepNav-pair, decomposes the problem of picking the optimal action into pairs of decisions, and employs a Siamese CNN architecture.|
|||We collect 5 labels  (a) DeepNav-distance (top), DeepNav-direction (bottom)  (b) DeepNav-pair  Figure 4: DeepNav CNN architectures.|
|||To relax the CNN loss function for making a wrong decision based on low-information visual features far away from the destinations, we modify the loss function by weighing the training samples geographically.|
|||It can be seen that the CNN correctly learns that center-city commercial areas have a high probability of having a McDonalds establishment, and gas stations are found around intersections of big streets with that have parked cars.|
||13 instances in total. (in cvpr2017)|
|271|Hariharan_Hypercolumns_for_Object_2015_CVPR_paper|To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel.|
|||Our hypothesis is that the information of interest is distributed over all levels of the CNN and should be exploited in this way.|
|||[15] combine CNN outputs from multiple scales of an image to do semantic segmentation.|
|||However, the features being combined still come from the same level of the CNN and hence have similar invariance.|
|||Toshev and Szegedy [38] use a CNN to regress to keypoint locations.|
|||Computing the hypercolumn representation: We take the cropped bounding box, resize it to a fixed size and feed it into a CNN as in [18].|
|||All the intermediate outputs in a CNN are feature maps (the output of a fully connected layer can be seen as a 1  1 feature map).|
|||Representation as a neural network: We can write our final hypercolumn classifier using additional layers grafted onto the original CNN as shown in Figure 2.|
|||Representing our pipeline as a neural network allows us to train the whole network (including the CNN from which we extract features) for this task.|
|||For each candidate in this set, we predict a segmentation, and score this candidate using CNN features computed on the segmentation.|
|||Layers of the original classification CNN are shown in red, and layers that we add are in blue.|
|||This pipeline scores bottom-up region proposals from [1] using CNN features computed on both the cropped bounding box of the region and the cropped region foreground.|
|||Next we segment each candidate using hypercolumns and score it using a CNN trained to classify regions.|
||13 instances in total. (in cvpr2015)|
|272|cvpr18-Perturbative Neural Networks|The past several years have witnessed the evolution of many successful CNN architectures such as AlexNet [14], VGG [27], GoogLeNet [30], ResNet [8, 9], MobileNet [10], and DenseNet [11], etc.|
|||This seemingly simple inner product operation f , whether it has been efficiently implemented in the frequency do main or spatial domain, is a major computational bottleneck of deep CNN models.|
|||Therefore, the standard CNN convolution operation is as follows, assuming no bias in the convolution:  CNN :  y = vec(mat(x)  w) =  xi,shift  wi  (3)  k2  Xi  where xi,shift is the i-th spatially shifted version of the input in vectorized form, and wi is the i-th element in the convolution filter w.  For PNN, the same input x will be perturbed with N random noise masks ni, and then linearly combined using weight vector v whose elements are vis to form the output response vector y.|
|||Given the CNN output vector y, we can always find the vector v for PNN such that the PNN output y is equal to or approximately equal to y.|
|||If N < d, X is a tall matrix, so a least square solution can be found for v as:  v = ( X X)1 Xy  (8)  Next, we will derive a relationship between the convolutional weights in CNN and the perturbation weights in PNN assuming y = y.|
|||Relating PNN and CNN: A Micro View  Now let us consider a single neighborhood (patch) in the input tensor where the convolution is taking place, and obtain a relation between PNN and CNN with some mild assumptions.|
|||=  xc + XiNc PN  xc + XiNc | {z  oiw i  = y  }  nc  (22)  (23)  oiwi = y  (24)  y i wi  PN  (25)  (26)  i behaves like additive perturbation noise, will allows us to relate the CNN forumulation to the PNN formulation.|
|||Therefore, this analysis of the CNN operation establishes a relation between the CNN and the PNN formulation.|
|||The ratio of the number of parameters in CNN and PNN is:  # param.|
|||As can be seen, compared to the state-ofthe-art ResNet-18 performance (single center-crop protocol) on a standard CNN [4, 8], the proposed PNN achieves comparable classification accuracy on ImageNet-1k.|
|||We have shown the parameter ratio of CNN over PNN in the last column in Table 1.|
|||The results in Table 2 show that PNN-ResNet-50 performs competitively with the corresponding network with CNN layers [4] on ImageNet-1k.|
|||We study both CNN architectures (VGG-16, ResNet-50) by replacing the convolution layers with the proposed PNN modules.|
||13 instances in total. (in cvpr2018)|
|273|Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper|In this network, the lower CNN layers are shared to learn view-independent representations.|
|||proposed C3D [27], which designs a 3D CNN model for video datasets by combining appearance features with motion information.|
|||Simonyan and Zisserman [24] first proposed the twostream CNN to extract features from the RGB key frames and the optical flow channel.|
|||[34] integrated the key factors from iDT and CNN and achieved significant performance improvement.|
|||also proposed the temporal segment network(TSN) [35] to utilize segments of videos under the two-stream CNN framework.|
|||[4,5] have utilized discrete CRF in CNN for human pose estimation.|
|||(1) Basic multi-branch module is composed of one shared CNN and several view-specific CNN branches.|
|||view-independent features) for all videos by using one shared CNN, and then extracts view-specific features by using multiple CNN branches, which will be described in Section 3.2.|
|||2, the basic multi-branch module consists of two parts: 1) shared CNN : Most of the convolutional layers are shared to save computation and generate the common features (i.e.|
|||Soft ensemble of prediction scores Different CNN branches share common information and have each own refined view-specific information, so the combination of results from all branches should achieve better classification results.|
|||The layers used in the shared CNN and CNN branches in the inception 5b block.|
|||The shared CNN can be any of the popular CNN architectures, which is followed with V view-specific branches, each corresponding to one view.|
|||The shared CNN layers include the ones from the input to the block inception_5a.|
||13 instances in total. (in eccv2018)|
|274|Cimpoi_Deep_Filter_Banks_2015_CVPR_paper|A second drawback (II) is that the input to the CNN has to be of fixed size to be compatible with the fully connected layers, which requires an expensive resizing of the input image, particularly when features are computed for many different regions [14, 15].|
|||However, in contrast to it we do not need repeated evaluations of the CNN since the convolutional features can be computed just once for the entire image and pooled differently.|
|||Finally, we analyze the utility of different network layers and architectures as filter banks, concluding that: SIFT is competitive only with the first few layers of a CNN (Fig.|
|||4) and that significant improvement to the underlying CNN architecture, such as the ones achieved by the very deep models of Simonyan and Zisserman [39], directly translate into much better filter banks for texture recognition.|
|||Both descriptors are based on the same CNN features [23] obtained from an off-the-shelf CNN pre-trained on the ImageNet ILSVRC 2012 data as suggested in [6, 21, 35].|
|||Since the underlying CNN is the same, it is meaningful to compare FCand FV-CNN directly.|
|||FV-CNN is related to the method of [15], which uses a similar underlying CNN and VLAD instead of FV for pooling.|
|||and marginally better (68.8%  69.1%) when the same CAFFE CNN is used (Tab.|
|||However, [46] uses a category-specific part detector and corresponding part descriptor as well as a CNN fine-tuned on the CUB data; by contrast, FV-CNN and FC-CNN are used here as global image descriptors which, furthermore, are the same for all the datasets considered.|
|||So far, the same underlying CNN features, trained on ImageNets ILSVCR, were used for all datasets.|
|||The FC-CNN results are in line with those reported in [47]  in scene recognition with FC-CNN the same CNN architecture performs better if trained on the Places dataset instead of the ImageNet data  (58.6%  65.0% accuracy4).|
|||Convolutional layer analysis  We study the performance of filter banks extracted from different layers of a CNN in the FV-CNN framework.|
|||The CNN filter banks from layer 3 and beyond significantly outperform SIFT.|
||13 instances in total. (in cvpr2015)|
|275|Narihira_Learning_Lightness_From_2015_CVPR_paper|Our model is built upon two key elements of recent work:  Image  our HSC results  our CNN results  Figure 3.|
|||We show HSC and CNN classifier results for some IIW [4] test images.|
|||Distinguishing characteristics as compared to CNN features include that HSC features are learned generatively and extracted from a multilayer slice of a deep network.|
|||We attempt to train the CNN from both randomly initialized weights and weights initialized by pre-training on ImageNet [7].|
|||Following Equation (3) and choosing a linear form for f with either the HSC or CNN representations of the preced ing section serving as features z, we have f (zi, zj) = wT (zi  zj)  (7)  We learn HSC classifier weights w by ridge ranking re gression on the human ground-truth data for reflectance:  (cid:88)  log(cid:0)1 + exp(JijwT (zi  zj))(cid:1)+wT w  (cid:40)  Jij =  i > Rh j i < Rh j Cij = confidence in Jij  Rh 1, 1, Rh  (8)  (9)  (10)  (6)  min (w) =  i,j  where:  i = Rh  Here Rh refers to human ratings of relative reflectance (lightness) on the IIW dataset.|
|||While dense evaluation of g() is also possible using CNN features, the CNN implementation we use [12] is not targeted to fully convolutional evaluation over an arbitrarysized input and it is prohibitively expensive to run independently for all patches in an image.|
|||As in Figure 7, we visualize predictions of our model (HSC and CNN versions) and the CRF model of [4].|
|||Right: Examples with significant performance gap between CRF and CNN models.|
|||of our local classifier model, our CNN performance is on par with that of the best method on the test set, and ahead of all others.|
|||Here, CNN refers to CNN trained from randomly initialized weights while CNN-ImageNet is initialized with pre-trained weights on ImageNet.|
|||Our method with CNN features provides a more uniform improvement over CRF across lighting conditions.|
|||Top Left: Plotting errors against patch intensity variance reveals that the the CNN model has lower mistake count in smooth areas (low pixel intensity variance).|
|||Our CNN model is uniformly better than others over the range of patch mean intensity.|
||13 instances in total. (in cvpr2015)|
|276|cvpr18-Feedback-Prop  Convolutional Neural Network Inference Under Partial Evidence|We demonstrate this behavior using our feedback-prop inference for multiple tasks and under multiple CNN models.|
|||Since the underlying shared representations of a CNN capture common patterns among target outputs, we find that they can act as pivoting variables to transfer knowledge among variables in the target space.|
|||Our contributions can be summarized as follows:   A general feedback-based propagation inference procedure (feedback-prop) for CNN inference under partial evidence.|
||| An extensive analysis of CNN architectures regarding optimal layers in terms of information sharing with respect to target variables using feedback-prop.|
|||One prominent example is the generation of adversarial examples that are constructed to fool a CNN model [15].|
|||We find that CNN layers that lie somewhere in the middle are more beneficial to optimize as pivot variables under our model than the input image.|
|||Feedback(cid:173)prop  Let us consider a feed-forward CNN already trained to predict multiple outputs for either a single task or multiple tasks.|
|||Layer(cid:173)wise Feedback(cid:173)prop (LF)  In this section we propose a more general version of feedback-prop that leverages multiple intermediate representations in a CNN across several layers: Layer-wise feedback-prop.|
|||We first train a multi-label prediction model by modifying a standard CNN to generate a 1000-dimensional output, and learn logistic regressors using the following loss function:  L =   d  X  i=1  1 N  N  X  j=1  j[yij log (fj(Ii, )) +  (6)  (1  yij) log(1  (fj(Ii, )))],  where (x) = 1/(1 + exp(x)) is the sigmoid function, fj(Ii, ) is the unnormalized output score for category j given image Ii, and  are the model parameters of the underlying CNN.|
|||We use a CNN + Softmax classifier as our first Baseline, and as a second baseline a CNN + Softmax classifier that uses true values for coarse categories in the form of a binary indicator vector as additional input to the classifier (Baseline + PL).|
|||We train a multi-task CNN model on the COCO dataset [22] to jointly perform caption generation and multilabel object categorization.|
|||In this section, we analyze where are the most useful intermediate representations in a CNN under feedback-prop.|
|||We proposed two variants of a feedback propagation inference approach to leverage this dynamic property of CNNs and showed their effectiveness for making predictions under partial evidence for general CNN models trained in a multi-label or multi-task setting.|
||13 instances in total. (in cvpr2018)|
|277|cvpr18-Pose-Robust Face Recognition via Deep Residual Equivariant Mapping|[20] enhances the performance of CNN through augmenting training and test data with face images differ in 3D shape, expression and pose.|
|||[35] propose a multi-task CNN that exploits side tasks, e.g., pose, to serve as regularizations for learning pose-specific identity features.|
|||We denote a CNN as a function  and the image representation it maps from image x as (x).|
|||We call the network a stem CNN or base network.|
|||The stem CNN can also be of any of the existing face recognition models [29, 25, 33].|
|||Note that this requirement does not add additional burden to the stem CNN since the face alignment process is a standard preprocessing step of many face recognition pipelines2.|
|||The proposed light-weight block can also be trained together with the stem CNN in an end-to-end manner.|
|||If the stem CNN is not plain but trained previously, we can first fine-tune the stem CNN while training the DREAM block end-to-end using an existing face recognition loss (e.g., verification loss, identification loss, or both).|
|||We train the stem CNN and DREAM block together then train the DREAM block separately with frontal-profile face pairs.|
|||To show the benefits of DREAM on different base networks, we perform our experiments using the following stem CNN architectures.|
|||[32] Pooling Faces [8] Deep Multi-Pose [1] Multi-task CNN [35] PAMs [19] DCNNfusion (f.t.)|
|||This observation ascertains our design of exerting a higher degree of correction to a face pose larger than 45, as this pose range is harder to be handled by the stem CNN (as observed from Figure 6).|
|||Unconstrained face verification using deep cnn features.|
||13 instances in total. (in cvpr2018)|
|278|Ubernet_ Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory|Detection  Semantic Boundaries & Segmentation Human Parts  Figure 1: We train in an end-to-end manner a CNN that jointly performs tasks spanning low-, midand highlevel vision; all results are obtained in 0.7 seconds per frame.|
|||In [50] a CNN is used for joint localization, detection and classification, [17] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [20] train a system for joint detection, pose estimation and region proposal generation.|
|||The problem of using a single network to solve multiple tasks has been recently pursued in the context of deep  Our first contribution enables us to train a CNN from diverse datasets that contain annotations for distinct tasks.|
|||This allows us to perform end-to-end CNN training, while using the union of the diverse datasets as a single training set.|
|||As in [32, 39, 45, 50], we use a fully-convolutional network, namely a CNN that provides a field of responses, its output.|
|||Figure 2: UberNet architecture: an image pyramid is formed by successive down-sampling operations, and each image is processed by a CNN with tied weights; the responses of the network at consecutive layers (Ci) are processed with Batch Normalization (Bi) and then fed to taskspecific skip layers (Et i); these are combined across network layers (F t) and resolutions (S t) and trained using task-specific loss functions (Lt), while the whole architec ture is jointly trained end-to-end.|
|||Our training objective is the sum of per-task losses and regularization terms applied to task-specific, as well as shared layers:  L(w0,1,...,T ) =R(w0)+  T  X  t=1  t(R(wt)+Lt (w0, wt)) , (1)  where t indexes tasks, w0 denotes shared CNN weights, wt are task-specific weights, t determines the relative importance of task t, R(w) =  2kwk2 is an l2 regularization, and Lt (w0, wt) is the task-specific loss:  Lt (w0, wt) =  1 N  N  X  i=1  t,iLt (cid:0)f i  t (w0, wt), yi  t(cid:1) .|
|||In our training we use an effective batchsize Bp of 2 for detection, 10 for all other task-specific parameters, and 30 for the shared CNN features, w0.|
|||Using a larger batch size for the shared CNN features allows their updates to absorb information from more images, containing multiple tasks, so that task-specific idiosyncracies will cancel out by averaging.|
|||If we consider that every layer requires N bytes of memory for its activation and gradient signals, and we have LC layers for a shared CNN trunk, T tasks, and LT layers per task, the memory complexity of a naive implementation would be 2N (LC + T LT ), as shown in Fig.|
|||This means that each task can be removed from memory once it has communicated its gradient signals to the shared CNN trunk.|
|||We argue however that this can be anticipated given (a) the diversity of the tasks and (b) the limited parameter budget of our common CNN trunk.|
|||Secondly, our networks common CNN trunk has a limited number of parameters and layers.|
||13 instances in total. (in cvpr2017)|
|279|Yan_Object_Detection_by_2015_CVPR_paper|The CNN based representation has shown great advantages and has been adopted by all the leading  methods in ImageNet [43].|
|||In [14], CRF is built on top of CNN features for scene parsing.|
|||Different CNN features can largely  affect the final performance and we leave the details in the Section 5.2.|
|||In this paper, we use the RCNN [20] detection result (the details of CNN can be found in Section 5.2) for initialization.|
|||Our best single CNN based model has a detection mAP of 42.5%.|
|||Our method shows that by carefully designing new detection method, there exists potentials to get better detection result although the CNN is not good enough.|
|||The baseline RCNN implementation3 uses a CNN with AlexNet [28] which is trained on imageNet classification data and fine-tuned on detection data.|
|||We independently find that the depth of CNN plays a key role to the final performance, which is in consistent with [46, 45] for classification and [46, 56] for detection.|
|||Directly changing the CNN used in this paper to GoogLeNet could further improve the detection performance4.|
|||To fairly compare our method with the RCNN baseline, we use exactly the same CNN feature extractor and the same object proposals, as in [20].|
|||Similar to the observations on ImageNet, the superpixel labeling algorithm has a 3% improvement compared to the RCNN when using the same CNN feature.|
|||In our current implementation, we use the RCNN framework with new CNN feature extractor based on the open source software Caffe [25].|
|||The CNN used in RCNN and the parameters in the energy function are learned sequentially, and we plan to jointly learn them for further performance gain.|
||13 instances in total. (in cvpr2015)|
|280|Henry_W._F._Yeung_Fast_Light_Field_ECCV_2018_paper|[14] proposed to use a blurdeblur scheme to counter the problem of information asymmetry between angular and spatial domain and a single CNN is used to map the blurred epipolar-plane images (EPIs) from low to high resolution.|
|||The first CNN performs disparity estimation based on a set of depth features pre-computed from the given input SAIs.|
|||The estimated disparities are then used to warp the given SAIs to the novel SAIs for the second CNN to perform color estimation.|
|||Advancement in single image super-resolution (SISR) is recently made possible by the adoption of deep CNN models [15,16,17,18].|
|||[19,20], developed a CNN model that jointly super-resolves the LF in both the spatial and angular domain.|
|||This model concatenates at the channel dimension a subset of the spatially super-resolved SAIs from a CNN that closely resembles the model proposed in [15].|
|||The concatenated SAIs are then passed into a second CNN for angular super-resolution.|
|||[14] developed a CNN model that inherits the basic architecture of [15] with an addition residual learning component as in [16].|
|||2, we propose a novel CNN model to provide direct end-to-end mapping between the luma component of the input SAIs, denoted as KY , and that of the  novel SAIs, denoted as bNY .|
|||4 Experimental Results  Our model was compared with two state-of-the-art CNN based methods that are specifically designed for densely-sampled LF reconstruction, i.e., Kalantari et al.|
|||To characterize the high-dimensional spatial-angular clues within LF data accurately and efficiently, we have designed an end-to-end trained CNN that extensively employs spatial-angular alternating convolutions for fast feature transformation and stride-2 4-D convolutions for rapid angular dimension reduction.|
|||Considering the efficiency and effectiveness of the proposed CNN model in processing LF data, we believe such a design has great potential on LF compression, as well as a wide range of LF image processing tasks, including but not limited to LF spatial super-resolution, temporal super-resolution and depth estimation.|
|||Wang, T.C., Zhu, J.Y., Hiroaki, E., Chandraker, M., Efros, A.A., Ramamoorthi, R.: A 4d In: Proceedings of the  light-field dataset and cnn architectures for material recognition.|
||13 instances in total. (in eccv2018)|
|281|cvpr18-V2V-PoseNet  Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation From a Single Depth Map|We design our model as a 3D CNN that provides accurate estimates while running in real-time.|
|||[44] firstly utilized CNN to localize hand keypoints by estimating 2D heatmaps for each hand joint.|
|||[9] extended this method by exploiting multi-view CNN to estimate 2D heatmaps for each view.|
|||[15] proposed the viewpointinvariant pose estimation method using CNN and multiple rounds of a recurrent neural network.|
|||Their proposed CNN architecture and occupancy grids outperform those of Wu et al.|
|||[31] estimated the per-voxel likelihood for each body keypoint via 2D CNN as in the Figure 2(b).|
|||To the best of our knowledge, our network is the first model to generate voxelized output from voxelized input using 3D CNN for 3D pose estimation.|
|||To overcome these limitations, we train a simple 2D CNN following Oberweger et al.|
|||Thus, it is based on the 3D CNN architecture that treats the Z-axis as an additional spatial axis so that the kernel shape is whd.|
|||As the table shows, converting the input representation type from the 2D depth map to 3D voxelized form (also converting the model from 2D CNN to 3D CNN) substantially improves performance, regardless of output representation.|
|||Comparison with state(cid:173)of(cid:173)the(cid:173)art methods  We compared the performance of the V2V-PoseNet on the three 3D hand pose estimation datasets (ICVL [40], NYU [44], and MSRA [38]) with most of the stateof-the-art methods, which include latent random forest (LRF) [40], cascaded hand pose regression (Cascade) [38], DeepPrior with refinement (DeepPrior) [29], feedback loop training method (Feedback) [30], hand model based method (DeepModel) [57], hierarchical sampling optimization (HSO) [41], local surface normals (LSN) [47], multiview CNN (MultiView) [9], DISCO [1], Hand3D [5], DeepHand [36], lie-x group based method (Lie-X) [49], improved DeepPrior (DeepPrior++) [28], region ensemble network (REN-466 [14], REN-966 [13]), CrossingNets [46], pose-guided REN (Pose-REN) [2], global-to-local prediction method (Global-to-Local) [23], classification-guided approach (Cls-Guide) [50], 3DCNN  5085  Team name  Average 3D distance error  mAP (front-view)  mAP (top-view)  V2V-PoseNet (Ours) NVResearch and UMontreal NTU THU VCLab NAIST RVLab  9.95 mm 10.18 mm 11.30 mm 11.70 mm 11.90 mm  Table 3: The top-5 results of the HANDS 2017 frame-based 3D hand pose estimation challenge.|
|||To overcome the drawbacks of previous works, we converted 2D depth map into the 3D voxel representation and processed it using our 3D CNN model.|
|||End-toend global to local cnn learning for hand pose recovery in depth data.|
||13 instances in total. (in cvpr2018)|
|282|Sun_SVDNet_for_Pedestrian_ICCV_2017_paper|However, when the weight vectors are correlated, the FC descriptor  the projection on these weight vectors of the output of a previous CNN layer  will have correlated entries.|
|||In fact, SVDNet manages a stronger connection between CNN and SVD.|
|||Truncated SVD [8, 28] is widely used for CNN model compression.|
|||Our network  3801  Algorithm 1: Training SVDNet  Input: a pre-trained CNN model, re-ID training data.|
|||Relaxation: Fine-tune the network with the  Eigenlayer unfixed  end Output: a fine-tuned CNN model, i.e., SVDNet.|
|||In the experiment, we will present the re-ID performance of the CNN model after Step 0.|
|||Our key idea is to find a set of orthogonal projection directions based on what CNN has already learned from training set.|
|||1, the discriminative ability (re-ID accuracy) of the finetuned CNN model is 100% preserved.|
|||But these methods do not preserve the discriminative ability of the CNN model.|
|||For example, in our baseline, when directly fine-tuning a CNN model (without SVDNet training) using CaffeNet, S(WFC7) = 0.0072, indicating that the weight vectors in the FC7 layer are highly correlated.|
|||Moreover, the performance of SVDNet based on relatively simple CNN architecture is impressive.|
|||Exhaustively, we test re-ID performance and S(W ) values of all the intermediate CNN models.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||13 instances in total. (in iccv2017)|
|283|Jain_SUBIC_A_Supervised_ICCV_2017_paper|The contributions of the present work can be summa rized as follows:   We introduce a simple, trainable, CNN layer that encodes images into structured binary codes that we coin SUBIC.|
|||Owing to the success of CNNs for image analysis at large, most recent variants use off-the-shelf or specific CNN features as input representation, e.g., [3, 27, 34, 44].|
|||error block-based entropies  base CNN training  cross-domain  yes yes yes yes  fine-tuning  yes no yes  no no no no no no no yes  h  {0, 1}B of B bits through final entry-wise thresholding (or sign function for centered codes), B typically ranging from 12 to 64 bits.|
|||1):  s , F C1  f2  f1(I),  (1)  where I is an input image, f1 a deep CNN with L convolutional layers (inc. pooling and normalization, if any) and Q fully-connected layers, f2 a binary encoding layer, F C1 a C-class classification layer, and s the C-dimensional vector of class-probability estimates.|
|||A feature is extracted from image I by a base CNN f1 and binarized using a block-structured encoding layer f2 consisting of a fully-connected layer followed by a block softmax during training, or a block 1-hot encoder during testing.|
|||The whole architecture can be learned this way, including the CNN feature extractor, the encoding layer and the classification layer (Fig.|
|||Alternatively, (some of) the weights of the base CNN f1 can be fixed to pre-trained values.|
|||In Section 4, we will consider  the following variants, depending on set-ups: Training of F C0/F C1 only (2-layer training), the base CNN staying fixed; Training of F C1/F C0/F C1 (3-layer training); Training of all layers, C1    CL and F CQ    F C1 (full training).|
|||For fairness of comparison, we also use as base CNN the same as introduced in [33].|
|||We compare in Table 2 with various methods based on the same base CNN (top four rows, DSH [33], DNNH [31], DLBHC [32] and CNNH+ [45]), as well as other published values.|
|||For reference, we include a method (KSH-CNN [35]) not based on neural hash functions but using activations of a deep CNN as input features.|
|||Using VGG-D with 128-D bottleneck (VGG-128) [9] as base CNN (L = 5, Q = 3 and d = 128), setting  and  to 1.0, we performed 2-layer and 3-layer learning of our network (see Section 3.2) on ILSVRC-ImageNet [22] training set.|
|||Indexing of CNN features for large scale image search.|
||13 instances in total. (in iccv2017)|
|284|Qi_Ye_Occlusion-aware_Hand_Pose_ECCV_2018_paper|(b) Multiple pose labels (visible joints are in blue and occluded joints in yellow) and the predicted pose (in red) by CNN trained using a mean squared error.|
|||(c) A closer look of the multiple labels and the CNN prediction for the occluded joints.|
|||The prediction of a CNN trained by the mean squared error function is shown in red.|
|||1d, where we are given two available poses for the same image and CNN trained with the mean squared error function produces the pose estimation in red.|
|||The hierarchical mixture density is topped upon the CNN output layer, and the whole network is trained end-to-end with the differentiable density functions.|
|||Different from the work, we model a conditional distribution and use CNN to discriminatively learn the model parameters.|
|||We choose to learn these functions by a CNN and the distribution is parameterized by the output of the CNN.|
|||5, the input of the CNN is an image xn and the outputs are the HMDN parameters: wd nj, for d = 1, ..., D and j = 1, ..., J.|
|||The CNN outputs the parameters of HMDN, i.e.|
|||4.2 Self-comparisons  The baseline of our comparison is Single Gaussian Network (SGN), which is the CNN trained with a uni-modal Gaussian distribution.|
|||In our experiments, we observed that the estimation error of SGN using the Gaussian center is about the same as that of the CNN trained with the mean squared error.|
|||The CNN network used is the U-net proposed in [30], by adapting the final layers to fully connected layers for regression.|
|||Note the best method [20] uses a 50-layer ResNet model [11] and 21 more CNN models to refine the estimation.|
||13 instances in total. (in eccv2018)|
|285|cvpr18-RayNet  Learning Volumetric 3D Reconstruction With Ray Potentials|RayNet integrates a CNN that learns view-invariant feature representations with an MRF that explicitly encodes the physics of perspective projection and occlusion.|
|||In particular, errors are backpropagated to the CNN based on the output of the MRF.|
|||This allows the CNN to specialize its representation to the joint task while explicitly considering the 3D fusion process.|
|||We show that the MRF acts as an effective regularizer and improves both the output of the CNN as well as the output of the joint model for challenging real-world reconstruction problems.|
|||1https://avg.is.tue.mpg.de/research projects/raynet  3898  Adjacent View Reference View Adjacent View  Feature Map  2D CNN  d e r a h S  (  F  2D CNN  d e r a h S  2D CNN  Surface  Probabilites  ( D  Depth  Distribution  Expected  Loss  ( D  Ground Truth  Depth Map  Reference  Camera  Multi-View CNN (Section 3.1)  Markov Random Field (Section 3.2)  Unrolled Message Passing  Depth Prediction & Loss  (a) RayNet Architecture in Plate Notation with Plates denoting Copies  (b) Multi-View Projection  Figure 2: RayNet.|
|||Given a reference view and its adjacent views, we extract features via a 2D CNN (blue).|
|||Instead, they rely on a generic 3D CNN to learn this mapping from data.|
|||Our architecture utilizes a CNN to learn a feature representation for image patches that are compared across nearby views to estimate a depth distribution for each ray in the input images.|
|||We first specify our CNN architecture which predicts depth distributions for each input pixel/ray.|
|||Provided noisy depth measurements from the CNN (sr i ), the goal of the inference algorithm is to aggregate these measurements using raypotentials into a 3D reconstruction and to estimate globally consistent depth distributions at every pixel.|
|||While training RayNet in an end-to-end fashion is feasible, we further speed it up by pretraining the CNN followed by fine-tuning the entire RayNet architecture.|
|||In contrast, our baseline CNN and the approach of Ulusoy et al.|
|||Conclusion  We propose RayNet, which is an end-to-end trained network that incorporates a CNN that learns multi-view image similarity with an MRF with ray potentials that explicitly models perspective projection and enforces occlusion constraints across viewpoints.|
||13 instances in total. (in cvpr2018)|
|286|Chen_Multi-Instance_Object_Segmentation_2015_CVPR_paper|Unlike R-CNN, SDS inputs both bounding boxes and segmentation foreground masks to a modified CNN architecture to extract CNN features.|
|||Most recently, a R-CNN detector [16] facilitate a large scale CNN network [26] to tackle detection and outperforms the state-of-the-art with a large margin on the challenging PASCAL VOC dataset.|
|||[16] extract CNN features from the CPMC segmentation proposals and then apply the same procedure as in O2P framework to tackle semantic segmentation.|
|||Then, a SDS CNN architecture [19] extracts CNN features for each object hypothesis, and subsequently the extracted features are fed into classspecific classifiers to obtain the categories of object hypotheses.|
|||Then, these segments are fed into a CNN network to extract features, and this CNN network is based on the R-CNN framework [16].|
|||the CNN architecture of SDS is shown in Figure 3.|
|||by computing another CNN features on another  input object hypotheses SDS CNN	 class-specific likelihood map Graph-cut with occlusion handling	 categorized object   hypotheses exemplars shape predictions ... ... ... ... output ... ... ... ... ... ... ... ... Exemplar-based	 shape prediction	 occluding regions ... bounding box where it only has the foreground contents.|
|||Third, The two resulting CNN feature vectors are concatenated and the result is given as the input to train classspecific classifiers.|
|||Figure 3: SDS CNN architecture [19].|
|||Foreground images and cropped images are fed into Region and Box CNN respectively to jointly train the CNN network.|
|||Finally, the grouped CNN features are used to train class-specific classifiers.|
|||The chamfer distance between the contour of the proposal U and the contour of the exemplar template T is given by the average of the distance between each point ti  T and its nearest point uj in U as  (cid:88)  tiT  dCM (T, U ) =  1 |T|  |ti  uj| ,  min ujU  (1)  Bottom-up segmentation proposals tend to undershoot (e.g., missing parts of an object) and overshoot (e.g., con where |T| indicates the number of the points on the contour of the exemplar template T and we use boldface to  object proposals and foreground masks cropped images foreground images feature extraction feature vectors Class-specific classifiers Box CNN Region CNN ......exemplar templates matched points inferred masks shape prior 0.28 ...0.64 0.30 A       B corner detector Chamfer matching Mn M1 Sn (cid:80)  represent a vector.|
|||Finally, the segmentation candidates of all the foreground masks are applied with class-specific classifiers trained on top of the CNN features extracted from the SDS CNN architecture.|
||13 instances in total. (in cvpr2015)|
|287|yitong_wang_Orthogonal_Deep_Features_ECCV_2018_paper|[42] extended the HFA method [10] to a deep CNN model called latent factor guided convolutional neural networks (LF-CNNs), which achieved the state-of-the-art recognition accuracy in this field.|
|||[47] also used the linear combination of jointlylearned deep features to represent identity and age information, which is similar to the HFA based deep CNN model.|
|||A related work Decoupled Network also discussed how to decouple the CNN with orthogonal geometry in details.|
|||), and the inevitable mixture of unrelated components in the deep features extracted from a general deep CNN model.|
|||The proposed ResNet-Like CNN architecture.|
|||An overview of the proposed CNN model is illustrated in Figure 2.|
|||Equation 4 is used to guide the learning of our CNN model in the training phase.|
|||It is noteworthy that only 10 individuals are used to train CNN models, and the output dimension is set to 2.|
|||For the foregoing reasons, our method is more recommendable to be embedded into CNN framework for the purpose of learning age-invariant features, as supported by our experimental results.|
|||Specifically, we train CNN models with 10 individuals and set the output dimension of feature x as 2.|
|||Specifically, WebFace [45], celebrity+ [25] and CACD [5] form the training set to train a CNN base model.|
|||The highly discriminative age-invariant features can be consequently extracted from a multi-task deep CNN model based on the proposed approach.|
|||Extensive evaluations of several face aging datasets have been done to show the effectiveness of our orthogonal embedding CNN (OE-CNN) approach.|
||13 instances in total. (in eccv2018)|
|288|Bhattarai_CP-mtML_Coupled_Projection_CVPR_2016_paper|Identity based face retrieval performance (1-call@K for different K) with and without distractors with CNN features.|
|||Identity based face retrieval, 1-call@10 at different projection dimension, d, (left) using LBP and (right) CNN features.|
|||The lightweight unsupervised LBP features perform lower than the more discriminative CNN features, which are trained on large amounts of extra data e.g.|
|||The performance gains for the proposed method are larger for LBP compared to CNN features e.g.|
|||This is highlighted by the time complexities of the features; in practice LBP are much faster than CNN to compute.|
|||While CNN features roughly take 450 milliseconds, the LBP features take only a few milliseconds on a 2.5 GHz processor.|
|||The performance changes with varying d in the presence of distractors for CNN features are more modest.|
|||higher dimensions for the stronger CNN features.|
|||compressions), deteriorating only at the saturated case of high dimensions with strong CNN features.|
|||These results are different and interesting from the identity based retrieval experiments above, as they show the limitation of CNN features, learnt on identities, to generalize to other tasks  the performances with LBP features are higher than those with CNN features.|
|||CPmtML is better than stML, it is reversed for CNN features.|
|||With CNN features, stML learns to distinguish between ages when trained with such data, however, CP-mtML ends up being biased, due to its construction, towards identity matching and degrades age retrieval performance when auxiliary task is identity matching.|
|||However, the performance of CPmtML with LBP features is much higher than of any of the methods with CNN features.|
||13 instances in total. (in cvpr2016)|
|289|Not Afraid of the Dark_ NIR-VIS Face Recognition via Cross-Spectral Hallucination and Low-Rank Embedding|First, we propose to modify the NIR probe images using deep cross-spectral hallucination based on a convolutional neural network (CNN).1 The CNN learns a NIR-VIS mapping on a patch-to-patch basis.|
|||The cross-spectral hallucination CNN is trained on pairs of corresponding NIR-VIS patches that are mined from a publicly available dataset, as will be described below.|
|||Architecture of the CNN used for cross-spectral hallucination.|
|||Note that a cross-spectral NIR-VIS patch similarity metric using a CNN was proposed in [1].|
|||Despite our methodology for mining aligned patches, it is not at all impossible that the CNN introduces small artifacts in unseen patches.|
|||To safeguard the valuable details of the original NIR, we propose to blend the CNN output with the original NIR image.|
|||The parameter  balances the amount of information retained from the NIR images and the information obtained with the CNN and allows to remove some of the artifacts introduced by the CNN ( = 0.6 in our experiments).|
|||Note how the blending helps correcting some of the remaining artifacts in the CNN output but maintaining a more natural-looking face than the NIR alone.|
|||From left to right: Input NIR image; Raw output of the hallucination CNN; output of the CNN after post-processing; one RGB sample for each subject.|
|||The postprocessing helps removing some of the artifacts in the CNN output.|
|||Note that the CNN was trained only on face patches so the color of the clothes cannot be hallucinated.|
|||As explained, our cross-spectral hallucination CNN requires more training data than the one available in the standard CASIA NIR-VIS 2.0 benchmark training set, so we define a new splitting of that dataset into 6 folds.|
|||Cross-spectral hallucination preprocesses the NIR image using a CNN that performs a cross-spectral conversion of the NIR image into the VIS spectrum.|
||13 instances in total. (in cvpr2017)|
|290|cvpr18-NISP  Pruning Networks Using Neuron Importance Score Propagation|The CNN is pruned by removing neurons with least importance, and it is then fine-tuned to recover its predictive power.|
|||We evaluate our approach on MNIST [21], CIFAR10 [19] and ImageNet [5] using multiple standard CNN architectures such as LeNet [21], AlexNet [20], GoogLeNet [34] and ResNet [14].|
|||Experiments demonstrate that NISP leads to fullnetwork acceleration and compression for all types of layers in a CNN with small accuracy loss.|
|||Sparsity regularization terms have been use to learn sparse CNN structure in [23, 35, 33].|
|||Other studies focused on fixed point computation rather than exploiting the CNN redundancy [4, 29].|
|||The exact importance of neurons in a CNN is very hard to obtain given the complexity introduced by nonlinearities.|
|||We employ the recently introduced filtering method Inf-FS [31] because of its efficiency and effectiveness on CNN feature selection.|
|||The importance propagation and layer pruning happens jointly in a single backward pass,  We evaluate our approach on standard datasets with popular CNN networks.|
|||We evaluate using five commonly used CNN architectures: LeNet [21], Cifar-net3, AlexNet [20], GoogLeNet [34] and ResNet [14].|
|||Comparison with Random Pruning and Train(cid:173)  from(cid:173)scratch Baselines  We compare to two baselines: (1) randomly pruning the pre-trained CNN and then fine-tuning, and (2) training a small CNN with the same number of neurons/filters per layer as our pruned model from scratch.|
|||So we choose a small CNN structure trained on the CIFAR10 dataset.|
|||Pruning a CNN is a tradeoff between efficiency and accuracy.|
|||Experiments demonstrated that our method effectively reduces CNN redundancy and achieves full-network acceleration and compression.|
||13 instances in total. (in cvpr2018)|
|291|Yang_Multi-Scale_Recognition_With_ICCV_2015_paper|We introduce multi-scale CNN architectures that use features at multiple scales for output prediction (Fig.|
|||Our approach is most related to [12], who also use spatially pooled CNN features for scene classification.|
|||They do so by pooling together multiple CNN descriptors (re)computed on varioussized patches within an image.|
|||Our entire model is still a feed-forward CNN that is no longer chain-structured, but a directed-acyclic graph (DAG).|
|||Finally, our work aligns with approaches that predict local pixel labels using features extracted from multiple CNN layers [14, 22].|
|||We evaluate multi-scale DAG-structured variants of existing CNN architectures (e.g., Caffe [15], Deep19 [30]) on a variety of scene recognition benchmarks including SUN397 [39], MIT67 [26], Scene15 [9].|
|||We carry out an analysis on existing CNN architectures, namely Caffe and Deep19.|
|||Caffe [15] is a broadly used CNN toolbox.|
|||Training: Let w1, ...wK be the CNN model parameters at 1, .., K-th layer, training data be (x(i), y(i)), where x(i) is the i-th input image and y(i) is the indicator vector of the class of x(i).|
|||Our results are particularly impressive given that the next-best method of [40] (with a score of 54.3) makes use of a ImageNet-trained CNN and a customtrained CNN using a new 7-million image dataset with 400 scene categories.|
|||Interestingly [12] also uses multi-scale CNN features, but do so by first extracting various-sized patches from an image, rescaling each to canonical size.|
|||Single-scale CNN features extracted from these patches are then vector-quantized into a large-vocabulary codebook, followed by a projection step to reduce dimensionality.|
|||However even with current CNN architectures, our results suggest that any system making use of off-the-shelf CNN features should explore multi-scale variants as a cheap baseline.|
||13 instances in total. (in iccv2015)|
|292|Feng_Learning_The_Structure_ICCV_2015_paper|We evaluate the performance of ibpCNN, on fullyand semi-supervised image classification tasks; ibpCNN surpasses standard CNN models on benchmark datasets with much smaller size and higher efficiency.|
|||Comprehensive experiments on benchmark datasets show that the end-to-end trained ibpCNN is indeed much more compact than standard CNN models, and more efficient for prediction.|
|||Related Works  Although very successful when provided very large labeled datasets [15, 8], CNN models may generalize poorly on smaller datasets because they require the estimation of millions of parameters.|
|||The above fully connected IBP layer differs from its counterpart in a CNN model: the IBP fully connected layer decomposes the input x over a learned basis W in order to minimize the reconstruction error (captured by the first term in the loss) and the model complexity (the second term); it thus pursues a compact representation of the input by learning a set of appropriate basis1.|
|||As for the deep learning baselines, we mainly compare with CNN model of AlexNet architecture [15] in the fully supervised learning setting, which has shown state-of-theart performance on several benchmark datasets in previous studies.|
|||We also report the performance of CNN and CAE models with the architecture  identified by the GAP algorithm, i.e., the CNN-GAP and CAE-GAP models.|
|||A common practice is to initialize parameters of CNN models with CAE models [19], denoted as CAE+CNN.|
|||For fair comparisons, the layer numbers of CNN and CAE models are set the same as ibpCNNs and the architecture of CAE exactly follows the one specified in [19].|
|||CNN-GAP is the CNN model with the architecture determined by GAP.|
|||Employing the architecture identified by GAP boosts the performance of CNN (CNN-GAP vs. CNN-AlexNet).|
|||Initializing the parameters of CNN with CAE and further fine-tuning the CNN on labeled training examples offers better performance than single CNN and CAE.|
|||CAE-GAP, CNN-GAP and CAE+CNN+GAP employ the architectures determined by the GAP algorithm, and CAE+CNN+GAP is the CNN model whose parameters are initialized by CAE.|
|||We demonstrated the superior performance and efficiency of ibpCNN compared with standard hand-crafted CNN models.|
||13 instances in total. (in iccv2015)|
|293|Lenc_Understanding_Image_Representations_2015_CVPR_paper|These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved.|
|||g i r  O  p i l  F H  p i l  F V  0 9 t o R  Figure 1: Equivariant transformation of CNN filters.|
|||To understand the concept of target-oriented loss, consider a CNN  trained end-to-end on a categorisation problem such as the ILSVRC 2012 image classification task (ILSVRC12) [19].|
|||f i s s a l C  None  Conv1  Conv2  Conv3  Conv4  Conv5  0  15  30  45  60  75  90  Rotation []  Figure 7: Learning equivariant CNN mappings for image rotations.|
|||This section extends these results to deep representations, using the ALEXN CNN [10] as a reference state-of-theart deep feature extractor using the MatConvNet framework [29].|
|||Next, we investigate which geometric transformations can be represented by different layers of a CNN (Tab.|
|||First, for transformations such as horizontal flips and scaling, learning equivariant mappings is not better than leaving the features unchanged: the reason is that the CNN is implicitly learned to be invariant to such factors.|
|||Flip Top1 Top5 Top1 Top5 Top1 Top5 Top1 Top5 None 0.54 0.44 Conv1 0.43 0.20 Conv2 0.45 0.22 Conv3 0.45 0.23 Conv4 0.44 0.25 Conv5 0.44 0.28  0.61 0.45 0.48 0.49 0.49 0.50  0.21 0.20 0.22 0.21 0.21 0.21  0.75 0.43 0.46 0.46 0.48 0.51  0.75 0.44 0.46 0.47 0.49 0.53  0.54 0.20 0.22 0.22 0.24 0.26  0.37 0.22 0.24 0.25 0.25 0.26  Table 2: CNN equivariance.|
|||Performance on the ILSVRC12 validation set of compensated CNN classifier using learned equivariant mappings for selected transformations.|
|||90  % Num  % Num  % Num  52 54.17 131 51.17 238 61.98 343 89.32 255 99.61  53 55.21 45 17.58 132 34.38 124 32.29 47 18.36  95 98.96 69 26.95 295 76.82 378 98.44 252 98.44  % 42 43.75 27 10.55 120 31.25 101 26.30 56 21.88  Table 3: CNN invariance.|
|||Layer  Conv1 Conv2 Conv3 Conv4 Conv5  IMNET  ALEXN PLCS  ALEXN PLCS-H  ALEXN Top5 Top1 0.20 0.43 0.22 0.46 0.23 0.46 0.24 0.46 0.50 0.27  Top5 Top1 0.43 0.20 0.46 0.23 0.47 0.25 0.49 0.29 0.39 0.52  Top5 Top1 0.43 0.20 0.47 0.22 0.50 0.22 0.54 0.22 0.25 0.65  Table 4: CNN equivalence.|
|||2   To validate this idea, the first several layers   1 of the ALEXN CNN  =  1 are swapped with layers 1 from IMNET, also trained on the ILSVRC12 data, PLCS [34], trained on the MIT Places data, and PLCS-H, trained on a mixture of MIT Places and ILSVRC12 images.|
|||Table 5 reports the accuracy and speed obtained for HOG and CNN Conv3, Conv4, and Conv5 features for direct and equivariant regression.|
||13 instances in total. (in cvpr2015)|
|294|Li_Deep_Scene_Image_ICCV_2017_paper|In fact, the implementation of a holistic scene classifier with a CNN is almost trivial.|
|||It suffices to train the CNN on whole scene images.|
|||Early methods adopted a BoFlike approach, based on the extraction of features from intermediate CNN layers, which were then fed to dictionary learning methods such as clustering [11] or sparse coding [17] and finally used to implement descriptors such as the VLAD [11] or Fisher vector [17].|
|||This is because the Fisher vector is defined with respect to the probability distribution of the CNN features, usually estimated with a mixture learned by maximum likelihood.|
|||All methods above avoid this difficulty by using the CNN to extract features and learning the Fisher vector indepeddently.|
|||Since CNN features are high dimensional, it is impractical to rely on Gaussians of full covariance.|
|||While the CNN is trained to produce linearly separable responses to the different classes, there is no guarantee CNN feature distributions are prone to modeling with the diagonal GMM.|
|||Another possibility is to embed the Fisher vector in the CNN architecture, by deriving a neural network implementation of its equations.|
|||An alternative strategy, proposed by [6], is to use a better model of CNN feature statistics than the diagonal GMM.|
|||Since end to end training is an important reason for the recent success of the deep CNN architecture, it appears natural to pursue this integration.|
|||The FV of [2] uses both Alexnet and VGG-16 as CNN model and 10 patch scales.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Bilinear cnn models for fine-grained visual recognition.|
||13 instances in total. (in iccv2017)|
|295|Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper|They formulate this issue as a two-stage problem and build a typical pipeline, where the first phase hypothesizes category-agnostic object proposals within the given image and the second phase classifies each proposal according to CNN based deep features.|
|||It is generally accepted that in these methods, CNN representation plays a crucial role, and the learned feature is expected to deliver a high discriminative power encoding object characteristics and a good robustness especially to moderate positional shifts (usually incurred by inaccurate boxes).|
|||For instance, [11] and [15] extract features from deeper CNN backbones, like ResNet [11] and Inception [31]; [19] introduces a top-down architecture to construct feature pyramids, integrating low-level and high-level information; and the latest top-performing Mask R-CNN [9] produces an RoIAlign layer to generate more precise regional features.|
|||Deformable CNN [4] attempts to adaptively adjust the spatial distribution of RFs according to the scale and shape of the object.|
|||Inspired by the structure of RFs in the human visual system, this paper proposes a novel module, namely Receptive Field Block (RFB), to strengthen the deep features learned from lightweight CNN models so that they can contribute to fast and accurate detectors.|
|||We propose the RFB module to simulate the configuration in terms of the size and eccentricity of RFs in human visual systems, aiming to enhance deep features of lightweight CNN networks.|
|||Actually, there exist several studies that discuss RFs in CNN, and the most related ones are the Inception family [33,32,31], ASPP [3], and Deformable CNN [4].|
|||Deformable CNN [4] learns distinctive resolutions of individual objects, unfortunately it holds the same downside as ASPP.|
|||1 https://pytorch.org/ 2 https://github.com/amdegroot/ssd.pytorch  10  Songtao Liu, Di Huang, and Yunhong Wang  Method  Backbone Data mAP(%) FPS 07+12 73.2 VGG Faster [26] Faster [11] ResNet-101 07+12 76.4 ResNet-101 07+12 80.5 R-FCN [17] Darknet 07+12 78.6 YOLOv2 544 [25] R-FCN w Deformable CNN [4] ResNet-101 07+12 82.6 VGG SSD300* [22] 07+12 77.2 ResNet-101 07+12 78.6 DSSD321 [6] 07+12 80.5 VGG RFB Net300 VGG SSD512* [22] 07+12 79.8 DSSD513 [6] ResNet-101 07+12 81.5 07+12 82.2 VGG RFB Net512  7 5 9 40 8 120 9.5 83 50 5.5 38   Extrapolated time  Tested in Pytorch-0.3.0 and CUDNN V6 for fair comparison  Table 1.|
|||Comparison with other architectures: We also compare our RFB with Inception [33], ASPP [3] and Deformable CNN [4].|
|||Architecture RFB Inception [33] Inception-L ASPP-S Deformable CNN [4]  #parameters VOC 2007 mAP (%) COCO minival mAP (%)  34.5M 32.9M 33.3M 33.4M 35.2M  80.1 78.4 79.5 79.7 79.5  29.7 27.3 28.5 28.1 27.6  Table 3.|
|||Precision, Area: 0.5:0.95 0.5  L  37.1 50.9 48.2 52.0 45.0 50.3 53.5  35.5     51.1 49.1 50.2 45.9 47.4 47.6  Faster [26]  Faster+++ [11]  Faster w FPN [19]  VGG ResNet-101 ResNet-101-FPN Inception-Resnet-v2 [31] ResNet-101 R-FCN w Deformable CNN [4] ResNet-101  Faster by G-RMI [15]  R-FCN [17]  Mask R-CNN [9]  ResNext-101-FPN  YOLOv2 [25] SSD300* [22] SSD512* [22] DSSD513 [6]  RetinaNet500 [20] RetinaNet800 [20]  RFB Net300 RFB Net512  RFB Net512-E  darknet VGG VGG ResNet-101 ResNet-101-FPN ResNet-101-FPN VGG VGG VGG  147 ms 3.36 s 240 ms  24.2 34.9 36.2 34.7 110 ms 29.9 125ms 34.5 210 ms 37.1    0.75 45.3 23.5 55.7 37.4 59.1 39.0 55.5 36.7 51.9 55.0 60.0 39.4     S M 7.7 26.4 15.6 38.7 18.2 39.0 13.5 38.1 10.8 32.8 14.0 37.7 16.9 39.9  21.6 25.1 28.8 33.2 34.4  44.0 19.2 25 ms 12 ms 43.1 25.8 28 ms 48.5 30.3 53.3 35.2 182 ms 90 ms 53.1 36.8 198 ms 39.1 59.1 42.3 15 ms 30.3 49.3 31.8 30 ms 33.8 54.2 35.9 33 ms 34.4 55.7 36.4  5.0 22.4       13.0 35.4 14.7 38.5 21.8 42.7 11.8 31.9 16.2 37.1 17.6 37.0   Extrapolated time  Tested in Pytorch-0.3.0 and CUDNN V6 for fair comparison  Table 4.|
|||RFB is equipped on the top of lightweight CNN based SSD, and the resulting detector delivers a significant performance gain on the Pascal VOC and MS COCO databases, where the final accuracies are even comparable to those of existing top-performing deeper model based detectors.|
||13 instances in total. (in eccv2018)|
|296|Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation|We specify a novel deep architecture which fuses three distinct computation processes toward semantic segmentation  namely, (i) the bottom-up computation of neural activations in a CNN for the image-level prediction of object classes; (ii) the top-down estimation of conditional likelihoods of the CNNs activations given the predicted objects, resulting in probabilistic attention maps per object class; and (iii) the lateral attention-message passing from neighboring neurons at the same CNN layer.|
|||Inspired by the success of these approaches, we also start off with a CNN aimed at two tasks: pixel labeling and predicting image classes  where image classification results on training data are used for an end-to-end MIL-based learning.|
|||Importantly, our CRF-based refinement of  13529  Figure 1: Overview: Given an image, we use a CNN to compute bottom-up segmentation maps for every object class (blue links for bottom-up computation).|
|||The same CNN is used for top-down estimation of the attention maps for every recognized object class (red links for top-down computation).|
|||To avoid oversegmentation, and thus improve the smoothness over spatial extents of objects, we use our CNN for yet another, third task, that of predicting top-down visual attention maps of the recognized image classes.|
|||This extends the recent approach [44] that uses a Markov chain to model parent-child dependencies of neural activations for estimating the attention map, since we estimate the rectified Gaussian distribution by accounting for three types of neural dependences in the CNN: (i) parentto-child; (ii) child-to-parent; and (iii) between activations of neighboring neurons at the same CNN layer.|
|||Importantly, we compute the attention map using the same CNN aimed at semantic segmentation, unlike related work that uses an external network for estimating object seeds [18].|
|||This, in turn, generates the image classification loss that is backpropagated through the FCL, CRF-RNN, and CNN for learning all network parameters.|
|||The segmentation loss is backpropagated through the CRF-RNN and CNN and serves to regularize the image classification loss.|
|||Recent approach [16] combines the two computation processes in a single CNN for human pose estimation using the rectified Gaussian distribution.|
|||But their CNN is trained under full supervision.|
|||First, following [44], we define dependence of p(al  i|y) on  the activations of parent neurons Pi in the CNN as  i(y) = X l jPi  p(al  i|al1  j  )p(al  j|y),  (5)  Figure 2: (Left) Previous work [44] computes a top-down Markov chain for estimating the attention map.|
|||Second, we normalize the feed-forward neural processing in the CNN at neuron i across all neurons at the same layer l, resulting in  l  i =  al i  Pi al  i  (7)  Finally, third, we use the standard bi-lateral filtering of pixels in the image to define the strength Dl = [l ii ] of dependencies between neighboring neurons at the same layer.|
||13 instances in total. (in cvpr2017)|
|297|Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper|By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people/head size due to perspective effect or image resolution.|
|||[28] has utilized the features extracted from a pre-trained CNN to train a support vector machine (SVM) that subsequently generates counts for still images.|
|||[33] has proposed a CNN based method to count crowd in different scenes.|
|||The reason for us to adopt a multi-column architecture here is rather natural: the three columns correspond to filters with receptive fields of different sizes (large, medium, small) so that the features learned by each column CNN is adaptive to (hence the overall network is robust to) large variation in people/head size due to perspective effect or across different image resolutions.  2.|
|||Multi-column CNN for Crowd Counting  2.1.|
|||Density map via geometry(cid:173)adaptive kernels  Since the CNN needs to be trained to estimate the crowd density map from an input image, the quality of density given in the training data very much determines the performance of our method.|
|||Multi(cid:173)column CNN for density map estimation  Due to perspective distortion, the images usually contain heads of very different sizes, hence filters with receptive fields of the same size are unlikely to capture characteristics of crowd density at different scales.|
|||Motivated by the success of Multi-column Deep Neural Networks (MDNNs) [8], we propose to use a Multi-column CNN (MCNN) to learn the target density maps.|
|||Motivated by the success of pre-training of RBM [11], we pre-train CNN in each single column separately by directly mapping the outputs of the fourth convolutional layer to the density map.|
|||It can be seen that MCNNs significantly outperforms each single column CNN for both MAE and MSE.|
|||[33] is based on crowd CNN model to estimate the crowd count of an image.|
|||The proposed MCNN model outperforms both the foreground segmentation based methods and CNN based method [33].|
|||Our method also achieves better performance than Fine-tuned Crowd CNN model [33] in terms of average MAE.|
||13 instances in total. (in cvpr2016)|
|298|Sindagi_Generating_High-Quality_Crowd_ICCV_2017_paper|GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes.|
|||DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN.|
|||Motivated by their success, we believe that availability of global context shall aid the learning process and help us achieve better  1861  method is dubbed as Contextual Pyramid CNN (CP-CNN).|
|||To summarize, the following are our main contributions:  We propose a novel Contextual Pyramid CNN (CPCNN) for crowd count and density estimation that encodes local and global context into the density estimation process.|
|||A Density Map Estimator (DME), which is a multi-column architecture-based CNN with appropriate max-pooling layers, is used to transform the image into high-dimensional feature maps.|
|||To this effect, a Local Context Estimator CNN (LCE) is trained on input image patches to encode local context information.|
|||The proposed method uses CNN networks to estimate context at various levels for achieving lower count error and better quality density maps.|
|||Similarly, Onoro-Rubio and L opez-Sastre in [23] addressed the scale issue by proposing a scale-aware counting model called Hydra CNN to estimate the object density maps.|
|||DME is a multi-column CNN that performs the initial task of transforming the input image to high-dimensional feature maps.|
|||[30] proposed an end-to-end CNN architecture consisting of three parts: pre-trained GoogLeNet model for feature generation, long short term memory (LSTM) decoders for local count and fully connected layers for the final count.|
|||in [23] proposed a scale-aware CNN to learn a multi-scale non-linear regression model using a pyramid of image patches extracted at multiple scales.|
|||[36] proposed a layered approach of learning CNNs for crowd counting by iteratively adding CNNs where every new CNN is trained on residual error of the previous layer.|
|||Learning to count with cnn boosting.|
||13 instances in total. (in iccv2017)|
|299|Zhao_Deep_Semantic_Ranking_2015_CVPR_paper|We evaluate the proposed DSRH method on a couple of multi-label image datasets and compare it with several state-of-the-art hashing methods based on both hand-crafted features and activation features from the CNN model.|
|||2, we construct hash functions through incorporating the CNN model whose architecture is the same as [12].|
|||We argue that the features from the second fully connected layer (FCb) of CNN are dependent on classes too much and have strong invariance, which is unfavorable for capturing subtle sematic distinction.|
|||The ImageNet ILSVRC-2012 dataset [22] is utilized to pre-train the CNN model by optimizing multinomial logistic regression objective function in the image classification task.|
|||These derivative values can be fed into the underlying CNN via the back-propagation algorithm to update the parameters of each layer.|
|||initialize the CNN part of hash functions in our method.|
|||In order to ensure fairness, we apply the features from the pretrained CNN model to the compared hashing methods as well.|
|||By using the CNN model to construct hash functions, our method have higher learning capability and is able to exploit more semantic information than the hashing methods trained on hand-crafted features which are usually extracted by unsupervised and shallow models.|
|||We further evaluate the compared hashing methods on the features obtained from the activation of the last hidden layer of the CNN model pre-trained on the ImageNet dataset.|
|||Comparison of ranking performance of our DSRH and other hashing methods based on activation features of fine-tuned CNN on two datasets: (a) MIRFLICKR-25K and (b) NUS-WIDE.|
|||Similar to the skipping layer in the hash function of  DSRH, we also attempt to concatenate the activations of the last two hidden layers of the CNN model as feature representations and apply them to the compared hashing methods.|
|||Conclusion  In this paper we have proposed to employ multilevel semantic ranking supervision to learn deep hash functions based on CNN which preserves the semantic structure of multi-label images.|
|||The CNN model with listwise ranking supervision is used to jointly learn feature representations and mappings from them to binary codes.|
||13 instances in total. (in cvpr2015)|
|300|Li_Identity-Aware_Textual-Visual_Matching_ICCV_2017_paper|Our stage-1 network consists of a CNN and a LSTM for learning textual and visual feature representations.|
|||Given an input textual description or image, both the visual CNN and language LSTM are trained to map the input image and description into a joint feature embedding space, such that the features representations belonging to the same identity should have small feature distances, while those of different identities should have large distances.|
|||The CNN and LSTM parameters are updated by backpropagation.|
|||Before the first iteration, image and textual features are obtained by the CNN and LSTM.|
|||The parameters of both visual CNN and language LSTM are updated via backpropgation.|
|||Unlike LSTM decoders for NLP [4, 31], whose each step corresponds to a specific output word, each step of our semantic  1893  word-fc Visual CNN The ... word-fc Encoder LSTM model Spatial Attention Module wears word-fc dress word-fc Latent Semantic Attention Module Decoder LSTM Encoder LSTM Encoder LSTM Encoder LSTM ... x1 x2 xt xT Decoder LSTM Decoder LSTM fc Binary  classifier decoder captures a latent semantic concept and the number of steps is predefined as the number of concepts.|
|||3.2.1 Encoder word-LSTM with spatial attention  For the visual CNN and encoder LSTM, our goal is to generate a joint word-visual feature representation at each input word.|
|||The LSTM is trained with the Adam optimizer with a learning rate of 0.0001 while the CNN is trained with the batched Stochastic Gradient Descent.|
|||During the training phase, we first train the language model and fix the CNN model, and then finetune the whole network jointly to effectively couple the image and text features.|
|||Based on the framework of Stage-2 w/o SMA, we further remove the spatial attention module from the stage-2 network, denoted as Stage-2 w/o SMA+SPA, which can be viewed as a simple concatenation of the visual and textual features from the CNN and LSTM, followed by two fully-connected layers for binary classification.|
|||Since stage-1 network chooses only 20 most closest images of each query text for stage 2 during the evaluation  1896  Methods BoW [9] Word2Vec [23] Attributes [2] Word CNN [26] Word CNN-RNN [26] GMM+HGLMM [12] Triplet  Stage-1 Stage-2  Image-Text  Top-1 Acc (%)  Text-Image AP@50 (%)  DA-SJE DS-SJE DA-SJE DS-SJE  43.4 38.7 50.9 50.5 54.3  44.1 38.6 50.4 51.0 56.8  24.6 7.5 20.4 3.4 4.8  39.6 33.5 50.0 43.3 48.7  36.5 52.5  61.5   35.6 52.4  55.5 57.6  Table 3.|
|||Different images of the same identity (the first,  Methods BoW [9] Word2Vec [23] Word CNN [26] Word CNN-RNN [26] GMM+HGLMM [12] Triplet  Stage-1 Stage-2  Image-Text  Top-1 Acc (%)  Text-Image AP@50 (%)  DA-SJE DS-SJE DA-SJE DS-SJE  56.7 54.6 60.2 60.9  57.7 54.2 60.7 65.6  28.2 16.3 8.7 7.6  57.3 52.1 56.3 59.6  54.8 64.3  68.4   52.8 64.9  68.0 70.1  Table 4.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||13 instances in total. (in iccv2017)|
|301|Li_Learning_Weight_Uncertainty_CVPR_2016_paper|Convolutional Neural Networks The CNN is a special class of FNNs, typically applied to data with spatial or temporal covariates.|
|||The CNN employs the convolution operation at each layer of the feedforward network.|
|||The nonlinear function gl in CNN is typically composed of convolution and pooling operators [38].|
|||The CNN can take advantage of the properties of natural signals [37] such as images and shapes, which exhibit high local correlations [7, 12] and rich shared components [8, 81].|
|||(ii) We also train CNN models and advanced variants on raw data with our approach.|
|||On MNIST, we built 2 networks: one is LeNet [38], composed of a 2-layer CNN followed by a 2-layer FNN; the other is 4-CNN, it is similar to LeNet except that a 4-layer CNN is used with max-pooling after the 1st, 2nd, and 4th convolutional layers.|
|||The results are shown in Table 5. pSGLD significantly outperforms RMSprop on both networks, indicating that learning weight uncertainty in CNN models can boost results.|
|||Test error of CNN on MNIST, Caltech, ModelNet and Cifar10.|
|||5.2.2  3D ModelNet  We next evaluate the CNN on a large-scale 3D model dataset, Princeton ModelNet [79].|
|||We use a 30  30  30 bounding box to represent each shape, and construct Vol-CNN network: 3-layer volumetric CNN followed by 1-layer FNN with ReLU.|
|||We employed 12 depth-views of size 224 224 to represent a 3D shape, and adopted the View-CNN based on the CNN-M network from [12]: a 5-layer CNN followed by a 2-layer FNN with ReLU.|
|||The only method with lower error views (22.37%) than ours is Multi-View CNN (9.9%) [66]; note that their model is pre-trained with ImageNet [61], we did not perform any pretraining.|
||12 instances in total. (in cvpr2016)|
|302|Zhang_SPDA-CNN_Unifying_Semantic_CVPR_2016_paper|In this paper, we propose a new CNN architecture that integrates semantic part detection and abstraction (SPDACNN) for fine-grained classification.|
|||In order to introduce such layers to facilitate the extraction of part-specific features, several works proposed part-based CNN methods [41, 5, 28, 39].|
|||These methods define and train a separate CNN network for each part.|
|||To tackle these above-mentioned challenges, we propose a new CNN architecture with built-in mid-level part abstraction layers.|
|||[28] directly regressed part bounding box coordinates from CNN features and proposed to use valve linkage function to join part localization, alignment and class prediction in one network for each part.|
|||Although [40] has shown some neurons in CNN might implicitly capture part or attribute information, there is no evidence that part-level features are well modeled in the current architecture.|
|||Part Abstraction and Classification  Our part abstraction and classification sub-network (CLS-NET) introduces a semantic part RoI pooling layer, a part-based fully connected layer (pfc) and a concatenation fully connected layer (cfc) to the traditional CNN architecture to adjust it to be an end-to-end framework for fine-grained classification.|
|||3.2.1 Semantic Part RoI Pooling Layer In the traditional CNN architecture, the pooling layer is used to increase the translation invariance and reduce the spatial size of the network.|
|||To the best of our knowledge, we are the first to propose adding a pfc layer in CNN for mid-level part abstraction.|
|||3.2.3 Concatenation Fully Connected Layer  Note that most previous part-based CNN approaches [41, 5, 28] train a separate CNN network for each part and concatenate the CNN features extracted for each part and then train a SVM on this concatenated feature vector.|
|||First, it is very important to build part layers (e.g., pfc layer) in the CNN framework to abstract and concatenate multiple parts; and it is important to use more semantic parts (67.02% vs 77.08% vs 79.46%).|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||12 instances in total. (in cvpr2016)|
|303|Yang_Stacked_Attention_Networks_CVPR_2016_paper|The SAN consists of three major components: (1) the image model, which uses  1 21  a CNN to extract high level image representations, e.g.|
|||one vector for each region of the image; (2) the question model, which uses a CNN or a LSTM to extract a semantic vector of the question and (3) the stacked attention model, which locates, via multi-step reasoning, the image regions that are relevant to the question for answer prediction.|
|||Different from [30, 27], the approach proposed in [6] first used a CNN to detect words given the images, then used a maximum entropy language model to generate a list of caption candidates, and finally used a deep multimodal similarity model (DMSM) to rerank the candidates.|
|||Instead of using a RNN or a LSTM, the DMSM uses a CNN to model the semantics of captions.|
|||Image Model  The image model uses a CNN [13, 23, 26] to get the representation of images.|
|||Specifically, the VGGNet [23] is used to extract the image feature map fI from a raw image I:  image  448  448  14  512  14  feature map  Figure 2: CNN based image model  fI = CNNvgg(I).|
|||3.2.2 CNN based question model  3.2.|
|||unigram  bigram  trigram  max pooling   over time  convolution  embedding  Question:  w h a t  a r e  s i t t i n g  ...  i  b c y c e  l  Figure 4: CNN based question model  In this study, we also explore to use a CNN similar to [11] for question representation.|
|||(17)  (18)  h = [ h1,  h2,  h3],  (14)  hence vQ = h is the CNN based question vector.|
|||The diagram of CNN model for question is shown in Fig.|
|||For the CNN based question model, we set the unigram, bigram and trigram convolution filter size to be 128, 256, 256 respectively.|
|||For VQA dataset, since it is larger than other data sets, we double the model size of the LSTM and the CNN to accommodate the large data set and the large number of classes.|
||12 instances in total. (in cvpr2016)|
|304|Deep View Morphing|In this paper, we propose a novel CNN architecture for view synthesis called Deep View Morphing that does not suffer from these issues.|
|||In this paper, we propose a novel CNN architecture that can efficiently synthesize novel views with detailed textures as well as well-preserved geometric shapes under the view interpolation setting .|
|||Following the spirit of View Morphing, our approach introduces a novel deep CNN architecture to generalize the procedure in [27]for that reason, we named it Deep View Morphing (DVM).|
|||In this paper, DVM generalizes the procedure in [27] using a single CNN architecture.|
|||[5] proposed a generative CNN architecture to synthesize images given the object identity and pose.|
|||[29] proposed a similar CNN architecture without explicit decoupling of such factors.|
|||Deep View Morphing  DVM is an end-to-end generalization of View Morphing by a single CNN architecture shown in Fig.|
|||Similarly to [4], we can consider two possible ways of such mechanisms: (i) early fusion by channel-wise concatenation of raw input images and (ii) late fusion by channel-wise concatenation of CNN features of input images.|
|||Rectification network  Figure 2 shows the CNN architecture of the rectification network.|
|||3 is to encode correlations between two input images R1 and R2 into CNN features.|
|||The CNN features from the two encoders are concatenated channel-wise by the late fusion and fed into the correspondence decoder and visibility decoder.|
|||Implementation details  The CNN architecture details of DVM such as number of layers and kernel sizes and other implementation details are shown in Appendix A of the arXiv version of the paper [17].|
||12 instances in total. (in cvpr2017)|
|305|Qi_3D_Graph_Neural_ICCV_2017_paper|(c) Prediction by the two-stream CNN with HHA encoding [29].|
|||Timestamp 0 represents the CNN baseline.|
|||Prediction Model  After constructing the graph, we use a CNN as the unary model to compute the features for each pixel.|
|||For most of the ablation experiments, we use a modified VGG-16 network, i.e., deeplab-LargeFov [3] with dilated convolutions as our unary CNN to extract the appearance features from the 2D images.|
|||The initial learning rates of the pre-trained unary CNN and GNN are 0.001 and 0.01 respectively.|
|||Note that we concatenate the initial hidden state, which is the output of the unary CNN to capture the 2D appearance information.|
|||We now compare our 3DGNN to the unary CNN in order to investigate how GNN can be enhanced by leveraging 3D geometric information.|
|||The statistics show that our 3DGNN outperforms the unary CNN by a large margin for classes like cabinet, bed, dresser, and refrigerator.|
|||Propagation Step Unary CNN 2DGNN 3DGNN  0 1 3 4 6  37.9    37.8 38.4 38.0 38.1   38.1 39.3 39.4 39.0  Table 4.|
|||Comparison with the unary CNN on NYUD2 and SUNRGBD test set.|
|||Unary CNN vs 3DGNN on SUNRGBD test set.|
|||This result proves that our 3DGNN can overcome the limited size of the receptive field of unary CNN and captures the longrange dependency in images.|
||12 instances in total. (in iccv2017)|
|306|Zhang_Salient_Object_Subitizing_2015_CVPR_paper|Finally, inspired by the remarkable progress made by Convolutional Neural Network (CNN) features in many computer vision problems [22, 30, 43, 46], we try the CNN feature.|
|||In [22], it is suggested that given limited domain specific data, fine-tuning a pre-trained CNN model can be an effective and highly practical approach for many problems.|
|||Thus, we fine-tune the pre-trained CNN model of [30] for our problem.|
|||In addition, we evaluate the performance of the pretrained CNN without fine-tuning (CNN wo FT).|
|||The fine-tuned CNN achieves consistently better performance vs. other baselines over all categories, giving a mean AP score of 0.69.|
|||Fine-tuning gives about 15% relative performance gain over the pre-trained CNN feature (CNN wo FT).|
|||The fine-tuned CNN attains over 90% AP scores in predicting images with no salient object and with a single salient object.|
|||The CNN feature significantly outperforms the other baselines.|
|||7, shows the confusion matrix for our best baseline method, using the fine-tuned CNN model.|
|||To gain further insight into the model learned by the best baseline, we used the method of [47] to visualize the finetuned CNN classifiers.|
|||The visualization results indicate that the CNN captures some common visual patterns for each category, especially for categories 2, 3 and 4+.|
|||Figure 7: Confusion matrix of our method using the finetuned CNN feature.|
||12 instances in total. (in cvpr2015)|
|307|Yang_A_Large-Scale_Car_2015_CVPR_paper|Surprisingly, the AllView model yields the best performance, although it did not This result reveals that the CNN model is capable of learning discriminative representation across different views.|
|||We investigate if the CNN model can mimic this strength.|
|||We train a CNN model using images from each of the eight car parts.|
|||Taillight wins among the different car parts, mostly likely due to the relatively more distinctive designs, and the model name printed close to the taillight, which is a very informative feature for the CNN model.|
|||We fine-tune the CNN with the sumof-square loss to model the continuous attributes, such as maximum speed and displacement, but a logistic loss to predict the discrete attributes such as door number, seat number, and car type.|
|||To study the effectiveness of different viewpoints for attribute prediction, we train CNN models for different viewpoints separately.|
|||The feature extracted from the CNN model has a dimension of 4, 096, which is reduced to 20 by PCA.|
|||The second method combines the CNN features and SVM, denoted as CNN feature + SVM.|
|||We observe that CNN feature + Joint Bayesian outperforms CNN feature + SVM with large margins, indicating the advantage of Joint Bayesian for this task.|
|||CNN feature + Joint Bayesian  CNN feature + SVM  random guess  Easy Medium Hard 0.833 0.761 0.659 0.700  0.824 0.690 0.500  However, its benefit in car verification is not as effective as in face verification, where CNN and Joint Bayesian nearly saturated the LFW dataset [8] and approached human performance [21].|
|||12 depicts several pairs of test images as well as their predictions by CNN feature + Joint Bayesian.|
|||MG3 XrossMG3 XrossLancia DeltaLancia DeltaPeugeot SXCPeugeot 508ABT A7ABT A4Same modelSame modelDifferent modelsSame model00.20.40.60.8100.10.20.30.40.50.60.70.80.91false positive ratetrue positive rate  CNN feature + SVMCNN feature + Joint Bayesian[7] E. Hsiao, S. N. Sinha, K. Ramnath, S. Baker, L. Zitnick, and R. Szeliski.|
||12 instances in total. (in cvpr2015)|
|308|ER3_ A Unified Framework for Event Retrieval, Recognition and Recounting|The video representations are usually constructed by directly aggregating the frame-level CNN features.|
|||As shown in Figure 3, some counting descriptors are meaningless since no frames  1With l1normalization and appropriate down-sampling, the feature maps (after ReLU) from convolutional layer of CNN model naturally satisfy this assumption.|
|||Given an input video, we sample 5 frames per second (5 fps) to extract the CNN features.|
|||We exlpore various pre-trained CNN models, i.e., AlexNet [20], VGG [33] and ResNet-50 [14] to evaluate our method.|
|||The CNN feature maps are down-sampled to 4 4 with linear interpolation to fit the TCG (we set S =4  4 in TCG for computation efficiency).|
|||alex and res denote two CNN models, AlexNet and ResNet-50.|
|||We evaluate the CGA on two different CNN models, AlexNet [20] and ResNet-50 [14].|
|||In addition, consistent improvement can be observed for different CNN models, i.e., 11.2% gain with AlexNet and 9.9% with ResNet-50.|
|||vgg and res denote two CNN model, VGG and ResNet-50.|
|||On MED14 dataset, we achieve comparable result (mAP = 36.9) with recent CNN model based methods.|
|||achieve state of the art result by fusing motion features  (IDT) with their CNN based results (mAP = 34.9).|
|||Exploiting image-trained CNN architectures for unconstrained video classification.|
||12 instances in total. (in cvpr2017)|
|309|Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects|Specifically, a CNN is employed to encode image content and then a decoder RNN is exploited to generate a natural sentence.|
|||Given an image, a CNN is utilized to extract visual features, which will be fed into LSTM at the initial time step for sentence generation.|
|||(b) The image representation extracted by CNN is injected into LSTM at the initial time for standard word-by-word sentence generation.|
|||The overall training of LSTM-C is similar to regular CNN plus RNN systems by minimizing the energy loss which estimates the contextual relationships among the generated words in the decoding stage.|
|||Finally, the overall objective and optimization strategy of LSTM-C are presented in a CNN plus RNN framework.|
|||More specifically, for the external images with single label (e.g., ImageNet [19]), the standard CNN architecture [20] is adopted to train the object detectors, while for the image data with multiple objects (e.g., MSCOCO [12]), we follow [5] and learn the detectors by using the weakly-supervised approach of Multiple Instance Learning (MIL).|
|||For the experiments on held-out MSCOCO, the object classifiers for copying mechanism are trained with all the MSCOCO training images including the eight novel objects and the LSTM for sequence modeling is pre-trained with all the sentences in MSCOCO training set, while the entire CNN plus RNN system are optimized with the paired image-sentence data only from held-out MSCOCO training set.|
|||In terms of the entire CNN plus RNN system, it is optimized with the paired image-sentence data in MSCOCO training set.|
|||The entire CNN plus RNN system in our LSTM-C is trained for 50 epoches on both datasets or we stop the training until the performance has no longer improvement on the corresponding validation set.|
|||By only adopting the MSCOCO as the training data for the CNN plus RNN system, our LSTM-C (One hot+Glove) makes the relative improvement over NOC (One hot+Glove) by 4.3%, 4.9% and 17.8% in Novel, F1 and Accuracy, respectively.|
|||The results basically indicate the advantage of exploiting both generative and copying mechanisms in the CNN plus RNN system for novel object captioning, even when scaling into ImageNet images with hundreds of novel objects.|
|||The detected objects are predicted by the standard CNN architecture [20], and the output sentences are generated by 1) LRCN and 2) our LSTM-C.  0.6.|
||12 instances in total. (in cvpr2017)|
|310|Deep Quantization_ Encoding Convolutional Activations With Deep Generative Model|tional layer in a pre-trained CNN as a universal visual representation and applying this representation to other visual recognition tasks (e.g., scene understanding and semantic segmentation), CNNs also manifest impressive performances.|
|||Related Work  In the literature, visual representation generation from a pre-trained CNN model has proceeded along two dimensions: global activations and convolutional activations.|
|||The first is to extract visual representation from global activations in a CNN directly, e.g., the outputs from fullyconnected layer in VGG [30] or pool5 layer in ResNet [7].|
|||In practice, this scheme often starts by pre-training CNN model on a large dataset (e.g., ImageNet) and then finetuning the CNN architecture with a small amount of taskspecific data to better characterize the intrinsic information in target scenario.|
|||Another alternative scheme is to utilize the activations from convolutional layers in CNN as regional and local descriptors.|
|||Compared to global activations, convolutional activations from CNN are embedded with rich spatial information, making them more transferable to different domains and more robust to translation and rotation, which have shown the effectiveness in several technological advances, e.g., Spatial Pyramid Pooling (SPP) [6], Fast RCNN [5] and Fully Convolutional Networks (FCNs) [22].|
|||Spatial Pyramid Pooling (SPP) is performed on the last pooling layer of CNN to aggregate the local descriptors of video frame, which applies four different max pooling operations and obtain (6  6), (3  3), (2  2) and (1  1) outputs for each convolutional filter, resulting a total of 50 descriptors.|
|||For image, an input with a higher resolution of (448  448) is fed into the CNN and the activations of the last convolutional layer conv5 4+relu in VGG 19 are extracted, leading to dense local descriptors of 28  28.|
|||Instead, we feed a higher resolution (e.g., 448  448) input into the CNN to fully utilize image information and extract the activations of the last convolutional layer (e.g., conv5 4+relu in VGG 19), resulting in dense local descriptors (e.g., 28  28) for image as in [20].|
|||Method  Accuracy  Two-stream ConvNet [29] C3D (3 nets) [33] Factorized ST-ConvNet [32] Two-stream + LSTM [45] Two-stream fusion [4] Long-term temporal ConvNet [34] Key-volume mining CNN [49] TSN (3 modalities) [42] IDT [40] C3D + IDT [33] TDD + IDT [41] Long-term temporal ConvNet + IDT [34] FV-VAE-pool5 FV-VAE-pool5 optical flow FV-VAE-res5c FV-VAE-(pool5 + pool5 optical flow) FV-VAE-(res5c + pool5 optical flow) FV-VAE-(res5c + pool5 optical flow) + IDT  88.1% 85.2% 88.1% 88.6% 92.5% 91.7% 93.1% 94.2% 85.9% 90.4% 91.5% 92.7% 83.9% 89.5% 86.6% 93.7% 94.2% 95.2%  VAE and GA are then with the same dimension of 4,096.|
|||As shown in Table 3, compared to pool5 in VGG 19, FVVAE on the outputs of res5c layer in ResNet 152 with a deeper CNN exhibits better performance.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||12 instances in total. (in cvpr2017)|
|311|Wu_Quantized_Convolutional_Neural_CVPR_2016_paper|However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions.|
|||In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models.|
|||Therefore, it is crucial to accelerate the computation and compress the memory consumption for CNN models.|
|||4820  volutional networks, namely Quantized CNN (Q-CNN), to simultaneously accelerate and compress CNN models with only minor performance degradation.|
|||Moreover, we implement the quantized CNN model on mobile devices, and dramatically improve the test-phase efficiency, as depicted in Figure 1.|
|||Moreover, the quantized CNN model can be implemented on mobile devices and classify an image within one second.|
|||2  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  3.3.3 Error Correction for Multiple Layers  The above quantization method can be sequentially applied to each layer in the CNN model.|
|||In practice, we can vary these two parameters to balance the tradeoff between the test-phase efficiency and accuracy loss of the quantized CNN model.|
|||Experiments  In this section, we evaluate our quantized CNN framework on two image classification benchmarks, MNIST [20] and ILSVRC-12 [26].|
|||4826  5.2.3 Quantizing the Whole Network  So far, we have evaluated the performance of CNN models with either convolutional or fully-connected layers quantized.|
|||The speed-up/compression rates and the increase of top1/5 error rates for the whole CNN model.|
|||Therefore, our proposed approach improves the run-time efficiency in multiple aspects, making the deployment of CNN models become tractable on mobile platforms.|
||12 instances in total. (in cvpr2016)|
|312|Weakly Supervised Cascaded Convolutional Networks|[11] proposed an inverse cascade method using various CNN feature maps to localize object proposals in a coarse to fine manner.|
|||WCCN (2stage): The pipeline of end-to-end 2-stage cascaded CNN for weakly supervised object detection.|
|||First stage (Location network): The first stage of our cascaded model is a fully-convolutional CNN with a global  average pooling (GAP) or global maximum pooling (GMP) layer, inspired by [36].|
|||Since multiple categories can exist in a single image [22], we use an independent loss function for each class in this branch of the CNN architecture, so the loss function is the sum of C binary logistic regression loss functions.|
|||WCCN (3stage): The pipeline of end-to-end 3-stage cascaded CNN for weakly supervised object detection.|
|||This extra stage will help the multi-loss CNN to have better initial locations for choosing candidate bounding boxes to pass to the next stage.|
|||So this new cascade has three stages: first stage, similar to previous cascade is a CNN with global pooling layer; middle stage, fully convolutional network  with segmentation loss; last stage, multiple instance learning with corresponding loss.|
|||Since the multiple stages of cascades contain different CNN networks losses, in the following we explain details of each part separately to have better overview of the implementation.|
|||a semantic relationship between improvement gains using different CNN architectures in our networks in comparison with using the same CNNs in other methods.|
|||Similar to the other works like [6, 13], EdgeBox performs better with CNN based object detectors.|
|||Table 2 presents the comparison on VOC 2007 with different CNN architectures for all of the methods.|
|||Each stage in our multi-stage cascaded CNN can be analyzed by comparison with the CNN-based methods in same context.|
||12 instances in total. (in cvpr2017)|
|313|Zhang_Beyond_Frontal_Faces_2015_CVPR_paper|It consists of three components:   The global classifier, a CNN trained on the full body  of the person.|
||| A set of 107 poselet classifiers, each is a CNN trained  on the specific poselet pattern using [2].|
|||Using the poselet patches of step 1, we train a CNN for each poselet to recognize the identities on our training set.|
|||In addition, we train a CNN for the global classifier using the patches corresponding to the full body images.|
|||Training the Part Classifiers Pi(y|X) 4.2.1 Global classifier P0(y|X) Using the FC7 layer of the CNN trained for the full body area of each instance, we train a multi-class SVM to predict each identity y.|
|||PIPER gets more than 3% gain over the very strong baseline of using the fine-tuned CNN combined with the DeepFace model.|
|||As a second baseline we trained an SVM using the FC7 features of the CNN proposed by Krizhevsky et al.|
|||5.1.2 Ablation Study  Our method consists of three components  the fine-tuned Krizhevsky CNN (the Global Model), the DeepFace recognizer, and a collection of 107 Poselet-based recognizers.|
|||As the figure shows, when the face is not present we can significantly outperform a fine-tuned CNN on the full image.|
|||The generalization capabilities of deep features are well studied, but we believe the mixture of multiple partbased classifiers also helps here, since our system improves faster than the global fine-tuned Krizhevskys CNN method.|
|||If we use the predictions of the Krizhevskys CNN trained on ImageNet and fine-tuned on our training set, which is known to be a very powerful baseline, the nearest neighbor is of the same class in only 50% of the examples.|
|||bining a state-of-the-art CNN on the full body fine-tuned on our dataset with a state-of-the-art frontal face recognizer.|
||12 instances in total. (in cvpr2015)|
|314|cvpr18-A Constrained Deep Neural Network for Ordinal Regression|An implementation based on the CNN framework is proposed to solve the problem such that high-level features can be extracted automatically, and the optimal solution can be learned through the traditional back-propagation method.|
|||Based on this formulation, CNN can be adapted to solve it.|
|||Section 3 describes the proposed objective function and the CNN architecture adapted for solving the optimization problem.|
|||[16] have recently adopted CNN for age estimation.|
|||The idea is very similar to RED-SVM, but they adapt a single CNN to combine all classifiers and output the k  1 predictions at the same time.|
|||For example, the outputs of the CNN predicts that yt is greater than k + 1 and smaller than k  1.|
|||follow the decoding  strategy of RED-SVM to assign yt =  m1 P k=1  [fk(xt) = 1] + 1,  where fk(xt) is the k-th output of the CNN for xt.|
|||The Proposed CNN based Optimization  Traditional feature based large-margin approaches often employ a function (xi) mapping the input feature vector xi to a high dimensional space.|
|||1, all the convolution layers Gh, G11, G12, G21 and G22 share weights, meaning that there is only one unique standard CNN to be trained.|
|||1: Initialize or update all weights in a CNN consisting of convolution net Gh and two fully-connected layers Gc and Gr both connected to Gh.|
||||T | P xtT  Three baseline methods are employed for comparison: the state-of-the-art handcarfted feature based ordinal regression method RED-SVM [12], the traditional CNN method for  multi-class classification CNNm and the CNN based ordinal regression method Niu et al.s method [16].|
|||Ordinal regression with multiple output cnn for age estimation.|
||12 instances in total. (in cvpr2018)|
|315|Tran_DeepCoder_Semi-Parametric_Variational_ICCV_2017_paper|The CAE architectures are typically similar to that of a CNN with additional inverse convolution operation [34].|
|||In [15], a CNN is jointly trained for detection and intensity estimation of AUs.|
|||These methods are parametric, with the CNN used to extract deep features; yet, the network output remains unstructured.|
|||For models in which it is not feasible to process high dimensional features from raw images, we extracted the 2000-D features (Z 0) from the CNN in our experiments, this size was found optimal for the competing methods.|
|||The CNN [15] model is a standard 2-layer CNN for multi-output classification (we used the same setting as in [15]).|
|||The model highlighted with  was trained with the deep features, extracted from the last layer of the best performing CNN [15], and, thus, is directly comparable to the proposed 2DC.|
|||Here, we evaluated the model on two sets of features: LBPs with facial landmarks, and deep features, extracted using the CNN (our first coder).|
|||We used the pre-processed raw images as input to the proposed DeepCoder and CNN based models.|
|||As the GP-based models and the other baselines are not directly applicable to high dimensional image data, we trained on the LBP+landmark features and/or deep features, extracted  from the last layer of the best performing CNN [15] model.|
|||On average, the CNN based models largely outperform the GPs in both measures across most of the AUs.|
|||The OR-CNN [37] model, which has the same architecture as CNN [15] but with the ordinal classifier, learns one binary classifier for each intensity level of each AU, resulting in a large number of parameters, easily prone to overfitting.|
|||Ordinal In  regression with multiple output cnn for age estimation.|
||12 instances in total. (in iccv2017)|
|316|Atoum_Monocular_Video-Based_Trailer_ICCV_2017_paper|This system is made possible through our proposed distance-driven Multiplexer-CNN method, which selects the most suitable CNN using the estimated couplerto-vehicle distance.|
|||The input of the multiplexer is a group made of a CNN detector, trackers, and 3D localizer.|
|||The Multiplexer-CNN system selects from five CNN architectures to perform detection operations.|
|||The current estimate of the coupler position drives the CNN selection.|
|||Each CNN is invoked independently based on the estimated distance between the vehicle and the coupler.|
|||We propose multiple CNN trackers, i.e., TCNNi, for accurate tracking at all scales and compare against state-of-the-art single trackers.|
|||Our Proposed Approach  Our Multiplexer-CNN system has five CNN inputs: DCNN, TCNN1, TCNN2, TCNN3, CCNN.|
|||In our Multiplexer system, the coupler-to-vehicle distance (CVD) is crucial in serving as the selector to choose one CNN among five.|
|||The first part represents the Euclidean loss, which is the regression error when having a  5479  Origin point (0,0,0)Ground distance map D0Coupler coordinate(x, y, z)TRAILERVEHICLECoupler distance map DhCameraCouplerDhD0Distance to groundhch0Table 1: CNN architectures.|
|||Tracking CNN networks The network architecture of TCNN is in Tab.|
|||For the second baseline, we learn a CNN named CCNNP similar to CCNN with the same structure, except that instead of producing a regression output for 38 contour points, CCNNP estimates an 8-dim vector (i, i) for i = 1    4.|
|||2 provides detailed efficiency analysis per CNN network and per system stage, where Alg 1, Alg 2, and Alg 3 are the detection, tracking and localization of the coupler when tested with N1 (for 1 iteration), N2 and N3 patches.|
||12 instances in total. (in iccv2017)|
|317|cvpr18-Recurrent Residual Module for Fast Inference in Videos|However, CNN inference on video is computationally expensive due to processing dense frames individually.|
|||In this work, we propose a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks.|
|||For example, for Youtube-8M dataset [1] with over 8 million video clips, it will take 50 years for a CPU to extract the deep features using a standard CNN model.|
|||One of the bottlenecks for video understanding using CNNs is the frame-by-frame CNN inference.|
|||Intuitively, we can leverage the frame similarity to reduce some redundant computation in the frame-by-frame video CNN inference.|
|||An attractive recursive schema is as follows:  R(It) = R(It1) + G(It  It1),  (1)  where R is the deep CNN feature, G is a fast and shallow network that only processes the frame difference between Ideally, G should be frame It and It1 in a video clip.|
|||[43] proposed XNOR-Networks that use both binary weights and binary inputs to achieve 58 faster convolution operations on a CNN trained on ImageNet.|
|||Besides our method is a generic framework that could be plugged in a variety of CNN models without retraining to speed up the forward pass.|
|||Deep CNN models can be sped up by binarizing the input and the weight of the network.|
|||Video pose estimation and object detection  In this section, we apply our RRM framework to several mainstream visual systems to improve the efficiency of their backbone CNN models.|
|||We have shown that the overall sparsity of different CNN models can be generally improved by our RRM framework.|
|||Exploiting image-trained cnn architectures for unconstrained video classification.|
||12 instances in total. (in cvpr2018)|
|318|Zhou_Fine-Grained_Image_Classification_CVPR_2016_paper|This paper shows how to incorporate the class bipartite graphs into CNN and learn the optimal classifiers through overall back-propagation.|
|||11124  Using BGL has several advantages: (1) BGL imposes additional constraints to regularize CNN training, thereby largely reducing the possibility of overfitting when only a small amount of training data is available.|
|||[35] proposed an architecture that uses two separate CNN feature extractors to model the appearance due to where the parts are and what the parts look like.|
|||The resulting CNN is optimized by a global backpropagation.|
|||In a nutshell, the last layer of CNN is to minimize the negative log-likelihood over the training data, i.e.,  W X min  (x,y)X   log P (y(cid:12)(cid:12)  x, W),  where the softmax score,  x, W) =  P (i(cid:12)(cid:12)  efi j=1 efj  Pk  .|
|||This hierarchical ranking guides CNN training to learn more discriminative features to capture subtle difference between finegrained classes.|
|||In summary, given the training data X and the graph label defined by {Gj}j , the last layer of CNN with BGL aims to minimize the joint negative log-likelihood with proper regularization over the weights:  min  W,{Wj }j X  (x,y)X  (cid:16)  log py   m  X  j=1  log pj   y(cid:17)  log pw.|
|||We test on three popular CNN frameworks, AlexNet (AN) [31], GoogLeNet (GN) [49], and VGGNet (VGG) [46].|
|||3f further compares BGL with several previous works using different CNN architectures.|
|||By exploring the label dependency, the proposed BGL further improves all the three CNN architectures using either single-view (SV) or multi-view (MV) cropping.|
|||5f summarizes the performance of our method using different CNN architectures.|
|||BGL formulation provides a way to regularize CNN training to alleviate its overfitting issue.|
||12 instances in total. (in cvpr2016)|
|319|Lee_Deep_Saliency_With_CVPR_2016_paper|LEGS first generates an initial rough saliency mask from deep CNN and refines the saliency map using an object proposal algorithm.|
|||This regular grid representation is efficient for CNN architecture because we can convert images with different resolutions and aspect ratios into a fixed size distance map without resizing and cropping.|
|||In the self-evaluation experiment in Table 3, we find that encoding the low level feature distance map with the deep CNN with 1  1 kernel enhances the performance of our method.|
|||For each input image, we process it with the pre-trained deep CNN only once and reuse the extracted high level feature map for all queried regions.|
|||It is also common for CNN to generate feature maps with much lower resolution than original input images.|
|||By providing strongly relevant information, the encoded low level distance map(ELD-map) complements the features from deep CNN and guides the classifier to learn properly.|
|||The model using only the high level feature map from the deep CNN detected the approximate location of the salient objects but was unable to capture detailed location because the high level feature maps had lower resolution than the resolution of the original input images.|
|||Also, we found that the ELD-map often helps to find salient objects that are difficult to detect using only CNN as shown in the second row.|
|||DRFI, DSR, GMR, HDCT and HS use low level features and MCDL, MDF and LEGS utilize deep CNN for high level context.|
|||In Figure 8,  the PR-graph indicates our algorithm achieves the better performance than the previous works including MDF and MCDL which also utilize CNN models.|
|||The training of our deep CNN took around 3 hours under the same environment.|
|||When concatenated with the high-level features from the deep CNN model (VGG16), our method shows the state-of-the-art performance in terms of both visual qualities and quantitative comparisons.|
||12 instances in total. (in cvpr2016)|
|320|cvpr18-Excitation Backprop for RNNs|CAM [36] removed the last fully connected layer of a CNN and exploited a weighted sum of the last convolutional feature maps to obtain the class activation maps.|
|||[34] generated class activation maps from any CNN architecture that uses nonlinearities producing non-negative activations.|
|||[13] used mid-level CNN outputs on overlapping patches, requiring multiple passes through the network.|
|||1, we have three main modules: RNN Backward, Temporal normalization, and CNN Backward.|
|||For every video frame ft at time step  t, we use the backprop of [34] for all CNN layers:  P t(ai|aj) =(Zjbat M apt(ai) = Xaj Pi  0,  iwij,  if wij  0, otherwise  P t(ai|aj)M apt(aj)  (8)  (9)  1442  wherebat  i is the activation when frame ft is passed through the CNN.|
|||M apt at the desired CNN layer is the cEB-R saliency map for ft. Computationally, the complexity of cEB-R is on the order of a single backward pass.|
|||The CNN is truncated at the fc7 layer such that the fc7 features of frames feed into the recurrent unit.|
|||Starting from the last LSTM time-step, cEB-R backpropagates the probability distribution through time and through the CNN at every time-step.|
|||Procedure:  1 Set a one-hot vector according to the desired action  class or caption word A at the desired nth time-step;  2 Backprop the indicator vector through time and down to the fc CNN layer using EB-R obtaining a saliency map M apt at every time step t;  3 Normalize the resulting frame-wise saliency maps  over time such thatPT  t=1 M apt = 1;  4 Repeat the above steps, with negated weights at the  top layer to get a second set of T saliency maps;  5 Contrastive Operation: Subtract the resulting maps at  the fc CNN layer to yield cEB for each time step;  6 Continue EB through the CNN to the desired conv  layer to obtain the spatial grounding;  7 The sum of each spatial saliency map over time can be  used to perform temporal grounding for A;  1443  5.|
|||We compare our formulation against spatial top-down saliency using a CNN (treating every video frame as an independent image).|
|||We use the following CNN model: VGG-16 of Ma et al.|
|||In order to obtain spatial saliency maps for a word in a video, c-EB-R requires one forward pass and one backward pass through the CNN-LSTM-LSTM, while [15] requires one forward pass through the CNN part, but m forward passes through the LSTM-LSTM part, where m = 64 is the area of the saliency map (vs. our single backward pass).|
||12 instances in total. (in cvpr2018)|
|321|Crivellaro_A_Novel_Representation_ICCV_2015_paper|We also use a CNN for this task, but another detection method could be used.|
|||We then predict the reprojections of the control points by applying a specific CNN to each hypothesis.|
|||Architecture of a CNN CNNcp-pred-j for predicting the projections of the control points.|
|||During an offline stage, we train a first CNN with a standard multi-class architecture shown in Fig.|
|||The input to this CNN is a 32  32 image patch q, its output consists of the likelihoods P (J = j | q) of the patch to correspond to one of the NP parts.|
|||We train the CNN with patches randomly extracted around the centers cij of the parts in images Ii and patches extracted from the background, and by optimizing the negative log-likelihood over the parameters w of the CNN:  bw = arg min  NPX  j=0  X  q2Tj   log softmax(CNNpart-det  w  (q))[j]  ,  (2) where Tj is a training set made of image patches centered on part j and T0 is a training set made of image patches from the background, CNNpart-det (q) is the NP + 1vector output by the CNN when applied to patch q, and softmax(CNNpart-det (q))[j] is the j-th coordinate of vector softmax(CNNpart-det (q)).|
|||w  w  w  At run time, we apply this CNN to each 32  32 patch in the input images captured by the camera.|
|||Predicting the Reprojections of the Control  Points and their Uncertainty  Once the parts are detected, we apply a second CNN to the patches centered on the candidates cjl to predict the projections of the control points for these candidates.|
|||We train each of these CNNs during an offline stage by simply minimizing over the parameters w of the CNN the squared loss of the predictions:  bw = arg min X  (q,w)2Vj  ||w  CNNcp-pred-j  w  (q)||2 ,  (3)  where Vj is a training set of image patches q centered on part j and the corresponding 2D locations of the control points concatenated in a (2NV )-vector w, and CNNcp-pred-j (q) is the prediction for these locations made by the CNN specific for part j, given patch q as input.|
|||In addition, we estimate the 2D uncertainty for the predictions, by propagating the image noise through the CNN that predicts the control point projections [43].|
|||Each CNN was trained to predict a different part pose representation:   Averaging Poses: The output of the CNN is a 3D rotation and a depth for each part.|
||| 3D Control Points: The output of the CNN are the  (a)  (b)  (c)  Figure 7.|
||12 instances in total. (in iccv2015)|
|322|Perronnin_Fisher_Vectors_Meet_2015_CVPR_paper|First, since the convolutional layers of the CNN share much with the coding/aggregation steps of the FV (see section 3.3), we wanted to understand whether FVs were competitive with representations learned by the convolutional layers of the CNN.|
|||Our experiments show that the accuracy of our architecture comes close to that of the CNN model of Krizhevsky et al.|
|||In section 3 we describe the  1  proposed architecture and explain how it relates to standard FV and CNN pipelines.|
|||[17] address the lack of geometric invariance of CNNs by extracting CNN activations from large patches, embedding them using VLAD encoding and aggregating them at multiple scales.|
|||We then highlight the main differences between this architecture and standard FV and CNN pipelines.|
|||Comparison with FV and CNN pipelines Relationship with FVs.|
|||The main difference between the proposed architecture and the standard CNN [28] is that our first layers are unsupervised while in [28] all layers are learned in a supervised fashion.|
|||However, it is somewhat below the CNN systems [28, 56, 46] and significantly below the recent very deep architecture by Simonyan  Figure 5.|
|||Transferring mid-level features  In standard CNN architectures trained on large datasets such as ImageNet, the outputs of intermediate layers have been used as mid-level features in a large variety of new tasks.|
|||[41] reports 84.3% on Holidays and 3.64 on UKB with a costly setting that involves extracting and matching multiple CNN features per image.|
|||[1] also report better results on Holidays (79.3%) for a CNN that has been retrained using an external dataset of landmarks.|
|||We also showed that mid-level representations extracted from our architecture were competitive with the mid-level features extracted from CNN representations, on several target tasks.|
||12 instances in total. (in cvpr2015)|
|323|cvpr18-Wing Loss for Robust Facial Landmark Localisation With Convolutional Neural Networks|The input for a regression CNN is usually an image patch enclosing the whole face region and the output is a vector consisting of the 2D coordinates of facial landmarks.|
|||Besides the classical CNN architecture, newly developed CNN systems have also been used for facial landmark localisation and shown promising results, e.g.|
|||However, a CNN using a global face image as input cannot meet this requirement.|
|||To address this issue, one solution is to extract CNN features from local patches around facial landmarks.|
|||The first CNN is a very simple one that can perform rough facial landmark localisation very quickly.|
|||Then the second CNN is used to perform finegrained landmark localisation.|
|||In such a case, the CNN is used as a regression model learned in a supervised manner.|
|||To empirically analyse different loss functions, we use a simple CNN architecture, in the following termed CNN6, for facial landmark localisation, to achieve high speed in model training and testing.|
|||Given a set of labelled training samples  = {Ii, si}N the target of CNN training is to find a  that minimises:  i=1,  The design of a proper loss function is crucial for CNNbased facial landmark localisation.|
|||method  average normalised error  CCL (CVPR2016) [75] DAC-CSR (CVPR2017) [21] TR-DRN (CVPR2017) [39]  CNN-6 (L2) CNN-6 (L1) CNN-6 (smooth L1) CNN-6 (Wing loss)  2.72102 2.27102 2.17102 2.41102 2.00102 2.02102 1.88102  the CNN-6 training, the performance in terms of accuracy improves significantly and outperforms all the state-of-theart baseline approaches, despite the CNN networks simplicity.|
|||To prove the effectiveness of the proposed Wing loss function, extensive experiments have been conducted using several CNN network architectures.|
|||Joint head pose estimation and face alignment framework using global and local cnn features.|
||12 instances in total. (in cvpr2018)|
|324|Local Binary Convolutional Neural Networks|The ratio of the number of parameters in CNN and LBC is:  3.1.|
|||21  CNN Module  Wl  xl  xl  LBCNN Module  xl+1  Vl  xl+1  Figure 3: Basic module in CNN and LBCNN.|
|||(7)  This analysis is valid for a single image patch that is convolved with CNN and LBCNN filters.|
|||Baselines: To ensure a fair comparison and to quantify the exact empirical difference between our LBCNN approach and a traditional CNN, we use the exact same architecture for both the networks, albeit with sparse, binary and fixed  q  16  32  64  128  192  256  384  512  LBCNN  LBCNN-share  Baseline  82.74 82.70 84.13  85.57 85.26 86.30  88.18 87.85 88.77  90.70 90.26 90.86  91.58 91.37 91.69  92.13 91.72 92.15  92.96 92.91 92.93  92.09 91.83 91.87  Table 1: Classification accuracy (%) on CIFAR-10 with 20 convolution layers and 512 LBC filters on LBCNN, LBCNN-share, and CNN baseline.|
|||Consequently in these experiments with 3  3 convolutional kernels, LBCNN has 10 fewer learnable parameters (the baseline CNN also includes a 1  1 convolutional layer).|
|||The best performing LBCNNs are compared to their corresponding CNN baseline, as well as to state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [5], ResNet [12], Maxout Network [9], Network in Network (NIN) [23].|
|||LBCNN column only shows the best performing model and the Baseline column shows the particular CNN counterpart.|
|||The LBC module allows us to train extremely deep CNN efficiently with 8848 convolutional layers (4424 LBC modules), dubbed NetEverest, using a single nVidia Titan X GPU.|
|||Both LBCNN and our baseline CNN share the same architecture: 48 convolutional layers (24 LBC modules), 512 LBC filters, 512 output channels, 0.9 sparsity, 4096 hidden units in the fully connected layer.|
|||(R) Normalized correlation measure for LBCNN and CNN filters.|
|||We choose the best-performing architecture on CIFAR-10 described in Section 4 for both the CNN and LBCNN.|
|||We make a few observations from our findings (see Figure 7 (R3)): (1) LBCNN converges faster than CNN, especially on small datasets and (2) LBCNN outperforms CNN on this task.|
||12 instances in total. (in cvpr2017)|
|325|Tychsen-Smith_DeNet_Scalable_Real-Time_ICCV_2017_paper|We introduce two novelties, a corner based region-of-interest estimator and a deconvolution based CNN model.|
|||In particular we highlight the relatively slow region based CNN approaches (R-CNN [4], Faster R-CNN [15]) and the more recent work on real-time detection (YOLO [14], SSD [12]).|
|||Related work  In region based CNN detection (R-CNN) [4] the image is first preprocessed with a region proposal algorithm e.g.|
|||bounding boxes) of interest (RoIs) which are then rescaled to fixed dimensions (normalizing scale and aspect ratio) and fed into a CNN based classifier.|
|||The CNN assigns a probability that the region bounds an object of interest or the null class and, via linear regression, identifies an improved bounding box.|
|||This approach has demonstrated state-of-the-art results, however, it is very expensive to train and evaluate, requiring multiple full CNN evaluations (one per region proposal) and an often expensive pre-processing step.|
|||Since the majority of CNN computation occurs in the first few layers, Fast R-CNN [3] addressed these issues by applying a shallow CNN to the image and then, for each region, extracting fixed sized features from the generated feature map for the final classification.|
|||In Faster R-CNN [15] the region proposal algorithm was integrated into the CNN providing an end-to-end solution, improved timings and demonstrating that both tasks (region proposal and classification) shared similar underlying features.|
|||With this approach the CNN is only evaluated once to produce the outcomes for all detectors resulting in significantly reduced training and evaluation In Single Shot Detector (SSD) [12] this approach times.|
|||Subsequently, we have developed a solution based on the state-of-the-art regression capabilities of a single end-to-end CNN which estimates the highly sparse distribution Pr(s|B) in a real-time (or computationally constrained) operational environment.|
|||We assert that due to the natural translation invariance of the problem, estimating the corner distribution can be efficiently performed with a standard CNN design trained on bounding box annotated image data (e.g.|
|||Classify RoI: The final classification CNN is executed, the classification distribution and bounding box regression outputs are transferred from GPU to CPU.|
||12 instances in total. (in iccv2017)|
|326|Wang_Deep_Networks_for_ICCV_2015_paper|This simple model, which is named sparse coding based network (SCN), achieves notable improvement over the generic CNN model [8] in terms  1370  of both recovery accuracy and human perception, and yet has a compact model size.|
|||Technically, our network is also a CNN and it has similar layers as the CNN model proposed in [8] for patch extraction and reconstruction.|
|||PSNR for 2 SR on Set5 using SCN and CNN with various network sizes.|
|||We also train a CNN model [8] of the same size as SCN, but find its convergence speed much It is reported in [8] that training a CNN takes slower.|
|||Table 2 shows the PSNR and structural similarity (SSIM) [31] for adjusted anchored neighborhood regression (A+) [29], CNN [8], CNN trained with larger model size and more data (CNN-L) [9], the proposed CSCN, and CSCN with our multi-view testing (CSCN-MV).|
|||CNN-L improves over CNN by increasing model parameters and training data.|
|||The visual qualities of the SR results generated by sparse coding (SC) [35], CNN and CSCN are compared in Fig.|
|||SR results given by SC [35] (first row), CNN [8] (second row) and our CSCN (third row).|
|||self-example regression (SER) [34], CNN [8] and CSCN.|
|||Each of the participants in the evaluation is shown a set of HR image pairs, which are  376  (a) bicubic  (b) SE [12]  (c) SC [35]  (d) DNC [6]  (e) CNN [8]  (f) CSCN  Figure 9.|
|||Subjective SR quality scores for different methods including bicubic, SC [35], SE [12], SER [34], CNN [8] and the proposed CSCN.|
|||The CNN model becomes less competitive in the subjective evaluation than it is in PSNR comparison.|
||12 instances in total. (in iccv2015)|
|327|Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper|We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data.|
|||Previous work proposed techniques for domain transfer with CNN models [12, 24] but did not utilize the learned source semantic structure for task transfer.|
|||We solve the two problems jointly using a new CNN architecture, outlined in Figure 2.|
|||Recently, supervised CNN based feature representations have been shown to be extremely effective for a variety of visual recognition tasks [22, 9, 14, 29].|
|||However, this method operated primarily in a generative context and therefore did not leverage the full representational power of supervised CNN representations.|
|||Training a joint source and target CNN architecture was proposed by [7], but was limited to two layers and so was significantly outperformed by the methods which used a deeper architecture [22], pre-trained on a large auxiliary data source (ex: ImageNet [4]).|
|||Joint CNN architecture for domain and task  transfer  We first give an overview of our convolutional network (CNN) architecture, depicted in Figure 2, that learns a representation which both aligns visual domains and transfers the semantic structure from a well labeled source domain to the sparsely labeled target domain.|
|||Our ideas of domain confusion and soft label loss for task transfer are generic and can be applied to any CNN classification architecture.|
|||Our overall CNN architecture for domain and task transfer.|
|||We then further fine-tune the network using the source labeled data in order to produce the soft label distributions and use the learned source CNN weights as the initial parameters for training our method.|
|||This difference is explained by their use of SURF BoW features, indicating that CNN features are a much stronger feature for use in adaptation tasks.|
|||Conclusion  We have presented a CNN architecture that effectively adapts to a new domain with limited or no labeled data per target category.|
||12 instances in total. (in iccv2015)|
|328|One-Shot Video Object Segmentation|*First two authors contributed equally  This paper presents One-Shot Video Object Segmentation (OSVOS), a CNN architecture to tackle the problem of semi-supervised video object segmentation, that is, the classification of all pixels of a video sequence into background and foreground, given the manual annotation of one (or more) of its frames.|
|||The first contribution of the paper is to adapt the CNN to a particular object instance given a single annotated image (hence one-shot).|
|||To do so, we adapt a CNN pre-trained on image recognition [44] to video object segmentation.|
|||Overview of OSVOS: (1) We start with a pre-trained base CNN for image labeling on ImageNet; its results in terms of segmentation, although conform with some image features, are not useful.|
|||In [21], the authors combine training of a CNN with ideas of bilateral filtering.|
|||In the case of visual tracking (bounding boxes instead of segmentation) Nam and Han [32] use a CNN to learn a representation of the object to be tracked, but only to look for the most similar window in frame n + 1 given the object in frame n. In contrast, our CNN learns a single model from frame 1 and segments the rest of the frames from this model.|
|||In most CNN architectures [24, 47, 19], activations of the intermediate layers gradually decrease in size, because of spatial pooling operations or convolutions with a stride.|
|||End(cid:173)to(cid:173)end trainable foreground FCN  Ideally, we would like our CNN architecture to satisfy  the following criteria:  1.|
|||We draw inspiration from the CNN architecture of [30], originally used for biomedical image segmentation.|
|||Training details  Offline training: The base CNN of our architecture [47] is pre-trained on ImageNet for image labeling, which has proven to be a very good initialization to other tasks [27, 51, 23, 29, 18, 52].|
|||To this end, we propose a complementary CNN in a second branch, that is trained to detect object contours.|
|||In this case, the quality is not very good (although comparable to some previous techniques), but we only need to do a forward pass of the CNN for each frame.  )|
||12 instances in total. (in cvpr2017)|
|329|Li_Deeper_Broader_and_ICCV_2017_paper|In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning.|
|||There has been more extensive work on CNN models for domain adaptation, with methods developed for encouraging CNN layers to learn transferable features [9, 17].|
|||Moreover, as analysis has shown that the transferability of different layers in CNNs varies significantly [32], these studies have had carefully hand designed the CNN sharing structure to address their particular DA problems.|
|||It is also related to lowrank neural network models, typically used to compress [13] and speed up [16] CNNs, and have very recently been explored for cross-category CNN knowledge transfer [31].|
|||Domain generalization Using one such parameter generating function per layer, we can dynamically generate the weights at every layer of a CNN based on the encoded vector of every domain.|
|||Inference and Learning To make predictions for a particular domain, we synthesize a concrete CNN by multiplying out the parameters {G, U1, .|
|||This CNN can then be used to classify an input instance x.|
|||Classifier Performance Analysis We also compare the datasets by the margin between multiclass classification accuracy of within-domain learning, and a simple crossdomain baseline of training a CNN on all the source domains before testing on the held out target domain (as we shall see later, this baseline is very competitive).|
|||We explore features including Classic SIFT features (for direct comparison with earlier work), DECAF pre-extracted deep features following [10], and E2E end-to-end CNN learning.|
|||Ours-Full: Our full low-rank parameterized CNN trained end-to-end on images.|
|||Deep-All: Pretrained Alexnet CNN [14], fine-tuned on the aggregation of all source domains.|
|||For Deep and OursFull, we fine-tune the CNN on the source domains.|
||12 instances in total. (in iccv2017)|
|330|Person Re-Identification in the Wild|First, we propose a cascaded fine-tuning strategy to make full use of the detection data provided by PRW, which results in improved CNN embeddings.|
|||Two CNN variants, are derived w.r.t the fine tuning strategies.|
|||While RCNN is slow with 2000 proposals, extracting CNN features from a small number of proposals is fast, so we use RCNN instead of the fast variant [14].|
|||In this paper, three CNN architectures are tested: AlexNet [19], VGGNet [31] and ResNet [17].|
|||During training, a CNN embedding is learned to discriminate different identities.|
|||The two fine-tuning process which is called cascaded fine-tuning, results in a new CNN embedding, denoted as IDEdet.|
|||The two types of CNN embeddings are summarized below:   IDEimgnet.|
|||The IDE model is directly fine-tuned on the ImageNet pre-trained CNN model.|
|||With the ImageNet pre-trained CNN model, we first train an R-CNN model on PRW which is a two-class recognition task comprising of pedestrians and the background.|
|||Specifically, we fine tune RCNN with three CNN models  AlexNet (Alex) [19], VGGNet (VGG) [31], and ResidualNet (Res) [17].|
|||Further, when different CNN models are used for a given proposal, we find that ResidualNet outperforms the others in general: AP of ResNet is +0.41% higher than AlexNet.|
|||The 3 recognizers are: 1) 5, 600-dimensional Bag-of-Words (BoW) descriptor [45], the state-of-the-art unsupervised descriptor, 2) 4, 096-dimensional CNN embedding feature described in Section 4.2 using AlexNet, and 3) LOMO+XQDA [22], a  (a) mAP, 3 boxes/img  (b) mAP, 5 boxes/img  (c) rank-1 acc., 3 boxes/img  (d) rank-1 acc., 5 boxes/img  Figure 8: Plots of mAP and rank-1 accuracy using two variants of the IDE with 5 detectors.|
||12 instances in total. (in cvpr2017)|
|331|Behl_Bounding_Boxes_Segmentations_ICCV_2017_paper|We train a CNN to predict object coordinates for car instances.|
|||We use a modified version of the encoder-decoder style CNN proposed in [32] for estimating the object coordinates at each pixel.|
|||As above, the input to the CNN is an RGB-XYZ image as  2576  Figure 2: Work flow for our approach.|
|||The output of the CNN is a 3 layer regression which stores the X, Y and Z coordinates for each point of the input.|
|||We refer to the architecture of our CNN in supplementary material.|
|||Additionally, we also report results of ISF-SegMask-CNNDisp (ISF-SegMask with higher quality CNN based disparity input).|
|||In contrast, as defined in section 3.3, our scene flow estimation model considers all car instances detected by our CNN as foreground.|
|||Experiments with CNN based disparity as input: Furthermore, we evaluated our method with the optimal level of recognition granularity selected based on the ablation study (ISF-SegMask) on higher quality disparity inputs computed from DispNetC [21] and MC-CNN-acrt [47].|
|||Furthermore, in order to compare the performance of our CNN model with existing state-of-the-art approaches for predicting the object coordinates, we train a random forest [3] with 3 trees and maximum depth of 64.|
|||We find the quality of 3D object coordinate predictions significantly better using our CNN architecture with an average Euclidean er 2580  PRSM* [40] OSF+TC* [26] OSF [22] ISF-SegMask-CNNDisp  D1 fg  10.52 9.64 12.03 6.17  bg 3.02 4.11 4.54 4.12  bg+fg 4.27 5.03 5.79 4.46  bg 5.13 5.18 5.45 4.88  D2 fg  15.11 15.12 19.41 11.34  bg+fg 6.79 6.84 7.77 5.95  bg 5.33 5.76 5.62 5.40  Fl fg  13.40 13.31 18.92 10.29  bg+fg 6.68 7.02 7.83 6.22  bg 6.61 7.08 7.01 6.58  SF fg  20.79 20.03 26.34 15.63  bg+fg 8.97 9.23 10.23 8.08  Table 2: Quantitative Results on the KITTI 2015 Scene Flow Evaluation Server.|
|||We attribute the lower error of the CNN based predictions to the networks ability to generalize well to cars with high intra class variation.|
|||We refer the reader to the last section of the supplementary material for a detailed description of the CNN architecture we employed and a comparison of the quality of our 3D object coordinate predictions to other state-of-the-art methods.|
||12 instances in total. (in iccv2017)|
|332|Uijlings_Situational_Object_Boundary_2015_CVPR_paper|For both the Fisher Vectors and CNN features, we train linear SVMs with Stochastic Gradient Descent using [40].|
|||Looking at the probability mass Z, at n = 5 it is 61% for Fisher vectors and 71% for CNN features.|
|||Hence for stable and efficient computation time with optimal performance, we fix n = 5  n = 25  Z CNN subclass specific Z Fisher subclass specific AP CNN subclass specific AP Fisher subclass specific AP Monolithic  85% 79% 0.295 0.291 0.260 Table 1.|
|||n = 5 71% 61% 0.296 0.290 0.260  n = 1 47% 29% 0.274 0.267 0.258  n = 3 65% 51% 0.289 0.283 0.259  precision at 20% recall  precision at 50% recall  average precision  monolithic CNN class specific CNN subclass specific CNN class agnostic Fisher class specific Fisher subclass specific Fisher class agnostic GT class specific monolithic CNN enhanced  0.382 0.435 0.451 0.446 0.426 0.442 0.429 0.433 0.385  0.282 0.311 0.317 0.315 0.305 0.312 0.307 0.311 0.278  0.260 0.289 0.296 0.295 0.283 0.290 0.284 0.290 0.259  Table 2.|
|||Table 2 shows that CNN features work generally better than Fisher vectors for situational object boundary detection.|
|||This confirms other observations on the strength of CNN features (e.g.|
|||We use CNN features for the remainder of this paper.|
|||As the table shows, there is almost no difference between GT class specific and CNN class specific.|
|||We tried this with CNN features, which are stronger and have a lower dimensionality than Fisher vectors.|
|||We name this setting monolithic CNN enhanced.|
|||In contrast, when using CNN features results are slightly better when the detectors are trained on all object boundaries in the images.|
|||[17]Ours: CNN  class specificAcknowledgements.|
||12 instances in total. (in cvpr2015)|
|333|Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper|Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices.|
|||In contrast to these methods, ESP is computationally efficient and can be used at different spatial levels of a CNN network.|
|||To analyze the performance of a CNN network on edge devices, we introduce several new performance metrics, such as sensitivity to GPU frequency and warp execution efficiency.|
|||This factorization has successfully shown its potential in reducing the computational complexity of deep CNN networks (e.g.|
|||Sparse CNN: To remove the redundancy in CNNs, sparse CNN methods, such as sparse decomposition [32], structural sparsity learning [33], and dictionary-based method [34], have been proposed.|
|||We note that compression-based methods, low-bit networks, and sparse CNN meth ods are equally applicable to ESPNets and are complementary to our work.|
|||Our ESP module also learns multi-scale representations using dilated convolutions in parallel; however, it is computationally efficient and can be used at any spatial level of a CNN network.|
|||We introduce a computationally efficient convolutional module that allows feature re-sampling at different spatial levels of a CNN network.|
|||3.2 Relationship with other CNN modules  The ESP module shares similarities with the following CNN modules.|
|||In addition to these metrics, we introduce several system-level metrics to characterize the performance of a CNN on resource-constrained devices [56, 57].|
|||Latency represents the amount of time a CNN network takes to process an image.|
|||In addition to legacy metrics, we introduced several new system-level metrics that help to analyze the performance of a CNN network.|
||12 instances in total. (in eccv2018)|
|334|SphereFace_ Deep Hypersphere Embedding for Face Recognition|In this toy experiment, we construct a CNN to learn 2-D features on a subset of the CASIA face dataset.|
|||216  Layer  4-layer CNN  10-layer CNN  20-layer CNN  36-layer CNN  64-layer CNN  Conv1.x  [33, 64]1, S2  [33, 64]1, S2  Conv2.x  [33, 128]1, S2  Conv3.x  [33, 256]1, S2  [33, 128]1, S2  3  3, 128#  1 " 3  3, 128 3  3, 256#  2 " 3  3, 256  [33, 256]1, S2  Conv4.x  [33, 512]1, S2  [33, 512]1, S2  [33, 64]1, S2  [33, 128]1, S2  3  3, 64#  1 " 3  3, 64 3  3, 128#  2 " 3  3, 128 3  3, 256#  4 " 3  3, 256 3  3, 512#  1 " 3  3, 512  [33, 256]1, S2  [33, 512]1, S2  [33, 64]1, S2  [33, 128]1, S2  3  3, 64#  2 " 3  3, 64 3  3, 128#  4 " 3  3, 128 3  3, 256#  8 " 3  3, 256 3  3, 512#  2 " 3  3, 512  [33, 512]1, S2  [33, 256]1, S2  [33, 64]1, S2  [33, 128]1, S2  3  3, 64#  3 " 3  3, 64 3  3, 128#  8 " 3  3, 128 3  3, 256#  16 " 3  3, 256 3  3, 512#  3 " 3  3, 512  [33, 256]1, S2  [33, 512]1, S2  FC1  512  512  512  512  512  Table 2: Our CNN architectures with different convolutional layers.|
|||We use residual units [4] in our CNN architecture.|
|||For fairness, all compared methods use the same CNN architecture (including residual units) as SphereFace.|
|||Besides visual comparison, we also perform face recognition on LFW and YTF to evaluate the effect of m. For fair comparison, we use 64-layer CNN (Table 2) for all losses.|
|||Effect of CNN architectures.|
|||Specific CNN architectures can be found in Table 2.|
|||For fair comparison, all loss functions (including ours) we implemented use 64-layer CNN architecture in Table 2.  loss functions share the same 64-layer CNN architecture.|
|||For fair comparison, all loss functions (including ours) we implemented use the same deep CNN architecture.|
|||For fair comparison, we also implement the softmax loss, contrastive loss, center loss, triplet loss, L-Softmax loss [16] and train them with the same 64-layer CNN architecture as A-Softmax loss.|
|||Moreover, in contrast to their sophisticated network design, we only employ typical CNN architecture supervised by A-Softamx to achieve such excellent performance.|
|||Compared to these loss functions trained with the same CNN architecture and dataset, SphereFace also shows significant and consistent improvements.|
||12 instances in total. (in cvpr2017)|
|335|Towards Accurate Multi-Person Pose Estimation in the Wild|For this we use the FasterRCNN method [37] on top of a ResNet-101 CNN [22], as implemented by [23].|
|||These results have been attained with single-scale evaluation and using a single CNN for box detection and a single CNN for pose estimation.|
|||Multiscale evaluation and CNN model ensembling might give additional gains.|
|||[27] trained a CNN on image patches, which was applied convolutionally at inference time to infer heatmaps (or activity-maps) for each keypoint independently.|
|||[10] added image-dependent priors to improve CNN performance.|
|||We have also experimented with an Inception-ResNet CNN backbone [42], which is an architecture integrating Inception layers [43] with residual connections [22], which performs slightly better at the cost of increased computation.|
|||The CNN backbone has been pre-trained for image classification on Imagenet.|
|||Heatmap and Offset Prediction with CNN We apply a ResNet with 101 layers [22] on the cropped image in a fully convolutional fashion to produce heatmaps (one channel per keypoint) and offsets (two channels per keypoint for the xand ydirections) for a total of 3  K output channels, where K = 17 is the number of keypoints.|
|||Training a CNN to produce directly the highly localized activations fk (ideally delta functions) on a fine resolution spatial grid is hard.|
|||We have experimented with alternative CNN setups for our pose estimation module.|
|||We have explored CNN network backbones based on either the faster ResNet-50 or the more accurate ResNet-101, while keeping ResNet-101 as CNN backbone for the Faster-RCNN box detection module.|
|||We report in Table 4 COCO test-dev results for the four CNN backbone/ crop size combinations, using COCO+int for pose estimator training.|
||12 instances in total. (in cvpr2017)|
|336|Xiong_Spatiotemporal_Modeling_for_ICCV_2017_paper|Following this work, Y. Zhang [36] proposed a multi-column CNN architecture et al.|
|||The multi-column CNN architecture also uses a different method for computing the crowd density.|
|||In the spirit of the gradient boosting approach, CNNs are added one at a time such that every new CNN is trained to estimate the residual error of the earlier prediction.|
|||After the first CNN has been trained, the second CNN is trained on the difference between the current estimate and the learning target.|
|||The third CNN is then added and the process continues.|
|||[19] proposed a framework called Hydra CNN which uses a pyramid of image patches extracted at multiple scales to perform the final density prediction.|
|||If we remove the connections between ConvLSTM cells, we can regard each ConvLSTM cell as a CNN model with gates.|
|||Method  MAE MSE  Head detection [22] Density map + MESA [15] Multi-source features [11] Crowd CNN [32] Multi-column CNN [36] ConvLSTM-nt  Shang et al.|
|||Results of different methods on the UCSD dataset Method MAE MSE  Gaussian process regression [4] Ridge regression [7] Cumulative attribute regression [6] Density map + MESA [15] Count forest [20] Crowd CNN [32] Multi-column CNN [36] Hydra CNN [19] CNN boosting [29] ConvLSTM-nt ConvLSTM Bidirectional ConvLSTM  2.24 2.25 2.07 1.70 1.60 1.60 1.07 1.65 1.10 1.73 1.30 1.13  7.97 7.82 6.90 4.40 3.31 1.35 3.52 1.79 1.43  glass surface reflections.|
|||column CNN to generate the density map according to the perspective map with the relation  = 0.2M (x), where M (x) denotes the number of pixels in the image representing one square meter at location x.|
|||Results of different methods on the WorldExpo dataset  Method  Scene 1  Scene 2  Scene 3  Scene 4  Scene 5 Average  LBP features + ridge regression Deep CNN [32] Multi-column CNN [36] ConvLSTM-nt ConvLSTM Bidirectional ConvLSTM  13.6 9.8 3.4 8.6 7.1 6.8  59.8 14.1 20.6 16.9 15.2 14.5  37.1 14.3 12.9 14.6 15.2 14.9  21.8 22.2 13.0 15.4 13.9 13.5  23.4 3.7 8.1 4.0 3.5 3.1  31.0 12.9 11.6 11.9 10.9 10.6  ing setting using the UCSD and Mall datasets, which were both captured using stationary cameras.|
|||Learning to count with CNN boost ing.|
||12 instances in total. (in iccv2017)|
|337|cvpr18-CNN Based Learning Using Reflection and Retinex Models for Intrinsic Image Decomposition|A standard CNN architecture is chosen to exploit the dichromatic reflection model [31] as a standard reflection model to steer the training process by introducing a physics-based loss function called the image formation loss, which takes into account the reconstructed image of the predicted reflectance and shading images.|
|||The goal is to analyze the contribution of exploiting the image formation process as a constraining factor in a standard CNN architecture for intrinsic image decomposition.|
|||Finally, we propose a new CNN architecture, RetiNet, which is a Retinex-inspired scheme that exploits image gradients in combination with the image formation loss.|
|||(8)  Finally, for a global, non-canonical light source we obtain:  I(~x) = R(~x)  S(~x)  E + H(~x)  E.  (9)  In the next section, the reflection model is considered to introduce different image formation losses within an encoderdecoder CNN model for intrinsic image decomposition.|
|||IntrinsicNet: CNN driven by Reflection Models  In this section, a physics-based deep learning network, IntrinsicNet, is proposed.|
|||We use a standard CNN architecture to constrain the training process by introducing a physics-based loss.|
|||The reason of using a standard CNN architecture is to analyze whether it is beneficial to constrain  6676  Figure 1: IntrinsicNet model architecture with one shared encoder and two separate decoders: one for shading and one for reflectance prediction.|
|||the CNN by the reflection model.|
|||Therefore, an end-toend trainable encoder-decoder CNN is considered.|
|||To augment both color reproduction and edge sharpness, in the next section, a two-stage Retinex-inspired CNN architecture is described which uses intrinsic gradients (for edge sharpness) and the image formation loss (for color reproduction).|
|||RetiNet  In this section, we exploit how a well-established, traditional approach such as Retinex can be used to steer the design of a CNN architecture for intrinsic image decomposition.|
|||It shows the positive contribution of exploiting the image formation process as a constraining factor in a standard CNN approach for intrinsic image decomposition.|
||12 instances in total. (in cvpr2018)|
|338|cvpr18-Learning for Disparity Estimation Through Feature Constancy|Existing CNN-based methods only adopt CNN to solve parts of the four steps, or use different networks to deal with different steps, making them difficult to obtain the overall optimal solution.|
|||Due to the powerful representative capability of deep convolution neural network (CNN) for various vision tasks [14, 31, 15], CNN has been employed to improve stereo matching performance and outperforms traditional methods significantly [28, 32, 12, 17, 16].|
|||Zbontar and LeCun [32] first introduced CNN to calculate the matching cost to measure the similarity of two pixels of two images.|
|||In contrast, CNN can learn more robust and discriminative features from images, and produces improved stereo matching cost.|
|||Alternatively, the matching cost calculation, matching cost aggregation and disparity calculation steps can be seamlessly integrated into a CNN to directly estimate the disparity from stereo images [17, 12].|
|||These methods can achieve higher accuracy and computational efficiency than the methods that use CNN for matching cost calculation only.|
|||Related works  Over the last few years, CNN has been introduced to solve various problems in stereo matching.|
|||CNN for Matching Cost Learning  In this category, CNN is used to learn the matching cost.|
|||Zbontar and LeCun [32] trained a CNN to compute the matching cost between two image patches (e.g., 9  9), which is followed by several post-processing steps, including cross-based cost aggregation, semi-global matching, left-right constancy check, sub-pixel enhancement, median filtering and bilateral filtering.|
|||CNN for Disparity Regression  In this category, CNN is carefully designed to directly estimate the disparity, which enables end-to-end training.|
|||[28] used their highway network for matching cost calculation and an additional global disparity CNN to replace the winner-takes-all strategy used in conventional matching cost aggregation and disparity calculation steps.|
|||Note that, since the four steps for stereo matching are integrated into a single CNN network, end-to-end training is ensured.|
||12 instances in total. (in cvpr2018)|
|339|Re-Sign_ Re-Aligned End-To-End Sequence Modelling With Deep Recurrent CNN-HMMs|We initialise the algorithm with a provided frame labelling or a frame-state-alignment generated by standard CNN training.|
|||We therefore aim at joining a powerful and deep CNN with several bidirectional LSTM layers [17, 27].|
|||In order to train the full network end-to-end, the CNN architecture of choice should have a low memory footprint, while still being very deep.|
|||After comparing different CNN architectures [34, 24, 37], we opted for the 22 layer deep GoogleNet [37] architecture, which we initially pre-train on the 1.4M images from the ILSVRC-2012 [30].|
|||We use stochastic gradient descent with an initial learning rate 0 = 0.001 for CNN-LSTM architectures and 0 = 0.01 for CNN networks.|
|||All images are directly feed as inputs to a deep CNN architecture.|
|||Both hand and full frame image inputs to the CNN are of the size 256x256 pixels.|
|||This is in contrast to the full frame experiment, in which the CNN is not only able to distinguish the hands on its own, but also recognise other modalities which leads to better results.|
|||The impact of this can be seen in Table 4 where the CNN-LSTM architecture is initialised with weights of a CNN (GoogLeNet) only training on ImageNet.|
|||The results CNN and CNN-2BLSTM without any re-alignments constitute performances with legacy-style GMM-HMM alignments.|
|||CNN-2BLSTM refers to a CNN jointly trained with 2 layers of bidirectional LSTMs.|
|||Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled.|
||12 instances in total. (in cvpr2017)|
|340|Fine-Grained Image Classification via Combining Vision and Language|[1] show that the classification accuracy declines when the number of parts increases from 8 to 15 in the experiments of their Part-stacked CNN method.|
|||Object and Part annotations are directly used in training phase to learn a strongly supervised deformable part-based model [16] or directly used to fine-tune the pretrained CNN model [17].|
|||A mid-level temporal CNN hidden layer is at the bottom of CNN-RNN model, and a recurrent network is stacked on it.|
|||the CNN model.|
|||3.2.1 Vision Stream  A natural candidate for the visual classification function f is a CNN model, which is consist of a hierarchy of convolutional and fully connected layers.|
|||Therefore, we use a CNN model pre-trained on the ImageNet dataset [33] as the base model in our experiments.|
|||And then, we fine-tune the pre-trained CNN model on the fine-grained dataset.|
|||It is important to note that the model used in our proposed method can be replaced with any CNN model.|
|||Then the pre-trained CNN model on ImageNet dataset is used as a filter net for selecting the patches relevant to the object.|
|||And for text encoder, the CNN input size (sequence length) is set to 201 for character-level model.|
|||Partstacked cnn for fine-grained visual categorization.|
|||Bilinear cnn models for fine-grained visual recognition.|
||12 instances in total. (in cvpr2017)|
|341|cvpr18-Aperture Supervision for Monocular Depth Estimation|Our light field model uses a CNN to predict a depth map that is then used to warp the input 2D all-in-focus image into an estimate of the 4D light field inside the camera, which is then focused and integrated to render a shallow depth-of-field image of the scene.|
|||Our compositional model uses a CNN to predict a probabilistic depth map (a probability distribution over a fixed set of depths for each pixel) and renders a shallow depth-offield image as a composition of the input all-in-focus image blurred with a representative kernel for each discrete depth, blended using the probabilistic depth map as weights.|
|||Therefore, we use a CNN g() with parameters e that takes the single input depth map Z(x; I) and expands it into a depth map  6395  D(x, u) for each view in the light field:  D(x, u) = ge (Z(x; I))  (1)  where x are spatial coordinates of the light field on the image plane and u are angular coordinates of the light field on the aperture plane (equivalent to the coordinates of the center of projection of each view in the light field).|
|||When using the light field model, CNN fl () is trained to predict a depth map from the input all-in-focus image, CNN ge () expands this depth map into a depth map for each view, the camera light field is rendered by warping the input image into each view using the expanded depth maps, and finally all views in the light field are integrated to render a shallow depth-of-field image.|
|||When using the compositional model, the input all-in-focus image is convolved with a discrete set of disk blur kernels, and CNN fc () predicts a probabilistic depth map that is used to blend these blurred images into a rendered shallow depth-of-field image.|
|||Monocular Depth Estimation  We integrate our differentiable aperture rendering functions into CNN pipelines to train functions for monocular depth estimation using aperture effects as supervision.|
|||The input to the full network is a single RGB all-in-focus image, and we train a CNN to predict the scene depths that minimize the difference between the ground-truth shallow depth-of-field images and those rendered by our differentiable aperture rendering functions.|
|||Please refer to our supplementary materials for detailed descriptions of the CNN architectures.|
|||Using Light Field Aperture Rendering  To incorporate our light field aperture rendering function into a pipeline for learning monocular depth estimation, we use a CNN f () with parameters l and the bilateral solver [5] to predict a depth map Z(x; I) from the input all-in-focus image I(x):  Z(x; I) = BilateralSolver(fl (I(x))).|
|||Using Compositional Aperture Rendering  To use our compositional aperture rendering function in a pipeline for learning monocular depth estimation, we have the depth estimation CNN fc () output values over n discrete depth planes instead of just a single depth map:  P (x, d; I) = fc (I(x)).|
|||Unsupervised CNN for single view depth estimation: geometry to the rescue.|
||11 instances in total. (in cvpr2018)|
|342|Caicedo_Active_Object_Localization_ICCV_2015_paper|[12, 25], which combines object proposals and CNN fea Sequence of attended regions to localize the object   States   Actions   Steps   1   ...  ...   ...  ...      +1   1      Figure 1.|
|||The feature vector o is extracted from the current region using a pre-trained CNN following the architecture of Zeiler and Fergus [36].|
|||Layer 6   Layer 1  Layer 2  Output   Input region   Pre-trained  CNN   Deep QNetwork   Figure 3.|
|||The input region is first warped to 224  224 pixels and processed by a pretrained CNN with 5 convolutional layers and 1 fully connected layer.|
|||The output of the CNN is concatenated with the action history vector to complete the state representation.|
|||Using a pre-trained CNN has two advantages: First, learning the Q function is faster because we need to update the parameters of the Q-Network only, while using the deep CNN just as a feed-forward feature extractor.|
|||The only baseline method that does not use CNN features is the DPM system [11].|
|||[38] adapts the regionlets framework with CNN features too.|
|||Our system is significantly better at localizing objects than other recent systems that predict bounding boxes from CNN features without object proposals.|
|||This time is divided between feature extraction with the CNN (4.5 ms) and decision making with the Q-network (3.2ms), imposing an overhead of  about 70% more computing power per region in our prototype, compared to systems that only extract features.|
|||Part of our future work includes training the system end-to-end instead of using a pre-trained CNN, and using deeper CNN architectures for improving the accuracy of predictions.|
||11 instances in total. (in iccv2015)|
|343|Li_VLAD3_Encoding_Dynamics_CVPR_2016_paper|This includes approaches based on hand-crafted descriptors and a number of recent CNN models, which use a few frames as inputs and a purely feedforward structure [11, 26, 31].|
|||ants [6, 9], or a combination of a short-term representation, such as CNN activations, and a pooling operator along video trajectories [33].|
|||At the medium-range level, the CNN features extracted by the short-term level are processed by a linear dynamic system (LDS).|
|||We derive a VLAD descriptor for the LDS likelihood of CNN responses and use it as the final, long-range level representation of the video hierarchy.|
|||This leads to the proposed combination of short-term CNN features, medium-range LDS models, and global VLAD descriptor.|
|||by stacking an LSTM upon a CNN [4] so as to learn the high-level temporal structure of low-level visual features, or by using an LSTM to model the dynamics of CNN activations [19].|
|||Similarly to the CNN approaches, these models have so far only been learned from sequences of 16 [4] to 30 [19] frames.|
|||Instead, we propose to apply the dynamic model (LDS) directly to CNN features, which are power 1953  ful in discriminating short-term dynamics.|
|||Overall, we rely on the CNN features for discrimination and on the LDS to capture the medium-range dynamics of these features.|
|||Given the sequence of CNN vectors extracted from the video sequence, we defined a temporal sliding window of length  and stride three.|
|||Following [2, 26, 21], we also tried to late-fuse the representations above with a holistic representation obtained by average pooling the CNN fea 1956  method  mAP(%)  Acc(%)  UCF101  Olympic mAP(%)  THUMOS15  mAP(%)  T3  CTR   HMM-FV  VLAD3   T3  CTR   HMM-FV  VLAD3   84.35 84.28 85.14 89.31  89.11 88.44 89.56 90.47  82.96 81.59 80.41 84.08  84.04 83.00 84.29 84.65  80.14 80.89 88.15 90.78  + holistic  86.53 86.19 88.41 90.81  56.50 61.52 72.30 76.84  72.30 72.03 75.49 78.15  Table 1: Activity recognition performance on UCF101, Olympic Sports, and THUMOS15.|
||11 instances in total. (in cvpr2016)|
|344|Mohammed_Fathy_Hierarchical_Metric_Learning_ECCV_2018_paper|Finally, we exploit the intermediate activation maps generated within the CNN itself as a proxy for image pyramids traditionally used to enable coarse-to-fine matching [17].|
|||While early work [59, 39, 36] leveraged intermediate activation maps of a CNN trained with an arbitrary loss for keypoint matching, most recent methods rely on an explicit metric loss [63, 22, 61, 16, 65, 60, 66] to learn descriptors.|
|||The hidden assumption behind using contrastive or triplet loss at the final layer of a CNN is that this explicit loss will cause the relevant features to emerge at the top of the feature hierarchy.|
|||But it has also been observed that early layers of the CNN are the ones that learn local geometric features [64].|
|||While this takes care of the issue that a deep layer may have an unnecessarily large receptive field when learning on the basis of contrastive loss, we argue that it is more elegant for the CNN to look at the image at multiple scales, rather than separately process multiple scales.|
|||Descriptors for matching in 3D voxel grid representations are learned by 3DMatch [65], employing a Siamese 3D CNN setup on a 30x30x30 cm3 voxel grid with a contrastive loss.|
|||Recent works [31, 33, 34] suggest that providing explicit supervision to intermediate layers of a CNN can yield higher performance on unseen data, by regularizing the training process.|
|||Recently, many techniques have been developed for fusing features from different layers within a CNN and producing output at high resolution, e.g.|
|||Inspired by [17] for image alignment, we argue that the growing receptive field in deep CNN layers [64] provides a natural way to parse an image at multiple scales.|
|||Such coarse-to-fine matching has been well-known in computer vision [41], however recent work highlights how employing CNN feature hierarchies for the task (at least in the context of image alignment [17]) is more robust.|
|||5 Conclusion and Future Work  We draw inspiration from recent studies [64, 68] as well as conventional intuitions about CNN architectures to enhance learned representations for dense 2D and 3D geometric matching.|
||11 instances in total. (in eccv2018)|
|345|Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper|First, we formulate the recurrent attention residual (RAR) module to combine the contexts in two adjacent CNN layers and learn an attention map to select a residual and then refine the context features.|
|||Second, we develop a bidirectional feature pyramid network (BFPN) to aggregate shadow contexts spanned across different CNN layers by deploying two series of RAR modules in the network to iteratively combine and refine context features: one series to refine context features from deep to shallow layers, and another series from shallow to deep layers.|
|||A key factor for the successes is that CNN is able to learn the global spatial contexts in shadow images, as demonstrated in very recent works [57].|
|||In detail, we have the following technical contributions in this work:   First, we develop the recurrent attention residual module, or RAR module for short, to combine and process spatial contexts in two adjacent CNN layers.|
|||Inside the BFPN, we first apply the convolutional neural network (CNN) to generate a set of feature maps (i.e., spatial contexts) in different resolutions, and then use two series of RAR modules to iteratively integrate spatial contexts over the CNN layers: one series of RAR modules from deep to shallow layers and another series from shallow to deep layers.|
|||They trained one CNN to detect shadow regions and another CNN to detect shadow boundaries, and then fed the prediction results to a conditional random field (CRF) for classifying image pixels as shadows/non-shadows.|
|||Later, a stacked CNN [4] was presented to detect shadows by considering the global prediction of an image and the shadow predictions of image patches.|
|||They first trained a fully convolutional network to obtain an image-level shadow prior, which was combined with local image patches to train a patch-based CNN for the final shadow map prediction.|
|||Recently, a fast deep shadow detection network [24] was introduced by obtaining a shadow prior map produced from hand-crafted features and then applying a patch level CNN to compute the improved shadow probability map of the input image.|
|||Compared to these methods, we suggest to develop a network by fully leveraging the global and local contexts in different layers of the CNN to detect shadows.|
|||Then, we develop RAR modules to progressively refine features at each layer of the CNN by taking two adjacent feature maps as inputs to learn an attention map and to select a residual for the refinement of context features.|
||11 instances in total. (in eccv2018)|
|346|Cheng_An_Exploration_of_ICCV_2015_paper|We use the LeNet network [24] as our basic CNN model, which is known to work well on digit classification tasks.|
|||We use a standard CNN network  AlexNet [23] as the building block.|
|||Circulant CNN 1 replaces the fully connected layers of AlexNet with circulant layers.|
|||Circulant CNN 2 uses fatter circulant layers compared to Circulant CNN 1: d of Circulant CNN 2 is set to be 214.|
||| The performance of Randomized Circulant CNN 14 is very competitive to the Randomized AlexNet.|
|||The performance of Circulant CNN 1 is very competitive to AlexNet, yet with fraction of the space cost.|
||| By tweaking the structure to include more parameters, Circulant CNN 2 further drops the error rate to 17.8%, yet it takes only a marginally larger amount of space compared to Circulant CNN 1, an 18x space saving compared to AlexNet.|
||| With the same memory cost, the Reduced AlexNet per forms much worse than Circulant CNN 1.|
|||In addition, one interesting finding is that dropout, which is widely used in training CNN, does not improve  4This is Circulant CNN 1 with randomized circulant projections.|
|||2862  Method  Top-5 Error Top-1 Error Memory (MB)  Randomized AlexNet  Randomized Circulant CNN 1  AlexNet  Circulant CNN 1 Circulant CNN 2 Reduced-AlexNet  33.5% 35.2% 17.1 % 19.4 % 17.8 % 37.2 %  61.7% 62.8% 42.8% 44.1% 43.2% 65.3%  233 12.5 233 12.5 12.7 12.7  Table 4.|
|||Test error when training with reduced dataset sizes of circulant CNN and conventional CNN.|
||11 instances in total. (in iccv2015)|
|347|Visual-Inertial-Semantic Scene Representation for 3D Object Detection|For this we use the same CNN in a bottom-up mode, where putative detection (high-likelihood regions) are used to initialize object hypotheses (or, rather, regions with no putative detections are assumed free of objects), and several heuristics are put in place for genetic phenomena (birth, death and merging of objects, Sect.|
|||Their subsequent work [70] trains a CNN to classify 3DVPs.|
|||3.3 we will show how a discriminatively-trained CNN can be leveraged to this end.|
|||, lK}, we leverage on a CNN trained discriminatively to classify a given image region bj into one of K + 1 classes, including the background class.|
|||The architecture has a soft-max layer preceded by K + 1 nodes, one per class, and is trained using the cross-entropy loss, providing a normalized score CNN (l|It |bj )[k] for each class and image bounding box bj .|
|||Since inertials ut are directly measured, up to a Gaussian noise, we have:  p(yt|zj, gt, x)  CNN (l|It |(gt sj )  )[k]N ( u; Q)  (7)  where  u are the inertial biases and Q the noise covariance; here the object attributes zj are the labels lj = lk and geometry sj .|
|||These provide samples from p(z|yt) in the right-hand side of (4), that are scored with the CNN to update the posterior.|
|||Visual Odometry and Baseline 2D CNN We use robust SLAM implemented from [63] to acquire sparse point clouds and camera pose x, gt at each t. This occurs in 10  20ms per VGA frame.|
|||The state predicts the projection of (each of the K instances of) each object onto the image plane, where the CNN evaluates the likelihood.|
|||Nevertheless, we use this for scoring the use of the likelihood produced by the CNN for each predicted class.|
|||Initialization Putative 2D CNN detections not associated to any object are used as (bottom-up) proposals for initialization.|
||11 instances in total. (in cvpr2017)|
|348|Jacobsen_Structured_Receptive_Fields_CVPR_2016_paper|We aim to make learning more effective for smaller sets by restricting CNNs param Figure 1: A subset of filters of the first structured receptive field CNN layer as trained on 100-class ILSVRC2012 and the Gaussian derivative basis they are learned from.|
|||[39] and Figure 2, it becomes evident that the filters as learned in a CNN are locally coherent and as a consequence can be decomposed into a smooth compact filter basis [12].|
|||The pooling layers in a CNN effectively reduce resolution of input feature maps.|
|||The deep structure in a CNN models the image on several discrete levels of resolution simultaneously, precisely in line with Scale-space theory.|
|||In cases where limited training data are available, CNN training quickly overfits regardless and the learned representations do not generalize well.|
|||Whereas normal CNNs treat images and their filters as pixel values, we aim for a CNN that treats images as functions in Scale-space.|
|||Thus, a convolution with a basis of weighted Gaussian derivatives receptive fields is the functional equivalent to pixel values in a standard CNN operating on a scaled infinitely differentiable version of the image.|
|||Thus, an RFNN is a general CNN when a complete polynomial up to infinite order is considered.|
|||Where a classical CNN learns pixel values of the convolutional kernel, a RFNN learns Gaussian derivative basis function weights that combine to a  Algorithm 1 RFNN Learning updating the parameters l ij between input map indexed by i and output map indexed by j of layer l in the Mini-batch Gradient Decent framework.|
|||Both are shown, to illustrate that good performance of the RFNN is not due to the CNN architecture, but due to RFNN decomposition.|
|||The CNN and Scattering results on the task are taken from [2, 26].|
||11 instances in total. (in cvpr2016)|
|349|Wang_Joint_Learning_of_CVPR_2016_paper|By using the deep CNN architecture, we propose a framework to jointly learn SIR and CIR for improving matching performance with the least increase of the computational cost.|
|||To save the computational cost, we can store the CNN feature maps from the shared subnetwork of the gallery images in advance, and reduce the depth of the CIR sub-network to include only one convolutional layer and one fully-connected layer.|
|||Related Work  The existing person re-identification methods can be divided into two categories depending on whether they use the hand-crafted or deep CNN features.|
|||For the methods based on deep CNN features, feature representation and classifier can be jointly optimized for learning either SIR or CIR features.|
|||proposed a FaceNet model for face verification [31], which adopts a deep CNN to learn the Euclidean embedding per image by using the triplet comparison loss.|
|||For each image pair, this model first stitchs its two images horizontally to form a holistic image, then feeds these images to a CNN to learn their representations.|
|||Besides, we present two deep CNN architectures for joint SIR and CIR learning based on pairwise and triplet comparison objectives, respectively, and the matching scores of these two networks can be combined to improve the matching accuracy.|
|||For each of the probe and gallery images, its CNN feature maps (yellow part) from the shared sub-network and the SIR feature are computed once.|
|||When we extract the CIR of (xi, xj), the CIR sub-network is feeded by the CNN feature maps of xi and xj from the shared subnetwork.|
|||We pretrain the deep network using CUHK03 dataset for 100,000 iterations, and fine-tune the CNN using the training set of CUHK01 for 50,000 iterations.|
|||Following the testing protocol in [1], we pretrain the CNN using CUHK03 and CUHK01 datasets, and fine-tune the network on the training set of VIPeR.|
||11 instances in total. (in cvpr2016)|
|350|Image Splicing Detection via Camera Response Function Analysis|We first train a CNN model on edge patches.|
|||Thus the transition region around the edge potentially contains a cue for splicing detection in a CNN framework.|
|||4.2.1 CNN on IGHs  Our final approach marries our IGH feature with the power of CNNs.|
|||IGH Feature Extraction  Given an image I, we extract edge patches of size n  n pixels (n  [20, 200]), which are used for direct CNN classification and IGH extraction.|
|||Experiments  We test our CNN and SVM methods on SpLogo, CUISDE datasets and a combination of the two.|
|||We use a histogram intersection kernel [28] for our SVM, LeNet [23] for CNN with a 1e6 base learning rate, 1e6 maximum number of iterations.|
|||Among the classification methods employing our hand-crafted feature, CNN classifiers obtain better results than SVM, and adding the absolute value of intensity and gradient to IGH increases classification accuracy.|
|||SVM and CNN results on SpLogo dataset.|
|||Both SVM classification and CNNs applied to the patches give false negatives on forged sharp edges, which are reduced significantly by applying the CNN to our histogram feature.|
|||Our CNN model trained on IGH performs the best, reducing the false alarm rate compare to the other two models.|
|||Our IGH feature captures these statistics, and provides a way to eliminate nuisance variables such as edge orientation and step height so that CNN methods can be applied despite a lack of a large training set.|
||11 instances in total. (in cvpr2017)|
|351|Opitz_BIER_-_Boosting_ICCV_2017_paper|BIER does not introduce any additional parameters into a CNN and has only negligible additional cost during training time and runtime.|
|||[19, 54] use CNN features in an offline boosting framework.|
|||In contrast to their work, which trains several copies of full CNN models, our method trains a single CNN with an online boosting method.|
|||To avoid this additional computational cost, we divide the embedding layer of our CNN into several non-overlapping groups, as illustrated in Figure 3.|
|||All our weak learners share the same underlying feature representation, which is a pre-trained ImageNet CNN in all our experiments.|
|||We divide the embedding of a metric CNN into several weak learners and cast training them as online gradient boosting problem.|
|||Training a metric CNN this way encourages successive learners to focus on different samples than previous learners and consequently reduces correlation between learners and their feature representation.|
|||For each image in the test set, we compute the feature vectors from our CNN and then retrieve the K most similar images from the remaining test set.|
|||We train a CNN with embedding sizes 384, 512, 1024 with our method and with a regular CNN.|
|||Our extensive experiments show that BIER significantly reduces correlation on the last hidden layer of a CNN and works with several different loss functions.|
|||Learning to Count with CNN Boost ing.|
||11 instances in total. (in iccv2017)|
|352|Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper|In [4] a CNN architecture and MSE loss are used for directly learning low to high resolution mapping.|
|||The subsequent works developed deeper and more complex CNN architectures (e.g., [10, 18, 16]).|
|||Usually, MSE is used  13277  as a target loss function and the proposed CNN architectures consist of 3 to 15 convolutional layers [14, 2, 6] or are bichannel CNNs [17].|
|||Among other solutions are a bi-channel CNN [29], a 17-layer CNN [26] and a recurrent CNN [24] that was reapplied several times to the produced results.|
|||Considerably better performance on this task was obtained using generative adversarial networks [8] or a 16-layer CNN with a multinomial crossentropy loss function [27].|
|||Training CNN on the aligned high-resolution images is infeasible, thus patches of size 100100px were extracted from these photos.|
|||A deep residual CNN FW parameterized by weights W is used to learn the underlying translation function.|
|||The discriminator CNN is applied to grayscale images so that it is targeted specifically on texture processing.|
|||The discriminator CNN consists of five convolutional layers each followed by a LeakyReLU nonlinearity and batch normalization.|
|||The method relies on a standard 3-layer CNN and MSE loss function and maps from low resolution / corrupted image to the restored image.|
|||It can be seen that in all cases both pictures taken with a DSLR as well as pictures enhanced by the proposed CNN are picked much more often than the original ones taken with the mobile devices.|
||11 instances in total. (in iccv2017)|
|353|Karpathy_Large-scale_Video_Classification_2014_CVPR_paper|To obtain sufficient amount of data needed to train our CNN architectures, we collected a new Sports-1M dataset, which consists of 1 million YouTube videos belonging to a taxonomy of 487 classes of sports.|
|||From a modeling perspective, we are interested in answering the following questions: what temporal connectivity pattern in a CNN architecture is best at taking advantage of local motion information present in the video?|
|||How does the additional motion information influence the predictions of a CNN and how much does it improve performance overall?|
|||We examine these questions empirically by evaluating multiple CNN architectures that each take a different approach to combining information across the time domain.|
|||[1] and [10] extend an image CNN to video domains by treating space and time as equivalent dimensions of the input and perform convolutions in both time and space.|
|||We first describe a baseline single-frame CNN and then discuss its extensions in time according to different types of fusion.|
|||Every clip is propagated through the network 4 times (with different crops and flips)  Figure 2: Multiresolution CNN architecture.|
|||Compared to the wide gap relative to the feature-based baseline, the variation among different CNN architectures turns out to be surprisingly insignificant.|
|||We conduct further exper Table 2: Classes for which a (motion-aware) Slow Fusion CNN performs better than the single-frame CNN (left) and vice versa (right), as measured by difference in per-class average precision.|
|||We treat the CNN as a fixed feature extractor and train a classifier on the last 4096-dimensional layer, with dropout regularization.|
|||An alternative theory is that more careful treatment of camera motion may be necessary (for example by extracting features in the local coordinate system of a tracked point, as seen in [25]), but this requires significant changes to a CNN architecture that we leave for future work.|
||11 instances in total. (in cvpr2014)|
|354|Li_Maximum-Margin_Structured_Learning_ICCV_2015_paper|[14] trains a CNN as a sliding-window detector for each body part, and the resulting body-joint detection maps are smoothed using a learned pairwise relationship between joints.|
|||[5] uses a deep CNN to predict the presence of joints and the pairwise relationships between joints, and the CNN output is then used as the input into a pictorial structure model for 2D pose estimation.|
|||[31] trains a cascade CNN to predict the 2D coordinates of joints in the image, where the CNN inputs are the image patches centered at the coordinates predicted from the previous stage.|
|||[19] use a multi-task framework to train a CNN to directly predict a 2D human pose, where auxiliary tasks consisting of body-part detection guide the feature learning.|
|||[22] apply structured learning and CNN for face detection and face pose estimation.|
|||The CNN was trained to map the face image to a manually-designed face pose space.|
|||For the first sub-network, a CNN extracts high-level image features from the raw image.|
|||Note that gradients from LP only affect the CNN and highlevel image features (FC1-FC3), and have no direct effect on the pose embedding network or image embedding layer (FC4).|
|||We also compare against multi-task deep networks (DconvMP-HML) [18], which trains a CNN using the pose prediction cost (Eq.|
|||Overall, the performance of the auxiliary pose prediction task is similar to that of [18], which also uses a CNN for 3D pose estimation, but inferior to the poses obtained using the score function.|
|||The network consists of a CNN for image feature extraction, and two separate sub-networks  2The action Direction is not included due to video corruption.|
||11 instances in total. (in iccv2015)|
|355|Zhang_Efficient_Training_of_CVPR_2016_paper|Our method can take in any form of vector input, such as raw image intensities, traditional features like GIST [33], or even CNN features [26].|
|||[47] proposed learning image representations for supervised hashing by approximating the data affinity matrix with CNN features.|
|||Most of the works learn hash functions on top of a deep CNN architecture.|
|||When CNN features are used, our method can be viewed as fine-tuning these networks for supervised hashing.|
|||In order to compare VDSH fairly with other deep hashing methods which learn the CNN features jointly with the hash codes, we utilize the pre-trained vgg-f model [6] to extract CNN features on MNIST and CIFAR-10 directly without any fine-tuning.|
|||We then apply VDSH, SDH, CCAITQ and FastHash on these CNN features to generate hash codes.|
|||Compared to fully optimized deep hashing methods such as DRSCH [48], this two-stage scheme has not  1492  )  %  (    i  i  n o s c e r P  100  80   60   40   20   0    100  95   )  %  (    16 32  64  96  128  Code length  (a)  MNIST: CNN feature  i  i  n o s c e r P  90   85   80   16 32  64  96  128  Code length  (e)  MNIST: raw pixel intensity  )  %  (   l l  a c e R  100  80   60   40   20   0    )  %  (    P A M  100  99   98   97   96   95   MNIST: raw pixel intensity  CIFAR-10: GIST feature  80  60  )  %  (    i  i  n o s c e r P  16 32  64  96  128  Code length  (b)  MNIST: CNN feature  )  %  (    i  i  n o s c e r P  16 32  64  96  128  Code length  (c)  CIFAR-10: CNN feature  40  20  0   90  80  70  60  50  40  16 32  64  96  128  16 32  64  96  128  Code length  (f)  Code length  (g)  VDSH SDH FastHash CCA-ITQ MLH BRE KSH  VDSH SDH FastHash CCA-ITQ DSRH DSCH DRSCH  )  %  (    P A M  )  %  (    P A M  80  60  40  20  0   90  80  70  60  50  CIFAR-10: GIST feature  16 32  64  96  128  Code length  (d)  CIFAR-10: CNN feature  16 32  64  96  128  Code length  (h)  Figure 6.|
|||(a) VDSH: GIST feature  (b) SDH: GIST feature  (c) VDSH: CNN feature  (d) SDH: CNN feature  Figure 7. t-SNE visualization of the 64-bit binary hash codes of all test images in CIFAR-10.|
|||The pre-learned CNN is agnostic to the hash codes that are intended to be generated.|
|||6(eh) with the same experimental settings as in [48] and [54] for the CNN features.|
|||Precision-recall comparison on CIFAR-10 by varying Hamming radius (denoted by R) using (a-b) GIST features and (c-d) CNN features.|
||11 instances in total. (in cvpr2016)|
|356|Yingjie_Yao_Joint_Representation_and_ECCV_2018_paper|[30] adopt the original CF form due to its model adaptation has the closed-form solution and can be interpreted as a differentiable CNN layer.|
|||Experiments show that combining CNN with advanced CF tracker can benefit tracking performance, and the joint learning of deep representation and truncated inference also improves tracking accuracy.|
|||Instead of combining CNN with the standard CF tracker [30], we show that the combination with the advanced CF tracker (i.e., BACF [17]) can improve the tracking performance with a large margin.|
|||Several Siamese networks, e.g., SINT [29], GOTURN [15], and SiameseFC [1], have been exploited for the off-line learning of CNN feature extractor for tracking, but both the feature extractor and tracker are fixed for the first frame, making them generally perform inferior to state-of-the-arts.|
|||Due to that the original CF has the closed-form solution, it can be interpreted as a differentiable CNN layer and enables the joint learning of deep representation and model adaptation.|
|||One reason that CFNet only considers the conventional CF is that it has closed-form solution and can be interpreted as a differentiable CNN layer.|
|||(2) defines an implicit function of the feature representation zt and model parameter , restricting its integration with CNN representation.|
|||Actually, a large part of computational cost in RTINet comes from the deeper CNN feature extraction.|
|||4.4 Comparison with the state-of-the-art methods  We compare RTINet with several state-of-the-art trackers, including CF-based trackers (i.e., ECO [6], C-COT [10], DeepSRDCF [7], BACF [17], STAPLECA [23]) and learning-based CNN trackers (i.e., MDNet [24], MetaSDNet [25], MetaCREST [25], SiameseFC [1], DSiamM [13] and SINT [29]).|
|||We adopt the deep convolutional network for feature representation and integrate the CNN with advanced BACF tracker.|
|||To solve the BACF in the CNN architecture, we design the model adaptation network as truncated inference by unrolling the ADMM optimization of the BACF model.|
||11 instances in total. (in eccv2018)|
|357|CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper|This motivates us to explore how to learn a deep CNN for accurately detecting both non-occluded and occluded pedestrians.|
|||We thus adapt the Fast R-CNN framework [16, 34, 4] to learn a deep CNN for simultaneous pedestrian classification, full body estimation and visible part estimation.|
|||Our deep CNN consists of two branches, one for full body estimation and the other for visible part estimation.|
|||The contributions of this paper are three-fold: (1) A bi-box regression approach is proposed to achieve simultaneous pedestrian detection and occlusion estimation by learning a deep CNN consisting of two branches, one for full body estimation and the other for visible part estimation; (2) A training strategy is proposed to improve the complementarity between the two branches such that their outputs can be fused to improve pedestrian detection performance; (3) A new criterion is introduced to select better positive pedestrian proposals, contributing to a large performance gain for heavily occluded pedestrian detection.|
|||A complexity-aware cascaded pedestrian detector [6] is learned by taking into account the computational cost and discriminative power of different types of features (including CNN features) to achieve a trade-off between detection accuracy and speed.|
|||A deep CNN is learned to jointly optimize pedestrian detection and other semantic tasks to improve pedestrian detection performance [32].|
|||In this paper, we explore how to learn a deep CNN to improve performance for detecting partially occluded pedestrians.|
|||Different from these approaches, we learn a deep CNN without using parts to handle various occlusions.|
|||These pedestrian proposals are then fed to a deep CNN which performs classification, full body estimation and visible part estimation for each proposal.|
|||We train the deep CNN in Fig.|
|||Tu, Z., Xie, W., Dauwels, J., Li, B., Yuan, J.: Semantic cues enhanced multimodality multi-stream cnn for action recognition.|
||11 instances in total. (in eccv2018)|
|358|Video Propagation Networks|This is followed by a standard spatial CNN on the bilateral network output to refine and predict for the present video frame.|
|||Recently, [67] proposed to retain the intermediate CNN representations while sliding a image CNN across the frames.|
|||Another approach is to take unary predictions from CNN and then propagate semantic information across the frames.|
|||, l} with full or partial knowledge of v are available, it is possible to learn F and for a complex and unknown input-output relationship, a deep CNN is a natural design choice.|
|||However, any learning based method has to face the challenge: the scene/camera motion and its effect on v. Since no motion in two different videos is the same, fixed-size static receptive fields of CNN are insufficient.|
|||We then add a small spatial CNN with 3 layers, each with 32 filters of size 3  3, interleaved with  ReLU non-linearities.|
|||The main role of this spatial CNN is to refine the information in frame t. Depending on the problem and the size of the available training data, other network designs are conceivable.|
|||Runtimes exclude CNN computations which are shown separately.|
|||VPN and Results Since we already have CNN predictions for every frame, we train a VPN that takes the CNN predictions of previous and present frames as input and predicts the refined semantics for the present frame.|
|||Input video frames and the corresponding ground truth (GT) segmentation together with the predictions of CNN [79] and with VPN-Flow.|
|||Acknowledgments We thank Vibhav Vineet for providing the trained image segmentation CNN models for CamVid dataset.|
||11 instances in total. (in cvpr2017)|
|359|Gao_TURN_TAP_Temporal_ICCV_2017_paper|For run-time performance, TURN runs at over 880 frames per second (FPS) with C3D features and 260 FPS with flow CNN features on a single TITAN X GPU.|
|||In our experiments, C3D [27], optical flow based CNN model and RGB image CNN model [24] are investigated.|
|||We test TURN with different unit sizes nu  {16, 32}, and different unit features, including C3D [27], optical flow based CNN feature and RGB CNN feature [24].|
|||For dense flow CNN features, we sample 6 consecutive frames at the center of a unit and calculate optical flow [5] between them.|
|||As shown in Figure 4, dense flow CNN feature (TURNFL) gives the best results, indicating optical flow can capture temporal action information effectively.|
|||In contrast, RGB CNN features (TURN-RGB) show inferior performance and C3D (TURN-C3D) gives competitive performance.|
|||C3D feature is faster than flow CNN feature, but with a lower performance.|
|||TURN trained with dense flow CNN features is used for comparison.|
|||On v1.1, TURN-FL-16 proposal is fed into one-vs-all SVM classifiers which trained with two-stream CNN features.|
|||TURN is robust on different visual features, including C3D and dense flow CNN features.|
|||For the temporal action localizer, SVM classifiers are trained with two-stream CNN features in Sports and Works subsets.|
||11 instances in total. (in iccv2017)|
|360|Zhu_Discriminative_Multi-Modal_Feature_CVPR_2016_paper|Recently, the feature extracted from deep convolutional neural network has produced state-of-the-art results for various computer vision tasks, which inspire researchers to explore incorporating CNN learned features for RGBD scene understanding.|
|||The results from the multimodal layer can be back-propagated to the lower CNN layers, hence the parameters of the CNN layers and multimodal layers are updated iteratively until convergence.|
|||Recently, [10] explored to incorporate segmentation in CNN learning framework and achieved the state-of-the-art for rgbd scene recognition.|
|||Moreover, in terms of initial feature formulation, we include CNN features fine tuned on the HHA encoded depth layer [4], which has proved to be more discriminative than raw depth image and surface normal image.|
|||Proposed method  Let X = [x1, x2, ..., xN ]  Rd1N and Y = [y1, y2, ..., yN ]  Rd2N denote the d1and d2-dimensions of the activations from the second fully connected (fc2) layer of color and depth CNN in one data batch of N images.|
|||Intra-modality discriminative term: The discriminative term D1(W1) for RGB modality is intended to find a W1 to project the RGB CNN activations X to a space in which the distance between xi and xj is small if they are of the same class, otherwise large if they are from different classes.|
|||Alternating optimization  yij(1  dW1 (xi, xj)) > 1  (2)  where the distance between a pair of the CNN activations xi and xj is computed as  dW1 (xi, xj) = (W1xi  W1xj)T  (W1xi  W1xj)  (3)  and the discriminative term is defined as follows  D1(W1) =Xi,j  h(1  yij(1  dW1 (xi, xj)))  (4)  where h is a smoothed hinge loss function h(x) = max(0, x), where D2(W2) is similarly defined for the depth modality.|
|||L(, ) =p =p +p +  (1 + 2  1)  1T1 + p 2T2  (1 + 2  1) 1(t1D1(W1) + t2R1(W1, F1)) 2(t1D2(W2) + t2R2(W2, F2))  (12)  By setting L(,)    and L(,)    to 0, i can be updated as:  i =  (1/Ti)1/(p1) i=1(1/Ti)1/(p1)  P2  (13)  Finally, the learned weight can be back-propagated to the  lower layer of CNN by  F xi  = p  1t1  D1(W1)  xi  +  C xi  +p  1t2  R1(W1, F1)  xi  (14)  where  D1(W1)  xi  =Xj  2yijW T  1 W1(xi  xj)  (15)  h(1  yij(1  dW1 (xi, xj)))  C(W1, W2)  xi  =  2W T  1 W1Xc Xi,jc Xc,d Xic,jd  c6=d   2  (xi  xj)s dW1 (xi, xj)  dW2 (yi, yj) (xi  xj)s dW1 (xi, xj)  dW2 (yi, yj)  dW1 (xi, xj)  dW1 (xi, xj)  (16)  R1(W1, F1)  xi  = W T  1 (W1xi  fi)  (W T  1 fi  xi) (17)  for color modality X and is similarly defined for depth modality Y .|
|||For scene categorization, the benchmark of  Architecture of CNNs The architecture of the CNN for scene classification are exactly the same as the AlexNet [7].|
|||After the HHA encoding, we fine-tune the RGB CNN on the encoded depth dataset.|
|||From the tables, it can be seen that the CNN trained with scene centric databases, such as Place-CNN, SSCNN  2974  bathroom  bedroom  classroom  computer_room  conference_room  corridor  dining_area  dining_room  discussion_area  furniture_store  home_office  kitchen  lab  lecture_theatre  library  living_room  office  rest_space  study_space        0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  Figure 5.|
||11 instances in total. (in cvpr2016)|
|361|Najibi_G-CNN_An_Iterative_CVPR_2016_paper|Next, a standard CNN is applied as feature extractor to each proposed bounding box and finally a classifier decides which object class is inside the box.|
|||Generally, around 2K proposals are generated; for each of them, the CNN is applied independently to extract features.|
|||This increases the size of the final layer of the CNN to 4096x800x5 and introduces a large set of additional parameters.|
|||The necessity of an object proposal algorithm in CNN based object detection systems has recently been challenged by Lenc et al.|
|||However, the considerable gap between the best detection accuracy of these systems and systems with an explicit proposal stage suggests that the identification of good object proposals is critical to the success of these CNN based detection systems.|
|||Network structure  G-CNN trains a CNN to move and scale a fixed multiscale grid of bounding boxes towards objects.|
|||The backbone of this architecture can be any CNN network (e.g.|
|||The performance of G-CNN is evaluated with AlexNet [11] as a small and VGG16 [18] as a very deep CNN structure.|
|||Both Fast R-CNN and our methods use AlexNet CNN structure.|
|||We showed how to learn the CNN architecture in a stepwise manner.|
|||2375  Figure 6: A sample of paths G-CNN found towards objects in the VOC2007 test set using AlexNet CNN structure.|
||11 instances in total. (in cvpr2016)|
|362|Liu_Deep_Relative_Distance_CVPR_2016_paper|Mixed difference network structure based on VGG CNN M 1024.  our coupled clusters loss.|
|||they belong to the same vehicle model, an extra instance difference measurement is needed(The dimension of fc8 is set to 1024 in accordance with the output dimension of standard VGG CNN M 1024 network to eliminate the influence of feature dimensional difference when performing evaluation experiments).|
|||fc7 2 in the mixed difference network is just the same as the output feature of a standard VGG CNN M 1024 network while fc8 is an enhanced one suitable for both inter-model difference and intra-model difference metric.|
|||Specifically in our experiments, our networks are all fine tuned on VGG CNN M 1024 which is pre-trained with the ImageNet dataset in ILSVRC-2012[15].|
|||The base network structure used in our experiments is VGG CNN M 1024[2].|
|||We use VGG CNN M 2048[2] and its mixed difference version in section 3.3 as the feature extractor in all our experiments.|
|||The third algorithm, VGG CNN M 1024 network with triplet loss function is trained with Part-I data of CompCars and the corresponding vehicle model labels.|
|||The training process of our method is quite similar to the triplet loss network except that the vehicle make information is also introduced to assist the model training and the standard VGG CNN M 1024 is replaced by its mixed difference version.|
|||The VGG CNN M 1024+Triplet Loss got no results because its loss function failed to converge during the training phase.|
|||MAP of Vehicle Retrieval Task MAP  Small Medium Large 0.444 0.373 0.386 0.492 0.546 0.455  0.391 0.448 0.481  VGG+Triplet Loss[4]  VGG+CCL(Ours)  Mixed Diff+CCL(Ours)  72173  using the same learning model(like the Joint Bayesian), GoogleNet beats VGG CNN M 1024 about 2% in top1 matching rate and owns significant advantages from top-5 to top-50(6%  7%).|
|||Considering the number of vehicle models in CompCars are far more larger than our VehicleID dataset and VGG CNN M 1024 is a relative small network for multi-class classification, we also trained a more powerful network, i.e.|
||11 instances in total. (in cvpr2016)|
|363|Wang_Event-Specific_Image_Importance_CVPR_2016_paper|We propose a new loss function and training procedure, and our CNN method greatly outperforms different baselines.|
|||CNN Structure  The design of our siamese CNN architecture is shown in Fig.|
|||4813  Figure 1: A siamese CNN architecture for joint training over events.|
|||In order to incorporate face information, we generate face heatmaps, and use them to train a shallow CNN to independently predict the importance score  of the photos.|
|||We train 18 CNN models for different face parts and concatenate the final fully-connected layers as the final face descriptor, following a similar pipeline as [25].|
|||The results of the above two experiments verify that the pre-trained CNN features can generalize to some extent to the event-based image importance prediction problem.|
|||We train a CNN with exactly the same architecture and training parameters except that the last layer of each of the  4816  Figure 3: Example results for one wedding album.|
|||5.2.2 Are pre-trained CNN features useful?|
|||Pre-trained CNN features have been shown to have a high generalization ability to new tasks [4, 9].|
|||This method is denoted as No Event CNN (NoEvent-CNN).|
|||We also train a CNN with only the second stage directly on 23 event types, as PR-CNN(Direct).|
||11 instances in total. (in cvpr2016)|
|364|Li_TGIF_A_New_CVPR_2016_paper|Each animated GIF is represented using the off-theshelf Hybrid CNN [46] and C3D [36] models; the former encodes static objects and scenes, while the latter encodes dynamic actions and events.|
|||From each animated GIF, we sample one random frame for the Hybrid CNN features and 16 random sequential frames for the C3D features.|
|||We sample frames at 10 FPS and encode each using a CNN [16].|
|||The model weights  CNN and encoder/decoder LSTM  are learned by minimizing the softmax loss L:  L =   1 N  N  T  X  X  i=1  t=1  p(yt = S i  t|ht1, S i  t1),  (1)  where S i t is the tth word of the ith sentence in the training data, ht is the hidden state and yt is the predicted word at timestamp t. The word probability p(yt = w) is computed as the softmax of the decoder LSTM output.|
|||average their CNN features.|
|||Variants on CNN weight optimization.|
|||We evaluate three variants on how the CNN weights are initialized and updated.|
|||The Rand model randomly initializes the CNN weights and fixes them throughout.|
|||To keep the CNN weights fixed, we limit the gradients of the loss function backpropagate only to the encoder LSTM.|
|||Finetune takes the pretrained parameters and finetunes them by backpropagating the gradients all the way down to the CNN part.|
|||three variants of different CNN weight initialization and update schemes (S2VT, Rand, Finetune), Finetune performs the best.|
||11 instances in total. (in cvpr2016)|
|365|Yang_Exploit_All_the_CVPR_2016_paper|Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale  Dependent Pooling and Cascaded Rejection Classifiers  Fan Yang1,2, Wongun Choi2, and Yuanqing Lin2  1Department of Computer Science, University of Maryland College Park  2NEC Laboratories America  fyang@cs.umd.edu, {wongun, ylin}@nec-labs.com  Abstract  In this paper, we investigate two new strategies to detect objects accurately and efficiently using deep convolutional neural network: 1) scale-dependent pooling and 2) layerwise cascaded rejection classifiers.|
|||Leveraging on the representational power of CNN, a number of methods are proposed to detect objects in natural images using CNN [13, 14, 16, 7, 42].|
|||In this work, we attempt to address aforementioned drawbacks and propose a new CNN architecture for an accurate and efficient object detection in images.|
|||With the exceptional power on image classification, CNN has been applied to object detection and achieves promising results [34, 14, 7, 44, 13, 27].|
|||Similar techniques are applied to recent CNN based object recognition methods: they treat the last convolutional layers outputs (conv5 of AlexNet) as the features to describe an object and apply a classifier (fc layers) on top of the extracted features.|
|||For instance, if an object proposal has a height between 0 to 64 pixels, the features are pooled from the 3rd convolutional layer of CNN (SDP 3).|
|||In following paragraphs, we assume that we have a CNN model trained with SDPs without loss of generality.|
|||We pool the corresponding features xi for Bi  B from the convolutional layer l using the CNN model trained with our SDPs.|
|||Given the 50 dimensional pool of weak-learners, we approximate the boosting classifier with 2 fc layers and a hyperbolic tangent tanh layer, so as to utilize the computational modules in the CNN framework.|
|||Our CNN model is initialized with a deep network architecture (VGG16 [32]) trained on the ImageNet classification dataset [30].|
|||A discriminative CNN video representation for event detection.|
||11 instances in total. (in cvpr2016)|
|366|cvpr18-Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks|Recent CNN architectures such as Residual Networks (ResNets) [10, 11] and Inception [36] require billions of floating-point operations (FLOPs) to perform inference on just one single input image.|
|||Models with fixed inference costs cannot work effectively in certain resource-constrained vision systems, where the computational budget that can be allocated to CNN inference depends on the real-time resource availability.|
|||As a simple solution to such a concern, one could train several CNN models such that each has a different inference cost, and then select the one that matches the given budget at inference time.|
|||A CNN with costadjustable inference only has to be trained once, and it allows users to control the trade-off of inference cost against network accuracy/performance.|
|||The same idea has also been applied to CNN training for other tasks [2, 30].|
|||Besides, the effectiveness of average pooling has been validated through its extensive roles in recent CNN architectures (e.g., global average pooling [24, 10], DenseNets transition [14]).|
|||In this work, the downsampling point candidates are the points between two consecutive CNN basic building blocks, mirroring the placements of fixed downsampling layers in conventional CNNs.|
|||Also, EE forces CNN features to be classification-ready in early stage, thus causing higher layers to rely heavily on the classification-ready features, instead of learning better features on their own.|
|||We reckon that a prolonged preservation of spatial details (i.e., no early downsampling) in CNN feature maps is not crucial to a dataset with relatively low label complexity such as CIFAR-10.|
|||So, for those difficult examples, it makes sense to preserve spatially informative object details longer in the CNN layer hierarchy, and downsample the feature maps only after they are semantically rich enough.|
|||An empirical study of language cnn for image captioning.|
||11 instances in total. (in cvpr2018)|
|367|Rao_Attention-Aware_Deep_Reinforcement_ICCV_2017_paper|Motivated by the fact that convolutional neural network (CNN) has achieved state-of-theart results on face recognition in recent years [31, 32, 35], we propose a local temporal representation for video face recognition by combining the CNN feature with recurrent layers with locality constraints to make better use of temporal information.|
|||combined reinforcement learning with CNN and achieved the human-level performance in the Atari game [30].|
|||The network processes the whole video with a deep CNN model, a recurrent layer, and a temporal pooling layer to produce temporal representations of each frame in the video, respectively.|
|||Assume the video A containing N A frames X A = 1 , xA [xA 2 , ..., xA N A ], C1(x) is a CNN feature representation, each frame xA i has a corresponding convolutional feature representation f A i ).|
|||In practice, the CNN model and the recurrent layer are trained separately.|
|||In other words, we employ the CNN model developed for still face recognition as convolutional feature extractor because sufficient labeled training samples can be used to train the model.|
|||Our CNN model was fine-tuned on training set of the corresponding video face dataset as [10] and supervised by the triplet loss with the learning rate 0.001.|
|||CNN is the result of mean-pooling CNN feature, TR is the result of temporal representation learning.|
|||We also compared our methods with our baseline CNN and presented the ROC curve in Figure 6.|
|||When we fine-tuned our model, the performance of our CNN model approaches their CNN model, and ADRL can further boost the final performance and outperform NAN more.|
|||Unconstrained face verification using deep cnn features.|
||11 instances in total. (in iccv2017)|
|368|Klein_A_Dynamic_Convolutional_2015_CVPR_paper|that linear regression and the CNN models were not used for the entire image, only for patches.|
|||The architecture of the CNN is as follows: The four radar image patches of size 7070 are given to a conventional convolutional layer (C1) with 32 filters of size 774 .|
|||Shown are the error of the Patch Based CNN (green) and the error of the Patch Based Dynamic Convolution Network (Red).|
|||The mean results,  j=1  i=1  as well as the standard errors (SE) are presented in Table  (8)     Last Frame      Global Motion   Estimator                        Patch Based   Linear Regression         Whole Image  Dynamic CNN                     Patch Based CNN               Patch Based  Dynamic CNN                                                                                                  Figure 7.|
|||The next best performing method is the patch based conventional CNN and the following best performing method is the whole image dynamic CNN.|
|||Additionally, due to the fact that the Tel-Aviv dataset lacks rain clouds , we compute the performance of the TelAviv dataset Patch Based CNN and Patch Based Dynamic CNN models that were learned on the US dataset.|
|||As a result, the Euclidean loss of the Patch Based Dynamic CNN improved to 10.766 0.414 and the Euclidean loss of Patch Based CNN degraded to 11.708  0.536.|
|||The convergence of the dynamic CNN method follows a typical pattern.|
|||In order to supply a fair comparison of the convergence, the running time of a single epoch was computed for the Patch Based Dynamic CNN and Patch Based CNN by taking the mean of 31 epochs.|
|||The running time of a single epoch in the Patch Based CNN is 189.935  3.950 and in the Patch Based Dynamic CNN is 233.67710.331.|
|||Therefore, a single epoch in the Patch Based Dynamic CNN is a bit slower than in the Patch Based CNN, but the overall convergence of the Patch Based Dynamic CNN is faster than the Patch Based CNN.|
||11 instances in total. (in cvpr2015)|
|369|Wang_Unsupervised_Learning_of_ICCV_2015_paper|We design a Siamese-triplet network with a ranking loss function to train this CNN representation.|
|||We design a Siamese-triplet network with ranking loss function to train the CNN representation.|
|||We believe this is the first time an unsupervised-pretrained CNN has been shown so competitive; that too on varied datasets and tasks.|
|||Our work is also related to [30], which used CNN pre-trained on ImageNet classification and detection dataset as initialization, and performed semisupervised learning in videos to tackle object detection in target domain.|
|||(1)  We want to train a CNN to obtain feature representation f (), so that the distance between query image patch and the tracked patch is small and the distance between query patch and other random patches is encouraged to be larger.|
|||Adapting for Supervised Tasks  Given the CNN learned by using unsupervised data, we want to transfer the learned representations to the tasks with supervised data.|
|||As a baseline, ImageNet CNN performs 62% (but note it already learns on semantics).|
|||The fine-tuned CNN was then used to extract features followed by training SVMs for each object class.|
|||Our approach (unsup + ft) is significantly better than network trained from scratch and comes very close to Imagenet-pretrained CNN ( 1%).|
|||Specifically, we track millions of patches and learn an embedding using CNN that keeps patches from same track closer in the embedding space as compared to any random third patch.|
|||Our unsupervised pre-trained CNN fine-tuned using VOC training data outperforms CNN trained from scratch by 3.5%.|
||11 instances in total. (in iccv2015)|
|370|Hu_Bottom-Up_and_Top-Down_CVPR_2016_paper|Interestingly, feed-forward inference on part models can be written as a CNN [9], but the proposed mapping does not hold for feedback inference.|
|||3 shows that such a sequence of bottom-up and top-down updates can be unrolled into a feed-forward CNN with skip connections between layers and tied weights.|
|||One can interpret such a model as a recurrent CNN that is capable of feedback, since lower-layer variables (capturing say, edges) can now be influenced by the activations of high-layer variables (capturing say, objects).|
|||On the right, we show that bottom-up updates can be computed with a feed-forward CNN, and bottom-up-and-top-down updates can be computed with an unrolled CNN with additional skip connections and tied weights (which we define as a recurrent CNN).|
|||When the associated weight matrix W is copositive, an infinitely-deep recurrent CNN must converge to the solution of the QP from (2).|
|||Because QPk is a deterministic function, its gradient with respect to (W, b) can be computed by backprop on the k-times unrolled recurrent CNN (Fig.|
|||Implementation  In this section, we provide details for implementing QP1 and QP2 with existing CNN toolboxes.|
|||QP1 corresponds to the left half of QP2, which essentially resembles the state-of-the-art VGG-16 CNN [38].|
|||w1  ...  pad & interlace   z1  T w2  z2  z2  w2  convolve &  subsample  z1  w1 x  Figure 5: Two-pass layer-wise coordinate descent for a twolayer Rectified Gaussian model can be implemented with modified CNN operations.|
|||TB:  It is worth contrasting our results with TB [45], which implicitly models feedback by (1) using a MRF to post-process CNN outputs to ensure kinematic consistency between keypoints and (2) using high-level predictions from a coarse CNN to adaptively crop high-res features for a fine CNN.|
|||Our single CNN endowed with top-down feedback is slightly more accurate without requiring any additional parameters, while being 2X faster (86.5 ms vs TBs 157.2 ms).|
||11 instances in total. (in cvpr2016)|
|371|Joint Detection and Identification Feature Learning for Person Search|Our CNN consists of two parts, given a whole input gallery image, a pedestrian proposal net is used to produce bounding boxes of candidate people, which are fed into an identification net to extract features for comparing with the target person.|
|||[1] designed specific CNN models for person re-id.|
|||Various factors, including CNN model structures, training data, and different training strategies are studied empirically in [15].|
|||[31] exploited pedestrian and scene attribute labels to train CNN pedestrian detectors in a multi-task manner.|
|||[2] proposed a complexityaware boosting algorithm for learning CNN detector cascades.|
|||Given as input a whole scene image, we first use a stem CNN to transform from raw pixels to convolutional feature maps.|
|||Below we will first detail the CNN model structure, and then elaborate on the OIM loss function.|
|||Model Structure  We adopt the ResNet-50 [13] as our base CNN model.|
|||Another interesting phenomenon is that although IDNet and LOMO+XQDA have similar performance when using GT or fine-tuned ACF and CNN detectors, IDNet is significantly better when using off-the-shelf CCF detector.|
|||OIM loss consistently outperforms Softmax loss, regardless of which base CNN is used.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||11 instances in total. (in cvpr2017)|
|372|Lu_Deep_Multi-Patch_Aggregation_ICCV_2015_paper|applied trained CNN to extract features on poselet patches [4].|
|||In [28], the MIL-based training of CNN was discussed and applied to object detection.|
|||As a supervised learning approach, CNN is commonly adopted to learn a function f : X  Y, from a collection of training examples {(xn, yn)}n[1,N ], where N is the size of the training set, xn is the image, and yn is the associated label.|
|||The problem of training CNN on downsized input images is that the network fails to encode fine-grained information existing in original-resolution images, which results in information loss.|
|||Denote by Tk the set of values of the  1The CNN refers to the convolutional neural networks drawn in Fig ure 1 from the input layer to the fc256 layer.|
|||First, CNN is trained2, then the weights of CNN in the DMA-Nets are initialized by the weights of the learned CNN, with which we intend to accelerate weight initialization in DMA-Net training.|
|||We used the convnet3 package to train CNN and implemented the two new layers and the aggregation structures.|
|||To evaluate the proposed approach, the DMA-Net (Ours-DMA-Netstat and Ours-DMA-Netfc) were compared with several baseline methods:  (i) CNN performs single-column CNN training and testing.|
|||used as training data, incorporating the SPP [12] layer in the CNN structure.|
|||The CNN includes four convolutional layers, and two pooling and normalization layers following the first and second convolutional layers, and two fully-connected layer of 1000 and 256 neurons respectively.|
|||ImageNet  5.3.3 Quality Estimation on Real-World Photos  The results of image quality estimation are presented in Table 4, where several conclusions can be drawn: (i) Multipatch aggregation network training improves the singlepatch network training results: DMA-Netmax improves the CNN by 1.5%, DMA-Netave improves the CNN by 1%, and Ours-DMA-Net improves the CNN by 56%.|
||11 instances in total. (in iccv2015)|
|373|Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper|Keywords: CNN architecture design, efficiency, practical  1  Introduction  The architecture of deep convolutional neutral networks (CNNs) has evolved for years, becoming more accurate and faster.|
|||(cid:9)(cid:66)(cid:10)(cid:1)(cid:40)(cid:49)(cid:54)(cid:9)(cid:67)(cid:10)(cid:1)(cid:34)(cid:51)(cid:46)(cid:9)(cid:68)(cid:10)(cid:1)(cid:40)(cid:49)(cid:54)(cid:9)(cid:69)(cid:10)(cid:1)(cid:34)(cid:51)(cid:46)ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  3  Fig.|
|||2 Practical Guidelines for Efficient Network Design  Our study is performed on two widely adopted hardwares with industry-level optimization of CNN library.|
|||We note that our CNN library is more efficient than most open source libraries.|
|||From mean value inequality, we have  ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  5  GPU (Batches/sec.)|
|||Specifically, each building block consists of from 1 to 4 1  1 convolutions, which are arranged in  ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  7  1x1 GConv   BN ReLU  Channel Shuffle  3x3 DWConv BN 1x1 GConv BN  3x3 AVG Pool  (stride = 2)   1x1 GConv   BN ReLU  Channel Shuffle  3x3 DWConv  (stride = 2)  BN 1x1 GConv BN  Channel Split   1x1 Conv   BN ReLU  3x3 DWConv BN  1x1 Conv  BN ReLU  3x3 DWConv  (stride = 2)  BN  1x1 Conv  BN ReLU  1x1 Conv   BN ReLU  3x3 DWConv  (stride = 2)  BN  1x1 Conv  BN ReLU  Add  ReLU  Concat  ReLU  Concat  Concat  Channel Shuffle  Channel Shuffle  (a)  (b)  (c)  (d)  Fig.|
|||Following G3, one branch remains as  ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  9  Fig.|
|||ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  11  Table 8 summarizes all the results.|
|||Recently, automatic model search [39, 18, 21, 32, 22, 38] has become a promising trend for CNN architecture design.|
|||ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  13  FLOPs Top-1 err.|
|||10.5 6.3 6.7 6.5 5.4 5.4 6.4 6.1  6.7 4.9 4.5  5.6 4.6 4.1   219 102 217 247 137 137 133 197   174 81 82  161 130 115  24.6 26.0 25.8  ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design  15  References  1.|
||11 instances in total. (in eccv2018)|
|374|Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network|First, we take a batch of persons as the input and feed them into the CNN deep network.|
|||Then, we use the cosine similarity to obtain the similarity matrix C. By using a specially designed gradient descent method, we obtain the globally optimal association matrices H. We further compute the difference as H  H, and propagate it back to deep model to update the CNN features.|
|||[42] proposed a siamese CNN (S-CNN) deep architecture for person re-identification, where three S-CNNs were employed for deep feature learning.|
|||[34] proposed a gated Siamese CNN architecture to selectively emphasize fine common local patterns by comparing the mid-level features across pairs of images.|
|||Settings  Due to the limitation of GPU memory, it is impossible to feed all person images for all cameras into CNN at once, so in our experiments, we considered 3 cameras each time and took 30 persons per camera to feed into our CNN for WARD and Market-1501 datasets.|
|||We considered all 4 cameras and took 21 persons per camera to feed into CNN for RAiD dataset.|
|||The CNN starts with 4 concatenated convolution layers followed by a pooling layer.|
|||After the final fully connected layer, the CNN produces features of length 256.|
|||The detailed structure of the pretrained CNN is described in our supplementary materials.|
|||Under our protocol, we obtained a more accurate and balanced CNN for different camera pairs using CADL.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||11 instances in total. (in cvpr2017)|
|375|Sekii_Pose_Proposal_Networks_ECCV_2018_paper|The entire architecture is constructed from a single, fully CNN with relatively lower-resolution feature maps and is optimized end-to-end directly using a loss function designed for pose detection performance; we call this architecture the pose proposal network (PPN).|
|||Run forward propagation of the CNN and obtain RPs of person instances and parts  and limb detections.|
|||The PPNs are constructed from a single CNN and produce a fixed-size collection of RPs for each detection target (person instances or each part) over the input image.|
|||The CNN divides the input image into a H  W grid, each cell of which corresponds to an image block, and produces a set of RP detections {Bi k}kK for each grid cell i  G = {1, .|
|||Conversely, for each grid cell i located at x, the CNN also produces a set of limb detections, {Ck1k2}(k1,k2)L, where L is a set of pairs of indices of detection targets that constitute limbs.|
|||Finally, the CNN outputs an H  W  {6(K + 1) + H W |L|} tensor.|
|||[1]  Pose Proposal Networks  7  demonstrated that such a minimal greedy inference well approximates the global solution at a fraction of the computational cost and concluded that the relationship between nonadjacent tree nodes can be implicitly modeled in their pairwise part association scores, which the CNN estimates.|
|||This leads to a global context for wider image regions than the receptive fields of the CNN is taken into account in the parsing.|
|||In this setting, the output grid cell size of the CNN on the image, which is described in  3.1, corresponds to 32  32 px2 and (H, W ) = (12, 12) for the normalized 384  384 input size of the CNN used in the training.|
|||The runtime of the forward propagation with the CNN and the parsing step are 4 ms and 0.3 ms, respectively.|
|||The colored dot lines, each of which corresponds to one of previous approaches, denote limits of speed in total processing as speed for processing other than the forward propagation of CNNs such as resizing of CNN feature maps [1, 2, 9], grouping of parts [1, 9], and NMS in human detection [2] or for part proposals [1, 9] (The colors represent each method).|
||11 instances in total. (in eccv2018)|
|376|Agrawal_Learning_to_See_ICCV_2015_paper|Performing this task is equivalent to training a CNN with two streams (i.e.|
|||Siamese Style CNN or SCNN[8]) that takes two images as inputs and predicts the egomotion that the agent underwent as it moved between the two spatial locations from which the two images were obtained.|
|||Finetuning is the process of modifying the weights of a pretrained CNN for the given target task.|
|||Two Stream Architecture  Each stream of the CNN independently computes features for one image.|
|||Features from two BCNNs are concatenated and passed downstream into another CNN called as the TopCNN (TCNN) (see figure 2).|
|||Shorthand for CNN architectures  The abbreviations Ck, Fk, P, D, Op refer to a convolutional(C) layer with k filters, a fully-connected(F) layer with k filters, pooling(P), dropout(D) and the output(Op) layers respectively.|
|||The outputs of two BCNNs are concatenated and passed as inputs to a second multilayer CNN called as the TopCNN (TCNN) (shown as layers F1, F2).|
|||After feature learning, TCNN is discarded and a single BCNN stream is used as a standard CNN for extracting features for performing target tasks like scene recognition.|
|||In this work, xt are features computed using a CNN with weights W and D was chosen to be the L2 distance.|
|||For evaluating the utility of CNN features produced by different layers, separate linear (SoftMax) classifiers were trained on features produced by individual CNN layers (i.e.|
|||Further, egomotion based features outperform features learnt by a CNN trained using class-label supervision on two orders of magnitude more data (AlexNet-1M) on the task of visual odometry and one order of magnitude more data on the task of intra-class keypoint matching.|
||11 instances in total. (in iccv2015)|
|377|cvpr18-Single-Image Depth Estimation Based on Fourier Domain Analysis|First, we develop a CNN based on the ResNet [11] architecture.|
|||The CNN structure of the proposed single-image depth estimator.|
|||[38] trained a CNN for joint depth estimation and semantic segmentation and adopted a CRF model to improve CNN prediction results.|
|||[23] proposed a scheme to learn the unary and pairwise potentials of a continuous CRF in a unified CNN framework.|
|||[20] performed the pixelwise refinement of superpixel-based CNN prediction results through CRF optimization.|
|||Depth Estimation Network  The CNN structure of the proposed algorithm is shown in Figures 1 and 2, which is based on ResNet-152 [11].|
|||Depth Map Candidate Generation  Using the proposed CNN trained with the DBE loss, we generate multiple depth map candidates for an input image.|
|||Second, we process each cropped image through the CNN to yield the corresponding depth map.|
|||Since the CNN parameters are not symmetric, a flipped image does not yield the flipped depth map.|
|||Specifically, we developed the CNN structure based  336  (a)  (b)  (c)  (d)  (e)  (f)  Figure 5.|
||10 instances in total. (in cvpr2018)|
|378|Hoffman_Detector_Discovery_in_2015_CVPR_paper|Most recently, it was shown that CNN architectures can be used to transfer generic information between the classification and detection tasks [19], improving detection performance for tasks which lack bounding box training data.|
|||We begin by initializing a feature representation and initial CNN classification weights using auxiliary weakly labeled data (blue boxes Figure 2).|
|||Now, we can optimize over all images to refine the representation and learn category specific parameters that can be used per region proposal to produce detection scores:  (cid:88)  k  min wk, kC  (wk) +   (cid:88)  I{WS}  (cid:96)(Y k  I , wT  k (I))  (3)  We optimize Equation 3 through fine-tuning our CNN architecture with a new K-way last fully connected layer, where K = |C|.|
|||The intermediate objective is:    (cid:35)  (cid:34)  (cid:88)  q  min wq,  q{CS ,b}  (cid:88)  (cid:88)  IS  iRI  (wq) +   (cid:96)(yq  i , wT  q (xi))  (4)  This is accomplished by fine-tuning our CNN architecture with the strongly labeled data, while keeping the detection weights for the categories with only weakly labeled data fixed.|
|||We collectively utilize the strong annotations of images in S and estimated annotations for weakly labelled set, W, to optimize for detector weights and feature representation, as follows:  min wk, k{C,b}  (wk)  (cid:88)  (cid:88)  I{WS}  iRI  (6)  (cid:35)  (cid:96)(yk  i , wT  k (xi))  This is achieved by re-finetuning the CNN architecture.|
|||Specifically, we use 1000 randomly chosen images per class from the train set for initializing our CNN weights.|
|||for the implementation,  We use open source deep learning framework, Caffe [21], training and finetuning of our CNN architecture.|
|||The feature descriptor used is the output of the fully connected layer, f c7, of the CNN which is produced after fine-tuning the feature representation with strongly annotated data from auxiliary tasks.|
|||Following our alternating minimization approach, these discovered top boxes are then used to re-estimate the weights and feature representations of our CNN architecture.|
|||More specifically, we learn a new feature representation by fine-tuning all fully connected layers in the CNN architecture.|
||10 instances in total. (in cvpr2015)|
|379|MuCaLe-Net_ Multi Categorical-Level Networks to Generate More Discriminating Features|Abstract  In a transfer-learning scheme, the intermediate layers of a pre-trained CNN are employed as universal image representation to tackle many visual classification problems.|
|||The current trend to generate such representation is to learn a CNN on a large set of images labeled among the most specific categories.|
|||In this paper, we propose Multi Categorical-Level Networks (MuCaLe-Net) that include human-categorization knowledge into the CNN learning process.|
|||A standard CNN (Net-S) solves a discriminative problem (D1) that consists to separate specific categories ((1) and (2)) from each other.|
|||More precisely, our method re-labels the training-images at a generic categoricallevel (simply by re-labeling the categories), then use these generic categories (3) as the discriminative problem (D2) to be solved by another CNN (Net-G) for generating different features (G2).|
|||By this enlargement, the CNN is forced to solve a different discriminative problem that desirably increases the universality of generated features.|
|||In this paper, we propose a new method to relevantly increase the diversity of the generated features based on the nesting of Human-categorization [16, 29] (e.g., categoricallevel labels of objects) into the CNN learning process.|
|||In fact, they solve only one discriminative problem by jointly training the CNN on the generic and specific data, resulting into a mix of generic and specific features in the intermediate layers.|
|||Let us denote the feature extracted from a CNN model WB by B K() where the K th first layers filter the images (e.g, {conv1, conv2} when K = 2 with AlexNet [18] network).|
|||Comparison with Strategies of the Literature The reference strategy and the strategies of the literature used for comparison are: Standard [18, 32]: Training a CNN on the 483 specific categories of the trainingdatabase.|
||10 instances in total. (in cvpr2017)|
|380|cvpr18-AMNet  Memorability Estimation With Attention|[37] used CNN features with SVR [6] to predict memorability with accuracy comparable to MemoNet [2].|
|||[18] collected a large memorability dataset LaMem with 60K images and introduced MemNet model based on the Hybrid-CNN, which is the AlexNet [21] CNN pretrained on the ImageNet [30] and the Places [39] datasets (3.6 million images in total).|
|||Method  The idea behind the AMNet architecture is based on four main components a deep CNN trained on large-scale image classification task, a soft attention network, a Long Short Term Memory (LSTM) [9] recurrent neural network followed by a fully connected neural network for memorability score regression.|
|||In the following section we introduce the details of the AMNet architecture as shown in Figure 2, starting with the pre-trained CNN (a) for transfer learning.|
|||Transfer Learning for Memorability Estima tion  It is common practice to use a pretrained CNN as a fixed feature extractor or to fine tune it for a similar application [32], mainly to reduce training time and overfitting on tasks with small datasets.|
|||[18] has already shown the benefits of fine tuning of pretrained CNN for this domain, however we decided to evaluate a much deeper model as a fixed feature extractor.|
|||6  x = get cnn features(X) h0 = finitc (x) c0 = finith (x) lstm init(h0, c0) y = 0 for t = 0 to T do  e = fatt(x, ht)  = sof tmax(e) z = [] for i = 0 to L do z = z + ixi  ht, ct = lstm step(z, ht, ct) y = y + fm(ht)  return y   for all locations, Eq.|
|||Even without attention the AMNet outperforms prior work by 3.6% which demonstrates that the pretrained, deep CNN with our recurrent and regression network layers still achieve high  accuracy.|
|||The comparatively low performance of the CNNMTLES [14] method can be attributed to the fact that this model uses various, specifically engineered visual features and features extracted from CNN networks trained on ImageNet [30] and Places [39].|
|||This network consists of a pre-trained, deep CNN followed by a modified visual attention mechanism with a recurrent network and network for memorability regression.|
||10 instances in total. (in cvpr2018)|
|381|Singh_Hide-And-Seek_Forcing_a_ICCV_2017_paper|Recent work modify CNN architectures designed for image classification so that the convolutional layers learn to localize objects while performing image classification [32, 61].|
|||However, rather than modifying the CNN architecture, we instead modify the input image by hiding random patches from it.|
|||Masking image patches has been applied for object localization [1], self-supervised feature learning [34], semantic segmentation [17, 10], generating hard occlusion training examples for object detection [54], and to visualize and understand what a CNN has learned [59].|
|||In particular, for object localization, [59, 1] train a CNN for image classification and then localize the regions whose masking leads to a large drop in classification performance.|
|||Each patch is then randomly hidden with probability phide and given as input to a CNN to learn image classification.|
|||In order to learn the object localizer, we train a CNN which simultaneously learns to localize the object while performing the image classification  task.|
|||We take the new image I  with the hidden patches, and feed it as a training input to a CNN for classification.|
|||Our apObject proach of hiding patches is independent of the network architecture and can be used with any CNN designed for object localization.|
|||This approach has shown state-of-the-art performance for the ILSVRC localization challenge [36] in the weakly-supervised setting, and existing CNN architectures like AlexNet [28] and GoogLeNet [46] can easily be modified to generate a CAM.|
|||For action classification, we feed C3D features as input to a CNN with two conv layers followed by a global max pooling and softmax classification layer.|
||10 instances in total. (in iccv2017)|
|382|cvpr18-GeoNet  Geometric Neural Network for Joint Depth and Surface Normal Estimation|However, our  1283  experiments in section 4.2 demonstrate that even with the common successful CNN architectures, e.g., VGG-16, we cannot obtain any reasonable normal results from depth, not even close.|
|||These extensive experiments manifest that current classification CNN architectures do not have the necessary ability to learn such geometric relationship from data.|
|||In [20], a continuous conditional random field (CRF) was built on top of CNN to smooth super-pixel-based depth prediction.|
|||To improve accuracy, we propose a residual module, which consists of a 3-layer CNN with skip-connection and 1  1 convolutional layer, as shown in Fig.|
|||In particular, before fed to the 1  1 convolution, the output of this CNN is concatenated with initial estimation of surface normal, which could be output of another network.|
|||(a) Image  (b) Deep3D [33]  (c) Multi-scale CNN [7]  (d) SkipNet [3]  (e) Ours  (f) GT  Figure 6: Visual comparison on surface normal prediction with VGG-16 being the backbone architecture.|
|||[36]  3DP (MW) [9] UNFOLD [10]  35.3 36.3 35.2 33.5 Multi-scale CNN [7] 23.7 26.9 19.8 20.6 19.4 SkipNet [3]+GeoNet 19.7 19.0  Deep3D [33] SkipNet [3] SURGE [32]  Baseline  GeoNet  31.2 19.2 17.9 23.1 15.5 14.8 12.0 12.2 12.5 11.7 11.8   28.2   27.0 28.4 26.9  16.4 39.2 40.5 27.7 39.2 42.0 47.9 47.3 46.0 48.8 48.4  36.6 48.2 52.9 57.8 54.1 58.9 49.0 58.7 62.0 71.1 61.2 68.2 70.0 77.8 68.9 76.6 70.3 78.9 70.5 78.2 71.5 79.5  Table 1: Performance of surface normal prediction on NYU v2 test set.|
|||Error  Accuracy  rmse log 10 rel   < 1.25  < 1.252  < 1.253  DepthTransfer [13] SemanticDepth [15]  1.214    0.349   0.447 0.542      NRF [24]  FCRN [16]  DC-depth [21]  Local Network [5]  SURGE [32] GCL/RCL [1]  Global-Depth [37] CNN + HCRF [31] Multi-scale CNN [7]  1.06 0.127 0.335 1.04 0.122 0.305 0.525).829 0.907 0.215 0.641 0.158 0.744 0.078 0.187 0.149 0.620 0.643 0.156 0.802 0.790 0.083 0.194 0.584 0.059 0.136 VGG+Multi-scale CRF [34] 0.655 0.069 0.163 ResNet+Multi-scale CRF [34] 0.586 0.052 0.121 0.626 0.068 0.155 0.608 0.065 0.149 0.569 0.057 0.128  0.605 0.769 0.801 0.806 0.768 0.605 0.629 0.822 0.706 0.811 0.768 0.786 0.834  GeoNet-VGG GeoNet-ResNet  FCRN-ResNet [16]  Baseline  0.745 0.829   0.941 0.890 0.950 0.950 0.958 0.951 0.890 0.889 0.955 0.925 0.954 0.951 0.956 0.960  0.897 0.941   0.970 0.988 0.986 0.987 0.989 0.970 0.971 0.971 0.981 0.988 0.988 0.990 0.990  Table 2: Performance of depth prediction on NYU v2 test set.|
|||Moreover, we compare results with those of other methods, including Deep3D [33], Multi-scale CNN [7] and SkipNet [3] on surface normal prediction in Fig.|
|||The table reveals that LS module is already significantly better than the vanilla CNN baselines in all aspects.|
||10 instances in total. (in cvpr2018)|
|383|Marcos_Rotation_Equivariant_Vector_ICCV_2017_paper|In this paper, we propose a CNN architecture that naturally encodes these three properties: RotEqNet.|
|||TI-pooling [16] inputs several rotated versions of a same image to the same CNN and then performs pooling across the different feature vectors at the first fully connected layer.|
|||Their representations after the first fully connected layer are then encouraged to be similar, forcing the CNN to learn rotation invariance.|
|||On the one hand, these methods have the advantage of exploiting conventional CNN implementations, since they only act on data representations.|
|||(4)  This block can be applied on conventional CNN feature maps (left side of Fig.|
|||4, a standard CNN requires 4 more filters per layer to saturate performance, compared to RotEqNet.|
|||Model: We test four CNN models with the same architecture, but different number of filters per layer.|
|||Comparison to data augmented training: we evaluated the RotEqNet model (N = 2) and an equivalent standard CNN with 10 more parameters on 5 held out validation images.|
|||RotEqNet seems not to profit as much from data augmentation as its standard CNN counterpart, but improves the CNN solution in all the cases considered, as illustrated in Table 4.2.|
|||Our rotation equivariant prediction provides results comparable to other stateof-the-art methods only relying on the raw CNN softmax output [8, 9, 22] (see Table 6).|
||10 instances in total. (in iccv2017)|
|384|Video Segmentation via Multiple Granularity Analysis|Since MIL often benefits from more discriminative features, we propose a multi-scale CNN feature based descriptor to strengthen the discriminative power of each superpixel.|
|||Multi-scale CNN superpixel feature.|
|||Superpixels of different sizes are then fed into VGG network to predict multiplelayered CNN features.|
|||To this end, we propose a multi-scale CNN descriptor (feature representation) for encoding each superpixel for further processing, which inherits the good property of robustness and discriminating power from multi-scale analysis and CNN.|
|||3012  Multi-scale CNN Superpixel Feature Sizes and numbers of grains define the clustering ability of superpixels.|
|||We then concatenate the DCNN features extracted from the superpixels of each level into a feature representation vector by dimension of 4099  Q with 3 dimensions RGB and 4096 dimensions CNN features.|
|||In pixel term, we use GMM model on RGB and SVM on CNN features, while in superpixel term, RGB feature as well as superpixel clustering CNN feature are fed in energy function.|
|||For multi-scale CNN feature extractor, we extract the 3th, 6th, 10th, 14th and 18th layers of VGG network and do two iterations of superpixels-clustering.|
|||On pixel level, we feed each frame into VGG-19 and then upsample the output to frame size to obtain CNN features at each pixel position.|
|||Moreover, the weight for CNN and color feature for pixels and superpixels are respectively 3, 1, 5, 1, while general weight for pixels and superpixels are 1 and 15 to leverage their difference in numbers.|
||10 instances in total. (in cvpr2017)|
|385|Zhengqin_Li_Materials_for_Masses_ECCV_2018_paper|We train a CNN to regress an SVBRDF and surface normals from this image.|
|||challenge by proposing a novel CNN architecture that is specifically designed to account for the physical form of BRDFs and the interaction of light with materials, which leads to a better learning objective.|
|||We introduce a novel CNN architecture that encodes the input image into a latent representation, which is decoded into components corresponding to surface normals, diffuse texture, and specular roughness.|
|||The inferred BRDF parameters from the CNN are quite accurate, but we achieve further improvement using densely-connected conditional random fields (DCRFs) with novel unary and smoothness terms that reflect the properties of the underlying microfacet BRDF model.|
||| A physically-motivated CNN and DCRF framework for joint SVBRDF recon struction and material classification.|
|||[29] train a CNN to reconstruct the reflectance map  a convolution of the BRDF with the illumination  from a single image of a shape from a known class.|
|||use supervised learning to train a CNN to predict SVBRDF and normals from a single image captured under environment illumination [19].|
|||We demonstrate that by using our novel CNN architecture, supervised training on a high-quality dataset and acquisition under flash illumination, we are able to (a) reconstruct all these parameters with a single network, (b) learn a latent representation that also enables material recognition and editing, (c) obtain results that are significantly better qualitatively and quantitatively.|
|||4 Network Design for SVBRDF Estimation  In this section, we describe the components of our CNN designed for single-image SVBRDF estimation.|
|||Thus, we adopt a datadriven approach with a custom-designed CNN that reflects physical intuitions.|
||10 instances in total. (in eccv2018)|
|386|Tewari_MoFA_Model-Based_Deep_ICCV_2017_paper|For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible.|
|||Our network architecture is inspired by recent progress on deep convolutional autoencoders, which, in their original form, couple a CNN encoder and a CNN decoder through a code-layer of reduced dimensionality [18, 33, 61].|
|||Moreover, our decoder is compact and does not need training of enormous sets of unintuitive CNN weights.|
|||Unlike previous CNN regression-based approaches for face reconstruction, a single forward pass of our network estimates a much more complete face model, including pose, shape, expression, skin reflectance, and illumination, at a high quality.|
|||Our new network architecture allows, for the first time, combined end-to-end training of a sophisticated model-based (generative) decoder and a CNN encoder, with error backpropagation through all layers.|
|||We focus on parametric model fitting and CNN approaches in the context of monocular face reconstruction.|
|||In [40], a multi-task CNN is trained to predict several face-related parameters (e.g.|
|||In [41], a CNN is trained using synthetic data for extracting the face geometry from a single image.|
|||[55] used photo collections to obtain the ground truth parameters from which a CNN is trained for regressing facial identity.|
|||In [29], a CNN is trained under controlled conditions in a supervised fashion for facial animation tasks.|
||10 instances in total. (in iccv2017)|
|387|Sankaranarayanan_Guided_Perturbations_Self-Corrective_ICCV_2017_paper|To understand how a CNN can learn complex and meaningful representations but at the same time be easily fooled by simple and imperceptible perturbations still remains an open research problem.|
|||While these approaches have focused on understanding the effect of adversarial noise in deep networks, in this work we present an interesting observation: input perturbations can enable a CNN to correct its mistakes.|
|||In almost all the CNN based approaches, the output is obtained using a single forward pass during the prediction time.|
||| We propose a generalized framework to improve the performance of any pretrained CNN model that is architecture independent and requires no learning assuming the network is trained end-to-end.|
|||[12] proposed an approach to invert the function learned by the CNN in order to generate as faithful a reconstruction of the input as possible.|
|||This leads to interesting results such as images that look like random noise but which the CNN classifies into different classes with high confidence.|
|||Their study shows that there exist a lot of adversarial examples which are the result of minor pertubations of the input that causes the CNN to misclassify input images on classification tasks; these examples can be generated by adding a fraction of the gradient that is generated by wiggling the classifier output in the direction of the target class.|
|||By casting this as a CNN layer they perform endto-end training.|
|||As long as the errors made by the CNN are sparse with respect to each pixels receptive fields, the error gradients when accumulated over the entire network and used to perturb the input image exhibit a corrective behavior.|
|||In this toy example, the CNN is trained to classify among 5 classes.|
||10 instances in total. (in iccv2017)|
|388|cvpr18-Harmonious Attention Network for Person Re-Identification|Specifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images.|
|||[19] design an end-to-end trainable part-aligning CNN network for locating latent discriminative regions (i.e.|
|||Moreover, we also design an efficient attention CNN architecture for improving the model deployment scalability, an under-studied but practically important issue for re-id.|
|||Unlike most existing works that simply adopting a standard CNN network typically with a large number of model parameters (likely overfit given small size labelled data) and high computational cost in model deployment [17, 29, 33, 10], we design a lightweight (less parameters) yet deep (maintaining strong discriminative power) CNN architecture by devising a holistic attention mechanism for locating the most discriminative pixels and regions in order to identify optimal visual patterns for re-id.|
|||We avoid simply stacking many CNN layers to gain model depth.|
|||In the CNN hierarchical framework, this naturally allows for hierarchical multi-level attention learning to progressively refine the attention maps, in the spirit of the divide and conquer design [7].|
|||For example, the performance by SVDNet relies on the heavy ResNet50 CNN model (23.5 million parameters) with the need for model pre-training on the ImageNet data (1.2 million images), whilst HA-CNN has only 2.7 million parameters with no data augmentation.|
|||Model Complexity We compare the proposed HA-CNN model with four popular CNN architectures (Alexnet [17], VGG16 [29], GoogLeNet [33], and ResNet50 [10]) in model size and complexity.|
|||most existing re-id methods that either ignore the matching misalignment problem or exploit stringent attention learning algorithms, the proposed model is capable of extracting/exploiting multiple complementary attention and maximising their latent complementary effect for person re-id in a unified lightweight CNN architecture.|
|||This is made possible by the Harmonious Attention module design in combination with a two-branches CNN architecture.|
||10 instances in total. (in cvpr2018)|
|389|Fan_Combining_Local_Appearance_2015_CVPR_paper|As a result, it has difficulty in estimating complex human poses, even using the CNN architecture.|
|||In this paper, we propose a dual-source CNN (DS-CNN) based method for human pose estimation, as illustrated in Fig.|
|||More recently, CNN architectures have been successfully applied to object localization and detection [30, 17, 31].|
|||In [31], a single shared CNN named Overfeat is used to simultaneously classify, locate and detect objects from an image by examining every sliding window.|
|||This may complicate the CNN training by artificially introducing unrealistic patterns into training samples.|
|||In this paper, we use the open-source CNN library Caffe [39] for implementing DS-CNN.|
|||We finetune a CNN network pretrained on ImageNet [29] for training the proposed DS-CNN.|
|||Average precision (%) of joint detection on LSP and FLIC testing datasets when CNN takes different types of patches as input.|
|||As a result, we still need to combine pp and pb and construct a dual-source CNN for pose estimation.|
|||The proposed method is implemented using the open-source CNN library Caffe and therefore has good expandability.|
||10 instances in total. (in cvpr2015)|
|390|Cui_Fine-Grained_Categorization_and_CVPR_2016_paper|In such scenarios, the final fully connected layer of a CNN before the softmax layer would contain too many nodes, thereby making the training infeasible.|
|||Fueled by the recent advances in Convolutional Neural Networks (CNN) [27, 16], researchers have gravitated to CNN features [6, 50, 25, 35, 32] rather than traditional hand-crafted features such as LLC [2] or Fisher Vectors [14].|
|||The goal is to use a CNN with either pairwise (contrastive) or triplet loss to learn a feature embedding that captures the semantic similarity among images.|
|||Comparison between CNN with softmax and CNN for metric learning in feature space, where ci denotes a group of images within the same category.|
|||3 illustrates the differences between CNN with softmax and CNN for metric learning in 3-dimensional feature space.|
|||We train a CNN to preserve this relative ordering under feature embedding f ().|
|||Then the triplet of three images are fed into an identical CNN simultaneously to get their non-linear feature embeddings f (x), f (xp) and f (xn).|
|||The CNN could be any arbitrary architecture such as AlexNet [27], VGGNet [37] or GoogLeNet-Inception [38].|
|||This hinge loss function will produce a non-zero penalty of kf (x)  f (xp)k2 2  kf (x)  f (xn)k2 2 + m if the L2 distance between x and xn is smaller than the L2 distance between x and xp adding a margin m in feature space: kf (x)  f (xn)k2 2 < kf (x)  f (xp)k2 2 + m. The loss will be back propagated to each layer of the CNN and their corresponding parameters are updated through stochastic gradient descent.|
|||1158  The CNN architecture we used is GoogLeNet-Inception [38], which achieved state-of-the-art performance in largescale image classification on ImageNet [12].|
||10 instances in total. (in cvpr2016)|
|391|cvpr18-Camera Style Adaptation for Person Re-Identification|During training, we use the new training set for re-ID CNN training following the baseline model in [43].|
|||First, it can be regarded as a data augmentation scheme that not only smooths the camera style disparities, but also reduces the impact of CNN overfitting.|
|||In [38], input image pairs are partitioned into three overlapping horizontal parts respectively, and through a siamese CNN model to learn the similarity of them using cosine distance.|
|||When a CNN model is excessively complex compared to the number of training samples, over-fitting might happen.|
|||[50] randomly select PseudoPositive samples from an independent dataset as addition training samples for training re-ID CNN to reduce the risk of over-fitting.|
|||This allows us to leverage the style-transferred images as well as their associated labels to train re-ID CNN in together with the original training samples.|
|||Baseline Deep Re(cid:173)ID Model  Given that both the real and fake (style-transferred) images have ID labels, we use the ID-discriminative embedding (IDE) [43] to train the re-ID CNN model.|
|||Baseline CNN model for re-ID.|
|||Training CNN with CamStyle.|
|||RF+RC is a common technique in CNN training [17] to improve the robustness to image flipping and object translation.|
||10 instances in total. (in cvpr2018)|
|392|Ouyang_Chained_Cascade_Network_ICCV_2017_paper|Design of better CNN model.|
|||Recent CNN based methods for object detection.|
|||In fast RCNN, 1) a set of regions of interest (RoIs) are generated by a region proposal approach; 2) CNN feature maps for the input image are generated by several convolutional layers; 3) the roi-pooling layer projects the RoIs onto the CNN feature maps and extracts feature maps of the same size for RoIs of different sizes; 4) the layers after roi-pooling are conducted to obtain the final features, from which the classification scores and the regressed coordinates for bounding-box relocalization are predicted.|
|||At a stage, the RoIs rejected by previous stages do not have their features roi-pooled from  1939  early cascade  contextual cascade  RoIs  Classifier chaining with multiple   detection scores  cascade stages  features  remaining   RoIs  remaining   RoIs  oI  R  ...  loss function for   training  rejected  RoI  rejected  RoI  softmax  softmax  softmax  classifier  classifier  classifier  roi-pooling  ...  ...  ...  image  convolution on image  chained CNN  features for RoI  chained class   detection  scores  results  Figure 2.|
|||These CNN features can be different in depth, learned parameters, resolution and contextual regions.|
|||The treatment for the module (4e)t are also applied for the inception modules (5a)t and (5b)t. The CNN features obtained from the inception modules (5b)t have different sizes.|
|||Therefore, the CNN layers at the stage t no longer need to represent the information existing in previous stages.|
|||And this sample will not be used for extracting its features in the latter CNN layers.|
|||Object detection via a multiIn  region and semantic segmentation-aware cnn model.|
|||Gated bi-directional cnn for object detection.|
||10 instances in total. (in iccv2017)|
|393|Cao_Egocentric_Gesture_Recognition_ICCV_2017_paper|The recurrent STTM (RSTTM) can be inserted into a 3D CNN between any two convolutional layers.|
|||Illustration of the recurrent 3D CNN with RSTTM.|
|||Video sequence is split to clips and then input to the 3D CNN with RSTTM for feature extraction.|
|||[17] pre-train a hand segmentation CNN to help find objects of interest for activity recognition.|
|||A CNN is used to predict a series of affine transformations applied on the current frame patches to generate the next frame.|
|||We include recurrence in the localization network as illustrated in Figure 2 to predict the current transformation parameters conditioned on the previous ones:  ct = f cnn ht = f rnn Ht = f f c  loc (It) loc (ct, ht1) loc (ht)  (2)  (3)  (4)  where f cnn is a 3D CNN which takes It as input and outloc puts a feature map ct. f rnn is an RNN with hidden state ht loc and f f c loc is a fully-connected layer to regress the transformation parameters.|
|||Both the spatial stream and the temporal stream use 2D CNN to extract features.|
|||Ego ConveNet [24] uses hand-crafted egocentric cues (including hand masks, head motions and saliency maps) as input to a 2D CNN or 3D CNN model.|
|||The 2D CNN and 3D CNN both are small networks with only 2 convolutional layers.|
|||Conclusion  In this work, we propose a novel recurrent 3D CNN model with recurrent spatiotemporal transformer module which can deal with the egocentric motion effectively.|
||10 instances in total. (in iccv2017)|
|394|Zhang_Co-Saliency_Detection_via_2015_CVPR_paper|In  summary,  the  novelties  of  this paper are threefold:  1) We  propose  to  explore  the  properties  of  the  co-salient  objects  by  using  CNN  with  additional  transfer  layers,  which  brings  deep  information  for  discovering  the  higher-level properties of the co-salient objects.|
|||Then  the  aim  of  our  work  is  to  transfer  higher-level  features  for  representing  object  proposal  windows (OPs) in the each image (section 3.1),  assigning  co-saliency  scores  to  the  OPs  via  a  Bayesian  formulation  (section  3.2),  and  finally  converting  the  obtained  window-level  co-saliency  scores  the  superpixel-level co-saliency maps (section 3.3)   to   {  Figure 3: Transfer CNN for higher-level representation.|
|||Transfer CNN for higher-level representation   image  regions  with  deep  information, we extract OPs in each image firstly, and then  build  higher-level  features  for  them  via  a  CNN  with  additional  transfer  RBM  layer.|
|||With  more  than  60  million  parameters,  CNN  [27]  has  been  demonstrated  to  have  the  capability  to  capture  higher-level  image  representations  and  achieve  good  performance  in object detection and image classification.|
|||However,  directly  learning  a  whole  CNN  from  the  co-saliency  dataset  is  problematic,  since  the  co-saliency  dataset  only  contains  hundreds  of  images,  which  is  too  scarce  to  train  such  many  parameters.|
|||In  addition,  the  overall co-saliency framework proposed in this paper is in  an unsupervised fashion, which means there is no available  image  labels  to  be  used  for  CNN  training.|
|||To  solve  this  problem,  we  are  inspired  by  the  technologies  in  transfer  learning and domain adaption to pre-train a CNN  [27] on  the  source  data  (i.e.|
|||Thus we can obtain a 512 dimensional higher-level feature  outputted by the proposed CNN to represent each OP in the  co-saliency dataset.|
|||Since OURS-NW is still better than  all  the  existing  methods  which  are  all  using  low-level  features,  the  importance  of  the  deep  information  is  also  demonstrated; 2) Directly using the CNN proposed in [27]  without  (OURS-NT)  obtains  worse  performance  than  the  proposed  method  (OURS),  which  demonstrates  the  effectiveness  of  the  additional  RBM  layers proposed in this paper for building domain-specific   transfer   layers   Table 2: AP and F-measure scores on the MSRC dataset.|
|||In addition, the comparision between OURS and  OURS-NT  also  demonstrates  the  effectiveness  of  the  proposed  CNN  architecture  for  transfering  higher-level  features.3   To perform further verification, we compared the scores  between  the  proposed  approach  and  other  state-of-the-art  co-saliency detection methods for each image group in the  MSRC  dataset.|
||10 instances in total. (in cvpr2015)|
|395|Ying_Fu_Joint_Camera_Spectral_ECCV_2018_paper|Experimental results show that our recovery network outperforms state-of-theart methods in terms of quantitative metrics and perceptive quality, and both the spectral CNN module and the spatial CNN module have contributed to this performance gain.|
|||In this work, we present a spectral CNN module to better account for spectral nonlinear mapping, and a spatial CNN module to further incorporate the spatial similarity.|
|||In the HSI recovery network, we design a spectral CNN to approximate the complex nonlinear mapping between the RGB space and the spectra space, and a spatial CNN for the spatial similarity.|
|||Compared with these approaches, our method utilizes multiple CNN layers in spectral CNN to deeply learn the nonlinear mapping between spectra and RGB space, and employs DenseNet blocks [21] and ResNet modules [20] in the spatial CNN to enlarge the receptive field and obtain more spatial similarity in the space domain.|
|||Spectral CNN Previous works for the spectral recovery from a single RGB image [38, 3, 42, 22] mainly consider the spectral mapping between the input RGB image and the recovered HSI.|
|||It is well-known that CNN can effectively learn the nonlinear mapping.|
|||Thus, we design a spectral CNN to learn the spectral nonlinear mapping between the RGB values and the corresponding spectrum, which consists of L layers.|
|||Inspired by this, we use the input RGB image to guide the spatial information reconstruction, which is modeled by stacking the input RGB image and the initialized HSI FL from the spectral CNN mentioned above.|
|||Spatial CNN Due to abundant self-repeating patterns in natural images [7, 13], the spatial information is usually similar in the neighboring area.|
|||We can see that the results under the selected optimal CSS are much close to the ground truth, which further demonstrates the effectiveness of joint optimal CSS selection and the accuracy of the learned CNN nonlinear mapping in HSI recovery.|
||10 instances in total. (in eccv2018)|
|396|Deep Affordance-Grounded Sensorimotor Object Recognition|The different CONV layers of the employed CNN now model affordance-related patterns of increasing spatial complexity.|
|||With respect to the development of the ST architecture, a composite CNN (VGG-16) Long-Short Term Memory (LSTM) [28] NN is considered, where the output of a CNN applied at every frame is subsequently provided as input to an LSTM.|
|||Top: appearance CNN for object recognition, and affordance CNN (TM architecture).|
|||The CNN layer notation used in this paper is depicted at the top figure.|
|||For the GST architecture, the late fusion scheme considers only the concatenation of the features of the last FC layers of the appearance CNN and the affordance LSTM model, as depicted in Fig.|
|||In particular, the features of the FC7 layer of the appearance CNN and the internal state vector [h(t)] of the last LSTM layer of the affordance stream are concatenated at every time instant (i.e.|
|||For realizing this, two scenarios are considered, which correspond to the fusion of information from the two aforementioned CNNs at different levels of granularity: a) combining the feature maps of the appearance and the affordance CNN from the same layer level; and b) combining the feature maps of the appearance and the affordance CNN from different layer levels.|
|||The latter achieves an absolute increase of 4.31% in the overall recognition performance (which corresponds to an approximately 29% relative error reduction), compared to the appearance CNN model (baseline method).|
|||3  3  , RL5af f  tion of the GTMSM L(RL5app , RL6) architecture and the appearance CNN are given in Fig.|
|||Conclusions  3  3  , RL5af f  The GTMSM L(RL5app  , RL6) architecture is also comparatively evaluated, apart from the appearance CNN model, with the following typical probabilistic fusion approaches of the literature: a) the product rule for fusing the appearance and the affordance CNN output probabilities, b) concatenation of appearance and affordance CNN features and usage of a SVM classifier (RBF kernel) [4, 15], and c) concatenation of appearance and affordance CNN features and usage of a naive Bayes classifier [13].|
||10 instances in total. (in cvpr2017)|
|397|Johns_Pairwise_Decomposition_of_CVPR_2016_paper|However, extending this to generalised recognition over trajectories of  3813  arbitrary paths and lengths is not readily adopted by traditional CNN architectures, due to the need for fixed-length input data.|
|||Contributions  In this paper, we present three key technical contribu tions all based on powerful CNN learning:  1.1.|
|||Given this decomposition, a CNN is then trained on a fixedlength input consisting of the image pair, together with the relative pose between the associated viewpoints.|
|||We propose to learn NBV prediction with a more powerful discriminative model, training a second CNN to map directly from an observed image to the rotation angle over which the camera should subsequently move.|
|||To achieve this, we train a third CNN in a similar manner to the above NBV CNN, but training for regression to a recognition confidence score for all possible next viewpoints, rather then classification for the overall best viewpoint.|
|||Recently, CNN architectures have been extended to allow for recognition from image sequences using a single network, by max pooling across all viewpoints [35], or by unwrapping an object shape into a panorama and max pooling across each row [33].|
|||(1)  To compute the class probability distribution p(y|wi) for each view pair, we designed a CNN architecture, denoted CNN-1 (see Figure 2), to predict an object class based on the provided view pair.|
|||This architecture was inspired by the Siamese CNN [4], which consists of two CNNs running in parallel, each taking in one image from the pair, and with weights shared across both networks.|
|||For the View Voting method, we trained a CNN with 5 convolutional layers and 3 fully-connected layers, similar to CNN-1, to classify views independently based on the image alone.|
|||Experiments show that our method outperforms the voxel-based generative ShapeNets method, together with the Multi-View CNN method, and we achieve state-of-the-art recognition on the ModelNet dataset.|
||10 instances in total. (in cvpr2016)|
|398|Paulin_Local_Convolutional_Features_ICCV_2015_paper|Patch-CKN descriptors yield competitive results compared to supervised CNN alternatives on patch and image retrieval.|
|||With a CNN learned on a sufficiently large labeled set such as ImageNet [9], the output of its intermediate layers can be used as image descriptors for a wide variety of tasks including image retrieval [3, 38]  the focus of this work.|
|||The output of one of the fully-connected layers is often chosen because it is compact, usually 4,096 D. However, global CNN descriptors lack geometric invariance [14], so they produce results below the state-of-the-art in instance-level image retrieval.|
|||In [38, 14], CNN responses at different scales and positions are extracted.|
|||Formally, the output f (x) of a CNN for some image x represented as a vector is  f (x) = K (K (WK .|
|||The most popular off-theshelf CNN is AlexNet [22], which won the ImageNet 2012 challenge.|
|||The embedding of [27] keeps the 2-D spatial structure, similar to CNN feature maps.|
|||There are two distinct approximations, one in the subsampling defined by |1|  || that corresponds to the stride of a CNN pooling operation, and one in the embedding of the Gaussian kernel of the subpatches: k1(pz, p  z )  h1(z; M )h1(z; M ).|
|||As CKN and CNN share the same architecture, the descriptor extraction time is similar for all convolutional methods.|
|||On the other hand, PhilippNet was trained to be invariant to colorimetric transformations, and therefore yields better results than its CNN counterpart.|
||10 instances in total. (in iccv2015)|
|399|Modeling Sub-Event Dynamics in First-Person Action Recognition|In the proposed method, we first map each frame in the sequence to a discriminative space by forward passing through a CNN model and taking the first-fully connected layer activations.|
|||Due to the remarkable success of CNN on image classification, several methods have been proposed where deep networks are trained for video-based action recognition [5, 8, 17, 19, 35].|
|||[26] extracted the fully connected layer activations of CNN as single frame features and showed that by simply taking the firstand secondorder statistics of the frame-wise features, dynamic texture and scene classification can be significantly improved.|
|||Max pooling, max: Given a time series f k(t), the max pooling operator find the peak value in the series which is defined as  max k  [ts, te] = max  t=ts,...te  f k(t)  (1)  Sum pooling, sum: all entries in the time series which can be expressed as  Similarly, sum pooling aggregates  te  (cid:2) k [ts, te] =  f k(t)  (2)  (cid:2)t=ts  1 ,  1 :  Histogram of time series gradient, + As the time series of CNN neurons are already in a discriminative space and we observe that the trends in time series of an action resembles those with the same label, we encode the information of how the trends move by calculating the number of positive and negative gradients within the time intervals.|
|||(b) Each frame within sub-segments is individually passed through a CNN model and the fc6 layers activations are extracted.|
|||Therefore, we propose to encode the sub-segment dynamics by employing a rank pooling algorithm [10] on the CNN activations of the frames within the time intervals.|
|||We use a CNN model which was discriminatively trained on ImageNet [3] dataset without any fine-tuning.|
|||feature representation in hand such as CNN features.|
|||It is important to emphasize that we do not fine-tune the CNN model to the target datasets which has shown improved performance for various applications [34].|
|||Exploiting image-trained CNN architectures for unconstrained video classification.|
||10 instances in total. (in cvpr2017)|
|400|Benjamin_Coors_SphereNet_Learning_Spherical_ECCV_2018_paper|SphereNet retains the original spherical image connectivity and, by building on regular convolutions, enables the transfer of perspective CNN models to omnidirectional inputs.|
|||Furthermore, unlike our work which builds on regular convolutions and is compatible with modern CNN architectures, it is non-trivial to integrate either graph or spherical convolutions into network architectures for more complex computer vision tasks like object detection.|
|||While currently only few large omnidirectional datasets exist, there are many trained perspective CNN models available, which our method enables to transfer to any omnidirectional vision task.|
|||3.1 Kernel Sampling Pattern  The central idea of SphereNet is to lift local CNN operations (e.g.|
|||This figure compares the sampling locations of SphereNet (red) to the sampling locations of a regular CNN (blue) at the boundaries of the equirectangular image.|
|||By changing the sampling locations of the convolutional kernels while keeping their size unchanged, our model additionally enables the transfer of CNN models between different image representations.|
|||Similarly, the graph convolutional baseline (GCNN) uses graph-conv layers with 32 and 64 filters of polynomial order 25 each, while the spherical CNN baseline (S2CNN) uses an S2-conv layer with 32 filters and a SO(3)-conv layer with 64 filters.|
|||Because the graph and spherical convolution baselines are not applicable to the object detection task, we compare the performance of SphereNet to a CNN operating on the cube map (CubeMapCNN) and equirectangular representation (EquirectCNN).|
|||The performance of the EquirectCNN model is improved when the kernel size is enlarged towards the poles (EquirectCNN++), but all EquirectCNN models perform worse than the CNN operating on a cube map representation (CubeMapCNN).|
|||By updating the sampling locations of the convolutional filters we allow for easily transferring perspective CNN models to handle omnidirectional inputs.|
||10 instances in total. (in eccv2018)|
|401|Hard Mixtures of Experts for Large Scale Weakly Supervised Vision|However, there is still no effective method for training large CNNs that do not fit in the memory of a few GPU cards, or for parallelizing CNN training.|
|||We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.|
|||Second, we give evidence that in the setting of weakly supervised tag prediction from images, on the large datasets that are available today, standard CNN models are underfitting.|
|||Advantages and liabilities of hard mixtures of  experts versus standard CNNs  The models proposed above have some important scalability advantages compared to a standard CNN with K times as many feature maps.|
|||large CNN with K times as many feature maps, a naive forward could cost as much as K times as much.|
|||On the other hand, the models described here are inefficient compared to a standard CNN in terms of modeling power per parameter and in terms of data usage.|
|||In these situations, one quickly runs up against hardware and algorithmic limitations of training a standard CNN with serial stochastic gradient descent, and hard mixtures of experts become attractive.|
|||In particular, to make a CNN that has K times as many parameters, it is infeasible to scale the number of feature maps by K, but making a K expert hard mixture model is practical.|
|||We can see that for both shared decoders and independent decoders, and for each base CNN architecture, the mixture of experts model has significantly better test loss, better q@m, and better p@m than the base model.|
|||After training a CNN for the tag prediction task, we follow standard practice and fix all the layers of the CNN except the decoder.|
||10 instances in total. (in cvpr2017)|
|402|Qiang_Qiu_ForestHash_Semantic_Hashing_ECCV_2018_paper|We propose to first randomly group arriving classes at each tree split node into two groups, obtaining a significantly simplified two-class classification problem that can be a handled with a light-weight CNN weak learner.|
|||Code uniqueness is achieved via the random class grouping, whilst code consistency is achieved using a low-rank loss in the CNN weak learners that encourages intra-class compactness for the two random class groups.|
|||The non-conventional low-rank loss adopted for CNN weak learners encourages code consistency by minimizing intra-class variations and maximizing inter-class distance for the two random class groups.|
|||ForestHashForestHash  3  We further adopt a non-conventional low-rank loss for CNN weak learners to encourage code consistency by minimizing intra-class variations and maximizing interclass distance for the two random class groups, thereby preserving similarity.|
|||Network structures of light-weight CNN learners.|
|||We use the CNN suffix when using a light-weight CNN as weak learner.|
|||Table 1 shows two network structures of light-weight CNN learners, CNN2 and CNN4, adopted in experiments.|
|||For reference, the bottom group shows the performance of ForestHash with CNN features extracted from the 32x32 RGB images.|
|||We also observe that using softmax loss only for CNN learners leads to performance degradation.|
|||ForestHash with a two-layer CNN significantly outperform all compared methods.|
||10 instances in total. (in eccv2018)|
|403|Chang_Aesthetic_Critiques_Generation_ICCV_2017_paper|The CNN model is also trained for regression and then used to select the most interesting aspect of the input image.|
|||Most of them [12, 11] apply a CNN-RNN framework comprised of high-level features extracted from a CNN model trained on object recognition and the Recurrent Neural Networks (RNN) language model.|
|||With the success of deep learning techniques, some approaches [19, 14, 17, 16] exploit deep CNN to learn powerful representations from the data in an end-to-end manner  3515  b r i g h t   g r e e n s  C o l o r : t h e   c o l o r   i s   g r e a t   i  l o v e   t h e    a n d   t h e   t r e e   b r a n c h e s  C o m p o s i t i o n : i  l i k e   t h e   w a y   y o u   h a v e    u s e d   t h e   r u l e   o f   t h i r d s   t o   m a k e   t h e    I   w o u l d   l i k e   t o   s e e   m o r e   o f   t h e   s k y    i m a g e   m o r e   i n t e r e s t i n g   c o m p o s i t i o n   a n d    Figure 3: Examples of the captions generated by our approach for different aspects (composition and color) of the same picture.|
|||[16] introduced a doublecolumn CNN architecture that uses holistic images and image patches as global and local features respectively.|
|||[17] proposed a multiple instance learning CNN model that generates multiple patches from a single image.|
|||Given a training caption (desired output) Ci comprised of the words {w1, w2,    , wT }, a total of T + 2 feature vectors {x1, x0, x1,    , xT } are fed into the LSTM model, where x1 is the feature vector extracted from the CNN for the input image i, x0 is a special START token, and xt are the feature vectors converted from wt in the feature-embedded layer for t = 1    T .|
|||To choose the aspect of appealing, we use the CNN model to train L predictors based on the pairs {(i; pi,l)} (l = 1    L) in our dataset.|
|||The output of the CNN model has L nodes, each of which has a regression output in the range [0,1].|
|||Because the aspect-oriented hidden annotations, together with the image CNN features and word sequences of captions, are fed into the LSTM model (blue part) to generate the output caption of our AF approach, the sentences in different aspects are likely to be softly merged in the learned model to enhance the formation of captions.|
|||As MSCOCO is large about object descriptions and PCCD is relatively small on the aesthetic critiques, when training either the AO or the CNN-LSTM-WD approach, the CNN pre-trained on MSCOCO are fixed to keep its object-description capability, and only the LSTMs are finetuned on PCCD.|
||10 instances in total. (in iccv2017)|
|404|Kuen_Recurrent_Attentional_Networks_CVPR_2016_paper|A deconvolutional network (DecNN) is a variant of CNN that performs convolution and unpooling to produce dense pixel-precise outputs.|
|||To tackle dense prediction tasks in the multi-layered convolutional learning setting, one can append a deconvolutional network (DecNN) to a CNN as shown in [36].|
|||In such a convolutional-deconvolutional (CNN-DecNN) framework, the CNN learns globally meaningful representations, while the DecNN upsizes feature maps and learns increasingly localized representations.|
|||In this paper, we employ the simple unpooling method demonstrated in [10], whereby each block (with spatial size 11) in the input feature maps is mapped to the top left corner of a blank output block with spatial size k  k. This effectively increases the spatial size of the whole feature maps by a factor of k.  In the processing pipeline of CNN-DecNN for saliency detection, the CNN first transforms the input image x to a spatially compact hidden representation z, as z = CNN(x).|
|||Then, RACDNN uses a recurrent-based CNN CNNr to encode the attended input xi into a spatially-compact hidden representation zi.|
|||CNNr is similar to CNN except that CNNr is used in the recurrent setting, and all recurrent instances of CNNr share the same network parameters.|
|||As mentioned in Section 3.1, preserving the spatial information of hidden representation between CNN and DecNN is favorable for DecNNs upsizing-related operations.|
|||In RACDNN, the hidden representations (h1  0) at the 0-th iteration are provided by a CNN (sharing the same architectural properties as CNNr) which accepts the whole image region as input.|
|||The CNN part is initialized from the weights of VGG-CNN-S [6], a relatively powerful CNN model pre-trained on ImageNet dataset.|
|||The CNN accepts 224  224 RGB images as inputs, and it outputs a 7  7 feature maps with 256 feature channels.|
||10 instances in total. (in cvpr2016)|
|405|Ma_Learning_Activity_Progression_CVPR_2016_paper|In [3], the detector is trained on CNN features extracted from the action tubes in space-time; however, evaluation is on relatively short video clips (i.e., sevIn [27] eral hundred frames) of relatively short actions.|
|||an LSTM is trained that takes CNN features of multiple neighboring frames as input to detect actions at every frame; while their model is similar to ours, they focus on detecting  1943  4.|
|||At each video frame, the model first computes CNN features (illustrated as fc7) and then the features are fed into the LSTM to compute detection scores of activities and non-activity (BG in the figure).|
|||It contains two major components: a CNN that computes visual features from each video frame, and an LSTM with a linear layer that computes activity detection scores based on the CNN features of the current frame and the hidden states and memory of the LSTM from the previous time step.|
|||We adopt the VGG19 [21] CNN architecture, whose output of the second fully connected layer (fc7) is fed into the LSTM.|
|||Model Training  For the CNN component (see Fig.|
|||In this training phase, the CNN fc7 layer is also further trained together with the LSTM but with a lower starting learning rate of 104.|
|||We evaluate the performance of four models: i) the finetuned VGG19 CNN model; ii) the LSTM model shown in Fig.|
|||The LSTM models greatly outperform the CNN model.|
|||The LSTM models greatly outperform the CNN model on the early detection task.|
||10 instances in total. (in cvpr2016)|
|406|Deep Level Sets for Salient Object Detection|The CNN is built on the VGG16 net and generates coarse saliency level set maps at a resolution of 56*56.|
|||3, we build a CNN based on VGG16 net and replaced the last three Max-pooling layers with dilated convolutional layers [57].|
|||To incorporate the deep convolutional network with the level set method, we linearly shift the saliency values output by the CNN into [-0.5,0.5] and treat it as the level set . Pixel space of the input image is referred as .|
|||The CNN is followed by a Guided Superpixel Filtering layer with a hyperparameter D. The GSF layer takes superpixels and a level  2304  set map  produced by the CNN as input.|
|||The VGG16-based CNN is trained with BCE loss for 15 epochs and fine-tuned with the proposed level set method for 15 epochs (denoted by CNN+LS).|
|||For comparison, another VGG16-based CNN is trained for 30 epochs  2306  Ground(a) Input (b) Truth  (c)  FT  (d)  HC  (e)GMR (f) BSCA (g)MTDS (h) MDF (i) MCDL (j) ELD  (k) LEGS  (l)  CNN  (m)  CNN +LS  Our (n)Final  Figure 7.|
|||CNN is output by the CNN trained with BCE loss.|
|||CNN+LS is output by the CNN trained with the level set method.|
|||CNN represents the CNN trained with BCE loss.|
|||CNN+LS is for the CNN trained with the level set method.|
||10 instances in total. (in cvpr2017)|
|407|End-To-End Training of Hybrid CNN-CRF Models for Stereo|Recent deep CNN models for stereo [12, 28, 55] learn from data to be robust to illumination changes, occlusions,  I0  I1  Unary CNN  Unary CNN  Correlation  CRF  D  Figure 1: Architecture: A convolutional neural network, which we call Unary-CNN computes features of the two images for each pixel.|
|||However, also deep CNN models for stereo rely a lot on post-processing, combining a set of filters and optimization-like heuristics, to produce final accurate results.|
|||We observed that the hybrid CNN+CRF network performs very well already with shallow CNN models, such as 3-7 layers.|
|||Related Work  CNNs for Stereo Most related to our work are CNN matching networks for stereo proposed by [12, 28] and the fast version of [55].|
|||CNN Matching General purpose matching networks are [52] used a matching CNN for also related to our work.|
|||Training Unary CNN in the Pixel(cid:173)wise Model  For the purpose of comparison, we train our UnaryCNN in a pixel-wise mode, similarly to [12, 28, 55].|
|||A suboptimal  (after a fixed number of iterations) will generally  2343  vary when the CNN parameters  and thus the CRF costs f are varied.|
|||We then unify the post-processing step by adding our CRF on top of the CNN outputs.|
|||While the raw output of our CNN is inferior to the compared methods, the post-processing with a CRF significantly decreases the difference in performance.|
|||Benchmark Method CNN +CRF +Joint +PW  Middlebury  Kitti 2015  CNN3 CNN7  CNN3 CNN7 [28] [55]  23.89 11.18 18.58 9.35  28.38 13.08 5.99 13.56  6.33 4.79 4.31 4.45  9.48 8.05  6.11 4.60   9.45 7.88  4.75 4.04   Table 1: Influence of the individual components of our method ( 5.2) and comparison with [28, 55] without post-processing ( 5.3).|
||10 instances in total. (in cvpr2017)|
|408|Croitoru_Unsupervised_Learning_From_ICCV_2017_paper|However, that runtime is still long and prohibitive for training the student CNN that requires millions of images.|
|||We chose this CNN based on its relative simplicity and strong performance.|
|||Post-processing: Our CNN outputs a 32  32 soft mask.|
|||Firstly, we compare the quality of the segmentations obtained by the feedforward CNN against its teacher, VideoPCA (Sec.|
|||On the VID dataset we evaluated the student CNN against its teacher pathway.|
|||Our baseline model is represented by a classic CNN having only the RGB image as input and no skip-connections.|
|||The refined model is our final student CNN model as presented in Figure 2.|
|||We highlight that the overall system performance improves with the amount of selectivity, which shows that a simple quality measure used for soft mask selection can improve the performance of the CNN image-based pathway.|
|||tendency of the single-image CNN net to improve over its teacher.|
|||Moreover, our CNN feed-forward net processes each image in 0.04 sec, being at least one to two orders of magnitude faster than all other methods (see Table 4).|
||10 instances in total. (in iccv2017)|
|409|Xiangyu_Xu_Rendering_Portraitures_from_ECCV_2018_paper|[10] propose a CNN architecture that integrates coarse-scale depth prediction with fine-scale prediction.|
|||Since the DoF blur is spatiallyvariant, we adopt the RNN filters [21] instead of using a CNN which has the  Rendering Portraitures from Monocular Camera and Beyond  9  Fig.|
|||The proposed network contains two groups of RNNs for image filtering and a deep CNN to learn the guidance map with our refined depth and segmentation estimation.|
|||We use an encoder-decoder CNN to generate the guidance map for the following RNN which combines two groups of recursive filters in a cascaded scheme.|
|||We also train a CNN network with refined depth and segmentation maps as additional input to compare with our spatially-variant  Table 2.|
|||[21] Ours CNN Ours RNN  PSNR (dB) SSIM  31.55 0.9235  33.55 0.9432  37.35 0.9723  40.74 0.9868  14  X. Xu, D. Sun, S. Liu, W. Ren, Y. Zhang, M. Yang, J.|
|||Sun  (a) Input  (b) Xu [31]  (c) Liu [21]  (d) Ours CNN (e) Ours RNN (f) Ours CRF  Fig.|
|||This CNN has the same encoder-decoder structure as the guidance network in Section 3.4.|
|||The CNN-based methods (Figure 12(b) and (d)) incorrectly blur the foreground, such as the textures of the clothes, because CNN uses uniform kernel at different spatial locations and cannot well handle the spatially-variant DoF case.|
|||: Unsupervised cnn for single view  depth estimation: Geometry to the rescue.|
||10 instances in total. (in eccv2018)|
|410|Multigrid Neural Architectures|On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.|
|||Together, our results suggest that continuous evolution of features on a multigrid pyramid is a more powerful alternative to existing CNN designs on a flat grid.|
|||In fact, there are shortcomings to the typical CNN pipeline:   It conflates abstraction and scale.|
|||The first layer in a standard CNN consists of many filters independently looking at tiny, almost meaningless regions of the image.|
|||Top: Standard CNN architectures conflate scale with abstraction (depth).|
|||As Section 2 reviews, recent CNN architectural innovations ignore scale-space routing capacity, focusing instead on aspects like depth.|
|||While prior CNN designs have varied filter size (e.g.|
|||We can think of a multigrid CNN as a standard CNN in which every grid is transformed into a pyramid.|
|||We view a multigrid CNN as a nonlinear process, on a similar pyramid, with the same communication structure.|
|||These new abilities suggest that multigrid could replace some adhoc designs in the current zoo of CNN architectures.|
||10 instances in total. (in cvpr2017)|
|411|cvpr18-Making Convolutional Networks Recurrent for Visual Sequence Learning|In accordance with different backbone CNN architectures, the traditional RNN in (a, c) stacks the recurrent layer on top of the last fc layer or conv layer, while our PreRNN in (b, d) makes the pre-trained CNNs recurrent by directly transforming the pre-trained fc layer or conv layer into the recurrent layer.|
|||It is well explored in the literature [11, 34] that CNN models, pre-trained on large-scale image or video datasets, retain strong semantic and generality properties.|
|||Prior methods typically introduce a single or a stack of recurrent layers on top of the last layer1 of a pretrained CNN and then train the whole network together.|
|||It thus requires the entire recurrent layers to be trained from scratch, even though a pre-trained CNN is used for feature extraction.|
|||This can mitigate the difficulty of training RNNs, as we reuse parts of a pre-trained CNN as a partially pre-trained RNN.|
|||It therefore pushes the generalization ability of a pre-trained CNN onto the RNN and ultimately improves the overall performance.|
|||Transformations for VRNN  To be comprehensive in term of different backbone CNN architectures, we assume that the last fc or conv layer of a pre-trained CNN has the structure:  y = H(W xy  x),  (4)  where W xy are the pre-trained feedforward weights, x and y are the input and output of this layer, and  indicates matrix multiplication for the fc layer or convolution operation for the conv layer.|
|||We employ the pre-trained VGG16 [38] on ImageNet [9] as the backbone CNN and the l2 loss as our objective function, and change the output layer to 136 units corre sponding to the locations of 68 facial landmarks.|
|||We use the ResNet50 model [20] pre-trained on ImageNet [9] as the backbone CNN and the negative log likelihood (NLL) as the loss function.|
|||However, [13] employs a more powerful CNN (i.e., ResNet152) to the temporal stream, and [46] relies on two more input modalities (i.e., warped flow in iDT and difference images).|
||10 instances in total. (in cvpr2018)|
|412|cvpr18-A Pose-Sensitive Embedding for Person Re-Identification With Expanded Cross Neighborhood Re-Ranking|We demonstrate that learning and combining view specific feature maps on a standard underlying CNN architecture results in a significantly better re-id embedding.|
|||In summary, our contributions are threefold: 1) We propose a new CNN embedding which incorporates coarse and fine-grained person pose information.|
|||Both methods can be simultaneously incorporated into the same baseline CNN architecture and our experiments show that a combination of the two achieves a higher accuracy than either one alone.|
|||An overview of our CNN architecture with both types of pose information is depicted in Figure 2.|
|||To further increase this flexibility, we do not rely on the final keypoint decisions of the DeeperCut approach, but instead provide our re-id CNN with the full confidence map for each keypoint.|
|||For all our CNN embeddings we employ the same train ing protocol.|
|||Study of Pose Information  We investigate the usefulness of including different granularities of pose information into the CNN by performing separate experiments with only view information, only pose information, and a combination of both.|
|||To show that our proposal is not strictly dependent on the underlying CNN ar 424  CNN  Method  Market-1501  mAP R-1  R-5  R-10 R-50 mAP R-1  Duke R-5 R-10 R-50  Inception-v4  Baseline  Views only Pose only  PSE  ResNet-50  Baseline  Views only Pose only  PSE  51.9 61.9 60.9 64.9 59.8 66.9 61.6 69.0  75.9 81.5 81.7 84.4 82.6 88.2 82.8 87.7  89.8 92.3 91.8 93.1 92.4 95.4 93.1 94.5  92.5 94.9 94.4 95.2 94.9 97.2 95.5 96.8  97.3 98.1 97.9 98.4 98.2 98.9 98.3 99.0  36.6 40.3 48.2 50.4 50.3 56.7 53.1 62.0  61.8 62.7 70.5 71.7 71.5 76.9 73.4 79.8  74.8 76.6 81.9 83.5 83.1 87.3 84.5 89.7  79.8 81.1 86.1 87.1 87.0 90.7 88.1 92.2  89.4 90.3 92.7 93.1 94.1 95.7 94.3 96.3  Table 1.|
|||We showed that both the fine and coarse body pose cues are important for re-id and proposed a new pose-sensitive CNN embedding which incorporates these.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||10 instances in total. (in cvpr2018)|
|413|Sun_ProNet_Learning_to_CVPR_2016_paper|It replaces the fullyconnected layers of a standard CNN (e.g.|
|||Among those utilizing bounding box annotations, RCNN [9] achieves competitive performance by directly representing image boxes with CNN features and learning classifiers on top of the features.|
|||A typical CNN architecture for image classification task (e.g.|
|||Compared with region sampling with sliding windows or bounding box proposals, FCNs offer a seamless solution for end-to-end training under the CNN framework, and also naturally allow the sharing of intermediate features over overlapping image regions.|
|||Similar to object detection frameworks, we run CNN classifiers on the selected boxes.|
|||A small subset of proposed boxes with high scores  images  Multi-scale FCN  boxes  Verification CNN  boxes  Multi-scale FCN  boxes  Verification CNN  Scores  Locations  Scores  Locations  Scores  Locations  Scores  Locations  images  Multi-scale FCN  Scores  Locations  subset   of boxes  subset   of boxes  subset   of boxes  Verification   Verification   Verification    CNN #1   CNN #2   CNN #3  Scores  Scores  Scores  Locations  Locations  Locations  Figure 4.|
|||Since ProNet uses cascades or trees of CNNs, it can apply a more powerful CNN model VGG-16 with small computational overhead.|
|||Since the boxes generated by our proposal CNN have fixed aspect ratios, we follow [19] to aggregate the heat maps over 1000 bounding box proposals generated by selective search per image.|
|||Cascade CNN is then used to verify the high-scoring proposals.|
|||The cascade CNN improves the mAP to 15.5%.|
||10 instances in total. (in cvpr2016)|
|414|Guosheng_Hu_Deep_Multi-Task_Learning_ECCV_2018_paper|Existing MTL methods follow a design pattern of shared bottom CNN layers and taskspecific top layers.|
|||Our approach is composed of (1) a novel MTL framework that automatically learns which layers to share through optimisation under tensor trace norm regularisation and (2) an invariant representation learning approach that allows the CNN to leverage tasks defined on disjoint datasets without suffering from dataset distribution shift.|
|||The conventional deep MTL predefines the first few CNN layers as shared by multiple tasks, and then forks into different layers for different tasks with different losses.|
|||This is particularly tricky with increasingly deep CNN architectures.|
|||This is the first database for subtle expression analysis, the first database for recognising cognitive states from facial expressions, and it is big enough for deep CNN training.|
|||E.g., using a CNN to find facial landmarks as well as recognise facial attributes [61, 41].|
|||Learning the CNN with tensor trace norm regularisation means that the ranks of these tensors are minimised where possible, and thus knowledge is shared where possible.|
|||However in order to apply trace norm-based regularisation end-to-end in CNNs, we wish to optimise trace norm and standard CNN losses using a single gradient-based optimiser such as Tensorflow [1].|
|||Task t is modelled by a CNN parametrised by t = {(1) t } where L is the number of layers, and we split t into two sets at the l-th layer.|
|||2.4 CNN Architecture for Deep MTL  In this study, we implement our deep MTL based on the well known Residual Network (ResNet) architecture [17].|
||10 instances in total. (in eccv2018)|
|415|Mimicking Very Efficient Network for Object Detection|Mimicking Very Efficient Network for Object Detection  Quanquan Li1, Shengying Jin2, Junjie Yan1  1SenseTime  2Beihang University  liquanquan@sensetime.com, jsychffy@gmail.com, yanjunjie@outlook.com  Abstract  Method  MR2 Parameters  test time (ms)  Current CNN based object detectors need initialization from pre-trained ImageNet classification models, which are usually time-consuming.|
|||In this paper, we present a fully convolutional feature mimic framework to train very efficient CNN based detectors, which do not need ImageNet pre-training and achieve competitive performance as the large and slow models.|
|||Compared with traditional methods such as DPM [12], these CNN based frameworks achieve good performance on challenging dataset.|
|||Since the pioneering work R-CNN [14], CNN based object detectors need a pre-trained ImageNet classification model for initialization to get the desired performance.|
|||Our solution for mimicking in object detection comes from observation of modern CNN based detectors, including Faster R-CNN [28], R-FCN [6], SSD [25] and YOLO [27].|
|||In CNN based detection, we only need 1/4 computation if we can reduce the width and height of the input image by half.|
|||Related Work  The related work includes recent CNN based object detections, network mimicking and network training, as well as network acceleration.|
|||A seminal CNN based object detection method is RCNN [14], which uses the fine-tuned CNN to extract features from object proposals and SVM to classify them.|
|||The mimicking technique we proposed is validated on Faster R-CNN and R-FCN, but it can be naturally extended to SSD, YOLO and other CNN feature map based methods.|
|||[7, 20, 24] accelerate single layer of CNN through linear decomposition , while [38] considers the nonlinear approximation.|
||10 instances in total. (in cvpr2017)|
|416|Xiao_Learning_From_Massive_2015_CVPR_paper|However, we find that training a CNN from scratch with limited clean labels and massive noisy labels is better than finetuning it only on clean labels.|
|||Our work is inspired by [24], which modifies a CNN by inserting a linear layer on top of the softmax layer to map clean labels to noisy labels.|
|||Researchers proposed to conquer this problem by first initializing CNN parameters with a model pretrained on a larger yet related dataset, and then finetuning it on the smaller dataset of specific task [1, 7,12,21].|
|||In our case of clothing classification, we find that training a CNN from scratch with limited clean labels and massive noisy labels is better than finetuning it only on the clean labels.|
|||Denote the parameter set of each CNN by (cid:18)1 and (cid:18)2.|
|||Images and their ground truth labels in the dataset Dc are used to train the CNN that predicts p(yjx).|
|||(8) are used to train the CNN that predicts p(zjx).|
|||After both CNN components are properly pretrained, we can start to train the whole network with massive noisy labeled data.|
|||Then each CNN receives two kinds of gradients, one is from the clean labels and the other is from the noisy labels.|
|||From row #1 we can see that training a CNN from scratch with only small amount of clean data can result in bad performance.|
||10 instances in total. (in cvpr2015)|
|417|NIKOLAOS_ZIOULIS_OmniDepth_Dense_Depth_ECCV_2018_paper|In this work, we train a CNN to learn to estimate a scenes depth given an omnidirectional (equirectangular) image as input1.|
|||We propose and validate, a CNN auto-encoder architecture specifically de signed for estimating depth directly on equirectangular images.|
|||After retrieving the scenes depth by stereo matching and subsequently calculating the normals, the equirectangular image is projected to the faces of a cube that are then fed to a CNN whose object predictions are fused into the 360o image to finally reconstruct the 3D layout.|
|||Nowadays, with the establishment of CNNs, there are two straightforward ways to apply current CNN processing pipelines to spherical input.|
|||Either directly on a projected (typically equirectangular) image, or by projecting the spherical content to the faces of a cube (cubemap) and running the CNN predictions on them, which are then merged by back-projecting them to the spherical domain.|
|||The former approach, applying a CNN directly to the equirectangular image, was opted for in [23] to increase the dynamic range of outdoor panoramas.|
|||[34] trained a CNN in a coarse-to-fine scheme using direct depth supervision from RGB-D images.|
|||Finally, in [52] it was shown that a CNN can be trained to estimate depth from monocular input with only relative depth annotations.|
|||4 Omnidirectional Depth Estimation  The majority of recent CNN architectures for dense estimation follow the autoencoder structure, in which an encoder encodes the input, by progressively decreasing its spatial dimensions, to a representation of much smaller size, and a decoder that regresses to the desired output by upscaling this representation.|
|||Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||10 instances in total. (in eccv2018)|
|418|Liu_Recurrent_Scale_Approximation_ICCV_2017_paper|The whole system could be trained end-to-end in a unified CNN framework.|
|||The location variations can be naturally solved via sliding window, which can be efficiently incorporated into CNN in a fully convolutional manner.|
|||However, CNN itself does not have an inherent mechanism to handle the scale variations.|
|||In this paper, we extend the spirit of fast feature pyramid to CNN and go a few steps further.|
|||Our solution to the feature pyramid in CNN descends from the observations of modern CNN based detectors, including Faster-RCNN [27], R-FCN [4], SSD [21], YOLO [26] and STN [2], where feature maps are first computed and the detection results are decoded from the maps afterwards.|
|||The three components can be incorporated into a unified CNN framework and trained end-to-end.|
|||To sum up, our contributions in this work are as follows: 1) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit (RSA), which fully leverages efficiency and accuracy.|
|||Recently, some CNN based methods [16, 28] also employ such a spirit to predict the objectness and class within a sliding window in each scale.|
|||Theoretically RSA can handle all scales of features in a deep CNN model and therefore can be branched out at any depth of the network.|
|||Conclusion  In this paper, we prove that deep CNN features for an image can be approximated from a large scale to smaller scales by the proposed RSA unit, which can significantly accelerate face detection and achieve comparable results in object detection.|
||10 instances in total. (in iccv2017)|
|419|Liu_Learning_Efficient_Convolutional_ICCV_2017_paper|Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models.|
|||We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets.|
|||As an example, storing a typical CNN trained on ImageNet consumes more than 300MB space, which is a big resource burden to embedded devices.|
|||A large CNN may take several minutes to process one single image on a mobile device, making it unrealistic to be adopted for real applications.|
|||Many works have been proposed to compress large CNNs or directly learn more efficient CNN models for fast inference.|
|||Our approach imposes L1 regularization on the scaling factors in batch normalization (BN) layers, thus it is easy to implement without introducing any change to existing CNN architectures.|
|||Experiments on several benchmark datasets and different network architectures show that we can obtain CNN models with up to 20x mode-size compression and 5x reduction in computing operations of the original ones, while achieving the same or even higher accuracy.|
|||It can be applied to any typical CNNs or fullyconnected networks (treat each neuron as a channel), and the resulting network is essentially a thinned version of the unpruned network, which can be efficiently inferenced on conventional CNN platforms.|
|||1), if we add scaling layers to a CNN without BN layer, the value of the scaling factors are not meaningful for evaluating the importance of a channel, because both convolution layers and scaling layers are linear transformations.|
|||The network slimming process introduced above can be directly applied to most plain CNN architectures such as AlexNet [22] and VGGNet [31].|
||10 instances in total. (in iccv2017)|
|420|Xingang_Pan_Two_at_Once_ECCV_2018_paper|Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNNs modeling ability on one domain (e.g.|
|||For instance, by finetuning a CNN pretrained on Cityscapes using the data from GTA5, we are able to adapt the features learned from Cityscapes to GTA5, where accuracy can be increased.|
|||But even so, the appearance gap is not eliminated, because when applying the finetuned CNN back to Cityscapes, the accuracy would be significantly degraded.|
|||First, different from previous CNN structures that isolate IN and BN, IBNNet unifies them by delving into their learned features.|
|||training a CNN on Cityscapes and evaluating it on GTA5 without finetuning and vice versa, ResNet50 integrated with IBN-Net improves its counterpart by 8.5% and 7.5% respectively.|
|||However, there are two limitations in the above CNN architectures.|
|||In this work, we increase the modeling capacity and generalization ability across domains by designing a new CNN architecture, IBN-Net.|
|||It has become a standard component in most prevalent CNN architectures like Inception [26], ResNet [8], DenseNet [13], etc.|
|||To demonstrate the stronger model capacity of IBN-Net over traditional CNNs, we compare its performance with a number of recently prevalent CNN architectures on the ImageNet validation set.|
|||Pan, X., Shi, J., Luo, P., Wang, X., Tang, X.: Spatial as deep: Spatial cnn for  traffic scene understanding.|
||10 instances in total. (in eccv2018)|
|421|Song_Deep_Spatial-Semantic_Attention_ICCV_2017_paper|However, recent efforts [6, 22] on visualising what each layer of a CNN actually learns  15551  show that higher-layers of the network capture more abstract semantic concepts but not fine-grained detail, motivating fine-grained recognition methods to work with convolutional feature maps instead [17].|
|||First, we introduce attention modelling in each branch of the CNN so that computation for representation learning is focused on specific discriminative local regions rather than being spread evenly over the whole image.|
|||Including fine-grained information in the CNN feature output enables discrimination based on subtle details, but has two risks: misalignment in the feature channels between the two branches, and greater feature noise due to each fine-grained feature having less supporting cues.|
|||Fusing different CNN layers in the model output has been exploited in many problems such as edge detection (e.g., [30, 43]), pose estimation (e.g., [27]) and scene classification (e.g., [7, 45, 19].|
|||However, instead of making the network deeper, we use it in the attention module to make the attention module output robust against imprecise attention mask caused by cross-domain feature misalignment, as well as in the final CNN output layer to preserve both coarse and fine-grained information in the learned representation.|
|||It is a Siamese network with three CNN branches, corresponding to a query sketch, a positive photo and a negative photo respectively.|
|||Similar to [46], the CNN base net is the Sketch-a-Net [47] which was originally designed for sketch recognition.|
|||In our model, the attention module is added to the output of the fifth convolutional+pooling  5553  layer of the CNN in each branch (see Fig.|
|||HOLEF Loss  Triplet Loss with a First-order Energy Function For a given triplet t = (s, p+, p) consisting of a query sketch s, a positive photo p+ and a negative photo p, a conventional triplet ranking loss can be written as:  L (cid:0)s, p+, p(cid:1) = max(0,  + D (cid:0)F (s) , F (cid:0)p+(cid:1)(cid:1)   D (cid:0)F (s) , F (cid:0)p(cid:1)(cid:1)),  (4)  where  are the parameters of the CNN with attention network, F() denotes the output of the corresponding network branch, i.e., f f inal,  is the required margin of ranking for the hinge loss, and D(, ) denotes a distance between the two input representations, typically Euclidean distance.|
|||Bilinear cnn mod [21] J. Lu, J. Yang, D. Batra, and D. Parikh.|
||10 instances in total. (in iccv2017)|
|422|Filip_Radenovic_Deep_Shape_Matching_ECCV_2018_paper|Top retrieved images from the Oxford Buildings dataset [1]: CNN with an RGB input [2] (left), and our shape matching network (right).|
|||Starting from a pre-trained classification network stripped off the fully connected layers, the CNN is fine-tuned using a simple contrastive loss function.|
|||Prior domain generalization methods [35,36,37] are shown effective on PACS, while simply training a CNN on all the available (seen) domains is a very good baseline [39].|
|||The process of fine-tuning the CNN is described in Section 3.1, while the final representation and the way it is used for retrieval and classification is detailed in Section 3.2.|
|||The CNN is initialized by the parameters learned on a large scale annotated image dataset, such as ImageNet [46].|
|||The CNN is trained with Stochastic Gradient Descent in a Siamese fashion with contrastive loss [51].|
|||Direct application of the off-the-shelf CNN on edge maps already outperforms most prior hand-crafted methods (see Table 3).|
|||Radenovi c, F., Tolias, G., Chum, O.: CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples.|
|||Tolias, G., Sicre, R., J egou, H.: Particular object retrieval with integral max pooling of cnn activations.|
|||Iscen, A., Tolias, G., Avrithis, Y., Furon, T., Chum, O.: Efficient diffusion on region manifolds: Recovering small objects with compact CNN representations.|
||10 instances in total. (in eccv2018)|
|423|MDNet_ A Semantically and Visually Interpretable Medical Image Diagnosis Network|They typically employ pre-trained powerful CNN models, such as GoogLeNet [28], to provide image features.|
|||Baseline We choose the well-known image captioning scheme [14, 33] (the source code of [14]) as the baseline, which is to first train a CNN to represent images, followed by training an LSTM to generate descriptions.|
|||P, F, and J denote whether a pre-trained CNN is used, whether fine-tuning pre-trained CNNs when training LSTM, and whether using our proposed joint training approach (i.e.|
|||For all trained models, we observe the DCA of the language model strongly relies on that of corresponding image model (see Figure 7(middle)), which motivates us to analyze more about CNN training itself.|
|||(12), module M provides a standard CNN loss.|
|||If we interpret LL from module L as noise added onto the gradient D LM , this noise disturbs the loss of module M and overD all CNN training.|
|||Therefore, our optimization behaves particular regularization on CNN to overcome overfitting.|
|||As compared in Figure 7(right), the image model of MDNet trained using our optimization approach outperforms pre-trained CNN models using stochastic gradient descent (SGD).|
|||Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning.|
|||Disturblabel:  Regularizing cnn on the loss layer.|
||10 instances in total. (in cvpr2017)|
|424|Lin_Deep_LAC_Deep_2015_CVPR_paper|The large flexibility of CNN structures makes finegrained recognition still have much room to improve.|
|||Although research of [17, 10] showed that CNN features are reasonably robust to scale and rotation variation, it is necessary to directly capture these types of change to increase the recognition accuracy [27, 4].|
|||In what follows, we first describe localization and classification sub-networks, which are implemented according to the CNN model [13].|
|||This classification CNN [13] is expressed as  y = fc(Wc; ),  (3)  where Wc is the weight parameter set in this sub-network.|
|||In implementation, we modify the Caffe platform [11] for CNN construction.|
|||All CNN models are fine-tuned using the pretrained ImageNet model.|
|||The 6th layer of the CNN classification models (i.e., two part models + one whole image model) is extracted to form a 4096  3D feature.|
|||Then we follow the popular CNN-SVM scheme [19] to train a SVM classifier on our CNN feature.|
|||We finally tune the CNN model based on the whole image using the pre-trained model [11].|
|||[27] also considers the same head and body parts and combines the CNN feature of the whole image.|
||10 instances in total. (in cvpr2015)|
|425|cvpr18-Recognizing Human Actions as the Evolution of Pose Estimation Maps|[1] deeply merged rank pooling method with CNN to generate an efficient dynamic image network.|
|||As CNN has achieved success in image classification task, we use CNN model that is pre-trained on Imagenet [7] for transfer learning.|
|||Since these two images contain significantly different spatial structure, we use separate CNN to explore deep features from them.|
|||To accommodate existing CNN models, the single channel of body shape evolution image is repeated three times to form a 3channel image, and two channels of body pose evolution image are combined with a zero-valued channel to form a 3-channel image.|
|||Implementing details: In our model, each CNN contains five convolutional layers and three f c layers.|
|||When the CNN model achieves 99% accuracy on the training set, the training procedure is stopped beforehand.|
|||To reduce the effect of random parameter initialization and random sampling, we repeat the training of CNN model for five times and report the average results.|
|||These results show that depth information can improve the recognition, but the influence of depth channel drops when large scale training data is used, as well-trained CNN model may infer depth cues from 2D pose.|
|||Generally,  Table 2: Comparisons between our proposed method and state-of-the-art methods on NTU RGB+D dataset  Method  HON4D [32]  Super Normal Vector [50]  Skeletal Quads [10]  Lie Group [40] HBRNN-L [9]  FTP Dynamic Skeletons [16]  Deep RNN [35] Deep LSTM [35]  2 Layer P-LSTM [35]  ST-LSTM + Trust Gate [25] Unsupervised Learning [29]  LieNet-3Blocks [17]  GCA-LSTM network [26]  Body-part appearance + skeleton [33]  Clips + CNN + MTLN [20]  View-invariant [28]  Proposed Method: H1 + H3 Proposed Method: S2 + H3  Year 2013 2014 2014 2014 2015 2015 2016 2016 2016 2016 2017 2017 2017 2017 2017 2017   CS  CV  30.56% 7.26% 31.82% 13.61% 38.60% 41.40% 50.10% 52.80% 59.07% 63.97% 60.23% 65.22% 59.29% 64.09% 60.69% 67.29% 62.93% 70.27% 69.20% 77.70% 56.00% 61.37% 66.95% 74.40% 82.80% 75.20% 83.10% 79.57% 84.83% 80.03% 87.21% 78.80% 84.21% 91.71% 95.26%   Table 3: Comparisons between our proposed method and state-of-the-art methods on UTD-MHAD dataset  Sensor Kinect Kinect Inertial  Method  Cov3DJ [18]  Kinect [6] Inertial [6]  Kinect + Inertial  Kinect&Inertial [6]  Kinect Kinect Kinect Kinect RGB Kinect  JTM [44]  Optical Spectra [15] 3DHOT-MBC [53]  JDM [23]  Proposed Method: H1 + H3 Proposed Method: S2 + H3  Year 2013 2015 2015 2015 2016 2016 2017 2017   CS  85.58% 66.10% 67.20% 79.10% 85.81% 86.97% 84.40% 88.10% 92.84% 94.51%  2D pose from video can only compete with that from depth video in simple scenes; 2D pose from video can barely achieve the performance of 3D pose from depth video.|
|||We further select one frame for each video and use CNN to extract deep features.|
||10 instances in total. (in cvpr2018)|
|426|Rad_BB8_A_Scalable_ICCV_2017_paper|This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging.|
|||We then apply a CNN to predict the 3D pose of the detected objects.|
|||As we will explain in more details, we can then always use the CNN trained on the restricted range to estimate any pose.|
|||[13] replaces this energy function by an energy computed from the output of a CNN trained to compare observed image features and features computed  3829  from a 3D rendering of the potentially detected object.|
|||Like us, [12] relies on a CNN to directly predict a 3D pose, but in the form of a translation and a rotation.|
|||[6] also uses a CNN to predict the 3D pose of generic objects but from RGB-D data.|
|||We therefore introduce a CNN classifier k() to predict at run-time if  is in r1 or r2: If  is in r1, we can estimate the pose as before; If  is in r2, one option would be to apply another g() network trained for this range.|
|||4, we train another CNN that predicts an update to improve the pose.|
|||Because this CNN takes 4 or 6 channels as input, it is not clear how we can use VGG, as we did for the previously introduced networks, and we use here one CNN per object.|
|||More formally we train this CNN by minimizing:  X  X  X  kProje,t(Mo  i )  Proje,t(Mo  i )  (e,t)N (e,t)  i  (W,e,t)T  mi(h(W, Render(e, t)))k2 , (5) where h denotes the CNN,  its parameters; N (e, t) is a set of poses sampled around pose (e, t), and Render(e, t) a function that returns a binary mask, or a color rendering, of the target object seen from pose (e, t).|
||10 instances in total. (in iccv2017)|
|427|cvpr18-Hand PointNet  3D Hand Pose Estimation Using Point Sets|[8] encodes the hand depth images as 3D volumes and applies a 3D CNN for inferring 3D hand  pose.|
|||[50] train a CNN to regress hand model parameters and infer hand pose via forward kinematics.|
|||[7, 8] apply multi-view CNNs and 3D CNN for 3D hand pose estimation which take projected images on multiple views and 3D volumes as input, respectively, in order to better utilize the depth information.|
|||When N = 512, the estimation accuracy is slightly lower than the other two results with larger number of sampled  8422  100 %  90 %  NYU Dataset     100 %  90 %  MSRA Dataset     100 %  90 %  ICVL Dataset        D <   r o r r e    t s r o w h     t i  w   s e m a r f   f     o n o  i t r o p o r P  80 %  70 %  60 %  50 %  40 %  30 %  20 %  10 %  0 %    0  t i  t i     D <   r o r r e    t s r o w h     w   s e m a r f   f     o n o  i t r o p o r P  80 %  70 %  60 %  50 %  40 %  30 %  20 %  Heatmap [38] (20.8mm) DeepPrior [20] (19.8mm) Feedback [21] (16.2mm) DeepModel [49] (16.9mm) DeepHand [29] Crossing Nets [41] (15.5mm) LieX [46] (14.5mm) 3D CNN [8] (14.1mm) Hallucination Heat [3] REN [9] (13.4mm) DeepPrior++ [19] (12.3mm) Ours (10.5mm)     D <   r o r r e    t s r o w h     w   s e m a r f   f     o n o  i t r o p o r P  80 %  70 %  60 %  50 %  40 %  30 %  20 %  Hierarchical [33] (15.2mm) Collaborative Filtering [4] Multiview CNNs [7] (13.1mm) LSN, Finger Jointly Regression [42] LSN, Pose Classification [42] Crossing Nets [41] (12.2mm) 3D CNN [8] (9.6mm) DeepPrior++ [19] (9.5mm) Ours (8.5mm)  10  20  30 50 D: error threshold (mm)  40  60  70  80  30  D: error threshold (mm)  40  50  60  70  80  10 %  0 %    0  10  20  10 %  0 %    0  10  20  LRF [34] (12.6mm) Hierarchical [33] (9.9mm) DeepPrior [20] (10.4mm) DeepModel [49] (11.6mm) LSN [42] (8.2mm) Crossing Nets [41] (10.2mm) REN [9] (7.6mm) DeepPrior++ [19] (8.1mm) Ours (6.9mm)  30  40  50  60  70  80  D: error threshold (mm)  Figure 8: Comparison with state-of-the-art methods on NYU [38] (left), MSRA [33] (middle) and ICVL [34] (right) datasets.|
|||30  25  )  NYU Dataset  LieX [46] (14.5mm) 3D CNN [8] (14.1mm) REN [9] (13.4mm) DeepPrior++ [19] (12.3mm) Ours (10.5mm)  m m  (    20  e c n a  t s d  i  15     20  18  16  )  14  m m  (    12  e c n a  t s d  i  10  MSRA Dataset  Multiview CNNs [7] (13.1mm) 3D CNN [8] (9.6mm) Ours (8.5mm)     20  18  16  )  14  m m  (    12  e c n a  ICVL Dataset     Hierarchical [33] (9.9mm) DeepModel [49] (11.6mm) LSN [42] (8.2mm) REN [9] (7.6mm) Ours (6.9mm)       r o r r e n a e M  10  5  0     l  m a P     R b m u h T     T b m u h T  R   x e d n  I  T   x e d n  I  l     R e d d M  i  l     T e d d M  i       r o r r e n a e M  8  6  4  2  0     R e     l t t i  L  T e     l t t i  L  n a e M  t s i r  W     R b m u h T     T b m u h T  R   x e d n  I  T   x e d n  I     T g n R  i     R g n R  i  l     R e d d M  i  l     T e d d M  i  t s d  i  10       r o r r e n a e M  8  6  4  2  0     R e     l t t i  L  T e     l t t i  L  n a e M  l  m a P     R b m u h T     T b m u h T  R   x e d n  I  T   x e d n  I     R g n R  i     T g n R  i  l     R e d d M  i  l     T e d d M  i     R g n R  i     T g n R  i  R e     l t t i  L  T e     l t t i  L  n a e M  Figure 9: Comparison with state-of-the-art methods on NYU [38] (left), MSRA [33] (middle) and ICVL [34] (right) datasets.|
|||Comparisons with State(cid:173)of(cid:173)the(cid:173)arts  We compare our point cloud based hand joints regression method with 16 state-of-the-art methods: latent random forest (LRF) [34], hierarchical regression with random forest (Hierarchical) [33], collaborative filtering [4], 2D CNN for heat-map regression (Heat-map) [38], 2D CNN with prior and refinement (DeepPrior) [20], 2D CNN with feed 8423  Figure 10: Qualitative results for NYU [38] (left), MSRA [33] (middle) and ICVL [34] (right) datasets.|
|||back loop (Feedback) [21], 2D CNN for hand model regression (DeepModel) [50], matrix completion with deep feature (DeepHand) [29], local surface normal based random forest (LSN) [42], multi-view CNNs [7], 2D CNN using Lie group theory (Lie-X) [46], Crossing Nets [41], 3D CNN [8], region ensemble network (REN) [9], improved DeepPrior (DeepPrior++) [19] and 2D CNN with hallucinating heat distribution (Hallucination) [3].|
|||On MSRA dataset [33], when the error threshold is between 15mm and 20mm, the proportions of good frames of our method is about 10% better than 3D CNN [8] and DeepPrior++ [19] methods.|
|||In addition, our network model size is 10.3MB, including 9.2MB for the hand pose regression network and 1.1MB for the fingertip refinement network, while the model size of the 3D CNN in [8] is about 420MB.|
|||Robust 3D hand pose estimation in single depth images: from singleview CNN to multi-view CNNs.|
||10 instances in total. (in cvpr2018)|
|428|Umar_Iqbal_Hand_Pose_Estimation_ECCV_2018_paper|As a second contribution, we propose a novel CNN architecture to estimate the 2.5D pose from images.|
|||We design the proposed CNN architecture such that the 2.5D heatmaps do not have to be designed by hand, but are learned in a latent way.|
|||Given an image of a hand, the proposed CNN architecture produces latent 2.5D heatmaps containing the latent 2D heatmaps H 2D and latent depth maps H z.|
|||In this section, we first describe an alternative formulation of the CNN (Sec.|
|||The CNN provides a 2K channel output with K channels for 2D localization heatmaps H 2D and K channels for depth maps H zr for the kth keypoint is defined as  .|
|||Therefore, we alleviate these problems by proposing a latent representation of 2.5D heatmaps, i.e., the CNN learns the optimal representation by minimizing a loss function in a differentiable way.|
|||k  k  and H zr  To this end, we consider the 2K channel output of the CNN as latent variables for 2D heatmaps and depth maps, respectively.|
|||We first examine the impact of different choices of CNN architectures for 2.5D pose regression.|
|||We also evaluate the impact of additional training data on all CNN architectures for 2.5D regression.|
|||We also proposed a CNN architecture to learn 2.5D heatmaps in a latent way using a differentiable loss function.|
||10 instances in total. (in eccv2018)|
|429|Fragkiadaki_Recurrent_Network_Models_ICCV_2015_paper|For video pose labeling and prediction, the encoder is a Convolutional Neural Network (CNN) [4] initialized by a CNN per frame body part detector and decoder is a fully connected network.|
|||For video pose labeling, ERDs outperforms a per frame body part CNN detector, particularly in the case of left-right confusions.|
|||Initialization of the CNN encoder with the weights of a body pose detector leads to a much better solution than random weight initialization.|
|||We compare our ERD video labeler against two baselines: a per frame CNN pose detector (PF) used as the encoder part of our ERD model, and a dynamic programming approach over multiple body pose hypotheses per frame (VITERBI) similar in spirit to [20, 2].|
|||For a video comparison between ERD and the per frame CNN detector, please see the video at https://sites.google.com/site/motionlstm/.|
|||Last, VITERBI is marginally better than the per frame CNN detector.|
|||Figure 3 compares ERD training and test losses during finetuning the encoder from (the first five layers of) our per frame CNN pose detector, versus training the encoder from scratch (random weights).|
|||Quantitative comparison of a per frame CNN body part detector of [38] (PF), dynamic programming for temporal coherence of the body pose sequence in the spirit of [20, 2] (VITERBI), and ERD video pose labeler.|
|||ERD corrects left-right confusions of the per frame CNN detector by aggregating appearance features (CONV5) across long temporal horizons.|
|||lar performance as in H3.6M, marginally exceeding the per frame CNN detector.|
||10 instances in total. (in iccv2015)|
|430|Deep Co-Occurrence Feature Learning for Visual Object Recognition|The co-occurrence layer is general in the sense that it can serve as a building block, and generalize an arbitrary subset of the convolutional layers in a CNN model.|
|||In the experiments, we illustrate the proposed co-occurrence layer by applying it to the last three convolutional layers of two widely-used CNN frameworks, VGG-16 [28] and ResNet-152 [17].|
|||These methods, however, rely on an exhaustive search of multiple part or viewpoint templates, or require multiple CNN models.|
|||Recent success in fine-grained recognition often makes use of CNN models with multiple network streams.|
|||A framework with multiple CNN models is trained in [35] to learn the multiple granularity descriptors.|
|||Given a target CNN architecture, our goal is to associate a subset of or even the whole convolutional layers with the co-occurrence  layers so that the co-occurrence properties between object parts can be leveraged to enhance the performance of finegrained recognition.|
|||Generalization  The philosophy of deeper-is-better has been adopted in many powerful CNN frameworks, such as VGG-16 [28] and ResNet [15].|
|||[44], showed that the earlier convolutional layers in deep CNN models tend to detect low-level patterns, such as edge-like and blob-like features, while the later layers tend to detect high-level patterns, such as object parts.|
|||The abundant data in ImageNet help a lot in initializing deep CNN models especially when the domain specific fine-grained datasets have no sufficient training images.|
|||In the future, we plan to generalize this work and apply it the vision applications where CNN with part-based information are appreciated, such as generic object recognition and weakly supervised object detection.|
||10 instances in total. (in cvpr2017)|
|431|Ionescu_How_Hard_Can_CVPR_2016_paper|Similarly, for the task of semi-supervised object classification, we use our measure to improve the accuracy of a classifier based on CNN features [38] by 1%.|
|||In Section 3, we show that we can learn an even better predictor capable of automatically assessing visual search difficulty based on CNN features, without information derived from image properties.|
|||Table 3 includes the mean Average Precision (mAP) performance of the best CNN classifier presented in [7].|
|||Our regression model  We build our predictive model based on CNN features and linear regression with -SVR [36] or Kernel Ridge Regression (KRR) [36].|
|||These CNN models are trained on the ILSVRC benchmark [34].|
|||We removed the last layer of the CNN models and used them to extract deep features as follows.|
|||The best approach is to combine the pyramid features from both CNN architectures and to train the model using -SVR.|
|||In this setting, our -SVR model based on CNN features obtains a Kendalls  correlation of 0.427, compared to 0.270 for the -SVR model that combines all the baselines.|
|||All models are linear SVM classifiers based on CNN features [38].|
|||We believe that our difficulty measure can be used in a curriculum learning setting to optimize the training of CNN models for various vision tasks.|
||10 instances in total. (in cvpr2016)|
|432|Roberto_Valle_A_Deeply-initialized_Coarse-to-fine_ECCV_2018_paper|It uses a simple CNN to generate probability maps of  2  R. Valle, J.M.|
|||In this paper we use the landmark probability maps produced by a CNN to find a robust starting point for the CSR.|
|||[26] were pioneers to apply a three-level CNN to obtain accurate landmark estimation.|
|||We train this CNN to obtain a set of probability maps, P(I), indicating the position of each landmark in the input image (see Fig.|
|||Compared to typical CNNbased approaches, e.g., [33], our CNN is simpler, since we only require a rough estimation of landmark locations.|
|||We train from scratch the CNN selecting the model parameters with lowest  A Deeply-initialized Coarse-to-fine Ensemble for Face Alignment  9  validation error.|
|||In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying a 22 pooling4.|
|||For the Mempo data set training the CNN and the coarse-to-fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 (8GB) GPU and an Intel Xeon E5-1650 at 3.50GHz (6 cores/12 threads, 32 GB of RAM).|
|||The CNN provides robust landmark estimations with no face shape enforcement.|
|||DCFE combines CNNs and ERT by fitting a 3D model to the initial CNN prediction and using it as initial shape of the ERT.|
||10 instances in total. (in eccv2018)|
|433|Video Captioning With Transferred Semantic Attributes|In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner.|
|||Specifically, given a video, a 2-D/3-D CNN is utilized to extract visual features of selected video frames/clips and the video representations are produced by mean pooling over these visual features.|
|||Approach  We devise our CNN plus RNN architecture to generate video descriptions under the umbrella of incorporating mined semantic attributes from images and videos.|
|||i  where pwa is the probability of the attribute wa predicted by region ri and can be calculated through a sigmoid layer after the last convolutional layer in the CNN architecture [6].|
|||Here the adopted CNN architecture is a fully convolutional network extended from recent popular CNN [23] that shows superior performance for video representation learning [7, 14].|
|||The basic idea of LSTM-TSAIV is to translate the video representation from a 2-D and 3-D CNN to the desired output sentence through LSTM-type RNN model by additionally injecting the high-level semantic attributes learnt from both images and videos.|
|||(1) LSTM [30]: LSTM attempts to directly translate from video pixels to natural language with a CNN plus RNN framework.|
|||(3) Temporal Attention (TA) [34]: TA combines the frame representation from GoogleNet [25] and video clip representation based on a 3-D CNN trained on hand-crafted descriptors.|
|||(4) Long Shot-Term Memory with visual-semantic Embedding (LSTM-E) [15]: LSTM-E utilizes both 2-D CNN and 3-D CNN to learn video representation, and simultaneously explores the learning of LSTM and visual-semantic embedding for video captioning.|
||9 instances in total. (in cvpr2017)|
|434|cvpr18-Toward Driving Scene Understanding  A Dataset for Learning Driver Behavior and Causal Reasoning|In turn, visual features are represented by spatial grid of CNN activations.|
|||Features from the last convolutional layer of InceptionResnet-V2 on RGB frames are provided as an input to LSTM in CNN models.|
|||The necessity to preserve the spatial resolution is illustrated by Table 2 where CNN conv demonstrates the substantial advantage in detection of turns.|
|||The next method (CNN conv) is a variant of the second method: instead of spatially pooling CNN feature encodings, we used a small convnet to reduce the dimensionality of the CNN encodings of the frames before passing them through the LSTM.|
|||Finally, the CNN+Sensors method adds sensor data to the CNN conv method.|
|||We can see that the performance of CNN pool is quite low.|
|||When it comes to actions like lane changes, visual information used in CNN conv allows for proper scene interpretation, thus, improving over Sensor model.|
|||7705  right turn  left turn  intersection passing  railroad passing  left lane branch  right lane change  left lane change  right lane branch  crosswalk passing  merge  u-turn  0.8  0.6  0.4  0.2  0.0  n r u t   t h g  i r  n r u t   t f e l     g n i s s a p d a o r l i a r  h c n a r b   e n a l   t f e l  e g n a h c   e n a l   t h g  i r  e g n a h c   e n a l   t f e l  h c n a r b   e n a l   t h g  i r  g n i s s a p   k l a w s s o r c     g n i s s a p n o i t c e s r e t n  i  e g r e m  n r u t u  Figure 7: Confusion matrices for Goal-oriented driver behavior classes using CNN conv model (left) and CNN+Sensors (right).|
|||A railroad passing is surprisingly hard for the CNN model because this behavior type includes not only railroad crossing in the designated locations which have discriminative visual features but also tram rails crossing.|
||9 instances in total. (in cvpr2018)|
|435|Diwen_Wan_TBN_Convolutional_Neural_ECCV_2018_paper|Thus, a highly-economical yet effective CNN that is authentically applicable to consumer electronics is at urgent need.|
|||TBN demonstrates its consistent effectiveness when applied to various CNN architectures (e.g., AlexNet and ResNet) on multiple datasets of different scales, and provides  32 memory savings and 40 faster convolutional operations.|
|||Consequently, either training a CNN model with large-scale data or deploying a CNN model for real-time   Corresponding author: Fumin Shen  2  D. Wan, F. Shen, L. Liu, F. Zhu, J. Qin, L. Shao, H. Shen  Proposed TBN  Accelerate dot product by binary operations  Ternary vector  -1, 0, 1, -1, ...,0, -1, 0, 1  +  -,+,-,+,...,+,-,+  Binary vector with scaling factor  AND,XOR,bitcount  Dot product  1.2  result  Full-precision input  Ternary input    1.1, 2.2, ...,3.3 -0.5,0.6, ...,1.3          -1, 0, 1,...,1,0,-1 1,-1,0,...,1,-1,0     im2col  Ternary matrix 1, 0, -1, ..., 1, -1 0, 1, -1, ..., -1, 0 1, 1, -1, ...,  0, 1 -1, -1, 0, ..., 1, -1  f ternary  sign  TBConvolution  1.2, 3.4,...,5.6 -1, 0.4,...,-2.9                              1.7, -2,..., 0.6 3.1, 1.5,...,2.7  col2im    5,2.0,...,1.3 1.4,-4,...,1.3 1.0,2.5,...,-1     Full-precision result  The product of  convolution layer   (full-precision)  + ...  3.1  4.1  5.9  2.6  +   + ...  +   im2col  +  -,+,-,...,+,+,-,-,...,-,+ +,-,+,..,+,-,+,-,...,-,+  Matrix   multiplication  Full-precision weights  Binary weights with scaling factors  Binary matrix  Fig.|
|||Existing CNN lightweighting techniques include pruning [16, 17, 54], quantization [5, 15, 35, 36, 48, 55, 58], factorization [2, 21, 23, 26, 30, 52, 57], network binarization [7, 42], distilling [19] and others [10, 56].|
|||Specifically, TBN can provide  32 memory saving and 40 speedup over its real-valued CNN counterparts.|
||| By incorporating with various CNN architectures (including LeNet-5, VGG7, AlexNet, ResNet-18 and ResNet-34 ), TBN can achieve the promising image classification and object detection performance on multiple datasets among quantized neural networks.|
|||The performance (in mAP) comparison of TBN, XNOR-Networks and fullprecision CNN models for object detection.|
|||An accelerated ternary-binary matrix multiplication that employs highly efficient XOR, AND and bitcount operations was introduced in TBN, which achieved  32 memory saving and 40 speedup over its full-precision CNN counterparts.|
|||TBN demonstrated its consistent effectiveness when applied to various CNN architectures on multiple datasets of different scales, and it also outperformed the XNORNetwork by up to 5.5% (top-1 accuracy) on the ImageNet classification task, and up to 4.4% (mAP score) on the PASCAL VOC object detection task.|
||9 instances in total. (in eccv2018)|
|436|cvpr18-Image Blind Denoising With Generative Adversarial Network Based Noise Modeling|Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep CNN for denoising the given noisy images.|
|||When dealing with unknown noise, GAN is utilized to solve the key issue of building paired training datasets, and then CNN is employed for denoising.|
|||In particular, DnCNN [32] trains  a very deep CNN with residual learning and batch normalization strategies and achieves state-of-the-art results in Gaussian denoising.|
|||Denoising with Deep CNN  Many previous works [33, 32, 16] have proposed to solve denoising problems by training a CNN with large datasets and achieved impressive results.|
|||Thus, a CNN is utilized in our framework for denoising.|
|||Once the paired training dataset is built, a CNN can be trained for denoising finally.|
|||This is because the number of Gaussian models and the explicitlydefined model may limit the performance of GMM while GAN leverages the great capability of CNN to learn the noise model implicitly and to capture more features of noises without human knowledge of image priors.|
|||The GAN is utilized to learn noise distribution and to build the paired training dataset to train the CNN for denoising.|
|||Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising.|
||9 instances in total. (in cvpr2018)|
|437|Hanyu_Wang_Learning_3D_Keypoint_ECCV_2018_paper|While the latter kind of methods generalizes the CNN paradigm to non-Euclidean manifolds [16], they are able to learn invariant shape signatures for non-rigid shape analysis.|
|||[30] employe a CNN architecture to learn invariant descriptors in arbitrary complex poses and clothings, where their system is trained with a large dataset of depth maps.|
|||Another family of methods are based on the notion of geometric deep learning[33], where they generalize CNN to non-Euclidean manifolds.|
|||Various frameworks have been introduced to solve descriptor learning or correspondence learning problems, including localized spectral CNN (LSCNN)[34], geodesic CNN (GCNN)[35], Anisotropic CNN (ACNN)[36], mixture model networks (MoNet)[16], deep functional maps (FMNet)[37], and so on.|
|||4 CNN Architecture and Training  In this section, we describe the details of our network architecture and how it can be trained automatically and efficiently to learn the embedding function.|
|||4.4 CNN Architecture and Configuration  Considering the particularity and complexity of our task, we design a special CNN architecture dedicated to processing geometry images in our triplet structure, which is presented below.|
|||5 illustrates the architecture of our CNN model.|
|||The detailed configuration of our triplet CNN is set to adapt our architecture and gain the best performance.|
|||Because triplet loss is not as stable as other frequently-used loss functions, our old-version CNN with traditional ReLU activation often suffers from dying ReLU problem that may reduce the effective capacity of our CNN model and then lead to failure in generating meaningful descriptors.|
||9 instances in total. (in eccv2018)|
|438|cvpr18-Deep Group-Shuffling Random Walk for Person Re-Identification|[1] proposed a Cross-Input Difference CNN to capture local relationships between the two input images based on mid-level features from each input image.|
|||We call the affinity scores between the probe and gallery images by the CNN the initial P2G affinities y(0)  Rn.|
|||The pairwise affinity CNN takes a pair of images as inputs and outputs K affinity scores between the two images for K feature groups.|
|||The network structure for the pairwise affinity CNN is shown in Figure 3.|
|||Given a probe image and n gallery images, the pairwise affinity CNN estimates initial P2G affinities y(0) k  Rn and G2G affinities Wk  Rnn for k = 1,    , K. The group-shuffling random walk layer takes the initial P2G and G2G affinities as inputs and output K 2 groups of refined P2G affinities y() for j, k = 1,    , K. The supervisions are applied to the refined P2G affinities with cross-entropy loss functions.|
|||It consists of a pairwise affinity CNN and the proposed group-shuffling random walk layer.|
|||Implementation details  The pairwise affinity CNN in our network adopts the ResNet-50 [11] network structure and is pretrained on the ImageNet dataset.|
|||In testing, given a probe image, we first utilize the trained pairwise affinity CNN to identify the top-75 gallery images.|
|||We utilize the pairwise affinity CNN in our framework with the group number K = 1 as our baseline model.|
||9 instances in total. (in cvpr2018)|
|439|Benitez-Quiroz_Recognition_of_Action_ICCV_2017_paper|A schematic comparison of a typical CNN versus ours, which employs our derived GL-criterion (GL-CNN), is shown in Figure 1.|
|||The derived CNN and comparisons with other state-of-theart nets are in Section 3.|
|||We use this loss to train a deep CNN to recognize AUs.|
|||And,  xi can either be the t outputs of our CNN  fi = {  fij, .|
|||Details on this first set of layers of our CNN are in Table 1.|
|||Instead of applying a regular CNN to the entire input image, Zhao et al.|
|||[23] proposed to add a region layer in CNN to identify specific regions for different AUs.|
|||A CNN solution is to define independent outputs for each detection/attribute using a region-based approach [29].|
|||We showed that with the addition of these global constraints, a single global (non-patch based) CNN can be successfully defined.|
||9 instances in total. (in iccv2017)|
|440|Jiaxin_Chen_Deep_Cross-modality_Adaptation_ECCV_2018_paper|Alternatively, freehand sketch is a more convenient way for human to interact with  2  J. Chen and Y. Fang  metric of 2D sketches, the CNN network f 2  Fig.|
|||Our model consists of the CNN network f 1 CNN and metric network f 1 CNN and metric network f 2 metric of rendered images of 3D shapes, together with the cross-modality transformation network ftrans.|
|||The CNN and metric networks for each single modality (i.e., 2D sketches or 3D shapes) is trained by importance-aware metric learning through mining the hardest training samples.|
|||1, our proposed framework mainly consists of five components, including the CNN networks for 2D sketches (denoted by f 1 CNN) and for 3D shapes (denoted by f 2 CNN), fully connected metric networks for 2D sketches (denoted by f 1 metric), together with  metric) and for 3D shapes (denoted by f 2  Deep Cross-modality Adaptation for Sketch-based 3D Shape Retrieval  5  the cross-modality transformation network ftrans, of which the parameters are 1 CNN, 2  metric and trans, respectively.|
|||1, we train the CNN and metric networks for sketches, i.e., f 1 metric, jointly by adopting an importance-aware metric learning (IAML).|
|||The CNN and metric networks for 3D shapes, i.e., f 2 CNN and f 2 metric, are also trained in the same way.|
|||3.1  Importance-Aware Feature Learning  Given a mini-batch I m, after successively passing I m through the CNN network f m CNN and the metric network f m  metric, we can obtain a set of feature vectors:  Zm = (cid:8)zm  1,1,    , zm  1,K ,    , zm  C,1,    , zm  C,K(cid:9) ,  where m  {1, 2}, and for i = 1,    , C, j = 1,    , K  z1 i,j = f 1  metric (cid:0)f 1  CNN(I 1  i,j)(cid:1) , z2  i,j = f 2  metric (cid:0)f 2  CNN(I2  i,j)(cid:1) .|
|||That is to say, by minimizing Lm IAM L, the minimal inter-class distance is compelled to be larger than the maximal intra-class distance in the feature space, whilst keeping a certain margin . Consequently, we can train the CNN network f m CNN and the metric network f m  metric to generate discriminative features for each modality.|
|||For CNN networks of both sketches and shapes, i.e., f 1 CNN and f 2 CNN, we utilize the ResNet-50 network [7].|
||9 instances in total. (in eccv2018)|
|441|Milbich_Unsupervised_Video_Understanding_ICCV_2017_paper|To compensate for this shortcoming, we train a CNN with the different, local sub-sequence solutions in subsequent minibatches as discussed in Sect.|
|||The CNN then reconciles  4396  Figure 2.|
|||The solution of this matching problem then provides us with correspondences of similar and dissimilar postures, which we then impose onto the CNN representation (I; ) to learn it.|
|||Sequence Matching for Self(cid:173)supervision  j }n  j=1 and S  = {I   We aim to learn a CNN representation (I; ) which encodes posture similarity, without being provided with any labels.|
|||From Local Correspondences to a Globally  Consistent Posture Representation  Our training procedure combines multiple exact solutions to local ILPs to obtain self-supervision for training a joint CNN representation (I; ) that reconciles all the sub-problems.|
|||Since our goal is to learn a CNN representation for encoding human activity in a fully unsupervised manner, the mini-batches for training are composed just by pairs of sequences {S, S }.|
|||Using this triplet self-supervision we update the CNN parameters  via backpropagation and the triplet ranking loss [50, 49].|
|||(2) provides the self-supervision information needed to train the CNN representation , whereas the CNN training using Eq.|
|||A combinatorial sequence matching algorithm proposes relations between frames from subsets of the training set, which a CNN uses to learn a single concerted pose embedding that reconciles transitivity conflicts.|
||9 instances in total. (in iccv2017)|
|442|Shi_Yan_DDRNet_Depth_Map_ECCV_2018_paper|To attack these challenges, we propose a new cascaded CNN structure to perform depth image denoising and refinement in order to lift the depth quality in low frequency and high frequency simultaneously.|
|||For the denoising net, we train a CNN with a structure similar to U-net [36].|
|||Also with a better reflectance estimation method than previous work, the reflectance influence can be further alleviated, resulting in a CNN network which extracts only geometry-related information to improve the depth quality.|
|||For denoising part, a function D mapping a noisy depth map Din to a smoothed one Ddn with high-quality low frequency is learned by a CNN with the supervision of near-groundtruth depth maps Dref , created from a state of the art of dynamic fusion.|
|||The albedo map for each frame is also estimated the CNN used in [25].|
|||Not only does this direct pass set a good initialization, it turns out that residual learning is able to speed up the training process of deep CNN as well.|
|||It penalizes both intensity and gradient of the difference value between the rendered image and the corresponding intensity image:  lsh(Ndt, Nref , I) = kB(l  , Ndt, R)  Ik2+gkB(l  , Ndt, R)  Ik2,  (5)  where Ndt represents the normal map of the regressed depth from the refinement net, g is the weight to balance shading loss term, R is the albedo map estimated  can using Nestmeyers CNN + filter method[25].|
|||The denoising net is supervised by temporally fused reference depth map, and the refinement CNN is trained in an unsupervised manner.|
|||At test time, our whole processing procedure includes data pre-processing and cascade CNN predicting.|
||9 instances in total. (in eccv2018)|
|443|More Is Less_ A More Complicated Network With Less Inference Complexity|Some algorithms exploit the sparsity property of convolutional kernels or response maps in CNN architecture.|
|||In [8], a CNN model was proposed to process spatially-sparse inputs, which can be exploited to increase the speed of the evaluation process.|
|||As shown in Table 2, the speedup ratio is highly dependent on r. The term 1/C costs little time since the channel of the input tensor is always wide in most CNN models and it barely affects the acceleration performance.|
|||However, it is obvious that our LCCN is more complicated than the original CNN model, which leads to a requirement for more training epochs to converge into a stable situation.|
|||On ILSVRC12, since we use different data argumentation strategies, we report the top-1 error of the original CNN models trained in the same way as ours, and we mainly compare the accuracy drop with other state-of-the-art acceleration algorithms including: (1) Binary-Weight-Networks (BWN) [27] that binarizes the convolutional weights; (2) XNORNetworks (XNOR) [27] that binarizes both the convolutional weights and the data tensor; (3) Pruning Filters for Ef 3https://github.com/facebook/fb.resnet.torch  55844  Figure 3.|
|||Hitherto, we have demonstrated the feasibility of training CNN models equipped with our LCCL using different low-cost collaborative kernels and strategies.|
|||Furthermore, we experiment with more CNN models[12, 35] accelerated by our LCCN on CIFAR-10 and CIFAR100.|
|||Top-1 Error and Speed-Up of eight different CNN models on CIFAR-10 (symbol * means the bottleneck structure).|
|||Top-1 error and speed-up of seven different CNN models on CIFAR-100 (symbol * means the bottleneck structure).|
||9 instances in total. (in cvpr2017)|
|444|Skeleton Key_ Image Captioning by Skeleton-Attribute Decomposition|Starting from the basic form of a CNN encoder-RNN decoder, there have been many attempts to improve the system.|
|||It is also found that feeding high level attributes instead of CNN features yields improvements [47, 44].|
|||[31] propose a Multimodal Recurrent Neural Network (MRNN) that uses an RNN to learn the text embedding, and a CNN to learn the image representation.|
|||Spatial information is maintained in the CNN image features, and an attention map is learned at every time step to focus attention to predict the current word.|
|||The original input sequence of the LSTM in [42] is:  x1 = CNN(I)  xt = Weyt, t = 0, 1, ..., N  1  (3)  (4)  where I is the image, CNN(I) is the CNN image features as a vector without spatial information, We is the learned word embedding, and yt is the ground-truth word encoded as a one-hot vector.|
|||Our empirical finding is that by simply adopting a better network architecture that provides better image features, and fine-tuning the CNN within the caption dataset, the features extracted are already excellent inputs to the LSTM.|
|||We fine-tune the CNN features as follows: first, the CNN features are fixed, and an LSTM is trained for full sentence generation.|
|||After the LSTM achieves reasonable results, we start finetuning the CNN with learning rate 1e-5.|
|||By just fine-tuning the CNN with the simple baseline algorithm, we outperform the approaches with augmentation of high level attributes [47, 44].|
||9 instances in total. (in cvpr2017)|
|445|cvpr18-RotationNet  Joint Object Categorization and Pose Estimation Using Multiviews From Unsupervised Viewpoints|[34] proposed multi-view CNN (MVCNN), which takes multi-view images of an object captured from surrounding virtual cameras as input and outputs the objects category label.|
|||[29] proposed a voxel-based CNN that outputs orientation labels as well as classification labels and demonstrated that it improved 3D object classification performance.|
|||For each input image, our CNN (RotationNet) outputs M histograms with N + 1 bins whose norm is 1.|
|||Finally, we update the CNN parameters in a standard back-propagation manner with the estimated viewpoint variables.|
|||Note that it is the same CNN that is being used.|
|||The baseline architecture of our CNN is based on AlexNet [16], which is smaller than the VGG-M network architecture that MVCNN [34] used.|
|||As a baseline method, we also fine-tuned the pre-trained weights of a standard AlexNet CNN that only predicts object categories.|
|||As another baseline approach to com pare, we learned a CNN with AlexNet architecture that outputs 612 (= 51  12) scores to distinguish both viewpoints and categories, which we call Fine-grained. Here, T denotes the number of iterations that the CNN parameters are updated in the training phase.|
|||A comparative analysis and study of multiview cnn models for joint object categorization and pose estimation.|
||9 instances in total. (in cvpr2018)|
|446|Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper|[9] 78.15 ResNet-50 PC-ResNet-50 80.21 Bilinear CNN [1] 84.10 PC-BilinearCNN 85.58 DenseNet-161 84.21 PC-DenseNet-161 86.87  (2.06)  (1.48)  (2.66)  (B) Cars  (C) Aircrafts  Method Top-1  Wang et al.|
|||[9] 92.40 ResNet-50 91.71 PC-ResNet-50 93.43 Bilinear CNN [1] 91.20 PC-Bilinear CNN 92.45 DenseNet-161 91.83 PC-DenseNet-161 92.86  (1.25)  (1.03)  (1.72)  Top-1   Method Simon et al.|
|||Fine-tuning from Baseline Models: We fine-tune from three baseline models using the PC optimization procedure: ResNet-50 [12], Bilinear CNN [1], and DenseNet-161 [11].|
|||Combining PC with Specialized FGVC models: Recent work in FGVC has proposed several novel CNN designs that take part-localization into account, such as bilinear pooling techniques [16,1,9] and spatial transformer networks [2].|
|||We train a Bilinear CNN [1] with PC, and obtain an average improvement of 1.7% on the 6 datasets.|
|||Since our technique increases classification accuracy, we wish to understand if the improvement is a result of enhanced CNN localization abilities due to PC.|
|||To measure the regions the CNN localizes on, we utilize Gradient-Weighted Class Activation Mapping (Grad-CAM) [53], a method that provides a heatmap of visual saliency as produced by the network.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition.|
|||Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson, S.: CNN features offthe-shelf: An astounding baseline for recognition.|
||9 instances in total. (in eccv2018)|
|447|cvpr18-Transductive Unbiased Embedding for Zero-Shot Learning|sual recognition models (e.g., deep CNN models) struggle to make correct predictions.|
|||So they do not fully exploit the power of deep CNN models.|
|||Here, we also adopt a pre-trained CNN model to perform visual embedding.|
|||The design of bridging function depends on the CNN architecture from the visual embedding subnet.|
|||Specifically, our design follows the fully connected layers of the selected CNN model.|
|||Model Selection and Training Four popularly used deep CNN models are involved in our following experiments: AlexNet [17], GoogLeNet [37], VGG19 [35] and ResNet101 [41].|
|||Most of them keep the trained CNN models fixed and do not optimize them during the training phase.|
|||To further verify that our method is not only effective to a specific CNN model, we implement our method with AlexNet, GoogleNet, and VGG respectively.|
|||Using such scarce data to train deep CNN models like ResNet101 usually leads to over-fitted models.|
||9 instances in total. (in cvpr2018)|
|448|Wohlhart_Learning_Descriptors_for_2015_CVPR_paper|We show below how to train such a CNN to enforce the two important properties already discussed in the introduction: a) The Euclidean distance between descriptors from two different objects should be large; b) The Euclidean distance between descriptors from the same object should be representative of the similarity between their poses.|
|||We introduce a cost function for such a triplet:  (cid:18) 0, 1  ||fw(xi)  fw(xk)||2 ||fw(xi)  fw(xj)||2 + m  c(si, sj, sk) = max  (cid:19)  ,  (2)  where fw(x) is the output of the CNN for an input image x and thus our descriptor for x, and m is a margin.|
|||Network structure: We use a CNN made of two convolutional layers with subsequent 2  2 max pooling layers, and two fully connected layers.|
|||We optimize the parameters w of the CNN by Stochastic Gradient Descent on mini-batches with Nesterov momentum [32].|
|||Dataset Compilation  We train a CNN using our method on a mixture of synthetic and real world data.|
|||Before applying the CNN we normalize the input images.|
|||Then we perform two rounds of bootstrapping triplet indices as explained in Section 3.3, and for each round we train the CNN for another 200 epochs on the augmented training set.|
|||To do so, we train the CNN on 14 out of the 15 objects.|
|||Conclusion  We have shown how to train a CNN to map raw input images from different input modalities to very compact output descriptors using pair-wise and triplet-wise constraints over training data and template views.|
||9 instances in total. (in cvpr2015)|
|449|Zhang_Supplementary_Meta-Learning_Towards_ICCV_2017_paper|In our experiments, we observed a 10% degradation when compared to the original CNN method.|
|||An important difference between traditional CNN and MLNN models lies in the dynamic nature of their weights/parameters.|
|||(a) The fixed 64 first-layer filters of the original CNN model.|
|||show that traditional super-resolution algorithms can be replaced by a deep CNN to get better performance and faster speed [8].|
|||Table 2 shows the PSNR for measuring the difference between testing results and ground truths after train 4349  Table 3: Evaluations of Arc1 on 5 types of images (PSNR: dB)  Table 5: Performance comparisons in image classification  Individual Models MLNN  Data Set  Model  Data Set  natural images  screenshots depth images cartoon images  oil paintings  CNN  29.81 28.09 43.89 32.77 30.22  29.97 28.30 44.23 32.92 30.32  30.02 28.38 44.11 33.02 30.38  Table 4: Evaluations using image denoising (PSNR: dB)  Architecture  With CNN Model  With MLNN Model  Arc 1 Arc 3 Arc 4  28.71 28.87 29.02  28.87 28.98 29.14  ing for 64 epochs.|
|||When compared to a single CNN trained on dataset-5, training the same CNN for each type of images to get five models leads to better performance.|
|||Efficiency Comparisons and Analysis  In this subsection, we compare the efficiency of MLNN with standard CNN models.|
|||Table 6 compares the computational complexity and the speed of the standard CNN and the dynamic MLNN models.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||9 instances in total. (in iccv2017)|
|450|Wu_Ask_Me_Anything_CVPR_2016_paper|Given an image-question pair, a CNN is first employed to predict a set of attributes of the image.|
|||Most recently,  inspired by the significant progress achieved using deep neural network models in both computer vision and natural language processing, an architecture which combines a CNN and RNN to learn the mapping from images to sentences has become the dominant trend.|
|||Our framework also exploits both CNNs and RNNs, but in contrast to preceding approaches which use only image features extracted from a CNN in answering a question, we employ multiple sources, including image content, generated image captions and mined external knowledge, to feed to an RNN to answer questions.|
|||Our proposed framework: given an image, a CNN is first applied to produce the attribute-based representation Vatt(I).|
|||A shared CNN is connected with each proposal, and the CNN outputs from different proposals are  4624  Top 5 Attributes: players, catch, bat, baseball, swing Generated Captions: A baseball player swing a bat at a ball.|
|||The shared CNN is then fine-tuned on the multi-label dataset, the MS COCO image-attribute training data [30].|
|||Caption(cid:173)based Image Representation  Currently the most successful approach to image captioning [6, 9, 14, 21, 28] is to attach a CNN to an RNN to learn the mapping from images to sentences directly.|
|||The Baseline method is implemented simply by connecting a CNN to an LSTM.|
|||The CNN is a pre-trained (on ImageNet) VggNet model from which we extract the coefficients of the last fully connected layer.|
||9 instances in total. (in cvpr2016)|
|451|Yaojie_Liu_Face_De-spoofing_ECCV_2018_paper|A CNN architecture with proper constraints and supervisions is proposed to overcome the problem of having no ground truth for the decomposition.|
|||With such constraints and auxiliary supervisions proposed in [18], a novel CNN architecture is presented in this paper.|
|||Given an image, one CNN is designed to synthesize the spoof noise pattern and reconstruct the corresponding live image.|
|||In order to examine the reconstructed live image, we train another CNN with auxiliary supervision and a GAN-like discriminator in an end-to-end fashion.|
||| A novel CNN architecture is proposed for face de-spoofing, where appropriate  constraints and auxiliary supervisions are imposed.|
|||Most of the CNN works treat face anti-spoofing as a binary classification problem and apply the softmax loss function.|
|||However, during the cross-testing (i.e., train and test in different datasets), these CNN models exhibit a poor generalization ability due to the overfitting to training data.|
|||We believe by decomposing the spoof image, CNN can analyze the spoof noise more directly and effectively, and gain more knowledge in tackling face anti-spoofing.|
|||Despite the strength of using a CNN model, we are still facing the challenge of learning without the ground truth of the noise pattern.|
||9 instances in total. (in eccv2018)|
|452|cvpr18-Structured Set Matching Networks for One-Shot Part Labeling|More recently [52] proposed a CNN architecture to produce state-of-the-art results on this set.|
|||They noted that off-the-shelf CNN architectures do not work well for sketches, and instead proposed a few modifications.|
|||Appearance Similarity  The appearance of each object part is encoded by an encoder network, a CNN whose architecture is akin to the early layers of VGG16 [38].|
|||The input to the CNN is an image patch extracted around the annotated part and resized to a canonical size.|
|||The output of the CNN is an embedding of the local appearance cues for the corresponding object part.|
|||Due to the sparse nature of diagrams and the absence of much color and texture information, CNN models pretrained on natural image datasets perform very poorly as encoder networks.|
|||Furthermore, off the shelf CNN architectures also perform poorly on diagrams, even when no pretraining is used [52], and require custom modifications such as larger filter sizes.|
|||This noticeably improves performance for diagrams, whilst using the CNN architectures designed for natural images.|
|||An interesting observation was that when we swept the space of filter sizes to find the best performing one for each configuration, the best filters for the original image were 15  15 as reported in [52] but the best filters for the DT image was 3  3, which is consistent with CNN architectures built for natural images.|
||9 instances in total. (in cvpr2018)|
|453|cvpr18-Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Faces|LDAN makes use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images.|
|||In this work, we show that this synthetic data with accurate labels can help train a deep CNN to regress lighting of real face images: denoising the unreliable labels.|
|||The proposed method is based on two assumptions: (1) A deep CNN trained with synthetic data is accurate, i.e., it is not affected by any noise; (2) Ground truth labels for real data are noisy, but still contain useful information.|
|||Label Denoising Adversarial Network  Training a regression deep CNN needs a lot of data with ground truth labels.|
|||We propose to use synthetic face images, whose ground truth lighting parameters are known, to help train a better deep CNN model.|
|||REAL in Table 1 represents a baseline method which uses SIRFS SH as ground truth to train a deep CNN without synthetic data.|
|||This suggests that by observing a large amount of data, the deep CNN itself can  6242  Table 2.|
|||The proposed deep CNN can predict 390 such face images on the CPU and 2, 400 face images on the GPU per second, so it is potentially 100, 000 times faster than an optimization based method such as SIRFS.|
|||To alleviate the effect of noise, we propose to apply the idea of adversarial networks and use synthetic face images with known ground truth to help train a deep CNN for lighting regression.|
||9 instances in total. (in cvpr2018)|
|454|cvpr18-LiteFlowNet  A Lightweight Convolutional Neural Network for Optical Flow Estimation|FlowNet [9] and its successor FlowNet2 [14], have marked a milestone by using CNN for optical flow estimation.|
|||The objective of this study is to explore alternative CNN architectures for accurate flow estimation yet with high efficiency.|
|||e.g., brightness constraint in data fidelity to pyramidal CNN features and image warping to CNN feature warping.|
|||We reduce the computational burden of feature matching by using a short-ranged matching of warped CNN features at sampled positions and a sub-pixel refinement at every pyramid level.|
|||While transformation in FlowNet2 and SPyNet is limited to images, our decider network is a more generic warping network that warps high-level CNN features.|
|||We use Fi to represent CNN features for Ii.|
|||We define a feature-driven CNN distance metric D that estimates local flow variation using pyramidal feature F1, flow field  xs  Figure 4: Folding and packing of f-lcon filters {g}.|
|||Each convolution layer is followed by a leaky rectified linear unit layer, except f-lcon and the last layer in M , S and R CNN units.|
|||Runtime and Parameters  We measure runtime of a CNN using a machine equipped with an Intel Xeon E5 2.2GHz and an NVIDIA GTX 1080.|
||9 instances in total. (in cvpr2018)|
|455|Zhang_Range_Loss_for_ICCV_2017_paper|Firstly, we deeply investigate how long tailed data impacts current deep CNN models for face recognition.|
|||More recently, [32] proposed center loss which takes account of class-clusters in CNN training.|
|||Analysis of Deep Feature Vectors  Well-trained CNN can map the input face image to feature vectors with rich identity information.|
|||For recognition tasks, we expect CNN model to output similar deep feature vectors for same persons and far apart vectors for different persons.|
|||Good CNN models are expected to have small intra-class standard deviation and average Euclidean metric while large for inter-class.|
|||We firstly implemented our range loss with VGGs [26] architecture and train CNN models with 50% (A-2) and 100% (A-0) tailed datasets constructed in Section 3.1.|
|||For comparison, we trained CNN models under the supervision of softmax loss only, contrastive loss, joint triplet loss, center loss, and range loss, respectively (the last four are jointly used with softmax loss).|
|||Comparison with state(cid:173)of(cid:173)the(cid:173)art methods  To further examine the ability of range loss, we utilize a residual CNN [9] for the next experiments, whose architecture is shown in Figure 5.|
|||The whole CNN is trained under the joint supervisory signals of soft-max and our range loss.|
||9 instances in total. (in iccv2017)|
|456|Tang_Improving_Image_Classification_ICCV_2015_paper|We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework.|
|||Building on the CNN architecture introduced in [26], the basis for most state-of-theart image classification and recognition results, we address how to represent and incorporate location features into the network architecture.|
|||We show how to incorporate these additional features into a CNN [26].|
|||Neural network architecture  We build on the CNN model introduced in [26], as this model and extensions to it are commonly used benchmarks in image classification and recognition [18, 35, 39].|
|||This makes intuitive sense, as the lower layers of the CNN model are aimed at learning effective image filters and features, and we are interested in incorporating our features later on at a higher semantic level.|
|||Our CNN architecture.|
|||To address this, we show how to construct a layer in the CNN that automatically learns the optimal radius used for pooling, which we denote as the radius learning layer.|
|||Second, we can visualize the radii that we have learned, providing insight into what the CNN is learning.|
|||The outputs of these histogram functions are treated as input features to the CNN in place of the concatenated histograms, with a radius parameter h for each hashtag/concept that selects the value of the function to treat as input to the neural network.|
||9 instances in total. (in iccv2015)|
|457|Mueller_Real-Time_Hand_Tracking_ICCV_2017_paper|The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time.|
|||Hand localization, an important task in the presence of scene clutter, is achieved by a CNN that estimates the 2D image location of the center of the hand.|
|||This output is fed into a second CNN that regresses the relative 3D locations of the 21 hand joints.|
|||First, hand localization (Section 4.1) is achieved by a CNN that estimates an image-level heatmap (that encodes position probabilities) of the root  a point which is either the hand center (knuckle of the middle finger, shown with a star shape in Figure 3a) or a point on an object that occludes the hand center.|
|||Second, 3D joint regression (Section 4.2) achieved with a CNN that regresses root-relative 3D joint locations from the cropped hand image.|
|||In particular, we estimate HR using a CNN which we call HALNet (HAnd Localization Net).|
|||We trained a CNN with the same architecture as JORNet but with the task of directly regressing 3D joint positions from full frame RGB-D images which often have large ocIn Figure 7, we show the 3D clusions and scene clutter.|
|||Figure 7: Comparison of our two-step RGB-D CNN architecture, the corresponding depth-only version and a single combined CNN which is trained to directly regress global 3D pose.|
|||fingertip error plot for this CNN (single RGB-D) which is worse that our two-step approach.|
||9 instances in total. (in iccv2017)|
|458|cvpr18-Learning Semantic Concepts and Order for Image and Sentence Matching|Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc.|
|||Most existing methods [12, 16, 23] jointly represent all the concepts by extracting a global CNN [30] feature vector, in which the concepts are tangled with each other.|
|||To learn the semantic concepts, we exploit a multi-regional multi-label CNN that can simultaneously predict multiple concepts in terms of objects, properties, actions, etc.|
|||The inputs of this CNN are multiple selectively extracted regions from the image, which can comprehensively capture all the concepts regardless of whether they are primary foreground ones.|
|||[7] propose the first visual-semantic embedding framework, in which ranking loss, CNN [17] and Skip-Gram [25] are used as the objective, image and word encoders, respectively.|
|||Given an image, its multi-hot representation of groundtruth semantic concepts is yi  {0, 1}K and the predicted score vector by the multi-label CNN is byi  [0, 1]K ,  then the model can be learned by optimizing the following objective:  Lcnn = XK  c=1  log(1 + e(yi,c  byi,c))  (1)  During testing, considering that the semantic concepts usually appear in image local regions and vary in size, we perform the concept prediction in a regional way.|
|||All modules of our model excepting for the multiregional multi-label CNN can constitute a whole deep network, which can be jointly trained in an end-to-end manner from raw image and sentence to their similarity score.|
|||From Figure 5, we can see that our multi-regional multilabel CNN can accurately predict the semantic concepts with high confidence scores for describing the detailed image content.|
|||In the future, we will replace the used VGGNet with ResNet in the multi-regional multi-label CNN to predict the semantic concepts more accurately.|
||9 instances in total. (in cvpr2018)|
|459|Park_RDFNet_RGB-D_Multi-Level_ICCV_2017_paper|Our network effectively captures multilevel RGB-D CNN features by including multi-modal feature fusion blocks and multi-level feature refinement blocks.|
|||Although previous approaches achieved meaningful results, there has been a lack of research that fully utilizes recent successful CNN architectures using skip-connections.|
|||[5] extended multi-scale RGB CNN architecture [8] to RGBD situation by simply concatenating input color and depth channels, i.e., early fusion (Figure 2 (a)).|
|||However, as they simply sum intermediate RGB and depth features only in encoder part, it does not fully exploit effective midlevel RGB-D features, reporting accuracy worse than the state-of-the-art RGB-only CNN architecture [27].|
|||In this paper, we employ a similar architecture for multi-modal CNN feature fusion while retaining the advantage of skip connection.|
|||We first compare our RDFNet with the existing indoor semantic segmentation methods using CNN features.|
|||Conclusion  We proposed a novel network that takes full advantage of residual learning with skip-connection to extract effective multi-modal CNN features for semantic segmentation.|
|||The residual architecture facilitates efficient end-to-end training of very deep RGB-D CNN features on a single GPU.|
|||Fusenet: Incorporating depth into semantic segmentation via fusionbased cnn architecture.|
||9 instances in total. (in iccv2017)|
|460|Unrolling the Shutter_ CNN to Correct Motion Distortions|Unrolling the Shutter: CNN to Correct Motion Distortions  Vijay Rengarajan1, Yogesh Balaji2, A.N.|
|||Experiments reveal that our proposed architecture performs better than the conventional CNN employing square kernels.|
||| A new CNN architecture designed specific to the exposure mechanism in rolling shutter cameras that learns row-oriented and column-oriented features.|
|||The size of each sampled ps 1 and ps 2 is K, making the length of the output camera trajectory tensor P of the CNN in Fig.|
|||VanillaCNN We propose two CNN architectures in this work: the first one is VanillaCNN as shown in Fig.|
|||P = (IRS; )  (3)  The power and complexity of the proposed method is in  which is based on CNN that extracts information from images to output the camera motion.|
|||The CNN output vector length is N=15 for the translation and rotationonly models, and it is N=30 for the combined model (15 each for translations and rotations).|
|||Human rating for CNN outputs against [13] and [4].|
|||Conclusions  We proposed a new CNN architecture based on long rectangular kernels to aid in correcting rolling shutter distortions from single-images.|
||9 instances in total. (in cvpr2017)|
|461|cvpr18-Learning Depth From Monocular Videos Using Direct Methods|In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error.|
|||Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor.|
|||[31] who proposed a strategy that learned separate pose and depth CNN predictors by minimizing the photometric consistency across monocular video datasets during training.|
|||Existing methods [28, 31] for learning depth from monocular video address these differences only partially by adding an extra CNN pose prediction module.|
|||Further, we advocate that the incorporation of an additional CNN pose prediction module is unnecessary.|
|||Second, we suggest that learning an additional pose predicting CNN (which we shall refer to herein as the Pose 12022  CNN) is not the most effective strategy for estimating a depth predicting CNN from monocular video.|
|||To solve this problem, we propose a simple yet effective approach apply a non-linear operator to normalize the output of the depth CNN before feeding it to the loss layer.|
|||Pose-CNN architecture We use a similar CNN architecture as in [31], which takes input from a concatenation of three sequential frames.|
|||Additionally, we have shown that Direct Visual Odometry can be used to estimate the relative camera pose between frames instead of learning a predictor with a CNN (Pose-CNN).|
||9 instances in total. (in cvpr2018)|
|462|Deep Image Harmonization|Toward this end, we train a deep CNN model that consists of an encoder to capture the context of the input image and a decoder to reconstruct the harmonized image using the learned representations from the encoder.|
|||Training an end-to-end deep CNN requires a large-scale training set including various and high-quality samples.|
|||Second, we demonstrate that our joint CNN model can effectively capture context and semantic information, and can be efficiently trained for both the harmonization and scene parsing tasks.|
|||[32] learn a CNN model to predict the realism of a composite image and incorporate the realism score into a color optimization function for appearance adjustment on the foreground region.|
|||Deep Image Harmonization  In this section, we describe the details of our proposed end-to-end CNN model for image harmonization.|
|||Figure 3 shows an overview of the proposed CNN architecture.|
|||This data acquisition method ensures that the ground truth images are always realistic so that the goal of the proposed CNN is to directly reconstruct a realistic output from a composite image.|
|||This enables us to train an end-toend CNN for image harmonization with several benefits.|
|||Context(cid:173)aware Encoder(cid:173)decoder  Motivated by the potential of the Context Encoders [20], our CNN learns feature representations of input images via an encoder and reconstruct the harmonized output results through a decoder.|
||9 instances in total. (in cvpr2017)|
|463|cvpr18-SGPN  Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation|We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.|
|||Although a minimalistic framework with no bells and whistles already gives visually pleasing results (Figure 1), we also demonstrate the flexibility of SGPN as we boost performance even more by seamlessly integrating CNN features from RGBD images.|
|||Faster R-CNN [39] leveraged a CNN learning scheme and proposed Region Proposal Networks(RPN).|
|||Song and Xiao [45] use a volumetric CNN to create 3D RPN on a voxelized 3D scene and then use both the color  and depth data of the image in a joint 3D and 2D object recognition network on each proposal.|
|||4  W  In [36], 2D CNN features are combined 3D point cloud for RGBD semantic segmentation.|
|||By leveraging the flexibility of SGPN, we also seamlessly integrate 2D CNN features from RGB images to boost performance.|
|||Figure 6: Incorporating CNN features in SGPN.|
|||SGPN can effectively utilize CNN features.|
|||Learning by tracking: siamese cnn for robust target association.|
||9 instances in total. (in cvpr2018)|
|464|Deep Supervision With Shape Concepts for Occlusion-Aware 3D Object Parsing|We introduce a novel CNN architecture which jointly models multiple shape concepts including object pose, keypoint locations and visibility in Section 3.|
|||Figure 1 introduces our framework and Figure 2 illustrates a particular instance of deeply supervised CNN using shape concepts.|
|||Pose Estimation and 2D Keypoint Detection Render for CNN [33] synthesizes 3D CAD model views as additional training data besides real images for object viewpoint estimation.|
|||Deep Supervision with Shape Concepts  In the following, we introduce a novel CNN architecture for 3D shape parsing which incorporates constraints through intermediate shape concepts such as object pose, keypoint locations, and visibility information.|
|||The associated optimization problem for a multi-layer CNN is:  W  = argmin  W  X  (x,y)Z  l(y, f (x, W ))  (1)  where l(., .)|
|||, Wi} as the weights for the first i layers of the CNN and hi = f (, W1:i) as the activation map of layer i.|
|||To obtain such large-scale training data, we extend the data generation pipeline of Render for CNN [33] with 2D/3D landmarks and visibility information.|
|||We use stochastic gradient descent with momentum 0.9 to train the proposed CNN from scratch.|
|||This indicates that the deep supervision achieves better regularization during training by coupling the sequential structure of shape concepts with the feedforward nature of a CNN .|
||9 instances in total. (in cvpr2017)|
|465|cvpr18-SketchMate  Deep Hashing for Million-Scale Human Sketch Retrieval|In this paper, we combine RNN stroke modeling with conventional CNN under a dual-branch setting to learn better sketch feature representations.|
|||We find that strokelevel temporal information is indeed helpful in sketch feature learning in that it alone can outperform CNN features for the sketch recognition task, and offers the best performance when combined with CNN features.|
|||In this work, we for the first time, propose to combine the best from the both world for human sketches  utilizing CNN to extract abstract visual concepts and RNN to model human sketching temporal orders.|
|||Input: K = {Kn = (Pn, Sn)}N  n=1, Y = {yn}N  1: Train CNN from scratch using {Pn}N 2: Train RNN from scratch using {Sn}N 3: Parallelly connect pretrained RNN and CNN branches via hashing quantization-encoding layer.|
|||end for Fix cnn, rnn, calculate bt+1  10: 11: end for Output: Network parameters: cnn and rnn.|
|||DSH-Supervised [15] corresponds to a single-branch CNN model, but with noticeable difference in how to model the category-level discrimination, where pairwise contrastive loss is used based on the semantic pairing labels.|
|||DLBHC [14] replaces our two-branch CNN-RNN module with a single-branch CNN module, with  4.3.|
|||(ii) The gap between our model and DLBHC suggests the benefits of combining segment-level temporal information exhibited in a vector sketch with static pixel visual cues, the basis forming our CNN-RNN two-branch network, which may credit to (1) despite human tends to draw abstractly, they do share certain category-level coherent drawing styles, i.e., starting with a circle when sketching a sun, such that introducing additional discriminative power; (2) CNN suffers from sparse pixel image input [34] but prevails at building conceptual hierarchy [17], where RNN-based vector input brings the complements.|
|||Model  Sketch-a-Net [33]  ResNet 50 [6]  our RNN branch  our CNN branch  our RNN&CNN + CEL  our RNN&CNN + CEL + SCL  Accuracy  0.6871  0.7864  0.7788  0.7376  0.7949  0.8051  Table 6.|
||9 instances in total. (in cvpr2018)|
|466|Baque_Deep_Occlusion_Reasoning_ICCV_2017_paper|The CRF is defined as a sum of innovative highorder terms whose values are computed by measuring the discrepancy between the predictions of a generative model that accounts for occlusions and those of a CNN that can infer that certain image patches look like specific body parts.|
|||The very popular method of [23] performs both steps in a single CNN pass through the image.|
|||Combining CNNs and CRFs  Using a CNN to compute potentials for a Conditional Random Field (CRF) and training them jointly for structured prediction purposes has received much attention in recent years [18, 10, 11, 29, 2, 15, 17, 3].|
|||Modeling Occlusions in a CNN Framework  The core motivation behind our approach is to properly handle occlusions, while still leveraging the power of CNNs and on perfectly calibrated, fixed cameras.|
|||, W c}, let Lc k be the set of such projections that contain k.  We also introduce a CNN that defines an operator F(; F ), which takes as input the RGB image of camera c and outputs a feature map F c = F(I c; F ), where F denotes the networks parameters.|
|||Our discriminative model relies on a CNN which tries to predict similar distributions of 2D vectors, directly by looking at the image.|
|||More precisely, the CNN fu that appears in the unary terms of Eq.|
|||It handles occlusion while taking full advantage of the power of a modern CNN and can be trained either in a supervised or unsupervised manner.|
|||A limitation, however, is that the CNN used to compute our unary potentials still operates in each image independently as opposed to pooling very early the information from multiple images and then leveraging the expected appearance consistency across views.|
||9 instances in total. (in iccv2017)|
|467|cvpr18-xUnit  Learning a Spatial Activation Function for Efficient Image Restoration|In this paper, we propose a different mechanism for improving CNN performance (see Fig.|
|||Therefore, it allows using far fewer layers to match the performance of a CNN with ReLU activations.|
|||Today, most CNN architectures are at one extreme of the spectrum, with 0% of the parameters invested in the activations.|
|||Specifically, we take state-of-theart CNN models for image denoising [48], super-resolution [9, 23], and de-raining [10], which are already considered to be very light-weight, and replace their activations with xUnits.|
|||[48] were the first to train a very deep CNN for denoising, yielding state-of-the-art results.|
|||3. xUnit  binary map which is a thresholded version of zk,  Although a variety of CNN architectures exist, their building blocks are quite similar, mainly comprising of convolutional layers and per-pixel activation units.|
|||Conclusion  Popular CNN architectures use simple nonlinear activation units (e.g.|
|||We illustrated how our approach can reduce the size of several state-of-the-art CNN models for denoising, de-raining and super-resolution, which are already considered to be very small, by almost 50%.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||9 instances in total. (in cvpr2018)|
|468|VidLoc_ A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization|The many layers of a pre-trained CNN form a hierarchical model with increasingly higher level representations of the input data as one moves up the layers.|
|||Posenet [10] demonstrated the feasibility of estimating the pose of a single RGB image by using a deep CNN model to regress directly on the pose.|
|||Our model processes the video image frames using CNN and integrates temporal information through a bidirectional LSTM.|
|||Image Features: CNN  The goal of the CNN part of our model is to extract relevant features from the input images that can be used to predict the global pose of an image.|
|||A CNN consists of stacked layers performing convolution and pooling operations on the input image.|
|||The input to the LSTM is the output of the CNN consisting of a sequence of feature vectors, xt.|
|||Posenet Posenet uses a CNN to predict the pose of an input RGB image.|
|||Predicting a pose thus only requires a forward pass of the image through the CNN and propagating the hidden state.|
|||Logo recognition using cnn features.|
||9 instances in total. (in cvpr2017)|
|469|Wilber_Learning_Concept_Embeddings_ICCV_2015_paper|For example, one can imagine building a CNN to compare food dishes based solely on their appearance.|
|||To generate K, we finetune a CNN to a classification task on all images in the 186 known classes.|
|||As baselines, we calculate CNN features and try to cluster them with KMeans and spectral clustering, which do not benefit from extra human effort.|
|||All of the algorithms which use K CN N generally outperform SNaCK when using a pre 985  0100200300400500600700800Human effort: How many expert annotations?0.00.10.20.30.40.50.60.70.80.91.0Fraction of correctly labeled imagesIncrementally labeling CU Birds 200, "Birdlets" setKMeans on CNNfeaturesSpectral Clusteringon CNN featuresNaive expert labeling,adjusted for chanceSNaCK, CNN kernelt-STEMPCKMeans, CNN kernelCSPKmeans, CNN kernelLabel Propagation,CNN kernelSNaCK, pre-trained kernelSNaCK, HOG kerneltrained GoogLeNet kernel.|
|||We then discover labels on the training images using varying numbers of expert annotations and train a linear SVM classifier on all CNN features using the discovered labels.|
|||This data contains a variety of meals, appetizers, and snacks  986  050100150200250300350Expert effort: Number of labels revealed in training set0.400.450.500.550.600.650.700.750.800.850.90Classification accuracy on testing setSemisupervised classificationCU Birds 200, "Birdlets" setTraining labels fromKMeans on CNNfeaturesTraining labels fromSpectral Clusteringon CNN featuresSNaCKdiscovers training labelsExpert reveals ntraining labelsLabel propagationdiscovers training labelsMPCKmeans discoverstraining labelsFigure 8.|
|||i,j  To build Food Kernel 2, we fine-tuned a CNN to predict a food label.|
|||Our CNN model provides an excellent kernel to start from: when trained via the standard Food-101 protocol, this model achieves rank 1 classification accuracy of 73.5%.|
|||Using CNN features pre-trained on ImageNet, we can create an embedding that does a good job of grouping visually similar Emoji together.|
||9 instances in total. (in iccv2015)|
|470|cvpr18-GraphBit  Bitwise Interaction Mining via Deep Reinforcement Learning|More specifically, we perform a sigmoid function at the end of CNN for normalization, modelling the normalized elements as the possibilities of being quantized into one in a binomial distribution.|
|||We simultaneously train the parameters of CNN and the structure of the graph in an unsupervised manner, maximizing the mutual information of each bit with the observed inputs and the related bits for ambiguity elimination.|
|||For each input image, we first learn a normalized feature with a pre-trained CNN by replacing the softmax layer with a fully connection layer followed by a sigmoid function.|
|||Then, we simultaneously train the parameters of CNN and the structure of the graph for ambiguity elimination, mining bitwise relationships through deep reinforcement learning.|
|||As shown in Figure 2, we initialize the CNN with the pre-trained 16 layers VGG network [40], substituting the softmax layer with a fully connection layer followed by an activation function of sigmoid.|
|||Unlike most existing binary descriptors which directly utilize a sign function for binarization, we first perform a sigmoid function at the end of CNN to normalize each element in the range of 0 to 1 for better reliability estimation.|
|||[log q(b  snp(bsn|xn,btn) (10)  We apply the stochastic gradient decent with backpropagation to train the CNN model in an unsupervised manner.|
|||We employ a CNN network with the deconvolution layer in the end as our policy network.|
|||At the end of the sequence, we retrain the parameters of CNN with the learned structure of graph under the guidance of the objective function.|
||9 instances in total. (in cvpr2018)|
|471|shi_jin_Learning_to_Dodge_ECCV_2018_paper|[24] propose a generative CNN to synthesize models given existing  4  Shi Jin and Ruiynag Liu and Yu Ji and Jinwei Ye and Jingyi Yu  instances.|
|||[25] use CNN to generate arbitrary perspectives of an object from one image and recover the objects 3D model using the synthesized views.|
|||[26, 27] apply CNN to interpolate video frames.|
|||These methods use CNN to directly predict pixel colors from scratch and often suffer from blurriness and distortions.|
|||[28] propose to insert differentiable layers to CNN in order to explicitly perform geometric transformations on images.|
|||This design allows CNN to exploit geometric cues (e.g., depths, optical flow, epipolar geometry, etc.)|
|||[30] apply CNN on light field view synthesis.|
|||[8] estimate appearance flow by CNN and use it to synthesize new perspectives of the input image.|
|||This work is closely related to ours since we apply CNN on similar morphing task.|
||9 instances in total. (in eccv2018)|
|472|cvpr18-Real-Time Seamless Single Shot 6D Object Pose Prediction|The key component of our method is a new CNN architecture inspired by [27, 28] that directly predicts the 2D image locations of the projected vertices of the objects 3D bounding box.|
|||BB8 [25] is a 6D object detection pipeline made of one CNN to coarsely segment the object and another to predict the 2D locations of the  projections of the objects 3D bounding box given the segmentation, which are then used to compute the 6D pose using a PnP algorithm [16].|
|||In this paper, we propose a single-shot deep CNN architecture that takes the image as input and directly detects the 2D projections of the 3D bounding box vertices.|
|||It extends single shot CNN architectures for 2D detection in a seamless and natural way to the 6D detection task.|
|||Techniques such as Viewpoints and Keypoints [34] and Render for CNN [33] cast object categorization and 3D pose estimation into classification tasks, specifically by discretizing the pose space.|
|||In parallel to these developments, on the 2D object detection task, there has been a progressive trend towards single shot CNN frameworks as an alternative to two-staged methods such as Faster-RCNN [29] that first find a few candidate locations in the image and then classifies them as objects or background.|
|||This led us to design the CNN architecture [27, 28] shown in Fig.|
|||This parameterization is general and can be  293  S  S  (9x2+1+C)  Figure 1: Overview: (a) The proposed CNN architecture.|
|||Conclusion  We have proposed a new CNN architecture for fast and accurate single-shot 6D pose prediction that naturally extends the single shot 2D object detection paradigm to 6D object detection.|
||9 instances in total. (in cvpr2018)|
|473|cvpr18-Video Person Re-Identification With Competitive Snippet-Similarity Aggregation and Co-Attentive Snippet Embedding|The powerful feature learning ability of CNN further facilitates the utilization of video data.|
|||[19] built a CNN to extract features from each frame and then adopted a Recurrent Neural Network (RNN) to pass mes[15] focused sages between different frames.|
|||3, which consists of a CNN module for learning per-frame visual feature, a co-attention module for snippet pair embedding and a similarity estimation module to output the snippet similarity.|
|||We employ a CNN for obtaining visual features (s).|
|||Both value and key features are generated by a fully connected layer followed by a BN-layer [9] with the CNN features as their inputs.|
|||The query feature is based on the CNN features of the probe snippet by applying LSTM for summarization.|
|||, (9)  Lid =   1 N  NXn=1  IXi=1  PI  where xn indicates the CNN feature of nth image.|
|||If the nth image belongs to the ith person, yi,n = 1, otherwise yi,n = 0. wi are the coefficients associated with the feature embedding of the ith person, which is online updated with the CNN feature of the ith person.|
|||To investigate how  1174  Tracklet  Optical  Network modules  iLIDS-VID  PRID2011  representation a. multi-shot images b. multi-shot images c. complete sequence d. one snippet (L=8) e. multiple snippets (L=8) f. multiple snippets (L=8)  flow  CNN                          CE             SSE             top-1 63.8 66.7 74.4 69.2 79.8 85.4  top-5 mAP 68.9 88.1 71.3 89.8 92.5 78.4 73.8 90.9 82.6 91.8 96.7 87.8  top-1 82.7 86.3 86.0 84.6 88.6 93.0  top-5 mAP 85.3 94.2 88.5 96.2 97.8 88.7 87.2 96.6 90.9 99.1 99.3 94.5  top-1 75.6 79.9 82.4 82.1 81.2 86.3  Mars top-5 mAP 63.7 90.2 66.6 91.6 92.9 67.5 71.1 93.4 69.4 92.1 94.7 76.1  Table 1: Comparison of different sequence representations with our proposed approach, where CNN, CE, SSE represent the CNN module for per-frame feature learning, temporal co-attentive embedding module and snippet-similarity estimation module, respectively.|
||9 instances in total. (in cvpr2018)|
|474|Chang_Chen_Deep_Boosting_for_ECCV_2018_paper|Following the successful paradigm of end-to-end training, we also adopt the CNN for image denoising.|
|||Different from its common usages, however, the employed CNN is just a component of our denoising model.|
|||Experimental results demonstrate the superior performance of our boosting framework compared with a single CNN model.|
|||Contrastively, our proposed DBF inherits both advantages of boosting and CNN and achieves a new state-of-the-art performance for image denoising.|
|||Note that, boosting and CNN have been combined for image classification tasks before, e.g., IB-CNN [23] and BoostCNN [24], yet our proposed DBF is the first deep boosting framework in the field of image restoration.|
|||Specifically, we introduce a CNN to learn the denoising model in each stage.|
|||3.3 Relationship to TNRD  The TNRD model proposed in [11] is also a stage-wise model trained jointly, which can be formulated as  xn  xn1 = D(xn1)  R(xn1, y),  (12)  where D() stands for the diffusion term which is implemented using a CNN with two layers and R() denotes the reaction term as R(xn1, y) = (xn1y), where  is a factor which denotes the strength of the reaction term.|
|||4.2 Dilated Convolution  Widening the receptive field of the CNN is a well-known strategy for enhancing the performance in both image classification [26] and restoration [27] tasks.|
|||Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||9 instances in total. (in eccv2018)|
|475|cvpr18-Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning|As we will further present in the experimental section, RL-Restore is superior to CNN approaches given similar complexity and it requires 82.2% fewer computations to achieve the same performance as a single large CNN.|
|||[44] propose a 20-layer deep CNN to address multiple restoration tasks simultaneously, including image denoising, JPEG artifacts reduction and super-resolution.|
|||We apply a three-layer CNN (as in [8]) for slight distortions and a deeper eight-layer CNN for severe distortions.|
|||Each tool is either a 3-layer CNN or an 8-layer CNN according to the distortion it targets to solve.|
|||The first module, named feature extractor, is a four-layer CNN followed by a fully-connected (fc) layer that outputs a 32-dimensional feature.|
|||The results on mild and moderate sets show that our approach is apparently superior to VDSR-s while comparable to DnCNN and VDSR, demonstrating that the proposed RL-Restore could achieve the same performance as a deep CNN with much lower complexity.|
|||It indicates that our RL-based approach is more flexible in handling unseen distortions, while it is more difficult for a fixed CNN to generalize towards unseen cases.|
|||When realworld distortions (e.g., slight out-of-focus blur, exposure noise and JPEG artifacts) are close to the training data, the proposed RL-Restore can be easily generalized to these problems and performs better than a single CNN model.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||9 instances in total. (in cvpr2018)|
|476|Efficient Diffusion on Region Manifolds_ Recovering Small Objects With Compact CNN Representations|Efficient Diffusion on Region Manifolds:  Recovering Small Objects with Compact CNN Representations  Ahmet Iscen1 Giorgos Tolias2 Yannis Avrithis1 Teddy Furon1 Ondrej Chum2  1Inria Rennes  2VRG, FEE, CTU in Prague  {ahmet.iscen,ioannis.avrithis,teddy.furon}@inria.fr  {giorgos.tolias,chum}@cmp.felk.cvut.cz  Abstract  Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations.|
|||Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image.|
|||Local CNN features from multiple image regions have been investigated for this purpose, either aggregated [17, 52] or represented as a set [44].|
|||We employ a CNN that is fine-tuned for image retrieval [43] to extract global and regional representation.|
|||This, due to the overlapping nature of the CNN regions, may filter out incorrect neighbors.|
|||AQE is also effective with CNN global representation [52, 30, 18].|
|||Using recent CNN architectures, we achieve state-ofthe-art and near optimal performance on two popular benchmarks and a recent more challenging dataset.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Particular object retrieval with integral max-pooling of cnn activations.|
||9 instances in total. (in cvpr2017)|
|477|Differential Angular Imaging for Material Recognition|A CNN is used on both streams of the network and then combined for the final prediction result.|
|||We employ the ImageNet [14] pre-trained VGG-M model [4] as the prediction unit (labeled CNN in Figure 5).|
|||A third method (see Figure 5 (c)) is a hybrid of the two architectures that preserves the original CNN path for the original image Iv by combining the layer M feature maps for both streams and by combining the prediction outputs for both streams as shown in Figure 5 (c).|
|||Additionally, we compare the results to recognition using a standard CNN without a differential image stream.|
|||Notice that single view DAIN achieves better recognition accuracy than multiview CNN with voting (79.4% vs. 78.1%).|
|||The last three rows of Table 3 show that recognition performance using multiview DAIN beats the performance of both single view DAIN and CNN methods with no differential image stream.|
|||The CNN module of our DAIN network can be replaced by other state-of-the-art deep learning methods to further improve results.|
|||The differential angular imaging network (DAIN) has superior performance over CNN even when comparing single view DAIN to multiview CNN.|
|||Bilinear cnn models for fine-grained visual recognition.|
||9 instances in total. (in cvpr2017)|
|478|Ushiku_Common_Subspace_for_ICCV_2015_paper|Such linear weight vectors are also employed in combination with high-dimensional image feature [37], and in the full connected layers in CNN [17].|
|||For image feature, we used the Fisher Vector (FV) [31] with SIFT [24] and CNN [17, 39] pretrained with 1.2M images from ILSVRC [34] using Caffe [13].|
|||For the CNN feature, we experimentally found that the output of the seventh layer was optimal.|
|||Comparison to neural networks + MS COCO  After the first submission of this paper, various works based on neural networks such as CNN and RNN are evaluated using larger dataset than PASCAL Sentence.|
|||In this subsection, we employed two kinds of CNN features: AlexNet [17] and VGG net [39].|
|||[20] 0.11 (0.11)  BLEU  NIST  2  3  4  5  FV+CoSMoS  0.20  0.09 0.04 0.02 1.15  Google NIC [44] combines LSTM net and GoogLeNet [40], a deeper CNN than VGG net.|
|||Obviously, we can introduce CoSMoS between CNN and LSTM because CoSMoS can propagate the gradient of loss backward, although a proposal of that combination is beyond the scope of this paper.|
|||This paper utilizes CNN only for feature extraction.|
|||In a future work, we can integrate CoSMoS into combined network of CNN and RNN [4, 14, 44].|
||9 instances in total. (in iccv2015)|
|479|Heber_Convolutional_Networks_for_CVPR_2016_paper|On the one hand, the generated LFs are used to train a CNN for the hyperplane prediction.|
|||Unlike all the above mentioned methods, we suggest to train a CNN that allows to predict for each imaged 3D scene point the corresponding 2D hyperplane orientation in the LF domain.|
|||In order to handle textureless regions a 4D regularization is applied to obtain the final result, where a confidence measure is used to gauge the reliability of the CNN prediction.|
|||In order to train the CNN we make use of the caffe framework [19], where we use Adam [20] as the optimization method to minimize the Euclidean loss.|
|||The data term in our model ensures that the final solution is close to the predicted measurements of the CNN and is thus defined as  D(u, f ) =  1 2  kc  (u  f )k2  2 ,  (4)  where c is a confidence measure (c.f .|
|||When considering Figure 6, that provides network predictions and refinement results for two examples of the POV-Ray test set, we see that the CNN is able to predict reasonable 2D hyperplane orientations.|
|||The figure shows, from left to right, the LF data, the color-coded ground truth, the CNN prediction, and the refinement result.|
|||However, it should be emphasized that a main drawback of the presented method is to apply the CNN in a sliding window fashion, which results in considerable high computational costs.|
|||We trained a CNN to predict 2D hyperplane orientations in the LF domain.|
||9 instances in total. (in cvpr2016)|
|480|cvpr18-End-to-End Convolutional Semantic Embeddings|The experimental results show that our proposed textual CNN models with the new learning objective lead to better performance than the state-of-the-art approaches.|
|||When designing our CNN architecture for semantic learning, we carefully include several onedimensional convolutional layers for encoding phrases with different length.|
|||Similar to the approach in [19], we map images and sentences into a common semantic space, but employing CNN for sentence encoding instead of RNN.|
|||For the proposed text CNN architecture, we have four one dimensional convolutional layers with kernel sizes one, three, five and seven.|
|||Different from the visual CNN, which is pre-trained on the ImageNet dataset, the textual CNN is relatively trained from scratch.|
|||Without sufficient data, the textual CNN can only learn to encode the sentence to be semantically close to its matching image.|
|||In particular, we design a CNN for text encoding and a simple yet effective intermediate objective function to assist the global semantic learning.|
|||The experimental results indicate that the proposed textual CNN improves the semantics for sentences and has better generalizability than the state-of-the-art.|
|||More analysis on both intermediate representations and the correlation between visual CNN and textual CNN is necessary for designing better models.|
||9 instances in total. (in cvpr2018)|
|481|Hu_Attribute-Enhanced_Face_Recognition_ICCV_2017_paper|After CNN training, SVMs are trained for attribute recognition, and the vector of SVM scores provide the new feature for face verification.|
|||Integration with CNNs: architecture  In this section, we introduce the CNN architectures used for face recognition (LeanFace) designed by ourselves and facial attribute recognition (AttNet) introduced by [50, 30].|
|||To detect facial attributes, our AttNet uses the architecture of Lighten CNN [50] to represent a face.|
|||As shown in Table 4, LeanFace and Light CNN [49] already achieve very impressive performance due to their big training data and effective deep learning architectures.|
|||It is noteworthy that the gallery and probe are VIS and NIR images respectively, while LeanFace and Light CNN are trained using only VIS images.|
|||(%)  C-CBFD+LDA [25]  Dictionary Learning [13]  Gabor+RBM [53] Light CNN [49]  LeanFace  AttNet  GTNN (LeanFace, AttNet)  81.8 78.46 86.16 91.88 97.27 2.38 99.94  Comparison with other fusion methods.|
|||A light cnn for deep face representation with noisy labels.|
|||A lightened cnn for deep face  representation.|
|||Face attribute prediction  using off-the-shelf cnn features.|
||9 instances in total. (in iccv2017)|
|482|Optical Flow Requires Multiple Strategies (but Only One Network)|For optical flow, a few CNN based models were proposed.|
|||In [37], a CNN is used to predict the flow from a single static image.|
|||FlowNet [11] is the first end-to-end CNN for optical flow and showed competitive results.|
|||In the PatchBatch [13] pipeline, a CNN was used for extracting patch descriptors that are then used for matching via the PatchMatch [4] Nearest Neighbor Field (NNF) algorithms.|
|||Architecture improvements  In this paper, we improve the CNN that generates the descriptors.|
|||2, consists of a CNN which generates per-pixel descriptors and an approximate nearest neighbor algorithm which is later used to compute the actual assignments.|
|||To create each pixels descriptor, the CNN uses a patch as an input.|
|||In most of the CNN configurations described in PatchBatch, the input is a 51  51 patch centered around the examined pixel.|
|||The CNN uses the grayscale data of the patch to extract a descriptor as similar as possible to the one extracted for the matching pixel on the second image.|
||9 instances in total. (in cvpr2017)|
|483|Mingtao_Feng_3D_Face_Reconstruction_ECCV_2018_paper|In this paper, we exploit the Epipolar Plane Images (EPI) obtained from light field cameras and learn CNN models that recover horizontal and vertical 3D facial curves from the respective horizontal and vertical EPIs.|
|||2  M Feng, SZ Gilani, Y Wang and A Mian  Most existing methods have resorted to the use of prior models such as the Basal Face Model (BFM) [43] and the Annotated Face Model(AFM) [12] to generate synthetic data with ground truth to train CNN [11, 40] models and to recover the model parameters at test time.|
|||Training a CNN requires massive amount of photo-realistic labeled data.|
|||Recently, various attempts were made to integrate 3DMMs with CNN for facial geometry reconstruction from a single image.|
|||[41] extended the work [40] and introduced an end-to-end CNN framework that recovers the coarse facial shape using a CoarseNet, followed by a FineNet to refine the facial details.|
|||[29] proposed a 3DMM fitting method for face alignment, which uses a cascaded CNN to regress camera matrix and 3DMM parameters.|
|||[20] presented a method for reconstructing the shape from light field images that applies a CNN for pixel wise depth estimation from EPI patches.|
|||Jackson, A.S., Bulat, A., Argyriou, V., Tzimiropoulos, G.: Large pose 3d face reconstruction from a single image via direct volumetric cnn regression.|
|||Wang, T.C., Zhu, J.Y., Hiroaki, E., Chandraker, M., Efros, A.A., Ramamoorthi, R.: A 4d light-field dataset and cnn architectures for material recognition.|
||9 instances in total. (in eccv2018)|
|484|Jiahui_Zhang_Efficient_Semantic_Scene_ECCV_2018_paper|But meanwhile, we observe that 3D data has some attractive characteristics, which inspire us to build efficient 3D CNN blocks.|
|||Network acceleration methods in 2D CNN such as weight pruning, quantization, and Group Convolution (GC) design [20,50] can also be used, but these methods have not been well explored in 3D CNNs for now.|
|||Our method is orthogonal to Group Convolution, which is an operation widely used in recent CNN architectures [24,46,3].|
|||2 Related works  2.1  3D Deep Learning  The success of deep learning in 2D computer vision areas has inspired researchers to employ CNN in 3D tasks, such as object recognition [45,30,32], shape completion [45,40,4], and segmentation [40,1].|
|||OctNet [37] and O-CNN [44] used Octree-based 3D CNN for 3D shape analysis.|
|||Different methods have been presented in the 3D segmentation challenge [49], such as SCN, Pd-Network, densely connected PointNet, and Point CNN [27].|
|||Semantic Scene Completion with Spatial Group Convolution  5  For 3D completion tasks, advanced Octree-based CNN methods [36,41,17] were also used for generating high resolution 3D outputs.|
|||However, there is still intensive computation in 3D sparse CNN as mentioned above.|
|||Thus reducing the computation of 3D sparse CNN is necessary for realtime applications.|
||9 instances in total. (in eccv2018)|
|485|Huang_Beyond_Face_Rotation_ICCV_2017_paper|Specifically, we model the synthesis function with a two-pathway CNN GG that is parametrized by G.|
|||Since the Light CNN is pre-trained to classify tens of thousands of identities, it can capture the most prominent feature or face structure for identity discrimination.|
|||Specifically, we define the identity preserving loss based on the activations of the last two layers of the Light CNN [35]:  Lip =  2  X  i=1  1  Wi  Hi  Wi X  Hi X  x=1  y=1  |F (I P )i  x,y  F (G(I pred))i  x,y|  (6)  Except for synthesizing natural looking frontal view images, the proposed TP-GAN also aims to generate identity preserving image for accurate face analysis with off-theshelf deep features.|
|||* [12] HPN [7] FIP 40 [41] c-CNN Forest [36] Light CNN [35] TP-GAN*   29.82 31.37 47.26 9.00 64.03  47.57 49.10 60.66 32.35 84.10   44.81 61.24 69.75 74.38 73.30 92.93  71.65 74.68 72.77 85.54 89.02 97.45 98.58  81.05 89.59 78.26 92.98 94.05 99.80 99.85  89.45 96.78 84.23 96.30 96.97 99.78 99.78  I P and the prediction I pred = GG(I P ).|
|||Method  90  75  60  45  30  15  FIP+LDA [41] MVP+LDA [42] CPF [38] DR-GAN [30] Light CNN [35] TP-GAN*   5.51 64.64  24.18 77.43  45.9 60.1 61.9 83.2 62.09 87.72  64.1 72.9 79.9 86.2 92.13 95.38  80.7 83.7 88.5 90.1 97.38 98.06  90.7 92.8 95.0 94.0 98.59 98.68  Table 3.|
|||Note that all the compared CNN based methods achieve their best performances with learned intermediate features, whereas we directly use the synthesized images following a recognition via generation procedure.|
|||The CNN for gender classification is of the same structure as the encoder Gg and is trained on batch1 of the UMD [3] dataset.|
|||In a recent study on face hallucination [34], the authors show that directly using a CNN synthesized high resolution face image for recognition will certainly degenerate the performance instead of improving it.|
|||A light cnn for deep face representation with noisy labels.|
||9 instances in total. (in iccv2017)|
|486|Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper|Only few works acknowledge this problem and try to apply CNN to the MVS reconstruction: SurfaceNet [14] constructs the Colored Voxel Cubes (CVC) in advance, which combines all image pixel color and camera information to a single volume as the input of the network.|
|||The recent state-of-the-art method is GCNet [17], which applies 3D CNN to regularize the cost volume and regress the disparity by the soft argmin operation.|
|||The first learning based pipeline for MVS problem is SurfaceNet [14], which pre-computes the cost volume with sophisticated voxel-wise view selection, and uses 3D CNN to regularize and infer the surface voxels.|
|||The most related approach to ours is the LSM [15], where camera parameters are encoded in the network as the projection operation to form the cost volume, and 3D CNN is used to classify if a voxel belongs to the surface.|
|||An eight-layer 2D CNN is applied, where the strides of layer 3 and 6 are set to two to divide the feature towers into three scales.|
|||We notice that recent work [11] applies the mean operation with multiple CNN layers to infer the multi-patch similarity.|
|||Here we choose the variance operation instead because the mean operation itself provides no information about the feature differences, and their network requires preand postCNN layers to help infer the similarity.|
|||Inspired by recent learning-based stereo [17] and MVS [14, 15] methods, we apply the multi-scale 3D CNN for cost volume regularization.|
|||Although the multi-scale 3D CNN has very strong ability to regularize the probability to the single modal distribution, we notice that for those falsely matched pixels, their probability distributions are scattered and cannot be concentrated to one peak (see Fig.|
||9 instances in total. (in eccv2018)|
|487|Gupta_Cross_Modal_Distillation_CVPR_2016_paper|More importantly, our work allows us to extend the success of recent deep CNN architectures to new imaging modalities without having to collect large scale labeled datasets necessary for training deep CNNs.|
|||More recently, with the introduction of supervised CNN models by Krizhevsky et al.|
|||All this analysis helped us boost the mean AP on the test set from 38.80% as reported by [14, 15] to 44.39%, using the same CNN network and supervision.|
|||[15] embed depth images into a geocentric embedding which they call HHA (HHA encodes horizontal disparity, height above ground and angle with gravity) and use the AlexNet architecture to learn HHA features and copy over the weights from the RGB CNN that was trained for 1000 way classification [26] on ImageNet [5] to initialize this network.|
|||High scoring patches from RGB CNN (AlexNet in this case), correspond to parts of object (g), high scoring patches from the depth CNN also corresponds to parts of the same object class (h and i).|
|||We follow the same experimental setup as proposed in [16], and fix the CNN parameters (to a network that was finetuned for detection on NYUD2 dataset) and only learn the classifier parameters and use features from pool2 and conv4 layers in addition to fc7 for figure ground prediction.|
|||Note that in obtaining these performance improvements we are using exactly the same CNN architecture and amount of la 2833  method  modality  RGB Arch.|
|||refers to the CNN architecture used by the detector.|
|||We also report performance on the SDS task in Table 3 and obtain state-of-the-art performance of 40.5% as compared to previous best 37.5% [14] when using AlexNet, using VGG CNN for the RGB image improves performance further to 42.1%.|
||9 instances in total. (in cvpr2016)|
|488|Rao_Learning_Discriminative_Aggregation_ICCV_2017_paper|This means with DAN we can greatly reduced the number of images to be processed, while the aggregated images still have more discriminative ability in  the feature space of certain CNN F .|
|||We also report the result by directly passing all the video frames through feature extraction CNN F with mean pooling for comparison, denoted as CNN.|
|||We measured the performance by mean pooling on corresponding CNN features and denoted results as Random + CNN and Hierarchical Pooling respectively.|
|||The results show that on all three datasets, DAN outperforms the original CNN for dense feature extraction.|
|||This is largely due to the baseline CNN which is comparatively weaker than those of [37] and [45].|
|||But the gained improvement over baseline CNN result has already proven the effectiveness.|
|||Introducing adversarial loss will contribute to more realistic and therefore more discriminative images than the MSE loss, but are below dense CNN feature extraction baseline.|
|||By combining discriminative loss LDis and feature embedding based reconstruction loss LRec F C we can obtain the best result beyond CNN baseline.|
|||Unconstrained face verification using deep cnn features.|
||9 instances in total. (in iccv2017)|
|489|A Low Power, Fully Event-Based Gesture Recognition System|The CNN achieves 96.5% out-of-sample accuracy on a newly collected DVS dataset (DvsGesture) comprising 11 hand gesture categories from 29 subjects under 3 illumination conditions.|
|||The event-based SpiNNaker processor has been used to run a 5-layer CNN on prerecorded DVS events to recognize playing cards [39], but we know of no prior work that combines an event-based sensor with an eventbased processor to perform gesture recognition in real-time.|
|||For example, a convolution layer with a 11 kernel receiving  47246  Figure 4: System block diagram showing how many of the 4096 TrueNorth cores are allocated to each component (for one of the experiments); examples of instantaneous event maps output by a) the DVS, b) the temporal filter cascade, and cg) layers 1, 3, 8, 12, and 15 of the CNN described in Table 1; and histograms of events corresponding to the 11 gesture categories after h) the final convolution layer, i) the winner-take-all decoder, and j) the sliding window filter.|
|||Since the only extant DVS dataset for gesture recognition is not large enough to reliably train a CNN [20], we used the DVS128 to create a new hand gesture dataset that includes timestamped DVS128 event files, RGB videos from a webcam mounted next to the DVS, and ground-truth files with gesture labels and start and stop times.|
|||Classifier training  In order to train the convolution layers on the appropriate signals, we first preprocessed the dataset by feeding the raw DVS events through the same temporal filter cascade that shapes the input to the CNN during runtime.|
|||We trained a CNN on the preprocessed data using a GPU-accelerated implementation of the Eedn algorithm.|
|||map size  6464 3131 1414 1414  77 77 77 77 77 33 33 33 33 11 11 11 11  feat -ures  6 12 252 256 256 512 512 512 512 512 1024 024 1024 1024 1024 968 2640  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  kernel  stride  pad  groups  33 44 11 22 33 11 11 11 22 33 11 11 22 11 11 11  2 2 1 2 1 1 1 1 2 1 1 1 2 1 1 1  0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0  1 2 2 2 32 4 4 4 16 64 8 8 32 8 8 8  Table 1: CNN specification for the gesture recognition system.|
|||We took the CNN structure and all training metaparameters without modification from [9], and explored the impact of dataset preprocessing, varying the input image sampling (32  32, 42  42, and 64  64), and lengthening the temporal footprint of each sample by adding stages to the temporal filter cascade or increasing the decay period in each stage.|
|||Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled.|
||9 instances in total. (in cvpr2017)|
|490|Kohei_Uehara_Visual_Question_Generation_ECCV_2018_paper|Recent methods perform object detection that performs both object region proposal and object classification at the same time via supervised learning using CNN [21, 20].|
|||Finally, the target region along with the whole image is coded into a feature vector, and a question for the unknown object is generated  object as unknown if its feature distribution extracted from the CNN hidden layers is distant from known classes.|
|||The common approach in VQG is encoding image features via CNN and generating a sentence by decoding those features using an RNN.|
|||Specifically, we perform unknown object classification on the classification results of a CNN as follows.|
|||The output of the softmax function of the CNN can be regarded as the confidence with which the input is classified into a certain class.|
|||From the viewpoint of the naturalness of the question, the difference between the proposed method and CNN + LSTM was small.|
|||This is because, CNN + LSTM does not specify a target object to the decoder, so it may generate questions that are related to the image but not the target region.|
|||As baseline methods, we used the CNN + LSTM method and the Nearest Neighbors Retrieval method described in Sec.|
|||Examples of input images (upper), the target words and generated questions by our proposed VQG method for unknown objects (middle), and the generated questions by the CNN + LSTM and retrieval baselines (lower).|
||9 instances in total. (in eccv2018)|
|491|Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper|The CNN is used to learn the mapping from the head poses and eye images to gaze directions in the camera coordinate system.|
|||Gaze Estimation With Multimodal CNNs  The task for the CNN is to learn the mapping from the input features (2D head angle h and eye image e) to gaze angles g in the normalised space.|
|||We train a linear regression layer on top of the fully connected layer to predict gaze angle vectors g. We use a multimodal CNN model to take advantage of both eye image and head pose information [32].|
|||We encode head pose information into our CNN model by concatenating h with the output of the fully connected layer (see Figure 6).|
|||To further discuss the performance limits of the CNNbased approach, we also show more detailed comparisons between RF and CNN models.|
|||We first show a comparison between different architectures of the CNN on the UT Multiview dataset with the same three-fold cross-validation setting as reported in [39]  CNN (ours) RF Clustered CNN Eye image CNN  for each person from the above subset, and the other 2,500 samples were used as training data.|
|||e e r g e d [   s r o r r e   n a e M  12  10  8  6  4  2  0  UT  UT subset  MPIIGaze  UT subset  (person-specific)  (person-specific)  Figure 10: Comparison of the different CNN models and RF on (from left to right): UT Multiview dataset, subset of the UT Multiview dataset which has the same head and gaze angle ranges as the MPIIGaze dataset, using person-specific training on the MPIIGaze dataset, and using person-specific training on the UT Multiview subset.|
|||As can be seen, our proposed multimodal CNN model outperformed the RF method with 5.9 degrees mean error.|
|||Although [39] reported that their poseclustered structure improved their RF performance, the performance of the CNN became worse if the same clustering structure was introduced.|
||9 instances in total. (in cvpr2015)|
|492|Network Dissection_ Quantifying Interpretability of Deep Visual Representations|We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.|
|||The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].|
|||Scoring Unit Interpretability  The proposed network dissection method evaluates every individual convolutional unit in a CNN as a solution to a binary segmentation task to every visual concept in Broden (Fig.|
|||Our method can be applied to any CNN using a forward pass without the need for training or backpropagation.|
|||Experiments  For testing we prepare a collection of CNN models with different network architectures and supervision of primary tasks, as listed in Table 2.|
|||Here one unit of the last convolutional layer of a given CNN is probed by evaluating its performance on 1197 segmentation tasks.|
|||9 shows some typical visual detectors identified in the self-supervised CNN models.|
|||For colorization and tracking, recognizing textures might be good enough for the CNN to solve primary tasks such as colorizing a desaturated natural image; thus it is unsurprising that the texture detectors dominate.|
|||We applied network dissection to investigate the effects on interpretability of state-of-the art CNN training techniques.|
||9 instances in total. (in cvpr2017)|
|493|Sanghyun_Son_Clustering_Kernels_for_ECCV_2018_paper|First, we propose a new method to compress and accelerate the CNN by applying kmeans clustering to 2D kernels.|
|||Lastly, our extensive experiments show that our method is generally applicable to various CNN architectures and datasets.|
|||1: We compress convolutional layers (a) of given CNN as (b).|
|||We also accelerate the CNN by removing redundant computations from overlapping kernels in red and blue boxes.|
|||3.1 Compact representation of the kernels  In general, a CNN is trained without any structural restrictions on the shapes of its weight tensors.|
|||4.3, we will demonstrate that TIC allows very compact representation of a CNN and acts as a strong regularization term.|
|||(6)  (7)  8  S. Son, S. Nah and K. M. Lee  4 Experiments  We apply the proposed method to recent popular CNN architectures for image classification task: VGG [31], ResNets [15], and DenseNets [16].|
|||However, we found that the model performances are  Model VGG-16 [31] ResNet-56 [15] DenseNet-12-40 [16]  C128 6.24 6.76 5.44  C256 6.23 6.54 5.49  C512 6.16 6.61 5.39  C1024 6.01 6.30 5.38  Baseline  5.98 6.28 5.26  Table 1: We compress various CNN architectures on CIFAR10 and report the classification error rates (%).|
|||In other words, over 10,000 kernels of the CNN are sharing their shapes together while maintaining the classification accuracy.|
||9 instances in total. (in eccv2018)|
|494|Oyallon_Scaling_the_Scattering_ICCV_2017_paper|The second, referred to as a hybrid CNN, is a cascade of a scattering network and a standard CNN architecture, such as a ResNet [13].|
|||In the sequel, we do not make any distinction between the 1  1 CNN operators and the operator acting on Sx(u), u.|
|||A scattering transform with scale J can be interpreted as a CNN with depth J [27], whose channels indexes correspond to different scattering frequency indexes, which is a structuration.|
|||Fourier analysis permits the measurement of the smoothness of the operator and, in the case of CNN operator, it is a natural basis.|
|||Numerical performances of hybrid networks  We now demonstrate cascading modern CNN architectures on top of the scattering network can produce high performance classification systems.|
|||In a sec ond experiment, we again use a hybrid CNN architecture with a ResNet built on top of the scattering transform.|
|||In this small sample setting, a hybrid network outperforms the purely CNN based baselines, particularly when the sample size is smaller.|
|||Method Supervised methods Scat + WRN 20-8 CNN[37] Unsupervised methods Exemplar CNN [11] Stacked what-where AE [44] Hierarchical Matching Pursuit (HMP) [3] Convolutional K-means Network [8]  Accuracy  76.0  0.6 70.1  0.6  75.4  0.3  74.33 64.51 60.11  Table 7.|
|||Mean accuracy of a hybrid CNN on the STL-10 dataset.|
||9 instances in total. (in iccv2017)|
|495|Yang_CRAFT_Objects_From_CVPR_2016_paper|Practically, in proposal generation task, we add another CNN based classifier to distinguish objects from background given the output of off-the-shelf proposal algorithm (eg, Region Proposal Network); and in object classification task, since the N+1 class (N object categories plus background) cross-entropy objective leads the feature representation to learn inter-category variance mainly, we add a binary classifier for each object category in order to focus more on intra-category variance.|
|||The Multi-box uses CNN to regress the object location in an end-to-end manner.|
|||Browsing the history of computer vision, the feature representation is becoming more and more sophisticated, from hand-craft Haar [35] and HOG [7] to learning based CNN [15].|
|||In recent three years, with the revival of CNN [20], CNN based representation achieves excellent performance in various computer vision tasks, including object recognition and detection.|
|||[12, 21] use CNN cascade to rank sliding windows or re-rank object proposals.|
|||With the help of strong abstraction ability of CNN deep feature hierarchies, RPN is able to capture similarities among diverse objects.|
|||In addition, due to the resolution loss caused by CNN pooling operation and the fixed aspect ratio of sliding window, RPN is weak at covering objects with extreme scales or shapes.|
|||All CNN based methods use VGG16 net as initialization and are trained on PASCAL VOC 07+12 trainval set.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||9 instances in total. (in cvpr2016)|
|496|Spatiotemporal Pyramid Network for Video Action Recognition|One reason is that these CNN frameworks are not specifically designed for videos and cannot fully exploit spatiotemporal features.|
|||In addition to capturing the appearance information using standard CNN stream, several recent approaches try out using optical flow data in a second CNN stream to capture the motion information [22, 37, 6, 38].|
|||[12] compare multiple CNN connectivity methods in time, including late fusion, early fusion and slow fusion.|
|||[28] train a deeper CNN model called C3D on Sports1M.|
|||[22] train a second stream of CNN on optical flow frames and propose a two-stream ensemble network.|
|||Method  UCF101 HMDB51  Slow Fusion CNN [12] LRCN [5] C3D [28] Two-Stream (AlexNet) [22] Two-Stream + LSTM [37] Two-Stream + Pooling [37] Transformation [33] Two-Stream (VGG-16) [6] Two-Stream + Fusion [6] TSN (BN-Inception) [32] Ours (VGG-16) Ours (ResNet-50) Ours (BN-Inception)  65.4% 82.9% 85.2% 88.0% 88.6% 88.2% 92.4% 90.6% 92.5% 94.0% 93.2% 93.8% 94.6%   59.4%   62.0% 58.2% 65.4% 68.5% 66.1% 66.5% 68.9%  Table 7.|
|||This result also illustrate that our approaches are not any deep-network exclusive, but can be widely applied to many fancy CNN models.|
|||Bilinear cnn models for fine-grained visual recognition.|
||8 instances in total. (in cvpr2017)|
|497|Ligeng_Zhu_Sparsely_Aggregated_Convolutional_ECCV_2018_paper|This is exemplified by a series of popular CNN architectures, most notably: AlexNet [25], VGG [32], Inception [35,34], ResNet [16,17], and DenseNet [20].|
|||It places our contribution in the context of much of the recent research focus on CNN architecture.|
|||Taken together with our experimental results, sparse aggregation appears to be a simple, general improvement that is likely to filter into the standard CNN backbone design.|
|||2 Related Work  Modern CNN architectures usually consist of a series of convolutional, ReLU, and batch normalization [23] operations, mixed with occasional max-pooling and subsampling stages.|
|||Such connections route outputs of earlier CNN layers directly to the input of far deeper layers, skipping over the sequence of intermediate layers.|
|||Taking the plausible view that a CNN learns to compute increasingly abstract visual representations when going from shallower to deeper layers, skip connections can provide a pathway for assembling features that combine many levels of abstraction.|
|||Here, a block refers to a sequence of layers running at the same spatial resolution, between pooling and subsampling stages of the CNN pipeline.|
|||5 Conclusion  We demonstrate that following a simple design rule, scaling aggregation link complexity in a logarithmic manner with network depth, yields a new stateof-the-art CNN architecture.|
||8 instances in total. (in eccv2018)|
|498|gao_peng_Question-Guided_Hybrid_Convolution_ECCV_2018_paper|The mid-level visual features and language features are first learned independently using CNN and RNN.|
|||Recent research found that the combination of depth-wise convolution and channel shuffle with group convolution could reduce the number of parameters in CNN without hindering the final performance.|
|||Motivated by Xception [13], ResNeXt [14], and ShuffleNet [25], we decompose the visual CNN kernels into several groups.|
|||Note that for existing CNN methods with group convolution, the convolutional parameters are solely learned via back-propagation.|
|||Mapping the question features to generate full CNN kernels contains a huge number of learnable parameters.|
|||The output of the CNN fv+q fuses the textual and visual information and infers the final answers.|
|||CNN-LSTM [6] encodes images and questions using CNN and LSTM respectively.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual In: Proceedings of the IEEE International Conference on Computer  recognition.|
||8 instances in total. (in eccv2018)|
|499|Castrejon_Learning_Aligned_Cross-Modal_CVPR_2016_paper|[13, 29] learn a semantic embedding that joins representations from a CNN trained on ImageNet and distributed word representations.|
|||Our joint representation is different from previous works in that it is initially obtained from a CNN and sentence embeddings are mapped to it.|
|||However, since text cannot be fed into a CNN (descriptions are not images), we instead encode each description into skip thought vectors [23] and  Specific to  Modality  Shared Across All   Modalities  Image  Scene  Specific to Text  Skip   Thought  Vector  Shared Across All   Modalities  Scene  (a) Images  (b) Descriptions  Figure 3: Scene Networks: We use two types of networks.|
|||a) For pixel based modalities, we use a CNN based off [45] to produce pool5.|
|||For each model we have a separate CNN initialized using the weights of Places-CNN [45].|
|||We compare our results against three different finetuning baselines:  Finetuning individual networks (BL-Ind):  In this baseline we finetune a separate CNN for each of the modalities.|
|||This is the current standard approach employed in the computer vision community, but it does not enforce the representations in higher CNN layers to be aligned across modalities.|
|||Finetuning with a single shared CNN (BL-ShAll): here we use a single instance of Places-CNN shared by all modalities.|
||8 instances in total. (in cvpr2016)|
|500|Ni_Progressively_Parsing_Interactional_CVPR_2016_paper|Each frame image is input to a VGG-19 [25] CNN model for extracting image representation.|
|||We denote this feature as CNN feature x.|
|||Namely, the resolution of the Conv5 CNN feature map x is sufficient for object localization.|
|||The D-dimensional CNN image representation is further input to a recurrent network structure with H LSTM nodes (in this work, H is larger than M , i.e., to allow sufficient number of iterative refinement steps).|
|||Each LSTM takes the D-dimensional CNN image representation and the object parsing status C(l1) vector from the last LSTM node and outputs the detection vector B(l) for the current LSTM node (we use l to index the LSTM node).|
|||Note that for each frame, the image CNN feature xt is input to all LSTM nodes.|
|||The CNN feature (VGG-19) model is initialized by the ImageNet pre-trained model.|
|||Both the CNN part and LSTM part are jointly trained.|
||8 instances in total. (in cvpr2016)|
|501|Zhu_Aligning_Books_and_ICCV_2015_paper|Figure 3: Our CNN for context-aware similarity computation.|
|||In this paper, we adopt a 3-layer CNN as illustrated in Fig.|
|||We directly use the output of the CNN from 4.3 as the unary potential u().|
|||worse recall, and our 3 layer CNN outperforms SVM by a large margin.|
|||Columns show different instantiations of our model: we show the leave-one-featureout setting ( indicates that all features were used), compare how different depths of the context-aware CNN influence performance, and compare it to our full model (CRF) in the last column.|
|||We get the highest boost with a deeper CNN  recall improves by 10%, AP doubles.|
|||Fight Club  The Green Mile  Harry Potter and the Sorcerers Stone  American Psycho  One Flew Over the Cuckoo Nest Shawshank Redemption  The Firm  Brokeback Mountain  The Road  No Country for Old Men  Mean Recall  AP  AP Recall AP Recall AP Recall AP Recall AP Recall AP Recall AP Recall AP Recall AP Recall AP Recall  UNI  1.22 2.36 0.00 0.00 0.00 0.00 0.00 0.27 0.00 1.01 0.00 1.79 0.05 1.38 2.36 27.0 0.00 1.12 0.00 1.12  3.88 0.40  SVM  0.73 10.38 14.05 51.42 10.30 44.35 14.78 34.25 5.68 25.25 8.94 46.43 4.46 18.62 24.91 74.00 13.77 41.90 12.11 33.46  38.01 10.97    0.45 12.26 14.12 62.46 8.09 51.05 16.76 67.12 8.14 41.41 8.60 78.57 7.91 33.79 16.55 88.00 6.58 43.02 9.00 48.90  52.66 9.62  1 layer CNN w/o one feature  BLEU 0.41 12.74 14.09 60.57 8.18 52.30 17.22 66.58 6.27 34.34 8.89 76.79 8.66 36.55 17.82 92.00 7.83 48.04 9.39 49.63  52.95 9.88  TF-IDF  0.40 11.79 6.92 53.94 5.66 46.03 12.29 60.82 1.93 32.32 4.35 73.21 2.02 26.90 14.60 86.00 3.04 32.96 8.22 46.69  47.07 5.94  SENT 0.50 11.79 10.12 57.10 7.84 48.54 14.88 64.66 8.49 36.36 7.99 73.21 6.22 23.45 15.16 86.00 5.11 38.55 9.40 47.79  48.75 8.57  VIS 0.64 12.74 9.83 55.52 7.95 48.54 14.95 63.56 8.51 37.37 8.91 78.57 7.15 26.90 15.58 88.00 5.47 37.99 9.35 51.10  50.03 8.83  SCENE  PRIOR  CNN-3  CRF  0.50 11.79 13.00 60.57 8.04 49.37 15.68 66.58 9.32 36.36 9.22 75.00 7.25 30.34 15.41 86.00 6.09 42.46 8.63 49.26  50.77 9.31  0.48 11.79 14.42 62.78 8.20 52.72 16.54 67.67 9.04 40.40 7.86 78.57 7.26 31.03 16.21 87.00 7.00 44.13 9.40 48.53  52.46 9.64  1.95 17.92 28.80 74.13 27.17 76.57 34.32 81.92 14.83 49.49 19.33 94.64 18.34 37.93 31.80 98.00 19.80 65.36 28.75 71.69  66.77 22.51  5.17 19.81 27.60 78.23 23.65 78.66 32.87 80.27 21.13 54.55 19.96 96.79 20.74 44.83 30.58 100.00 19.58 65.10 30.45 72.79  69.10 23.17  Table 5: Performance of our model for movies in our dataset under different settings and metrics.|
|||The 1-layer CNN columns evaluate the leave-one-fature-out setting, where  indicates all features are used, BLEU: no BLEU sentence similarity measure is used, TF-IDF: no tf-idf sentence similarity measure, SENT: our sentence embedding is not used, VIS: our video-text embedding not used, SCENE: no hybrid-CNN [42] is used in video representation, PRIOR: no uniform prior is used.|
||8 instances in total. (in iccv2015)|
|502|cvpr18-Tagging Like Humans  Diverse and Distinct Image Annotation|The main difference of these works lies in designing an interface between CNN and RNN.|
|||In [9], features extracted by a CNN model were used as the hidden states of a RNN.|
|||In [21], the CNN features were integrated with the output of a RNN.|
|||In [14], the predictions of a CNN were used as the hidden states of a RNN, and the ground-truth tags of images were used to supervise the training of the CNN.|
|||1  The inner part qT (I, z) = (cid:0)W[fG(I); z] + bG(cid:1)   [0, 1]|T | is a CNN based soft classifier.|
|||Then, we formulate D(I, T ) as (4)  1  |T| XiT  (cid:0)w  D[fD(I); ti] + bD(cid:1) ,  D(I, T ) =  where fD(I) denotes the output vector of the fullyconnected layer of a CNN model (different from that used  in the generator).|
||| includes wD  R|fD(I)|+50, bD  R and the parameters of fD(I) in the CNN model.|
|||We firstly fix the CNN models in both G and D as the VGG-F model2 pre-trained on ImageNet [3].|
||8 instances in total. (in cvpr2018)|
|503|cvpr18-Decoupled Networks|Email:{wyliu,liuzhen1994}@gatech.edu  Figure 1: CNN learned features are naturally decoupled.|
|||These 2D features are output directly from the CNN by setting the feature dimension as 2.  whether the two samples have large semantic/label difference or have large intra-class variation.|
|||In order to better study the properties of CNN representation and further improve existing frameworks, we propose to explicitly decouple semantic difference and intra-class variation1.|
|||To this end, we use a CNN model that has the same structure and is pretrained on the same training set to initialize the DCNet.|
|||Consider a standard CNN with ReLU, we write the convolution and ReLU as max(0, kwkkxk cos()) which can be written as kwkkxk  max(0, cos()).|
|||(16))  Decoupled Operator (Standard Gradients) Decoupled Operator (Weight Projection) Decoupled Operator (Weighted Gradients)  Error  22.95 23.03 23.38 23.09 21.17 21.45  to significantly better accuracy than the baseline CNN with ReLU, even if our DCNet does not use ReLU at all.|
|||The results further verify that the intra-class and inter-class variation assumptions of the original CNN are not optimal.|
|||* indicates we use the pretrained model of original CNN on ImageNet-2012 as initialization (see Section 4.3).|
||8 instances in total. (in cvpr2018)|
|504|cvpr18-PAD-Net  Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing|Take the task of scene parsing as an example, a CNN trained with  1675  RGB-D data should perform better than the CNN trained with only the RGB data.|
|||If we do not have the depth data available, we can use a CNN to predict the depth maps and then use them as input.|
|||semantic and depth maps, as the multi-modal input, the powerful CNN is able to predict more information related, such as contour and surface normal.|
|||Specifically, we first learn to use a front-end deep CNN and the input RGB data to produce a set of intermediate auxiliary tasks (see Fig.|
|||[10] developed a multi-scale CNN for fusing both coarse and fine predictions from different semantic layers of the CNN.|
|||Front(cid:173)End Network Structure  The front-end backbone CNN could employ any network structures, such as the commonly used AlexNet [25], VGG [49] and ResNet [17].|
|||On the depth estimation task, we compare with several state-of-the-art methods, including: methods adopting hand-crafted features and deep representations [46, 46, 20, 27, 10, 9, 29, 28], and methods considering graphical modeling with CNN [33, 32, 61, 51, 54].|
|||Analyzing modular cnn architectures for joint depth prediction and semantic segmentation.|
||8 instances in total. (in cvpr2018)|
|505|Cheron_P-CNN_Pose-Based_CNN_ICCV_2015_paper|P-CNN: Pose-based CNN Features for Action Recognition  Guilhem Ch eron   Ivan Laptev  Cordelia Schmid  INRIA  Abstract  This work targets human action recognition in video.|
|||Given the recent success of Convolutional Neural Networks (CNN) [20, 23], we explore CNN features obtained separately for each body part in each frame.|
|||We use appearance and motion-based CNN features computed for each track of body parts, and investigate different schemes of temporal aggregation.|
|||Each patch is resized to 224  224 pixels to match the CNN input layer.|
|||Additionally, we want to follow the temporal evolution of CNN features in the video by looking at their dynamics (Dyn).|
|||The final dimension of our P-CNN is (5 4 4K) 2 = 160K, i.e., 5 body parts, 4 different aggregation schemes, 4K-dimensional CNN descriptor for appearance and motion.|
|||Mean and Max aggregation are widely used methods in CNN video representations.|
|||Table 2 shows the impact of adding minaggregation (Max/Min-aggr) and the first-order difference between CNN features (All-Dyn).|
||8 instances in total. (in iccv2015)|
|506|cvpr18-MapNet  An Allocentric Spatial Memory for Mapping Environments|A CNN extracts features from the image and a ground projection module (sec.|
|||Results on synthetic mazes  To validate our approach without the added complexities of a complex CNN for ingesting images and performing feature extraction and ground projection, we first experimented with simple 2D environments.|
|||The CNN embeddings associated with these 3D points are accumulated into the corresponding cell of o using max pooling (see sec.|
|||Instead of an image-space CNN and ground projection (sec.|
|||This CNN has two layers of 33 filters followed by batch-normalization, with 20 hidden channels and a ReLU.|
|||3.4, and input to a final 2-layers CNN with 64 hidden channels (same as in sec.|
|||It uses the same CNN as ours, followed by global average pooling, which improves performance.|
|||We also reimplemented the state-of-the-art DeepVO method [24], which is similar but uses a FlowNet CNN [5].|
||8 instances in total. (in cvpr2018)|
|507|Huang_Learning_Deep_Representation_CVPR_2016_paper|The CNN is trained with instances selected through a new quintuplet sampling scheme and the associated triple-header hinge loss.|
|||2) The quintuplet sampling is repeated during CNN training, thus avoiding large information loss as in traditional random under-sampling.|
|||: max(cid:0)0, g1 + D(f (xi), f (xp+ max(cid:0)0, g2 + D(f (xi), f (xp max(cid:0)0, g3 + D(f (xi), f (xp i, i  0, i  0, i  0  i  i  i  ))  D(f (xi), f (xp ))  D(f (xi), f (xp ))  D(f (xi), f (xn  i  i  ))(cid:1)  i, ))(cid:1)  i, i ))(cid:1)  i,  (2)  where i, i, i are the slack variables, g1, g2, g3 are the margins, W represents the parameters of the CNN embedding function f (), and  is a regularization parameter.|
|||Back-propagation is used to update the CNN parameters.|
|||For CNN training, repeatedly sample mini-batches equally from each class and retrieve the corresponding quintuplets from the offline table.|
|||Back-propagate the gradients to update the CNN pa rameters and feature embeddings.|
|||For each task, we list its used CNN architecture (top left), prior features for clustering (top right), and class-specific cost (bottom).|
|||Parameters: Our CNN is trained using batch size 40, momentum 0.9, and  = 0.0005 in Eq.|
||8 instances in total. (in cvpr2016)|
|508|Yuge_Shi_Action_Anticipation_with_ECCV_2018_paper|To over come some of these issues, we resort to use deep convolutional neural networks (CNNs) and take the deep feature on the penultimate layer of CNN as video  2  Y. Shi et al.|
|||1: Overview of Proposed feature mapping RNN : Given a frame extracted from video data, the algorithm first passes the RGB image I(t) through a deep CNN to acquire high level features of the image xt.|
|||[49], we propose a method to generate future features tailored for action anticipation task: given an observed sequence of deep CNN features, a novel Recurrent Neural Network (RNN) model is used to generate the most plausible future features and thereby predicting the action depicted in video data.|
|||First, we extract some CNN feature vectors from frames and predict the future features based on the past features.|
|||Furthermore, a naive CNN feature mapping using a LSTM from past to the future is also prone to over-fitting.|
|||We believe that the effectiveness of such models can be largely improved by utilising the correlation of high level activations of modern CNN architectures [47,13].|
|||In addition, to show the progression of how our method is able to outperform the baseline by such a large margin, we also implemented the Feature Mapping RNN on top of VGG16 so that the deep CNN pre-processing is consistent with other methods in Table 1.|
|||To fully explore functional capacity of RBF function, in future studies, we aim to implement kernel RBFs on fully connected layer of popular deep CNN models such as ResNet [13], AlexNet [24] and DenseNet [15].|
||8 instances in total. (in eccv2018)|
|509|Wolf_Learned_Watershed_End-To-End_ICCV_2017_paper|Results could be improved further by progress in CNN architectures and more sophisticated data augmentation, using e.g.|
|||Subsequently, [31] employed a CNN to learn features and boundary probabilities simultaneously.|
|||To combine the advantages of both approaches, we propose to augment the input image I with an additional channel holding node boundary probabilities predicted by an unstructured model g(w|I; UL):   I := [I  g] : V  RD+1  (24)  We train the CNN g separately beforehand and replace I with the augmented input  I everywhere in fstatic and fdyn.|
|||Here, independent workers fetch the current CNN parameters  from the master and compute loss gradients for randomly selected training images in parallel.|
|||Since these algorithms work best on slightly smoothed inputs, we apply Gaussian smoothing to the CNN output.|
|||The boundary probability prediction g (the same g as in equation (24)) was provided by a deep CNN trained with an unstructured lossfunction.|
|||We found the following aspects to be critical success factors: First, we train a very powerful CNN to control region growing.|
|||Specifically, feeding the current partial segmentation into the CNN provides shape clues for the next assignment decision, and maintaining a latent history along assignment paths allows to adjust growing parameters locally.|
||8 instances in total. (in iccv2017)|
|510|Fine-Grained Recognition as HSnet Search for Informative Image Parts|HSnet is grounded via the CNN to an image, and consists of the three components: H-layer for computing the heuristic function, S-layer for realizing the successor function, and Long Short-Term Memory (LSTM) [14] for capturing long-range dependencies along the search trajectory.|
|||Also, in [16], informative object parts are learned without needing part annotations by augmenting an existing CNN architecture with a differentiable spatial transformation module.|
|||0  RHW C  The CNN maps an image to a deep feature map, x  , where H is the height, W is the width, and C is the number of channels.|
|||The CNN extracts a deep feature map x from the image.|
|||CNN: Given an image, a CNN directly predicts the  class.|
|||CNN with ground truth bounding boxes: Given an image, a CNN produces a feature map, then a neural network predicts the class based only on the contents of k bounding boxes, initialized to the ground truth part locations.|
|||Notably, B4 performs significantly better than B1, which again supports that sequential reasoning performs better than recognition with a CNN in one shot.|
|||Bilinear cnn models for fine-grained visual recognition.|
||8 instances in total. (in cvpr2017)|
|511|Zhang_Unconstrained_Salient_Object_CVPR_2016_paper|It first generates a set of scored location proposals using a CNN model.|
|||2 DecrementPass(O)  while O 6=  do  b  arg maxbO h(O \ {b}) if h(O \ {b}) > h(O) then  O  O \ {b}  else  return  by [17, 46], we train a CNN model to produce a fixed number of scored window proposals.|
|||As our CNN model takes the whole image as input, it is able to capture context information for localizing salient objects.|
|||Our CNN model predicts scores for a predefined set of exemplar windows.|
|||We then apply our CNN model on each of these sub-images to augment our proposal set.|
|||Let (ci)K i=1 denote the output of our CNN model.|
|||To train our CNN model, we use about 5500 images from the training split of the Salient Object Subitizing (SOS) dataset [53].|
|||which is expected as we explicitly trained our CNN model to suppress detections on background images.|
||8 instances in total. (in cvpr2016)|
|512|Singh_GPLAC_Generalizing_Vision-Based_ICCV_2017_paper|A conventional CNN trained with this multitask objective does not attain good performance, as shown in Section 6.|
|||The standard CNN methods achieve poor results on all tasks, even when provided with more supervised data than our approach (CNN-40).|
|||This is an upper bound on the performance of a standard CNN model.|
|||Surprisingly, our approach with just one labeled environment outperforms a conventional CNN model even when that model is trained with labels on all 40 environments, suggesting that the strong lateral inhibition and distractor rejection of the attentional model greatly improves its ability to generalize to unseen settings.|
|||The non-attentional CNN with our classifier (CNN-C) performs much worse on both tasks, likely due to the CNN allocating different units to handle the action prediction and classification tasks, instead of benefitting from their shared structure.|
|||Our approach relies on two components: an attentional CNN architecture that focuses on objects of interest while ignoring distractors, and a multitask learning setup that trains this network simultaneously on actions and an auxiliary binary classification task on the passively collected images.|
|||In comparisons, our approach outperforms domain adaptation, semi-supervised learning with autoencoders, and non-attentional CNN models.|
|||Object detection via a multiIn  region and semantic segmentation-aware CNN model.|
||8 instances in total. (in iccv2017)|
|513|cvpr18-Compare and Contrast  Learning Prominent Visual Differences|Let xi  RD represent the images D-dimensional descriptor, which could be comprised of GIST [32], color, part descriptors, CNN features, or just raw pixels.|
|||(1)  (2)  Different learning objectives are used to quantify constraint satisfaction, such as a wide margin classification objective [34] for an SVM [15] ranker, or a RankNet objective [5, 40, 41, 45] for a deep CNN ranker.|
|||CNN Ranker Deep CNN based rankers have emerged as a strong alternative for predicting relative attributes [40, 41, 45].|
|||These models generally use a CNN optimized for paired ranking loss [5].|
|||For the SVM ranker and binary dominance, we generate CNN features from the fc7 layer of AlexNet [22].|
|||For the deep CNN ranker [40], the inputs are raw pixels.|
|||Comparing the use of the two attribute rankers, both yield similar performance on Zap50K but we benefit from the CNN ranker scores on LFW10.|
|||We compare our approach to the three strongest baselines and report results with the CNN attribute ranker in Figure 9 (see Supp for similar results with other ranker).|
||8 instances in total. (in cvpr2018)|
|514|cvpr18-Learning a Discriminative Prior for Blind Image Deblurring|Specifically, we first train a deep CNN to classify blurred (labeled as 1) and clear (labeled as 0) images.|
|||We then take the learned CNN classifier as a regularization term w.r.t.|
|||The contributions of this work are as follows:  We propose an effective discriminative image prior which can be learned by a deep CNN classifier for blind image deblurring.|
|||[10] propose an end-to-end CNN to deblur text images.|
|||We embed the learned CNN prior into the coarse-to-fine MAP framework for solving the blind image deblurring problem.|
|||We train a deep CNN by predicting blurred images as positive (labeled as 1) and clear images as negative (labeled as 0) samples.|
|||Effectiveness of the proposed CNN prior.|
|||As shown in Figure 10(b), while the L0 gradient prior helps to preserve more image structures, the integration with the proposed CNN prior leads to state-ofthe-art performance.|
||8 instances in total. (in cvpr2018)|
|515|cvpr18-Coupled End-to-End Transfer Learning With Generalized Fisher Information|Knowledge distillation [17] can be considered as another special case of transfer learning, in which the knowledge from a teacher CNN is transferred to a much more concise student CNN by emulating teachers soft-targets (a variation of softmax outputs).|
|||The research of transfer learning on deep CNN emerged recently.|
|||mitigated the domain discrepancy by layer-wise pre-training a CNN using a series of autoencoders.|
|||[10] designed the model combining a traditional CNN for source label prediction with a convolutional autoencoder for target data reconstruction.|
|||Specifically, by passing the target data through S, we obtain the feature maps from each layer in S. Then, by connecting a specific layer in S to a reversed target CNN T1, we consider S as an encoder and T1 as a decoder.|
|||Since the feature maps in S reflect the activations of the source CNN with the input of target data, by decoding these feature back into the input space, T1 is updated to represent the weights encoded in S in a backward manner.|
|||The source CNN S is pre-trained using a supervised cross entropy loss based on the source data:  Ls c(S(S), xsrc, ysrc) =  1  N  N  i=1  log P(yi  src|xi  src, S)  (1)  where S denotes the model parameters while T1 is trained using an unsupervised reconstruction loss on the target data:  r(T1(T1 ), xtgt) = |T1  S(xtgt)  xtgt|2 Ls  (2)  By doing so, we find an underlying feature representation across two datasets Dsrc and Dtgt.|
|||The target CNN denoted as T2 is also connected with decoder T1 to conduct the coupled learning, in which the following combined loss is minimized:   Lt  c(T2(T2), xtgt, ytgt) + (1   )Lt  r(T1(T1 ), T2(T2), xtgt)  where  Lt c(T2(T2), xtgt, ytgt) =  1  N  N  i=1  log P(yi  tgt|xi  tgt, T2 )  (3)  (4)  is the classification loss on the target data, and  r(T1(T1 ), T2(T2 ), xtgt) = |T1  T2(xtgt)  T1  S(xtgt)|2 (5) Lt  is the loss of reconstructing output of T1  S using xtgt as the input.|
||8 instances in total. (in cvpr2018)|
|516|cvpr18-Deep Texture Manifold for Ground Terrain Recognition|[38] introduce Deep Texture Encoding Network (Deep-TEN) that ports the dictionary learning and feature pooling approaches into the CNN pipeline for an end-to-end material/texture recognition network.|
|||Deep Encoding Pooling Network  Encoding Layer The texture encoding layer [38] integrates the entire dictionary learning and visual encoding pipeline into a single CNN layer, which provides an orderless representation for texture modeling.|
|||Recognition Experiments  We compare the DEP network with the following three methods based on ImageNet [28] pre-trained 18-layer ResNet [12]: (1) CNN with ResNet, (2) CNN with DeepTen and(3) CNN with bilinear models.|
|||Outputs from convolutional layers of two CNN streams are multiplied using outer product at each location and pooled for recognition.|
|||The input image size is 224  224.  reduce the number of CNN streams outputs channels from 512 to 128 with a 11 convolutional layer.|
|||The class names are (in the order of top-left to bottomright): asphalt, steel, stone-cement, glass, leaves, grass,  562  ResNet [12] Bilinear CNN [19] Deep-TEN [38] DEP (ours)  Single scale  70.82  Multi scale  73.16  72.03  75.43  74.22  76.12  76.07  82.18  Table 2: Comparison our Deep Encoding Pooling Network (DEP) with ResNet (left) [12], Bilinear CNN (mid) [19] and Deep-TEN (right) [38] on GTOS-mobile dataset with single scale and multi scale training.|
|||For an equal comparison, we build DEP based on a 50-layer ResNet [12], the feature maps channels from CNN streams are reduced from 2048 to 512 with a 11 convolutional layer.|
|||Bilinear cnn models for fine-grained visual recognition.|
||8 instances in total. (in cvpr2018)|
|517|cvpr18-Multi-Frame Quality Enhancement for Compressed Video|designed a Decoder-side Scalable CNN (DS-CNN) for video quality enhancement [38, 39].|
|||Then, a novel Multi-Frame CNN (MF-CNN) architecture is proposed for quality enhancement, in which both the current frame and its adjacent PQFs are as the inputs.|
|||For the quality enhancement of compressed video, the  Variable-filter-size Residue-learning CNN (VRCNN) [8] was proposed to replace the inloop filters for HEVC intracoding.|
|||However, the CNN in [8] was designed as a component of the video encoder, so that it is not practical for already compressed video.|
|||Most recently, a Deep CNNbased Auto Decoder (DCAD), which contains 10 CNN layers, was proposed in [34] to reduce the distortion of compressed video.|
|||proposed a Video Super-Resolution network (VSRnet) [20], in which the neighboring frames are warped according to the estimated motion, and both the current and warped neighboring frames are fed into a super-resolution CNN to enlarge the resolution of the current frame.|
|||Then, we proposed a novel CNN framework, called MF-CNN, to enhance the quality of each non-PQF.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||8 instances in total. (in cvpr2018)|
|518|Sifei_Liu_Switchable_Temporal_Propagation_ECCV_2018_paper|A number of recent algorithms, such as optical-flow based warping [1], similarity-guided filtering [2, 3] and the bilateral CNN model [4], explore the local relationships between frames to propagate information.|
|||It contains a propagation module that transfers a property (e.g., color) of one frame to a nearby frame through a global, linear transformation matrix which is learned with a CNN from any available guidance information (e.g., lightness).|
|||It uses bi-directional training for a pair of frames in the propagation module, which is guided by switched output maps from the guidance CNN network.|
|||Recently, a deep CNN model is proposed to learn task-dependent affinity metric [7] by modeling the propagation of pixels as an image diffusion process.|
|||3.1 Learning Pixel Transitions via the Basic TPN  Directly learning the transformation matrix G via a CNN is prohibitive, since G has a huge dimension (e.g., n2  n2).|
|||Since the upper-triangular matrix: (a) corresponds to propagating in the right-to-left direction, and (b) contains the same set of weight sub-matrices, switching the CNN output channels w.r.t.|
|||We adopt a symmetric U-net shaped, light-weight deep CNN with skip links for all tasks, but with slightly different numbers of layers to accomondate the different input resolutions (see Fig.|
|||The guidance CNN takes as input a pair of lightness images (L) for the two frames.|
||8 instances in total. (in eccv2018)|
|519|Poggi_Quantitative_Evaluation_of_ICCV_2017_paper|Concerning methodologies based on Convolutional Neural Networks (CNN), Seki and Pollefeys [40] proposed to infer a confidence measure by processing features extracted from the left and right disparity maps while Poggi and Mattoccia [32] learned from scratch a confidence measure by feeding to a CNN the left disparity map.|
|||Le Cun proposed the first successful attempt to infer an effective matching cost from a stereo pair with a CNN now deployed by almost any top-performing stereo method on K12, K15 and M14 datasets.|
|||In this case, given a stereo pair, the left-right stereo correspondence is regressed from scratch with a CNN trained end-to-end.|
|||This method infers a confidence measure by feeding to a random forest, trained  3.7.2 CNN approaches  As for many other computer vision fields, convolutional neural networks have recently proven to be very effective  5231  also for confidence estimation.|
|||In patch based confidence prediction (PBCP) [40] the input of a CNN consists of two channels p1 and p2 computed, on a patch basis, from left and right disparity maps.|
|||A step towards a further abstraction is represented by confidence CNN (CCNN) [32].|
|||In fact, in this approach confidence prediction is regressed by a CNN without extracting any cue from the input data.|
|||For approaches based on random forests we train on 10 trees as suggested in [28] and adopting a fixed number of iteration as termination criteria (e.g., proportional to the number of trees), while we train CNN based measures for 25 epochs (resulting in about 1 million iterations), with a batch of size  64, learning rate of 0.001 and momentum of 0.9, by minimizing the loss functions reported in [32, 40].|
||8 instances in total. (in iccv2017)|
|520|cvpr18-Between-Class Learning for Image Classification|We used the output of the 10-th layer of an 11-layer CNN trained on CIFAR-10.|
|||The 11-layer CNN was also incorporated into one of them.|
|||We trained each model 10 times for the 11-layer CNN and ResNet-29, and 5 times for other networks.|
|||4https://github.com/facebookresearch/ResNeXt, https://github.com/  liuzhuang13/DenseNet, https://github.com/xgastaldi/shake-shake  5491  20  15  10  )  %  t  (   e a r   r o r r e  5  0  11-layer CNN Standard  11-layer CNN BC (ours)  11-layer CNN BC+ (ours)  )  %  t  (   e a r   r o r r e  12  10  8  6  4  2  0  DenseNet Standard  DenseNet BC (ours)  DenseNet BC+ (ours)  12  10  )  %  t  (   e a r   r o r r e  8  6  4  2  0  Shake-Shake Standard  Shake-Shake BC (ours)  Shake-Shake BC+ (ours)  0  50  100  150  200  250  0  75  150  225  300  375  0  200  400  600  800  1000  1200  1400  1600  1800  epochs  epochs  epochs  Figure 5.|
|||Furthermore, the last 75 training epochs for 11-layer CNN and DenseNet leads to a lower testing error when using BC learning.|
|||We trained an 11-layer CNN on CIFAR-10 and CIFAR-100 using various settings.|
|||We trained 11-layer CNN on CIFAR10 and CIFAR-100 using various settings.|
|||We applied PCA to the activations of the 10-th layer of the 11-layer CNN trained on CIFAR-10 against the training data.|
||8 instances in total. (in cvpr2018)|
|521|Tai_MemNet_A_Persistent_ICCV_2017_paper|construct a 20-layer CNN structure named VDSR for SISR [20], and adopts residual learning to ease training difficulty.|
|||However, none of above CNN models has such mechanism to achieve persistent memory.|
|||As the pioneer CNN model for SISR, superresolution convolutional neural network (SRCNN) [8] predicts the nonlinear LR-HR mapping via a fully deep convolutional network, which significantly outperforms classical shallow methods.|
|||The authors further proposed an extended CNN model, named Artifacts Reduction Convolutional Neural Networks (ARCNN) [7], to effectively handle JPEG compression artifacts.|
|||In very deep networks, some of the mid/high-frequency information can get lost at latter layers during a typical feedforward CNN process, and dense connections from previous layers can compensate such loss and further enhance  MemNet_NL_4  MemNet_NL_6  MemNet_4  MemNet_6  27.31/0.9078  27.45/0.9101  27.29/0.9070  27.71/0.9142  Low frequency  High frequency  (a)  (b)  (c)  MemNet_NL_4-MemNet_NL_6  MemNet_4-MemNet_6  MemNet_4-MemNet_NL_4 MemNet_6-MemNet_NL_6  Figure 4.|
|||Discussions  Difference to Highway Network First, we discuss how the memory block accomplishes the gating mechanism and present the difference between MemNet and Highway Network  a very deep CNN model using a gate unit to regulate information flow [32].|
|||Comparision with Non(cid:173)Persistent CNN Models  In this subsection, we compare MemNet with three existing non-persistent CNN models, i.e., VDSR [20], DRCN [21] and RED [27], to demonstrate the superiority of our persistent memory structure.|
|||Conclusions  In this paper, a very deep end-to-end persistent memory network (MemNet) is proposed for image restoration, where a memory block accomplishes the gating mechanism for tackling the long-term dependency problem in the previous CNN architectures.|
||8 instances in total. (in iccv2017)|
|522|Fragkiadaki_Learning_to_Segment_2015_CVPR_paper|The multimodal class of moving objects is represented with a dual pathway CNN architecture on both RGB and motion fields; its neurons capture parts of fish, people, cars, animals etc.|
|||Moving Objectness Detector  We train a Moving Objectness Detector (MOD) using a CNN [24] with a dual-pathway architecture operating on both image and flow fields, shown in Figure 3.|
|||Last, we benchmark our moving objectness detector on ranking per frame segments as well as spatio-temporal tube proposals, and compare with alternative CNN architectures, centersurround saliency and static image objectness.|
|||Our dual-pathway CNN regressor outperforms other CNN alternatives and hand-coded center-surround saliency.|
|||We compare our dual-pathway CNN regressor from image and flow fields (piCNN-regress) against a dual pathway classification CNN (piCNN-class), an image only CNN (imgCNN), a flow only CNN (flowCNN), our implementation of a standard center-surround saliency measure from optical flow magnitude (center-surround) [14], and an objectness detector using the 7000 category detector from the Large Scale Domain Adaptation (LSDA) work of [20].|
|||Our dual pathway classification CNN is trained to classify boxes as positive or negatives using a threshold of 50% of IoU, instead of regressing to their IoU score.|
|||Our dual-pathway CNN regressor performs best among the alternatives considered, though has close performance with the dual-pathway classification CNN.|
|||Our CNN networks operate on the bounding box of a segment rather than its segmentation mask.|
||8 instances in total. (in cvpr2015)|
|523|cvpr18-Temporal Deformable Residual Networks for Action Segmentation in Videos|V Temporal Unpooling  V Temporal Pooling  +  Element-wise Summation  Temporal Residual Stream  Temporal Pooling Stream  DTRM  Deformable Temporal Residual Module  Conv Temporal Convolution Layer  FC  Conv  V  +  +  +  DTRM  V  DTRM  V  FC  Fully-connected Layer  Conv  V  Input:  1  T  T  DTRM  CNN  CNN  CNN  CNN  CNN CNN CNN CNN CNN CNN CNN  Figure 1: For action segmentation, TDRN takes frame-level CNN features as input and outputs frame-wise action labels.|
|||The residual network takes frame-level CNN features as input and computes deformable convolutions along time at multiple temporal scales, starting at the frame-level resolution.|
|||We use the same CNN features as in [22].|
|||As input to TDRN, we use the same spatial CNN features as in [22], where the CNN is trained on RGB images showing 17 mid-level action classes.|
|||For fair comparison, input to TDRN are the same CNN features as those used in [22].|
|||We also compare with the following closely related work: (3) Spatial CNN [23]: Frame-wise classification using CNN features of a single RGB frame that capture object texture and spatial location; (4) ST-CNN [23]: Temporal convolutional filter that builds on top of spatial CNN to capture scene changes over the course of action; (5) Bi-LSTM [37]: Bi-directional temporal LSTM; 6) ED-TCN  50 Salads (mid)  F1@{10,25,50}  Edit  Acc  Spatial CNN [23]  32.3,27.1,18.9  IDT+LM [32]  44.4,38.9,27.8  Dilated TCN [22]  52.2,47.6,37.4  ST-CNN [23]  55.9,49.6,37.1  Bi-LSTM [37]  62.6,58.3,47.0  ED-TCN [22]  68.0,63.9,52.6  TRN  70.2,65.4,56.3  TDRN+UNet  69.6,65.0,53.6  TDRN  72.9,68.5,57.2  24.8  45.8  43.1  45.9  55.6  59.8  63.7  62.2  66.0  54.9  48.7  59.3  59.4  55.7  64.7  66.9  66.1  68.1  Table 3: Results on 50 Salads (mid).|
|||GTEA  F1@{10,25,50}  Edit  Acc  Spatial CNN [23]  41.8,36.0,25.1  ST-CNN [23]  58.7,54.4,41.9  Bi-LSTM [37]  66.5,59.0,43.6  Dilated TCN [22]  58.8,52.2,42.2  ED-TCN [22]  72.2,69.3,56.0  EgoNet+TDD [39]         TRN  77.4,71.3,59.1  TDRN+UNet  78.1,73.8,62.2  TDRN  79.2,74.4,62.7  72.2  73.7  74.1  54.1  60.6  55.5  58.3  64.0  64.4  67.8  69.3  70.1  Table 4: Results on GTEA.|
|||JIGSAWS  F1@{10}  Edit  Acc  MSM-CRF [42]  Spatial CNN [23]  ST-CNN [23]  Bi-LSTM [37]  ED-TCN [22]  TCN [25]  TRN  TDRN+UNet  TDRN    78.3  77.8  89.2   91.4  92.1  92.9   37.7  68.6  66.8  84.7  83.1  87.7  89.4  90.2  71.7  74.0  78.4  77.4  80.8  81.4  83.3  83.9  84.6  Table 5: Results on JIGSAWS.|
||8 instances in total. (in cvpr2018)|
|524|Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps|More recently, confidence measures have been inferred [26, 30] processing disparity maps with a CNN (Convolutional Neural Network).|
|||Concerning the first goal, in [30] a confidence measure is inferred with a CNN analyzing hand-crafted features extracted from left-right and right-left disparity maps.|
|||Finally, in [24] a CNN was trained to combine the outcome of multiple stereo algorithms in order to obtain more accurate results.|
|||Proposed CNN architecture to prediction match reliability enforcing local consistency on the input confidence map.|
|||Deep network architecture  To learn a locally consistent confidence prediction, we propose to train a custom CNN to assign the new value for the pixel under investigation, using image patches extracted from confidence maps.|
|||In order to infer the final pixel-wise confidence score, in our experiments we evaluated different CNN architectures made of different convolutional layers, depending on the perceptive field of the network, and fully-connected layers.|
|||The resulting CNN architecture has more than 600 thousand parameters and, with a full resolution confidence map of the KITTI dataset, it requires just 5 GB of memory and about 0.1 sec to infer a new confidence estimation with a Titan X GPU.|
|||The improvements are remarkable also for top-performing confidence measures O1 and CNN being k, respectively, greater than 14% and 9% in the worst case.|
||8 instances in total. (in cvpr2017)|
|525|Gedas_Bertasius_Object_Detection_in_ECCV_2018_paper|One of the earliest deep CNN object detection systems was RCNN [14], which involved a two-stage pipeline where object proposals were extracted in the first stage, and then each proposal was classified using a CNN.|
|||For example, we note that it would take about four days to train an optical flow CNN of FGFA [24] from scratch and then four additional days to train FGFA [24] for video object detection, making it eight days of total training time.|
|||As backbone network, we use a Deformable CNN [25] based on the ResNet-101 [5] architecture, which is one of the top-performing object detection systems at the moment.|
|||We also note that even though we use a Deformable CNN architecture, our system can easily integrate other architectures and thus it can benefit from future improvements in still-image object detection.|
|||For our backbone network we adopt a state-of-the-art Deformable CNN [25] based on the ResNet-101 [5] architecture.|
|||Also note, that D&T+ and STSN+ refer to D&T and STSN baselines with temporal post-processing applied on top of the CNN outputs.|
|||Finally, we demonstrate that our method performs better than the D&T [27] method in two scenarios: 1) when we only use CNN-level outputs for video object detection, and also 2) when we allow temporal post-processing techniques such as Seq-NMS to be applied on top of the CNN outputs.|
|||SeqNMS [22], object-tube based linking [27], etc) applied on top of the CNN outputs.|
||8 instances in total. (in eccv2018)|
|526|Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors|Specifically, we utilize two CNN streams: the source and target networks fused at the classifier level.|
|||Moreover, the problem in hand may be homogeneous or heterogeneous [39, 47] in nature e.g., identical source and target representations using RGB images vs. a source represented by a CNN trained on images [7, 22] and a target using an LSTM [16, 15] which is trained on text corpora or video data [13].|
|||Noteworthy is also recent trend in the CNN fine-tuning which by itself is a powerful domain adaptation and transfer learning tool [10, 33] which requires large training datasets.|
|||Where stated, we use the 16-layer VGG model [35] per stream to quantify the impact of different CNN models on our algorithm.|
|||73.33 78.2 [3] 94.81.2 87.75 [6] 80.72.3 [9] 53.60.2 71.20.0 69.43 [40] 56.50.3 64.60.4 42.70.1 93.60.2 47.60.1 92.40.3 66.23 [40] 80.50.5 81.81.0 59.90.3 81.81.0 59.90.3 80.50.5 74.06 Source+Target CNN [40] 82.50.9 85.21.1 65.20.7 96.30.5 65.80.5 93.90.5 81.48 Dom.|
|||[40] 82.70.8 86.11.2 65.00.5 97.60.2 66.20.3 95.70.5 82.22 82.42.0 85.50.9 65.11.4 95.80.8 66.01.2 94.30.6 81.53 84.51.7 86.30.8 65.71.7 97.50.7 66.51.0 95.50.6 82.68  Source CNN Target CNN  sSource+Target CNN (S+T) r u O  Second-order (So)   83.50.0  D(cid:1)A   Table 1: Comparison of our second-order alignment loss (So) to the state of the art on the Office dataset.|
|||for which the source and target training samples were used together to fine-tune a standard CNN network.|
|||a multiscale multi-patch CNN approach [23] by 0.6% on A(cid:1) W. Weighted vs. Unweighted Alignment.|
||8 instances in total. (in cvpr2017)|
|527|cvpr18-Rotation-Sensitive Regression for Oriented Scene Text Detection|Object Detection  Recent object detectors [9, 8, 36, 34, 28, 6, 35, 27] leverage the powerful learning ability of CNN to detect objects, and achieve impressive performances.|
|||Both classification and regression rely on shared translation and rotation-invariant features attributing to pooling layers involved in classical CNN architecture.|
|||This paper focuses on a typical example of such object detection, i.e., scene text detection, by explicitly introducing rotation-sensitive features to the CNN pipeline.|
|||Compared to these modern multi-oriented scene text detectors, this paper proposes to explicitly use rotation-sensitive CNN features for oriented bounding box regression while adopting rotationinvariant features for classification.|
|||Rotation(cid:173)sensitive CNN Features  Rotation-invariant features are important for a robust classification.|
|||Modern CNN architectures usually involve pooling layers achieving rotation invariance to a certain extent.|
|||Some recent works [10, 7, 52] focus on enhancing the rotation invariance of the CNN features to further improve the classification performance.|
|||Different  from standard CNN features, RRD extracts rotation-sensitive features with active rotating filters (ARF) [52].|
||8 instances in total. (in cvpr2018)|
|528|Improving Interpretability of Deep Neural Networks With Semantic Information|We stack a CNN model and a bi-directional LSTM model as encoder to extract video features {v1, ...vn}, and then feed them to an LSTM decoder to generate descriptions.|
|||For example, some works have shown the effectiveness of DNNs on video analysis [2, 35] when stacking a hierarchical RNN on top of some CNN layers.|
|||A key difference from previous works [32, 34, 26] which use CNN features as video representations is that we stack a bi-directional LSTM model [29] on top of a CNN model to characterize the video temporal variation in both input directions.|
|||Interpretive Loss  The above architecture for video captioning incorporates both CNN and RNN to encode the spatial and temporal information.|
|||The complex architecture makes internal neurons learn more abstract features than a single CNN or RNN, and these features are typically hard to interpret by human users.|
|||We only use 2-D CNN features for simplicity in this work.|
|||For fair comparison, we also show the extensive results of SA, LSTM-E and h-RNN which only incorporate 2-D CNN features.|
|||By comparing our baseline method LSTM-B to SA, which only uses a single CNN model as encoder and a similar decoder architecture, we can see that our baseline model achieves higher BLEU and METEOR scores, suggesting that the bi-directional LSTM can help us capture temporal variation and lead to better video representations.|
||8 instances in total. (in cvpr2017)|
|529|cvpr18-PointFusion  Deep Sensor Fusion for 3D Bounding Box Estimation|The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively.|
|||Our deep network for 3D object box regression from images and sparse point clouds has three main components: an off-the-shelf CNN [13] that extracts appearance and geometry features from input RGB image crops, a variant of PointNet [23] that processes the raw 3D point cloud, and a fusion sub-network that combines the two outputs to predict 3D bounding boxes.|
|||[12] propose to predict a semantic segmentation map as well as object pose hypotheses using a CNN and then align the hypotheses with known object CAD models using ICP.|
|||PointFusion has two feature extractors: a PointNet variant that processes raw point cloud data (A), and a CNN that extracts visual features from an input image (B).|
|||2A), a CNN that extracts image appearance features (Fig.|
|||Fusion Network  The fusion network takes as input an image feature extracted using a standard CNN and the corresponding point cloud feature produced by the PointNet sub-network.|
|||Among our variants, final achieves the best performance, while the homogeneous CNN architecture rgb-d has the worst performance, which underscores the effectiveness of our heterogeneous model design.|
|||This is a much smaller gap than in the KITTI dataset, which shows that the CNN performs well when it is given dense depth information (rgb-d cameras provide a depth measurement for every rgb image pixel).|
||8 instances in total. (in cvpr2018)|
|530|CNN-SLAM_ Real-Time Dense Monocular SLAM With Learned Depth Prediction|To maintain a high frame-rate, we propose to predict a depth map via CNN only on key-frames.|
|||Since in most cases the camera used for SLAM differs from the one used to acquire the dataset on which the CNN is trained, we propose a specific normalization procedure of the depth map designed to gain robustness towards different intrinsic camera parameters.|
|||One limitation of using a pre-trained CNN for depth prediction is that, if the sensor used for SLAM has different intrinsic parameters from those used to capture the training set, the resulting absolute scale of the 3D reconstruction will be inaccurate.|
|||To ameliorate this issue, we propose to adjust the depth regressed via CNN with the ratio between the focal length of the current camera, fcur and that of the sensor used for training, ftr as  Dki (u) =  fcur ftr   Dki (u)  (5)  where  Dki is the depth map directly regressed by the CNN from the current key-frame image Ii.|
|||Since the CNN is trained to provide semantic labels in addition to depth maps, semantic information can be also associated to each element of the 3D global model, through a process that we denote as semantic label fusion.|
|||As for the implementation of our method, although the CNN network works on an input/output resolution of 304228 [16], both the input frame and the predicted depth map are converted to 320240 as input for all other stages.|
|||In all our experiments, we used the CNN model trained on the indoor sequences of the NYU Depth v2 dataset [25], to test the generalization capability of the network to unseen environments; also because this dataset includes both depth ground-truth (represented by depth maps acquired with a Microsoft Kinect camera) and pixel-wise semantic label annotations, necessary for semantic label fusion.|
|||The increased accuracy with respect to the depth maps estimated by the CNN (as employed in [16]) and by RE MODE, as well as the higher density with respect to LSDSLAM is also shown in Fig.|
||8 instances in total. (in cvpr2017)|
|531|Anil_Baslamisli_Joint_Learning_of_ECCV_2018_paper|To that end, we propose a supervised end-to-end CNN architecture to jointly learn intrinsic image decomposition and semantic segmentation.|
|||Moreover, new cascade CNN architectures for intrinsic-for-segmentation and segmentation-for-intrinsic are proposed as single tasks.|
|||The joint learning includes an end-to-end trainable encoderdecoder CNN with one shared encoder and three separate decoders: one for reflectance prediction, one for shading prediction, and one for semantic segmentation prediction.|
|||Our contributions are: (1) a CNN architecture for joint learning of intrinsic image decomposition and semantic segmentation, (2) analysis on the gains of addressing those two problems jointly, (3) new cascade CNN architectures for intrinsic-for-segmentation and segmentation-for-intrinsic, and (4) a very largescale dataset of synthetic images of natural environments with scene level intrinsic image decomposition and semantic segmentation ground-truths.|
|||On the other hand, contemporary semantic segmentation methods such as [24,25,26] benefit from the powerful CNN models and large-scale datasets such as [27,28].|
|||reflectance) helps the CNN to improve semantic segmentation performance.|
|||New CNN architectures are proposed for joint learning, and single intrinsic-for-segmentation and segmentation-for-intrinsic learning.|
|||Baslamisli, A.S., Le, H.A., Gevers, T.: Cnn based learning using reflection and retinex models for intrinsic image decomposition.|
||8 instances in total. (in eccv2018)|
|532|Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework|Inspired by these works, we use the CNN to generate a visual sensitivity map which refers to a weighting map of describing the degree of visual importance of each pixel to the HVS.|
|||The CNN model in our approach is dedicated to learn the HVS properties.|
|||A novel deep CNN based FR-IQA framework is proposed.|
|||first applied a CNN to the NR-IQA problem without using any handcrafted features [8].|
|||The visual sensitivity map is obtained from the CNN  1678  models  s1 = CN N1( Id; 1) s2 = CN N2( Id, e; 2)  (2)  (3)  where CN N1() and CN N2() indicate the CNN models of DeepQAs and DeepQA with the parameters 1 and 2 respectively.|
|||SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC 0.876 0.704 0.767 0.948 0.809 0.951 0.826 0.963 0.960 0.890 0.893 0.960 0.940 0.963 0.975 0.977 0.981 0.934 0.956 0.960 0.958  0.872 0.945 0.949 0.960 0.960 0.961 0.966 0.977 0.975 0.982 0.948 0.953 0.972 0.960  0.573 0.773 0.657 0.808 0.879 0.876 0.937  0.706 0.691 0.833 0.772 0.859 0.877 0.934  0.666 0.745 0.842 0.765 0.867 0.884 0.937  0.636 0.637 0.786 0.677 0.804 0.851 0.926  0.892 0.951  0.957 0.961  0.956 0.965  0.876 0.955  0.835  0.855  0.878 0.947  0.766 0.939  0.818 0.947  0.848 0.949                    marked: SESANIA [7], CNN [8], a Patchwise method in [2], and BIECON [9].|
|||From this  observation, it is obvious that taking the error map as an input helps the CNN model extract more useful features to achieve a higher accuracy.|
|||Conclusion  In this paper, we have described a new FR-IQA framework where the CNN model learns the human visual sensitivity.|
||8 instances in total. (in cvpr2017)|
|533|Ohnishi_Recognizing_Activities_of_CVPR_2016_paper|CNN descriptors: CNN has achieved superior results to the standard pipeline for object recognition [16].|
|||[12] brought CNN to action recognition.|
|||They obtain the state of a fully connected layer from each frame in videos and calculate the video representation by averaging all CNN features.|
|||Experiment protocols  We used 16-layer VGG net [27] pre-trained on the ImageNet 2012dataset [6] for the CNN architecture in the same manner as the LCD [36].|
|||Therefore, we evaluated our dataset not only with CNN descriptors but also with iDT [34].|
|||Comparing the iDT on both cameras, we found that iDT on HCD showed better performance than on WCD unlike the CNN descriptors.|
|||[11] showed that combining object features extracted by CNN with motion features such as iDT boosts  0.2!|
|||Additionally, we proposed a novel video representation that aggregated CNN descriptors spatially and temporally, and optimized their weights both iteratively and alternately.|
||8 instances in total. (in cvpr2016)|
|534|cvpr18-Weakly Supervised Coupled Networks for Visual Sentiment Analysis|Our proposed framework performs favorably against the state-of-the-art methods and off-the-shelf CNN classifiers on six benchmark datasets for visual sentiment analysis.|
|||To cope with limited training data, most approaches incorporate the CNN weights learned from a large-scale general dataset [9] and fine-tune the model for sentiment prediction [4, 5, 30].|
|||As stated in Section 2.2, there are only a few end-to-end CNN frameworks for weakly supervised object detection that do not use additional localization informa 7586  tion.|
|||Implementation Details  Our framework is based on the state-of-the-art CNN architecture ResNet-101 [12].|
|||For the basic CNN models, we report the results of using three classical deep learning methods pre-trained on ImageNet and fine-tuned on the affective datasets: AlexNet [15], VGGNet [22] with 16 layers and ResNet101 [12].|
|||We also show the results of fullyconnected features extracted from the ImageNet CNN with LIBSVM.|
|||Table 3 shows that with an increasing number of feature maps, our method is able to achieve better performance compared with the standard classification strategy in the CNN (i.e.|
|||Our baseline is the WSCNet with  = 1 and without the coupling operation, where the classification branch is the original classification layer in the CNN (i.e.|
||8 instances in total. (in cvpr2018)|
|535|Noh_Large-Scale_Image_Retrieval_ICCV_2017_paper|In our approach, the attention model is tightly coupled with the proposed descriptor; it reuses the same CNN architecture and generates feature scores using very little extra computation (in the spirit of recent advances in object detection [30]).|
|||Dense Localized Feature Extraction  We extract dense features from an image by applying a fully convolutional network (FCN), which is constructed by using the feature extraction layers of a CNN trained with a classification loss.|
|||,  (1)  where W  RM d represents the weights of the final fullyconnected layer of the CNN trained to predict M classes.|
|||The score function is designed using a 2-layer CNN with a softplus [9] activation at the top.|
|||The CNN based on VGG16 [32] extracts 512 dimensional global descriptor.|
|||DELF-noFT means that extracted features are based on the pretrained CNN on ImageNet without finetuning and attention learning.|
|||NetVLAD: CNN Architecture for Weakly Supervised Place Recognition.|
|||with Integral Max-Pooling of CNN Activations.|
||8 instances in total. (in iccv2017)|
|536|Zhang_Embedding_Label_Structures_CVPR_2016_paper|For example, [35, 48, 2] proposed to combine the softmax and contrastive loss in CNN via joint optimization.|
|||It improved traditional CNN because contrastive constraints might augment the information for training the network.|
|||Jointly Optimize Classification and Similarity  Constraints  Traditional classification constraints such as softmax with loss are usually employed in CNN for fine-grained image categorization, which can distinguish different subordinate classes with high accuracy.|
|||To address these limitations, we explicitly model the similarity constraint in CNN using a multi-task learning strategy.|
|||To  enforce this constraint in CNN training, a common relaxation [27] of Eq.|
|||Furthermore, without the explicit constraints for classification, the accuracy of differentiating classes can be inferior to the traditional CNN using softmax, especially in fine-grained problems where the differences of subordinate classes are very subtle.|
|||2 shows the CNN architecture of our joint learning.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||8 instances in total. (in cvpr2016)|
|537|Kim_Interpretable_Learning_for_ICCV_2017_paper|But attention networks need to find all potentially salient image areas and pass them to the main recognition network (a CNN here) for a final verdict.|
|||To this end, we first encode images with a CNN and decode this feature into a heat map of attention, which is also used to control a vehicle.|
|||A CNN is trained to produce these features, and a simple controller maps them to steering angle.|
|||Training and Evaluation Details  To obtain a convolutional feature cube xt, we train the 5-layer CNN explained in Section 3.2 by using additional 5layer fully connected layers (i.e., # hidden variables: 1164, 100, 50, and 10, respectively), of which output predicts the measured inverse turning radius ut.|
|||Control accuracy is not degraded by incorporation of attention compared to an identical base CNN without attention.|
|||[3], which used an identical base CNN and a fully-connected network (FCN) without attention.|
|||To see the contribution of LSTM, we also test a CNN and LSTM, which is identical to ours but does not use the attention mechanism.|
|||We showed that (i) incorporation of attention does not degrade control accuracy compared to an identical base CNN without attention (ii) raw attention highlights interpretable features in the image and (iii) causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output.|
||8 instances in total. (in iccv2017)|
|538|Zhang_Zero-Shot_Learning_via_ICCV_2015_paper|For all the datasets, we utilize MatConvNet [36] with the imagenet-vgg-verydeep-19 pretrained model [32] to extract a 4096-dim CNN feature vector (i.e.|
|||To better understand our SSE learning method, we visualize the target domain CNN features as well as the learned SSE features using t-SNE [35] in Fig.|
|||4(b), CNN features seem to form clusters for different classes with some overlaps, and there is a small gap between an (a) Cosine similarity matrix  (b) CNN features  (c) SSE embeddings (auto-deer)  (d) SSE embeddings (cat-dog)  Figure 4.|
|||(b) shows the 4096-dim original target domain CNN features.|
|||[16] Romera-Paredes and Torr [30] SSE-INT SSE-ReLU  32.5 37.93 45.05  19.1 26.020.05 27.271.62  38.16 24.222.89 44.150.34 46.230.53  Non-CNN  AlexNet  vgg-verydeep-19  42.78 42.7 48.30 43.5 47.1  40.5 43.010.07 49.300.21  61.9  57.23 75.322.28 71.520.79 76.330.83  18.0  14.4  40.3  30.190.59 30.410.20  52.50 56.180.27 65.750.51  72.00 82.100.32 82.170.76 82.501.32  aThe results listed here are the ones with 4096-dim CNN features and the continuous attribute vectors provided in the datasets for fair comparison.|
|||(a) decaf  (b) verydeep-19  Figure 5. t-SNE visualization comparison of SSE distributions using the two CNN features on AwA testing data.|
|||We show the SSE distribution comparison using decaf CNN features and vggverydeep-19 CNN features in Fig.|
|||However, it can return the prediction results on any of these 5 datasets within 30 minutes using a multi-thread CPU (Xeon E5-2696 v2), starting from loading CNN features.|
||8 instances in total. (in iccv2015)|
|539|Li_Weakly_Supervised_Object_CVPR_2016_paper|Second, many approaches use a pre-trained CNN as a feature extractor and do not adapt the weights from whole-image classification to object detection.|
|||3512  (2) Detection adaptation uses confident object candidates to optimize the CNN representations for the target domain.|
|||The key ingredient for the success lies in end-to-end training CNN in a fully supervised fashion.|
|||(3) Instead of training a classifier over pre-trained CNN features, we fine-tune the parameters of all the CNN layers for training object detectors.|
|||We denote the CNN as a function p() that maps an input image I to a 2C dimensional output p(I)  R2C .|
|||The idea of masking out the input of CNN has been previously explored in [44, 1].|
|||Implementation details  For multi-label image classification training, we use the AlexNet [21] as our base CNN model, initialized with the parameters pre-trained on ImageNet dataset.|
|||Compared to the topic model [30], we incorporate inter-class relations by jointly training CNN with all object classes and background class while they rely on handcrafted features.|
||8 instances in total. (in cvpr2016)|
|540|Liu_Matching-CNN_Meets_KNN_2015_CVPR_paper|The M-CNN differs from the classic CNN [12] in that the tailored cross image matching filters are introduced to characterize the matching between the testing image and the semantic region of a KNN image.|
|||in traditional pipelines into one unified deep CNN framework.|
|||There exist some works on semantic segmentation with CNN architectures.|
|||[6] and its extension work [9] proposed to classify the candidate regions by CNN for semantic segmentation.|
|||[5] trained a multi-scale CNN from raw pixels to extract deep features for assigning the label to each pixel.|
|||The recurrent CNN [22] was proposed to speed up scene parsing and achieved the state-of-the-art performance.|
|||Our M-CNN inherits the merit of existing CNN parsing models in our single image convolutional path.|
|||Ablation of Our Networks: We also extensively explore different CNN architectures to demonstrate the effectiveness of each component in M-CNN more transparently.|
||8 instances in total. (in cvpr2015)|
|541|Harley_Segmentation-Aware_Convolutional_Networks_ICCV_2017_paper|To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance.|
|||We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues, while at the same time remaining fully-convolutional.|
|||Recently, the CRF approach has been integrated more closely with the CNN, by framing the CRF as a recurrent network, and chaining it to the backpropagation of the underlying CNN [61].|
|||Given an input image (left), a CNN typically produces a smooth prediction map (middle top).|
|||We train a fullyconvolutional CNN to minimize this loss through stochastic gradient descent.|
|||In semantic segmentation, the unary term u is typically chosen to be the negative log probability provided by a CNN trained for per-pixel classification.|
|||Note that the network can always fall back to a standard CNN by simply learning a setting of  = 0.|
|||Implementation details  This section first describes how the basic ideas of the technical approach are integrated in a CNN architecture, and then provides details on how the individual components are implemented efficiently as convolution-like layers.|
||8 instances in total. (in iccv2017)|
|542|cvpr18-CVM-Net  Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization|Specifically, we embed the NetVLAD layer [3] on top of a CNN to extract descriptors that are invariant against large viewpoint changes.|
|||Towards this goal, we adopt the VLAD descriptor by embedding NetVLAD layers on top of each CNN branch.|
|||Specifically, the CNN layers for extracting local features Us and Ug remain the same.|
|||This suggests that NetVLAD used in both our CVM-Nets is capable of learning much more discriminative features compared to the CNN and/or fully connected layers architectures utilized by the other approaches.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Multi-scale triplet cnn for person re-identification.|
||8 instances in total. (in cvpr2018)|
|543|Lifting From the Deep_ Convolutional 3D Pose Estimation From a Single Image|We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations.|
|||We propose a novel CNN architecture that learns to combine the image appearance based predictions provided by convolutional-pose-machine style 2D landmark detectors [44], with the geometric 3D skeletal information en coded in a novel pretrained model of 3D human pose.|
|||Information captured by the 3D human pose model is embedded in the CNN architecture as an additional layer that lifts 2D landmark coordinates into 3D while imposing that they lie on the space of physically plausible poses.|
|||Instead, we show how such a model can be used as part of the CNN architecture itself, and how the architecture can learn to use physically plausible 3D reconstructions in its search for better 2D landmark locations.|
|||[22] incorporate model joint dependencies in the CNN via a max-margin formalism, others [48] impose kinematic constraints by embedding a dif 2501  STAGE 12D joint prediction3D lifting &projectionFusionFeature extraction 2D LossSTAGE 22D joint prediction3D lifting &projectionFusionFeature extraction2D LossSTAGE 62D joint prediction3D lifting &projectionFusionFeature extraction2D LossProbabilistic 3Dpose model3D pose3D/2Dprojectionpredictedbelief mapsprojected posebelief maps999999991111predictedbelief mapspredictedbelief mapsprojected posebelief maps2Dfusionfusedbelief mapsInput imageProbabilistic 3Dpose modelOutput 2D poseFinal 3D poseferentiable kinematic model into the deep learning architecture.|
|||it only needs 2D pose annotations to train the CNN joint regressor and a separate 3D mocap dataset to learn the 3D sparse basis.|
|||Network Architecture  Figure 1 illustrates the main contribution of our approach, a new multi-stage CNN architecture that can be trained end-to-end to estimate jointly 2D and 3D joint locations.|
|||Section 5 describes all the new components and layers of the CNN architecture.|
||8 instances in total. (in cvpr2017)|
|544|cvpr18-Where and Why Are They Looking  Jointly Inferring Human Attention and Intentions in Complex Tasks|(3)  Attention feature matching 2(at, lt) describes the compatibility between the attention feature at and the intention lt. We train a CNN classifier with the VGG16 model [30] on the square attention patch samples.|
|||Intention prediction accuracy is defined as the ratio of the  6805  Methods  Accuracy  SVM-JF NN-JF RGB Frame CNN [30]  H (human pose) A (attention patch) O (object feature) H + Relation of H and A A + Relation of H and A A + Relation of A and O O + Relation of A and O Our HAO  0.26 0.29 0.17  0.34 0.07 0.18 0.36 0.28 0.25 0.28 0.40  Table 1: Comparison of overall inttention prediction accuracy.|
|||The method RGB Frame CNN uses whole RGB frames as inputs.|
|||The RGB Frame CNN [30] method uses whole frames as inputs.|
|||We compare our HAO with several methods: 4DHOI [39], Frame CNN [15], and Two-Stream CNN [29].|
|||Frame CNN [15] labels videos by voting based on the frame classification with CNN.|
|||Two-Stream CNN [29] combines the RGB and optical flow features with convolutional neural network to label videos.|
|||Our work develops video understanding from recognizing what humans are doing to inferring what humans are think 4DHOI [39] Frame CNN [15] Two-Stream CNN [29]  H (human pose) A (attention patch) O (object feature) H + Relation of H and A A + Relation of H and A A + Relation of A and O O + Relation of A and O Our HAO  0.62 0.39 0.54  0.58 0.20 0.50 0.61 0.46 0.54 0.66 0.73  Table 3: Comparison of overall task recognition accuracy.|
||8 instances in total. (in cvpr2018)|
|545|cvpr18-TOM-Net  Learning Transparent Object Matting From a Single Image|To the best of our knowledge, TOM-Net is the first CNN that is capable of learning transparent object matting.|
|||Related Work  In this section, we briefly review representative works on environment matting and recent works on CNN based image mating.|
|||[18] introduced a CNN for image matting of color portrait images.|
|||Encoder(cid:173)Decoder for Coarse Prediction  The first stage of our TOM-Net (i.e., CoarseNet) is based on mirror-link CNN introduced in [19].|
|||Therefore, it is reasonable for us to adapt mirror-link CNN for our CoarseNet.|
|||The mirror-link CNN adapted for our CoarseNet consists of one shared encoder and three distinct decoders.|
|||Since we formulate the problem of transparent object matting as refractive flow estimation, which is a dense prediction task, we augment our mirror-link CNN with multi-scale loss similar to [9].|
|||Conclusion and Discussion  We have introduced a simple and efficient model for transparent object matting, and proposed a CNN architecture, called TOM-Net, that takes a single image as input and predicts environment matte as an object mask, an attenuation mask, and a refractive flow field in a fast feed-forward pass.|
||8 instances in total. (in cvpr2018)|
|546|Zihang_Meng_Efficient_Relative_Attribute_ECCV_2018_paper|The initial features f () for the nodes are generated using a CNN on the images and the edge features and following updates are performed using GNNs (details in Fig.|
|||The weights in the entire framework including those in the CNN and GNN are trained end-to-end.|
|||The representations derived from this network yield the initial representations of the node features as  x(0) i = f (Ii),  (1)  where f () refers to a CNN which operates on the images.|
|||[21] and DeepPermNet [2] use VGG CNN model in their experiments, while we choose the simpler Alexnet [15] in all experiments, which has far fewer parameters.|
|||The reader may contrast this with most multi-task learning networks, such as [22, 12, 1], many of which use an additional CNN or several more fully connected layers for each additional attribute, which contribute to more parameters compared to our model.|
|||The multi-task CNN model [1] is a natural choice for the baseline.|
|||For the number of parameters, [1] needs one CNN for each attribute, while we only add 4096  1 parameters twice.|
|||Abdulnabi, A.H., Wang, G., Lu, J., Jia, K.: Multi-task cnn model for attribute  prediction.|
||8 instances in total. (in eccv2018)|
|547|Panda_Weakly_Supervised_Summarization_ICCV_2017_paper|Casting the problem as a weakly supervised learning problem, we propose a flexible deep 3D CNN architecture to learn the notion of importance using only video-level annotation, and without any human-crafted training data.|
|||Furthermore, to unleash the full potential of our 3D CNN architecture, we also explored a series of good practices to reduce the influence of limited training data while summarizing videos.|
|||However, whether and how an end-to-end 3D CNN architecture could be exploited for video summarization still remains as a novel and rarely addressed problem.|
|||We accomplish this via a flexible 3D CNN architecture, namely Deep Summarization Network (DeSumNet), which can assign an importance score to each segment without requiring any human-annotated training data.|
|||As an overview of our approach for summarizing a given video, (1) we perform a forward pass on the input video which generates a distribution of scores over the video categories; (2) calculate the CNN derivatives with respect to each video segment via back-propagation guided by the category with highest score in the forward pass; (3) compute spatio-temporal importance score, and then generate summaries of a given length based on the computed importance scores.|
|||(1) a weakly supervised approach based on 3D CNN that advances the frontier of learning for video summarization; (2) computing spatiotemporal importance scores based on CNN derivatives without resorting to additional training steps; (3) study on a series of good practices for learning 3D CNN with limited training data while extracting video summaries.|
|||Gradient-based deep CNN visualizations have shown to be effective in localizing objects in images without relying on bounding box or pixel-level annotations [51, 49, 56, 37].|
|||However, the gap is still significant due to the two introduced components working in concert: exploiting temporal aspects of activities via an end-to-end 3D CNN architecture and learning the notion of importance from similar category-related videos.|
||8 instances in total. (in iccv2017)|
|548|Song_Embedding_3D_Geometric_ICCV_2017_paper|To this end, we design a 2Dconvolution based CNN structure to extract 3D geometric features from 3D volume, which is named VolNet.|
|||Experimental results verify our conjecture and the effectiveness of both the proposed 2-stream CNN and VolNet.|
|||2) A 2D convolution based CNN model, named VolNet, is proposed to extract 3D geometric features from 3D volume effectively, which also serves as a teacher model to teach GeoNet how to extract 3D geometric features.|
|||These methods mainly are based on 3D CNN with 3D volume as input, while our VolNet adopts 2D CNN.|
|||When using the same number of feature maps, 2D CNN takes less computation and storage than 3D CNN.|
|||We design a 2-stream CNN under FCN framework.|
|||Overview  In order to simultaneously exploit the appearance and 3D geometric features, we design a 2-stream CNN under FCN framework.|
|||The best performance is achieved when simultaneously exploit these fea To verify the observation that 2D CNN is effective to process 3D volume data, we adapt VolNet to assign the right class label to an 3D model on Dataset ModelNet-40 [32].|
||8 instances in total. (in iccv2017)|
|549|Jongbin_Ryu_DFT-based_Transformation_Invariant_ECCV_2018_paper|To address this misalignment problem, recently several CNN models, e.g., GoogleNet [3], ResNet [4], and Inception [5], use an average pooling layer.|
|||2.1 Transformation Invariant Pooling  In addition to rich hierarchical feature representations, one of the reasons for the success of CNN is the robustness to certain object deformations.|
|||DFT-based Transformation Invariant Pooling Layer for Visual Classification  11  4.2 Transferring to Other Domains  The transferred CNN models have been applied to numerous domain-specific classification tasks such as scene classification and fine-grained object recognition.|
|||4.3 Comparison with state-of-the-art methods  We also compare proposed DFT based method with state-of-the-art methods such as the Fisher Vector(FV) [21] with CNN feature [20], the bilinear pooling [9,34], the compact bilinear pooling [24] and the texture feature descriptor e.g.Deep-TEN [30].|
|||The proposed model can be easily incorporated with existing state-of-the-art CNN models by replacing the pooling layer.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
|||Tolias, G., Sicre, R., J egou, H.: Particular object retrieval with integral max-pooling  of cnn activations.|
|||Zheng, L., Zhao, Y., Wang, S., Wang, J., Tian, Q.: Good practice in cnn feature  transfer.|
||8 instances in total. (in eccv2018)|
|550|RON_ Reverse Connection With Objectness Prior Networks for Object Detection|(b) YOLO detects objects with the top-most CNN layer, without deeply exploring the detection capacities of different layers.|
|||The SPP-Net [13] and Fast R-CNN [10] speed up the R-CNN approach with RoI-Pooling (Spatial-PyramidPooling) that allows the classification layers to reuse features computed over CNN feature maps.|
|||YOLO [22] uses the top most CNN feature maps to predict both confidences and locations for multiple categories.|
|||Originated from YOLO, SSD [19] tries to predict detection results at multiple CNN layers.|
|||The region-based networks usually fuse multiple CNN layers into a single feature map [3][16].|
|||SSD [19] detects objects on multiple CNN layers.|
|||Inspired from the success of residual connection [14] which eases the training of much deeper networks, we propose the reverse connection on traditional CNN architectures.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||8 instances in total. (in cvpr2017)|
|551|Zheng_Learning_Multi-Attention_Convolutional_ICCV_2017_paper|Although promising results have been reported, the performance for both part localization and feature learning are heavily restricted by the discrimination ability of the category-level CNN without explicit part constrains.|
|||Besides, some methods (e.g., [35]) propose to unify CNN with spatially-weighted representation by Fisher Vector [23], which show superior results on both bird [30] and dog datasets [12].|
|||A visual attention-based approach proposes a two-level domainnet on both objects and parts, where the part templates are obtained by clustering scheme from the internal hidden representations in CNN [31].|
|||Multi(cid:173)Attention CNN for Part Localization  Given an input image X, we first extract region-based deep features by feeding the images into pre-trained convolutional layers.|
|||All the baselines are listed as following:   PN-CNN [1]: pose normalized CNN proposes to com pute local features by estimating the objects pose.|
||| RA-CNN [5]: recurrent attention CNN framework which can locate the discriminative area recurrently for better classification performance.|
|||Part-stacked cnn for In CVPR, pages 1173  fine-grained visual categorization.|
|||Bilinear cnn models for fine-grained visual recognition.|
||8 instances in total. (in iccv2017)|
|552|Sun_Human_Action_Recognition_ICCV_2015_paper|Without using auxiliary training videos to boost the performance, FSTCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.|
|||The 3D CNN model [7] learns convolution kernels in both space and time based on a straightforward extension of the established 2D CNN deep architectures [17, 13] to the 3D spatio-temporal domain.|
|||By using the optical flow to capture motion features, the two-stream CNN is less effective for characterizing long-range or slow motion patterns which may be more relevant to the semantic categorization of human actions [35, 27].|
|||Possibly due to the increased complexity and difficulty of training 3D kernels without sufficient training video data (as compared to massive image datasets [3]), 3D CNN [7] does not perform well even on the less challenging KTH dataset [25].|
|||To extend the conventional CNN [17] to the spatio-temporal domain, it is necessary to learn a hierarchy of 3D convolution kernels and convolve the learned kernels with the input video data.|
|||In Table 1, we also compare with [10] in the setting of using temporal convolution pipeline only, i.e., using the TCL path with V dif f clip as the input for FSTCN and the optical flow CNN stream for [10].|
|||Our result (48.4%) of TCL path is better than that (46.6%) of optical flow CNN stream in [10] when only split 1 of HMDB-51 is used as training videos.|
|||Compared with the state-of-the-art CNN based method [10], our method outperforms it by about 1% on both datasets, when averaging fusion is adopted.|
||8 instances in total. (in iccv2015)|
|553|Deep Cross-Modal Hashing|Configuration of the CNN for image modality.|
|||There are eight layers in this CNN model.|
|||3.1.2 Hash-Code Learning Part  Let  (x; )  R denote the learned image feature for point , which corresponds to the output of the CNN for image modality.|
|||Here,  is the network parameter of the CNN for image modality, and  is the network parameter of the deep neural network for text modality.|
|||3.2.1 Learn , with  and B Fixed  When  and B are fixed, we learn the CNN parameter  of the image modality by using a back-propagation (BP) algorithm.|
|||We exploit the CNN-F network [5] pre-trained on ImageNet dataset [27] to initialize the first seven layers of the CNN for image modality.|
|||To further verify the effectiveness of DCMH, we exploit the CNN-F deep network [5] pre-trained on ImageNet dataset, which is the same as the initial CNN of the image modality in DCMH, to extract CNN features.|
|||All the baselines are trained based on these CNN features.|
||8 instances in total. (in cvpr2017)|
|554|Yanting_Pei_Does_Haze_Removal_ECCV_2018_paper|To guarantee the comprehensiveness of empirical study, we use both synthetic data of hazy images and real hazy images for experiments and use AlexNet [14], VGGNet [22] and ResNet [10] for CNN implementation.|
|||In [15], a hazeremoval method was proposed by directly generating the underlying clean image through a light-weight CNN and it can be embedded into other deep models easily.|
|||In [28], CNN is combined with recurrent neural networks (RNN) for improving the performance of image classification.|
|||While the proposed CNN model can use AlexNet, VGGNet, ResNet or another network structures, for simplicity, we use AlexNet, VGGNet-16, ResNet50 on Caffe in this paper.|
|||The CNN architectures are pre-trained on ImageNet dataset that consists of 1,000 classes with 1.2 million training images.|
|||We can see that when we train CNN models on clear images and test them on hazy images with and without haze removal (e.g., AlexNet 2, VGGNet 2 and ResNet 2 ), the classification performance drop significantly.|
|||13  4.6 Feature Reconstruction  The CNN networks used for image classification consists of multiple layers to extract deep image features.|
|||One interesting question is whether certain layers in the trained CNN actually perform image dehazing implicitly.|
||8 instances in total. (in eccv2018)|
|555|Seong_Tae_Kim_Facial_Dynamics_Interpreter_ECCV_2018_paper|In [23], a long short-term memory (LSTM) network has been designed on top of CNN features to encode dynamics in video.|
|||By using the LSTM, the temporal correlation of CNN features was effectively encoded.|
|||[31] reported VGG-style CNN learned from large-scale static face images.|
|||In [35], the authors defined an object as a neuron on feature map obtained from CNN and designed a neural network for relational reasoning.|
|||3.1 Facial Local Dynamic Feature Encoding Network  Given a face sequence, appearance features are computed by CNN on each frame.|
|||The pre-trained VGG face model is used to get off-the-shelf CNN features in this study.|
|||Method  Classification Accuracy (%)  Spontaneous Age<=19 Age>19 Age<=19 Age>19  Posed  All  60.80 how-old.net + dynamics(Tree) [6] N/A how-old.net + dynamics(SVM) [6] COTS + dynamics (Tree) [6] 76.92 COTS + dynamics (Bagged Trees) [6] N/A Image-based CNN [27] 80.72 74.38 Holistic dynamic approach Proposed method 80.17  93.46 N/A 93.00 N/A 89.85 93.52 94.65  N/A 60.80 N/A 76.92 80.58 77.51 85.14  N/A N/A 92.89 N/A N/A N/A 92.89 N/A 92.36 86.94 93.91 87.10 95.18 90.08  female, the relation among forehead, nose, and cheek was important.|
|||The accuracy of how-old.net+dynamics and COTS+dynamics were directly from [6] and the accuracy of the image-based CNN and the holistic dynamic approach were calculated in this study.|
||8 instances in total. (in eccv2018)|
|556|Tsai_Adaptive_Region_Pooling_2015_CVPR_paper|We learn a 8192 dimensional codebook, and the SIFT histogram is then built by locality constrained linear coding [35] with 5 nearest neighbors and maximum pooling.1 For extracting CNN features, we use the output of the seventh layer [22], where the CNN models are pre-trained as described in Krizhevskys framework [24].|
|||In the test stage, it takes 1 to 3 seconds (depending on the number of parts) to extract CNN features on a PC with 3.4GHz Core i7 CPU, and the rest takes around 0.55 seconds for testing each image on one model.|
|||Object Detection with CNN Features.|
|||Instead of pooling SIFT features in each part, we use the bounding box of each part as the input to CNN models to obtain features.|
|||Table 4 shows the results compared with other state-ofthe-art methods.2 Our method performs favorably against methods that utilize CNN features (we compare the best results of [39, 30] without bounding box regression).|
|||This is shown in Table 3 and 4, where the mean mAP for DPM improves less than 10% with CNN features, while our algorithm improves more than 20% using CNN features.|
|||We also illustrate that our method is flexible to use any features such as CNN features to achieve state-of-the-art results.|
|||Deformable part models with cnn features.|
||8 instances in total. (in cvpr2015)|
|557|Ren_Scene-Domain_Active_Part_ICCV_2015_paper|Various methods have utilized the CNN feature in part-based models in order to leverage the discriminative power of CNN feature and the fine-grained modeling of part-based models [18, 33, 38].|
|||Thus in the experiments on 2D object and parts detection, we construct our method based on the DPM structure as in [12], but with various types of features: HOG feature as DPM [12], Segmentation feature as SegDPM [15], and CNN feature as DeepPyramid DPM [18].|
|||While in the experiments on 2D, 3D pose and viewpoint estimation, we construct our model based on the Mixture-of-Parts structure as [41] of 10 part types, based on HOG feature and CNN feature as [7].|
|||We use the Caffe [23] to compute the CNN feature.|
|||And the last group of baseline models uses CNN feature, including DeepPyramid DPM (DP-DPM) [18], C-DPM [33] and Conv-DPM [38].|
|||6.4.1  2D pose and 3D landmark shape estimation  We construct our model following the mixture-of-parts structure [41], with HOG and CNN features respectively.|
|||As shown in the first row of Table 5, SDAPM outperforms [20], especially with CNN feature, which validates the effectiveness of modeling in the scenedomain via a unified process other than two separated steps.|
|||Deformable part models with cnn features.|
||8 instances in total. (in iccv2015)|
|558|Shi_Chen_Boosted_Attention_Leveraging_ECCV_2018_paper|[32] combine the memory vector of LSTM with visual features from CNN and feed the fused features to an attention network to compute the weights for features at different spatial locations.|
|||3) Instead of using the spatial map for encoding stimulus-based attention like [3,28,29], we integrate the attention via attentional CNN features.|
|||The model first takes a single raw image as input and encodes it with a CNN Visual Encoder to obtain the visual features.|
|||The proposed Stimulus-based Attention Module mainly consists of three parts, a convolutional layer Wsal pre-trained on saliency prediction for producing the stimulus-based attention features (attentional CNN features, section 4.1), a convolutional layer Wv that further encodes the visual features, and an integration module  that combines stimulus-based attention and visual features.|
|||Boosted Attention Captioning  7  4.1 Attentional CNN Features  Instead of using the final output of the saliency prediction network (i.e., the saliency map), we propose to make use of features from intermediate layers of the network which could encode richer information about stimulus-based attention.|
|||Therefore, we in this work use Wsal to produce attentional CNN features for encoding stimulus-based attention, constructing not only spatial attention widely used in various captioning models but also channel-wise (filter-wise) attention [2] that recently found beneficial for image captioning.|
|||In Figure 4 we visualize attention maps computed with the CNN features, the results demonstrate that attentional CNN features utilized by our model are capable of highlighting various regions of interest.|
|||Stimulus-based attention maps are generated by normalizing the average activation within the CNN features at different spatial locations.|
||8 instances in total. (in eccv2018)|
|559|Wu_Harnessing_Object_and_CVPR_2016_paper|Our semantic fusion network combines three streams of information using a three-layer neural network: (i) frame-based low-level CNN features, (ii) object features from a state-of-the-art large-scale CNN object-detector trained to recognize 20K classes, and (iii) scene features from a state-of-the-art CNN scene-detector trained to recognize 205 scenes.|
|||Recent CNN approaches have shown remarkable improvements in performance on datasets where large amount of labeled data is available [31].|
|||OSF combines three streams of information using a three-layer fusion neural network: (i) frame-based low-level CNN features, (ii) object features from a stateof-the-art large-scale CNN object-detector with 20K classes and (iii) scene features from a state-of-the-art CNN scenedetector trained to recognize 205 scenes.|
|||Deep models (CNNs/LSTMs): More recently, driven by the great success of Convolutional Neural Networks (CNN) on image analysis tasks [6, 31, 32], a few works attempted to leverage CNN models to learn feature representations for video classification.|
|||Final predictions were generated by averaging scores from the two corresponding CNN streams.|
|||CNN model visualization: Our work is also partly inspired by the techniques for visualization and understanding of CNN networks.|
|||OSF consists of a three-layer neural network that fuses information from three CNN streams: (i) a generic image feature stream, designed to capture low-level features of video frames, such as texture and color, (ii) an object stream that captures confidences among 20K object categories it is pre-trained to detect, and (iii) a scene stream, that similarly captures confidences among 205 scene categories it is pre-trained to detect.|
|||Here we use VGG16 CNN model provided by [41].|
||8 instances in total. (in cvpr2016)|
|560|Levy_Live_Repetition_Counting_ICCV_2015_paper|The CNN is the only learned part in our system.|
|||In another domain, state of the art results in text detection were achieved using a CNN trained on synthetic text examples [16].|
|||(third row) The 20 non sequential frames as passed to the CNN classifier.|
|||The lower the entropy, the higher the confidence we have in the result of the CNN classification.|
|||CNN architecture and training  We train our CNN to classify the cycle length within a fixed number of frames.|
|||After the computation of the ROI, the CNN classifier is applied to the video block.|
|||The CNN classifier outputs cycle lengths between 3 and 10 frames, which translate to a repeated action of a duration ranging from 3N/30 seconds to 10N/30 seconds, assuming 30 frames per second.|
|||We present multiple technical novelties: the online design that uses a sliding classifier for counting video repetitions, the use of a CNN in such a framework, the reliance on purely unrealistic (i.e., not using realistic CG) synthetic data for training a computer vision system, changing the state of the system based on entropy, automatically working at multiple time scales, and more.|
||8 instances in total. (in iccv2015)|
|561|Peng_A_Mixed_Bag_2015_CVPR_paper|We also predict emotions in the traditional setting of affective image classification, showing that CNN outperforms Wangs method [30] on Artphoto dataset [20].|
|||Classification performance of CNN and Wangs method [30] with Artphoto dataset [20].|
|||In 6 out of 8 emotion categories, CNN outperforms Wangs method [30].|
|||model [13] and fine-tune the convolutional neural network with our training set in both CNN and CNNR.|
|||To show the efficacy of classification with the convolutional neural network, we use CNN to perform binary emotion classification with Artphoto dataset [20] under the same experimental setting of Wangs method [30].|
|||Figure 4 shows that CNN outperforms Wangs method [30] in 6 out of 8 emotion categories.|
|||In terms of the average of average true positive per class of all 8 emotion categories, CNN (64.724%) also outperforms Wangs method [30] (63.163%).|
|||The preceding experiment shows that CNN achieves state-of-art performance for emotion classification of images.|
||8 instances in total. (in cvpr2015)|
|562|Teng_Robust_Object_Tracking_ICCV_2017_paper|Deep learning models, especially CNN models, have been widely applied to the visual tracking problem, and most of which employ a pre-trained neural network model as the feature extractor [3, 7, 21, 23, 26, 32, 37, 42, 45, 54, 55].|
|||Some CNN based tracking methods combine a CNN model with conventional tracking techniques such as the saliency map and SVM used in the DSCNN tracker [23], correlation filters employed in the HCFT tracker [37], particle filters utilized in the MDNet tracker [42], feature selection described in the FCNT tracker [54], etc.|
|||Other CNN based methods either combine several CNN models (e.g.|
|||the DeepTrack model [32]) or establish an end-to-end CNN model, such as the GOTURN tracker [21] with no online training but offline learn a generic relationship between object motion and appearance from large number of videos.|
|||Apart from CNN models, other deep models, such as Siamese network [3, 5, 51] and Recurrent Neural Networks (RNNs), are also employed in the tracking problem.|
|||The DeepTrack model [32] represents temporal adaptation through the update of CNNs in the CNN pool.|
|||The R-CNN [18], FCNT [54], and HCFT [37] combine features from different layers of CNN to describe the spatial information of the target.|
|||DeepSRDCF employs CNN features in a correlation filter but could not exert the best performance of CNN features due to insufficient learning.|
||8 instances in total. (in iccv2017)|
|563|Liu_Sparse_Convolutional_Neural_2015_CVPR_paper|[12] obtain 4.5x speedup with less than 1% drop in accuracy of a 4 layer CNN trained on a scene character classification dataset.|
|||None of them show that their method can work on kernels as small as 3  3, which are extensively used in state-of-the-art CNN models.|
|||There are also several works that try to optimize the speed of CNN from other perspectives.|
|||[6] implement a large scale CNN based on FPGA infrastructure that can perform embedded real-time recognition tasks.|
|||[8] first proposed to use CNN to solve the object detection problem.|
|||They warp each candidate window generated by the Selective Search[21] method to a fixed size and use CNN to generate high level discriminative features, with whick linear SVM with hard negative mining is adopted to train object detection model.|
|||We start from a pre-trained Caffe[13] reference CNN model, which is almost identical to the model described in [14].|
|||We show the variation of both the accuracy and the average sparsity of our sparse CNN during the training process.|
||8 instances in total. (in cvpr2015)|
|564|Jiabei_Zeng_Facial_Expression_Recognition_ECCV_2018_paper|The CNN feature extractor was trained from a set of clean data.|
|||To this end, we integrate the Dawid&Skenes[5] and the CNN into an end-to-end trainable architecture LTNet.|
|||In AIR, we trained a CNN from the mixture of the noisy labels and used the features from the trained CNN to do the afterward L12-norm regularization.|
|||EM+CNN is similar to the 2-step solution in Section 3.3, where we used EM algorithms to estimate the latent truth, and then trained a CNN on the latent truth.|
|||We also report the test accuracy of the basic CNN trained on clean data.|
|||Within the methods trained on mixture data, we observe that the end-to-end methods (e.g., basic CNN on mixture data or majority ratings, NAL, LTNet) are significantly better than the step-by-step methods (e.g., AIR, EM+CNN).|
|||Among all the end-to-end methods, the proposed LTNet achieves the highest test accuracy and has a comparable performance to the CNN trained from clean data.|
|||As can be seen, the test accuracy curves of LTNet, CNN(clean data), CNN(mixed all), and ANL keep increasing during the training, while those of CNN (with 40%, 30%, or 20% noisy labels) and EM+CNN(spectral or major init) reach a peak value and then decrease as the training iterates.|
||8 instances in total. (in eccv2018)|
|565|One-Shot Metric Learning for Person Re-Identification|When training a CNN using only intensity images, the learned embedding is color-invariant and shows high performance even on unseen datasets without fine-tuning.|
|||Deep texture features T are trained with CNN on intensity images to enforce color invariance without having to fine-tune.|
|||We can achieve this if we  2990  use only intensity images and train a single CNN through a challenging multi-classification task on multiple datasets.|
|||In contrast, a CNN learned on color images would most likely require fine-tuning when testing [60], since the training dataset would have to be extremely large to cover all possible inter-camera color variations.|
|||We adopt the CNN model from [55] and train it from scratch using only intensity images to obtain highly robust color-invariant features for person reidentification.|
|||Compared to KISSME [28] for both color and intensity images, it is apparent that a single CNN is flexible enough to handle multiple dataset variations.|
|||However, as we are interested in generalization properties of this CNN (for unsupervised case), we also evaluate JSTL performance on unseen camera pairs.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||8 instances in total. (in cvpr2017)|
|566|Wang_Sketch-Based_3D_Shape_2015_CVPR_paper|Since the two input sources have distinctive intrinsic properties, we use two different CNN models, one for handling the sketches and the other for the views.|
|||Most importantly, we define a loss function to align the results of the two CNN models.|
|||Specifically, CNN has set records on standard object recognition benchmarks [16].|
|||With a deep structure, the CNN can effectively learn complicated mappings from raw images to  the target, which requires less domain knowledge compared to handcrafted features and shallow learning frameworks.|
|||The goal of CNN is to learn a hierarchy of feature representations.|
|||The learning of CNN is based on Stochastic Gradient Descent (SGD).|
|||The structure of the single CNN has three convolutional layers, each with a max pooling, one fully connected layer to generate the features, and one output layer to compute the loss (Eq.|
|||Computational cost The implementation of the proposed Siamese CNN is based on the Theano [2] library.|
||8 instances in total. (in cvpr2015)|
|567|Gonzalez-Garcia_An_Active_Search_2015_CVPR_paper|When computing CNN features on the CPU [29], the processing time for one test image reduces from 320s to 36s (9 speed-up).|
|||The state-ofthe-art detector [22] follows this approach, using CNN features [30] with Selective Search proposals [49].|
|||This detector is based on the CNN model of [30], which achieved winning results on the ILSVRC-2012 image classification competition [1].|
|||The CNN is then finetuned from a image classifier to a window classifier on ground-truth bounding-boxes.|
|||We embed the 4096dimensional CNN appearance features [29] in a Hamming space with 512 bits using [24].|
|||The most expensive component is computing CNN features, which takes 4.5 ms per window on the GPU.|
|||The small overhead added by our method is mainly due to the RF query performed at each iteration for the context  2Extracting CNN descriptors on a GPU is more efficient in batches than one at a time, and is done in R-CNN [22] by batching many proposals in a single image.|
|||Extracting CNN features on the CPU takes 100 ms per window [29].|
||8 instances in total. (in cvpr2015)|
|568|Xiaofeng_Liu_Dependency-aware_Attention_Control_ECCV_2018_paper|The CNN is an ideal approximate function to address the infinite state space [37].|
|||We adopt a modern CNN module to embed an image into a latent space, which can largely reduce the computation costs and offer a practicable state space for RL.|
|||To utilize the millions of available still images, we train our CNN embedding module separately.|
|||We use Titan Xp for CNN processing.|
|||The previous work NAN [16] uses the same CNN structure as our framework, but adopts a neural network module for independently quality assessment of each image.|
|||It also indicates that DAC achieves a very competitive performance without highly-engineered CNN models.|
|||Comparisons of the average verification accuracy with the recently state-of-the-art results on the YTF dataset. fine-tuned the CNN model with YTF.|
|||With the help of end-to-end learning and large volume training data for CNN model, deep learning methods outperform [61, 12] by a large margin.|
||8 instances in total. (in eccv2018)|
|569|cvpr18-Triplet-Center Loss for Multi-View 3D Object Retrieval|Model-based methods [11, 38] obtain the 3D shape features directly from the original 3D representations so that 3D CNN is preferred.|
|||In particular, training a plain CNN model with the triplet loss for end-to-end metric learning has shown its advantages in face recognition [29] and person re-identification (re-ID) [12].|
|||argue that training a CNN with the triplet loss [29] or center loss [39] that is specific for distance measure, can also bring the performance benefits to 3D object retrieval, significantly outperforming the state-of-the-art approaches on the most popular benchmark datasets of 3D object retrieval, such as ModelNet40 and ShapeNet Core55.|
|||In summary, we make the following contributions: 1) We firstly introduce two kinds of typical loss functions that are suggested for 3D object retrieval, and fully investigate their impact on the retrieval performance; 2) We propose a novel loss function named triplet-center loss (TCL), and show that the state-of-the-art results are obtained when using TCL based on the same CNN model, superior to other alternatives.|
|||With TCL, our CNN model for 3D object retrieval is built upon the framework of MVCNN [33], as illustrated in Figure 1.|
|||[38] propose a 3D CNN based on octree representation, which can largely improve the computation efficiency compared with traditional full-voxel-based representations.|
|||The view-based methods usually render a single view or multiple views for a 3D shape firstly, such that sophisticated image feature extractors like CNN can be exploited to extract features from the 2D rendered view, then these extracted view features are assembled into a compact shape descriptor which is finally employed for the retrieval or classification task.|
|||Usually, this can be partly achieved by exploiting softmax loss to train a CNN on the labeled training set.|
||8 instances in total. (in cvpr2018)|
|570|Dmytro_Mishkin_Repeatability_Is_Not_ECCV_2018_paper|The patch is described by a differentiable CNN descriptor.|
|||2.6  Implementation details  The CNN architecture is adopted from HardNet[25], see Fig.|
|||We believe it is because now the CNN always outputs the same affine transformation for a patch, unlike in the previous experiment, where repeated features may end up with different shapes.|
|||Baseline HesAff + dominant gradient orientation + SIFT: no CNN components  0.4 sec.|
|||HesAffNet (CNN) + dominant gradient orientation + SIFT  0.8s, 3 CNN components: HesAffNet + OriNet + HardNet  1.2 s. Now the data is naively transferred from CPU to GPU and back each of the stages, which generates the major bottleneck.|
|||Radenovic, F., Tolias, G., Chum, O.: CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples.|
|||Radenovi c, F., Tolias, G., Chum, O.: Fine-tuning cnn image retrieval with no human annota tion.|
|||Iscen, A., Tolias, G., Avrithis, Y., Furon, T., Chum, O.: Efficient diffusion on region manifolds:  Recovering small objects with compact cnn representations.|
||8 instances in total. (in eccv2018)|
|571|Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper|In these architectures, a full-frame CNN representation is used to compute a spatial weighting (attention) over the CNN grid cells.|
|||4  M. Malinowski, C. Doersch, A. Santoro and P. Battaglia  The alternative is to select CNN grid cells in a discrete way, but due to many challenges in training non-differentiable architectures, such hard attention alternatives are severely under-explored.|
|||We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63], or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64].|
|||In most prior work, soft attention is implemented as a weighted mask over the spatial cells of the CNN representation.|
|||The correlation  spatial aggregation +MLPCNNembedding +LSTM1x1convnet/ MLPClassifier LogitsAre all dogs on red leashes?L2normthresholdBroadcasting6  M. Malinowski, C. Doersch, A. Santoro and P. Battaglia  between L2-norm and relevance is an emergent property of the trained CNN features, which requires no additional constraints or objectives.|
|||As above, let xij and q be a CNN cell at the spatial position i, j, and a question representation respectively.|
|||In our experiments we use a simple CNN built of: 1 layer with 64 filters and 7-by-7 filter size followed up by 2 layers with 256 filters and 2 layers with 512 filters, all with 3-by-3 filter size.|
|||Due to the visual simplicity of CLEVR, we follow up the work of [25], and instead of relying on the ImageNet pre-trained features, we train our HAN+sum and HAN+RN (hard attention with relation network) architectures end-to-end together with a relatively small CNN (following [25]).|
||8 instances in total. (in eccv2018)|
|572|Yang_From_Facial_Parts_ICCV_2015_paper|Interestingly, in our method, part detectors emerge within CNN trained to classify attributes from uncropped face images, without any part supervision.|
|||Each CNN outputs a partness map, which is obtained by weighted averaging over all the label maps at its top convolutional layer.|
|||3 depicts the structure and hyperparameters of the CNN in Fig.|
|||To obtain a cleaner partness map, we employ the merit from object categorization, where CNN is pre-trained with massive general object categories in ImageNet [27] as in [16].|
|||To improve it further, we refine these windows by joint training face classification and bounding box regression using a CNN similar to the AlexNet [16].|
|||If the proposed window is a false positive, the CNN outputs a vector of [1, 1, 1, 1].|
|||(0.861535) Acfmultiscale (0.860762) Cascade CNN (0.856701) Boosted Exampler (0.856507) DDFD (0.848356) SURF Cascade multiview (0.840843) PEPAdapt (0.819184) XZJY (0.802553) Zhu et al.|
|||[11], who show that a CNN structure can enjoy a 2.5 speedup with no loss in accuracy by approximating non-linear filtering with lowrank expansions.|
||8 instances in total. (in iccv2015)|
|573|Eric_Muller-Budack_Geolocation_Estimation_of_ECCV_2018_paper|We combine the outputs from all scales to exploit the hierarchical information of a CNN that is trained simultaneously with labels from multiple partitionings to encode local and global information.|
|||This should enable the CNN to learn specific features for estimating the GPS (Global Positioning System) coordinate of images in different environmental surroundings.|
|||Furthermore, we have used a state of the art CNN architecture and our comprehensive experiments include an evaluation of the impact of different scene concepts.|
|||[39] to learn a feature representation with a CNN to improve the Im2GPS framework.|
|||As a consequence, the CNN is able to learn geographical features at different scales resulting in a more discriminative classifier.|
|||As described in the previous sections, the geolocalization can be further increased by training the CNN with multiple partitionings and exploiting the hierarchical knowledge at all spatial resolutions.|
|||Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Radenovi c, F., Tolias, G., Chum, O.: Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples.|
||8 instances in total. (in eccv2018)|
|574|Bolei_Zhou_Temporal_Relational_Reasoning_ECCV_2018_paper|TRN is a general and extensible module that can be used in a plug-and-play fashion with any existing CNN architecture.|
|||A CNN+LSTM model, which uses a CNN to extract frame features and an LSTM to integrate features over time, is also used to recognize activities in videos [14].|
|||Notice that for any given sample of d frames for each Td, all the temporal relation functions are end-to-end differentiable, so they can all be trained together with the base CNN used to extract features for each video frame.|
|||The CNN feature is extracted from incoming key frame only once then enqueued, thus TRN-equipped networks is able to run in real-time on a desktop to processing streaming video from a webcam.|
|||Thus, we fix the base network architecture to be the same throughout all the experiments and compare the performance of the CNN model with and without the proposed TRN modules.|
|||The CNN features for a given frame is the activation from the BN-Inceptions global average pooling layer (before the final classification layer).|
|||The base CNN is BNInception.|
|||Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson, S.: Cnn features offIn: Proceedings of the IEEE  the-shelf: an astounding baseline for recognition.|
||8 instances in total. (in eccv2018)|
|575|Fu_Zero-Shot_Object_Recognition_2015_CVPR_paper|After training, for each test image, the 4,096 dimensional top-layer hidden unit activations (fc7) of the CNN are taken as the features.|
|||On AwA, we also use the CNN feature originally provided [28] as it has been shown recently to be much more powerful than the low-level features originally provided in [15].|
|||The mapping/embedding of visual feature vector (4,096D) into the 1000/100D word vector space is achieved using the deep CNN model DeViSE [10].|
|||Two types of features are used: low-level (L) and CNN (C) features.|
|||However, more recently the CNN features (C) have been used [4].|
|||Table 2 shows that the best result is obtained using the proposed AMP (SR+SE) method, with two observations: (1) In general using the CNN features leads to better performance.|
|||Given CNN features, our model outperforms significantly the other existing methods.|
|||This is partly because we use the deep CNN model directly to learn the projection rather than just extract the features.|
||8 instances in total. (in cvpr2015)|
|576|cvpr18-Non-Blind Deblurring  Handling Kernel Uncertainty With CNNs|The fact that we need discriminatory feature learning from multiple inputs renders CNN as a natural choice.|
||| We train our CNN with large numbers of synthetic and real noisy kernels to achieve state-of-the-art performance in non-blind deblurring.|
|||[39] trained a deep CNN to perform inversion of a single kernel in the presence of non-linearities in the blurred image.|
|||Some of the recent works on NBD employ machine learning frameworks such as Gaussian conditional random fields [29], shrinkage fields [28], whereas the most recent work in [18] uses CNN based regularization.|
|||Our approach for image restoration consists of two modules: a conventional NBD unit to obtain multiple initial estimates of the latent image followed by the use of a deep CNN to remove any undesired artifacts present in the initial estimates, and to provide enhanced details.|
|||In this paper, our primary focus is on training the CNN by initializing from restored results of [16] (i.e., Eq.|
|||Motivation  Our idea of using deep CNN to improve the output from a deconvolution unit is motivated by the fact that  plays a crucial role in deciding the restoration quality.|
|||Since CNN can learn feature discrimination as well as patch specific priors from the training data, our approach also attempts to restore enhanced details in output.|
||8 instances in total. (in cvpr2018)|
|577|cvpr18-Disentangling 3D Pose in a Dendritic CNN for Unconstrained 2D Face Alignment|Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto 15% reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG.|
|||A generic CNN is used for auxiliary tasks such as fine-grained localization or occlusion detection.|
|||With an intent to solve these, we propose a tree structure model in a single Dendritic CNN (PCD-CNN), which is able to capture the shape constraint in a deep learning framework.|
|||Researchers have proposed many different network architectures like Hourglass [35], Binarized CNN (based on hourglass) [10] in order to achieve accuracy in keypoints estimation.|
|||Proposed Pose Conditioned Dendritic CNN : To capture the structural relationship between different keypoints, we propose the dendritic structure of facial landmarks as shown in figure 2b where the nose tip is assumed to be the root node.|
|||Following this, the keypoint network is modeled with a single CNN in a tree structure composed of convolution and deconvolution layers.|
|||Conclusion and Future Work  In this paper, we present a dendritic CNN which processes images at full scale looking at the images globally and capturing local interactions through convolutions.|
|||Large pose 3d face reconstruction from a single image via direct volumetric cnn regression.|
||8 instances in total. (in cvpr2018)|
|578|Yu_Visual_Relationship_Detection_ICCV_2017_paper|Although the visual input analyzed by the CNN in [19] includes the subject and object, predicates are predicted without any knowledge about the object categories present in the image or their relative locations.|
|||Our Approach  the optimization problem is:  A straightforward way to predict relationship predicates is to train a CNN on the union of the two bounding boxes that contain the two objects of interest as the visual input, fuse semantic features (that encode the object categories) and spatial features (that encode the relative positions of the objects) with the CNN features (that encode the appearance of the objects), and feed them into a fully connected (FC) layer to yield an end-to-end prediction framework.|
|||While the subject, predicate, and object are not statistically independent, a CNN would require a massive amount of data to discover the dependence structure while also learning the mapping from visual features to semantic relationships.|
|||Entire Set  Zero-shot  R@100/502 R@100 R@50 R@100/50 R@100 R@50 k=70  k=70  k=70  k=1  k=70  k=1  Part 1: Training images VRD only Visual Phrases [26] Joint CNN [6] VRD-V only [19] VRD-Full [19] Baseline: U only Baseline: L only U+W U+W+L:S U+W+L:T U+SF U+SF+L:S U+SF+L:T U+W+SF U+W+SF+L: S U+W+SF+L: T U+W+SF+L: T+S  1.91 2.03 7.11 47.87 34.82 51.34 37.15 42.98 52.96 36.33 41.06 51.67 41.33 47.50 54.13 55.16   37.20 3 84.34 83.15 85.34 83.78 84.94 88.98 83.68 84.81 87.71 84.89 86.97 89.41 94.65  Part 2: Training images VRD + VG Baseline: U U+W+SF U+W+SF+L: S U+W+SF+L: T U+W+SF+L: T+S  36.97 42.08 48.61 54.61 55.67  84.49 85.89 87.15 90.09 95.19   28.36 70.97 70.02 80.64 70.75 71.83 83.26 69.87 71.27 83.84 72.29 74.98 82.54 85.64  70.19 72.83 75.45 82.97 86.14   3.52 8.45 12.75 3.68 13.44 13.89 7.81 14.33 15.14 8.05 14.13 16.98 8.80   13.31 14.51 17.16 9.23    32.34 50.04 69.42 18.22 69.77 72.53 40.15 69.01 72.72 41.51 69.41 74.65 41.53   70.56 70.79 75.26 43.21    23.95 29.77 47.84 8.13 49.01 51.37 32.62 48.32 51.62 32.77 48.13 54.20 32.81   50.34 50.64 55.41 33.40   4.1.|
|||The Visual Phrases method [26] trains deformable parts models for each relationship; Joint CNN [6] trains a 270-way CNN to predict the subject, object and predicate together.|
|||End-to-end CNN training with semantic and spa 3The recall of different ks are not reported in [19].We calculate those  recall values using their code.|
|||Comparing our baseline, which uses the same visual representation (BB-Union) as [19], and the VRD-V only model, our huge recall improvement (R@100/50, k=1 increases from 7.11% [19] to 34.82%) reveals that the end-to-end training with soft-max prediction outperforms extracting features from a fixed CNN + linear model method in [19], highlighting the importance of finetuning.|
|||Phrase Detection  Relationship Detection  R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, R@100, R@50, k=70  k=70  k=10  k=10  k=10  k=70  k=10  k=70  k=1  k=1  k=1  k=1  0.07 0.09 2.61 17.03  Part 1: Training images VRD only Visual Phrases [26] Joint CNN [6] VRD V only [19] VRD Full [19] Linguistic Cues [25] VIP-CNN [17] VRL [18] U+W+SF+L: S U+W+SF+L: T U+W+SF+L: T+S  27.91 22.60 19.98 23.57 24.03   0.04 0.07 2.24 16.17   22.78 21.37 19.15 22.46 23.14  Part 2: Training images VRD + VG U+W+SF+L: S U+W+SF+L: T U+W+SF+L: T+S  20.32 23.89 24.42  19.96 22.92 23.51      25.52 20.70  20.42 16.89   25.16 29.14 29.76  25.71 29.82 30.13   22.95 25.96 26.47  23.34 26.34 26.73  24.90  20.04      25.54 29.09 29.43  25.97 29.97 30.01      22.59 25.86 26.32  22.83 26.15 26.58   0.09 1.85 14.70    20.01 20.79 17.69 20.61 21.34  18.32 20.94 21.72   0.07 1.58 13.86    17.32 18.19 16.57 18.56 19.17  16.98 18.93 19.68      22.03 18.37  17.43 15.08   27.98 29.41 29.89  28.24 29.95 30.45   19.92 21.92 22.56  20.15 22.62 22.84  21.51  17.35      28.94 31.13 31.89  29.85 31.78 32.56      20.12 21.98 22.68  21.88 22.65 23.18  Table 3.|
||8 instances in total. (in iccv2017)|
|579|Budget-Aware Deep Semantic Video Segmentation|In deep video segmentation, the most time consuming step represents the application of a CNN to every frame for assigning class labels to every pixel, typically taking 6-9 times of the video footage.|
|||Also, the runtime of these feed-forward CNN architectures is typically 6-9 times the video length [35].|
|||These approaches are not easy to generalize to the state-of-the-art CNN based semantic segmentation, since such approaches do not explicitly extract features during a preprocessing step.|
|||ferent from theirs in : 1) Unlike their approach, which is based on CRF based supervoxel labeling along the video, we use CNN based frame-level model aumgneted with a label propagation network.|
|||We specify f as a CNN following two prior approaches [2, 23].|
|||Based on the result, although both LSTM and CNN provides slight improvement in accuracy when applied independent of each other, they improve the accuracy with a larger margin when learned together.|
|||We have specified a budget-aware inference for this problem that intelligently selects a subset of frames to run a deep CNN for semantic segmentation.|
|||Since CNN computation often dominates the cost of infer ence, our framework can provide substantial time savings in a principled manner.|
||8 instances in total. (in cvpr2017)|
|580|cvpr18-End-to-End Weakly-Supervised Semantic Alignment|r i a p   t u p n I  s r e  i l  n I  s r e  i l t u O  Figure 1: We describe a CNN architecture that, given an input image pair (top), outputs dense semantic correspondence between the two images together with the aligning geometric transformation (middle) and discards geometrically inconsistent matches (bottom).|
|||We show that our approach allows to significantly improve the performance of the baseline deep CNN alignment model, achieving stateof-the-art performance on multiple standard benchmarks for semantic alignment.|
|||   s VGG-16 ResNet-101 CNN Geo.|
|||   s  Proposed method  ResNet-101 CNN Geo.|
|||We build on the Siamese CNN architecture described in [29], illustrated in the left section of Fig.|
|||The input source and target images, (I s, I t), are passed through two fully-convolutional feature extraction CNN branches, F , with shared weights.|
|||Implementation as a CNN layer.|
|||For the underlying semantic alignment network, we use the best-performing architecture from [27] which employs a ResNet-101 [12], cropped after conv4-23, as the feature extraction CNN F .|
||8 instances in total. (in cvpr2018)|
|581|Tang_Large_Scale_Semi-Supervised_CVPR_2016_paper|Related Work  With the remarkable success of deep CNN on large-scale object recognition [16] in recent years, a substantial number of CNN-based object detection frameworks have emerged [11, 12, 13, 25, 31, 35].|
|||Such approaches only adopt CNN as a feature extractor, and exhaustively mine image regions extracted by region proposal approaches, such as Selective Search [36], BING [5], and EdgeBoxes [41].|
|||[23] develop a weakly supervised CNN end-to-end learning pipeline that learns from complex cluttered scenes containing multiple objects by explicitly searching over possible object locations and scales in the image, which can predict image labels and coarse locations (but not exact bounding boxes) of objects.|
|||[14] propose a Large Scale Detection through  2120  Adaptation (LSDA) algorithm that learns the difference between the CNN parameters of the image classifier and object detector of a fully labeled category, and transfers this knowledge to CNN classifiers for categories without bounding box annotated data, turning them into detectors.|
|||The LSDA algorithm learns to convert (K m) image classifiers (from A) into their corresponding object detectors through the following steps: Pre-training: First, an 8-layer (5 convolutional layers and 3 fully-connected (f c) layers) Alex-Net [16] CNN is pre-trained on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 classification dataset [30], which contains 1.2 million images of 1,000 categories.|
|||Fine-tuning for classification: The final weight layer (1,000 linear classifiers) of the pre-trained CNN is then replaced with K linear classifiers.|
|||This weight layer is randomly initialized and the whole CNN is then fine-tuned on the dataset D. This produces a classification network that can classify K categories (i.e., K-way softmax classifier), given an image or an image region as input.|
|||The visual similarity (denoted sv) between a weakly labeled category j  A and a fully labeled category i  B is defined as:  sv(j, i)   1 N  N  X  n=1  CNN(In)i  (2)  where In is a positive image from category j of the validation set of A, N is the number of positive images for this category, and CNN(In)i is the ith CNN output of the softmax layer on In, namely, the probability of In being category i  B as predicted by the fine-tuned classification network.|
||8 instances in total. (in cvpr2016)|
|582|Annotating Object Instances With a Polygon-RNN|Most of these ap proaches [14, 24, 36, 34, 20, 21] operate on the pixel-level, typically exploiting a CNN inside a box or a patch to perform the labeling.|
|||As input in each step of the RNN we use a CNN representation of the image crop, as well as the vertices predicted one and two time steps ago, plus the first point.|
|||3.1.1  Image Representation via a CNN with Skip Connections  We adopt the VGG-16 architecture [27] and modify it for the purpose of our task.|
|||This allows the CNN to extract features that contain both low-level information about the edges and corners, as well as semantic information about the object.|
|||2 for a visualization and further details about the architecture (the CNN is highlighted in green).|
|||Let yt denote the one-hot encoding of a vertex, output at time step t.  Our ConvLSTM gets as input a tensor xt at time step t, that concatenates multiple features: the CNN feature repre 5232  sentation of the image, yt1 and yt2, i.e., a one-hot encoding of the previous predicted vertex and the vertex predicted from two time steps ago, as well as the one-hot encoding of the first predicted vertex y1.|
|||We reuse the same architecture of the CNN as in Sec.|
|||Given an input image patch, DeepMask uses a CNN to output a pixel labeling of an object, and does so agnostic to the class.|
||8 instances in total. (in cvpr2017)|
|583|Semi-Supervised Deep Learning for Monocular Depth Map Prediction|We concurrently train a CNN from unsupervised and supervised depth cues to achieve state-of-the-art performance in single image depth prediction.|
|||[5] propose a CNN architecture that integrates coarse-scale depth prediction with fine-scale prediction.|
|||Their deep convolutional neural fields allow for training CNN features of unary and pairwise potentials end-to-end, exploiting continuous depth and Gaussian assumptions on the pairwise potentials.|
|||[1] incorporates the side-task of depth ranking of pairs of pixels for training a CNN on single image depth prediction.|
|||We train the CNN to predict the inverse depth (x) at each pixel x   from the RGB image I.|
|||With  we denote the CNN network parameters that generate the inverse depth maps r/l,.|
|||(6)  6649  We use the berHu norm kk as introduced in [16] to focus training on larger depth residuals during CNN training,  Channels I/O Scaling  Inputs  (7)  (8)  (9)  kdk =(|d|, d    d2+2  2  , d >   .|
|||10  t  To train the CNN on KITTI we use stochastic gradient descent with momentum with a learning rate of 0.01 and momentum of 0.9.|
||8 instances in total. (in cvpr2017)|
|584|Liu_Box_Aggregation_for_ICCV_2015_paper|This framework consists of three steps: (1) object proposal generation, (2) CNN feature extraction and class-specific scoring, (3) and object box finding from thousands of scored box proposals.|
|||The representative work includes proposal generation with high recall [25, 1] and developing deeper CNN models [20, 21] or new structure [15] to boost the performance.|
|||With the same proposal-generation and CNN steps, detection accuracies increase.|
|||Then CNN features for these box proposals are extracted and scores are assigned to the proposals by learned classspecific classifiers.|
|||New training strategies and deformable layers were designed in [17] to make CNN work better for object detection.|
|||In [22], object proposal generator and CNN feature extractor were combined to further increase detection performance.|
|||This phenomenon indicates when the detection score is high enough, localization accuracy becomes not strongly related to it even using the complicated and advanced CNN model [20].|
|||Box Group Generation  As aforementioned, we use the RCNN framework that outputs thousands of object proposals with scores after object proposal generation, CNN feature extraction and classification.|
||8 instances in total. (in iccv2015)|
|585|Workman_A_Unified_Model_ICCV_2017_paper|We use a CNN to extract features, fg(Gi), from each image and interpolate using Nadaraya Watson kernel regression,  fG(l) =  P wifg(Gi)  P wi  ,  (1)  where wi = exp(d(l, li; )2) is a Gaussian kernel function where a diagonal covariance matrix  controls the kernel bandwidth and d(l, li; ) is the Mahalanobis distance from l to li.|
|||Overhead Feature Map Construction  This section describes the CNN we use to extract features from the overhead image and how we integrate the ground-level feature map.|
|||The CNN is based on the VGG16 architecture [25], which has 13 convolutional layers, each using 3  3 convolutions, and three fully connected layers.|
|||We estimate these bandwidth parameters using a CNN applied to the overhead image.|
|||, 33}, with the overhead image CNN defined in Section 4.3.|
|||We use the unified (uniform) architecture, but do not incorporate the ground-level feature map in the overhead image CNN or the hypercolumn.|
|||Starting from unified (uniform), we omit all layers from the overhead image CNN prior to concatenating in the ground-level feature map from the hypercolumn.|
||7 instances in total. (in iccv2017)|
|586|Liu_Semantic_Image_Segmentation_ICCV_2015_paper|Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms.|
|||First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF.|
|||[22] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [6].|
|||[3] improved [22] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components.|
|||A recent advance was obtained by [30], which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) [27] is required for each training image during backpropagation (BP).|
|||We found that directly combing CNN and MRF as above is inefficient, because CNN typically has millions of parameters while MRF infers thousands of latent variables; and even worse, incorporating complex pairwise terms into MRF becomes impractical, limiting the performance of the entire system.|
|||This work proposes a novel Deep Parsing Network (DPN), which is able to jointly train CNN and complex pairwise terms.|
||7 instances in total. (in iccv2015)|
|587|Kuan-Chuan_Peng_Zero-Shot_Deep_Domain_ECCV_2018_paper|The newly added CNN takes the T-R depth images as input, and shares all the weights with the original source CNN, so we use s2 to refer to both of them.|
|||For each BNA, We specify the layer separating the source/target CNN and the source classifier in Fig.|
|||The layer name in the right column is based on the official Caffe [24] and SqueezeNet v1.1 [23] implementation of each BNA  base network  source/target CNN architecture architecture (BNA) (up to where in BNA (inclusive))  LeNet [5]  GoogleNet [38]  AlexNet [26]  ip1  pool5/7x7 s1  fc7  SqueezeNet v1.1 [23]  fire9/concat   blending the gray scale images with the patches randomly extracted from the BSDS500 dataset [2].|
|||Table 4 lists the base network architecture (BNA) we use and the layer separating the source/target CNN and the source  Zero-Shot Deep Domain Adaptation  9  classifier in Fig.|
|||For instance, in the case when the BNA is LeNet [5], the architecture of each source/target CNN in Fig.|
|||2 from scratch except that the target CNN is pre-trained from the T-I dataset and fixed afterwards.|
|||For example, when using DF and DF -M as the T-I data in the DA task DM  DM M, we train a CNN (denoted as CN Nref ) with the LeNet [5] architecture from scratch using the images and labels of DF -M, and pre-train the target CNNs in Fig.|
||7 instances in total. (in eccv2018)|
|588|Fan_A_Generic_Deep_ICCV_2017_paper|The above approaches focus on solving a single/major problem using a plain CNN model followed by more traditional, inflexible operations inspired by fixed filtering methods.|
|||However, because accurately capturing smoothing effects with a fully convolutional deep network can be challenging, [38] trains a shallow CNN on the gradient domain followed by an optimized image reconstruction post-processing step with sensitive parameters tuned for each different smoothing filter.|
|||From a somewhat different perspective, by treating spatially-variant recursive networks as surrogates for a group of distinct filters, [24] combines sparse salient structure prediction implemented as CNN with image filtering in a hybrid neural network.|
|||Two CNN networks, E-CNN and I-CNN, are used for edge prediction and image reconstruction, respectively.|
|||(b) The detailed CNN structure shared by E-CNN and I-CNN.|
|||Similarly, given a source image Is, we apply a CNN to learn an edge map Et of the target image It (i.e., the background layer for reflection removal or the smoothed image for image smoothing).|
|||Details of CNN Layers  For simplicity, we employ the deep CNN structure shown in Fig.|
||7 instances in total. (in iccv2017)|
|589|cvpr18-PoTion  Pose MoTion Representation for Action Recognition|It can thus be passed to a conventional CNN for classification without having to resort to recurrent networks or more sophisticated schemes.|
|||Given this representation, we train a shallow CNN architecture with 6 convolutional layers and one fully-connected layer to perform action classification.|
|||In this paper, we use CNN pose features with a colorization scheme to aggregate the feature maps.|
|||Once that we have precomputed our compact PoTion representation for every video clip of the dataset, it takes approximatively 4 hours to train our CNN for HMDB on a NVIDIA Titan X GPU.|
|||Data augmentation plays a central role in CNN training.|
|||After introducing the datasets and metrics in Section 5.1, we study the parameters of PoTion in Section 5.2 and of the CNN architecture in Section 5.3.|
|||On JHMDB we significantly outperform P-CNN [6], a method that leverages pose estimation to pool CNN features.|
||7 instances in total. (in cvpr2018)|
|590|Dasgupta_DeLay_Robust_Spatial_CVPR_2016_paper|Our CNN uses the architecture proposed by Chen et al.|
|||Refinement  3.4.1 The Problem  Given the CNN output T, a straightforward way to obtain a labeling is to simply pick the label with the highest score for each pixel:  However, note that there are no guarantees that this layout will be consistent with the model described in Sec.|
|||This is because our CNN does not enforce any smoothness or geometric constraints.|
|||These usually use the CNN output as the unary potentials and define a pairwise potential over color intensities [2].|
|||Efficiency  The CNN used in our implementation can process 8 frames per second on an Nvidia Titan X.|
|||Occasionally, we encounter cases where the CNN predictions differ so drastically from the ground truth that the optimizer fails to provide any improvements.|
|||A promising approach here would be to train a CNN for performing joint segmentation of both layout and object classes.|
||7 instances in total. (in cvpr2016)|
|591|Low-Rank Bilinear Pooling for Fine-Grained Classification|Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model [8], and three orders smaller than the standard bilinear CNN model [19].|
|||It is worth noting that the set of parameters learned in our model is ten times smaller than the recently proposed compact bilinear model [8], and a hundred times smaller than the original full bilinear CNN model [19].|
|||xixT  In the bilinear CNN model [19] as depicted in Figure 1 (b), the bilinear pooled feature is reshaped into a vector z = vec(XXT )  Rc2  and then fed into a linear classifier1.|
|||Instead, we optimize the objective function 9 using stochastic gradient descent to allow end-to-end training of both the classifier and CNN feature extractor via standard backpropagation.|
|||The full bilinear CNN and compact bilinear CNN consistently apply sign square root and l2 normalization on the bilinear features.|
|||Finally, we produce a third visualization by repeatedly remove superpixels from the input image, se Xk2  +  7371  Table 2: Classification accuracy and parameter size of: a fully connected network over VGG16 [28], Fisher vector [5], Full bilinear CNN [19], Random Maclaurin [8], Tensor Sketch [8], and our method.|
|||Bilinear cnn models for fine-grained visual recognition.|
||7 instances in total. (in cvpr2017)|
|592|Poznanski_CNN-N-Gram_for_Handwriting_CVPR_2016_paper|CNN-N-Gram for Handwriting Word Recognition  Arik Poznanski and Lior Wolf  The Blavatnik School of Computer Science  Tel Aviv University  arik.com@gmail.com, wolf@cs.tau.ac.il  Abstract  Given an image of a handwritten word, a CNN is employed to estimate its n-gram frequency profile, which is the set of n-grams contained in the word.|
|||The CNN that is used employs several novelties such as the use of multiple fully connected branches.|
|||That n-gram based CNN achieves inferior results, compared to their other CNNs, on both SVT and SVT-50.|
|||The architecture we propose employs multiple fully connected arms diverging from the CNN body.|
|||A shared set of filters can tackle these problems successfully, and the CNN benefits from solving multiple classification problems at once.|
|||Network architecture  The basic layout of our CNN is a VGG [47] style network consisting of small (33) convolution filters.|
|||The employed CNN has nine convolutional layers and  three fully connected layers.|
||7 instances in total. (in cvpr2016)|
|593|Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper|Efficient CNN designs like ResNets and DenseNet were proposed to improve accuracy vs efficiency trade-offs.|
|||Inspired by these techniques, we propose to model connections between filters of a CNN using graphs which are simultaneously sparse and well connected.|
|||3 Approach  Recent breakthroughs in CNN architectures like ResNet[47] and DenseNet-BC[3] are ideas based on increasing connectivity, which resulted in better performance trade-offs.|
|||Hence the kernel of this convolutional layer has M  c  c parameters as compared to |V |  |U |  c  c, which is the number of parameters in a vanilla CNN layer.|
|||In this section, we benchmark and empirically demonstrate the effectiveness of X-Nets on a variety of CNN architectures.|
|||We choose MobileNet as the base model for this experiment, since it is the state-of-the-art in efficient CNN architectures.|
|||5.2 Comparison with Efficient CNN Architectures  In this section, we test whether Expander Graphs can improve the performance trade-offs even in state-of-the-art architectures such as DenseNet-BCs [3] and ResNets[47] on the ImageNet [50] dataset.|
||7 instances in total. (in eccv2018)|
|594|Roffo_Infinite_Feature_Selection_ICCV_2015_paper|2: CNN on object recognition datasets  This section starts with a set of tests on the object recognition datasets, that is, Caltech 101-256 and PASCAL VOC 2007-2012.|
|||The CNN features have been pre-trained on ILSVRC (we adopt the MatConvNet distribution [33]), using the 4,096dimension activations of the penultimate layer as image features, L2-normalized afterwards.|
|||Table 6 presents interesting results since Inf-FS is the more  Stability analysis PASCAL VOC 2007 Method Relief-F  #Images mAP  DJS  Object recognition by CNN features  Datasets  VOC07 VOC12 Cal.101 Cal.-256  Fisher  Methods  Relief-F  Fisher  MutInf  FSV  Ours  80.4 (81%) 80.7 (81%) 80.6 (88%) 80.8 (86%) 83.5 (88%)  82.7 (96%) 82.9 (87%) 82.8 (92%) 81.6 (89%) 84.0 (89%)  90.8 (81%) 90.9 (81%) 90.9 (81%) 89.7 (81%) 91.8 (81%)  79.8 (81%) 79.9 (81%) 79.9 (81%) 79.6 (81%) 81.5 (81%)  Table 5.|
|||BoWs have been concatenated to CNN features, resulting in a 5,120 feature set.|
|||As for the protocol, we have fixed the number of features to be selected at 85%, representing a valid compromise among the percentages chosen by the different approaches on the sole CNN (see Table 5).|
|||The numbers enclosed in square brackets (Table 7) show the percentages of the kept features, with CNN in the first position and BoW in the second position.|
|||Moreover, the ordering of the features (not shown here) indicates that in almost all the cases, most of the CNN features (95% circa) are ranked ahead of the BoW ones.|
||7 instances in total. (in iccv2015)|
|595|Kuo_DeepBox_Learning_Objectness_ICCV_2015_paper|We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster.|
|||This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP.|
|||Concretely, we train a CNN to rerank a large pool of object proposals produced by a bottom-up proposal method (we use Edge boxes [30] for most experiments in this paper).|
|||We also provide evidence that what our network learns is category-agnostic: our improvements in performance generalize to categories that the CNN has not seen before (16% improvement over Edge boxes on COCO [20]).|
|||Sharing computation for faster reranking  Running the CNN separately on highly overlapping boxes wastes a lot of computation.|
|||Fast R-CNN implements this pooling as a layer in the CNN (the RoI Pooling layer) allowing us to train the network end-to-end.|
|||Discussion and Conclusion  We have presented an efficient CNN architecture that learns a semantic notion of objectness that generalizes to unseen categories.|
||7 instances in total. (in iccv2015)|
|596|Rematas_Deep_Reflectance_Maps_CVPR_2016_paper| A CNN addressing a data-interpolation task of sparse  unstructured data.|
|||In a typical CNN regression architecture, there is a spatial correspondence between input and output, e.g.|
|||The first and fourth step are model by CNN architectures, while the second and third step are prescribed transformations, related to the parametrization of the reflectance map.|
|||We train a CNN to learn the s, t coordinates.|
|||The employed CNN architecture is shown in Fig.|
|||Methods and Metrics  i)  ii)  iii)  iv)  v)  vi)  We include six different methods to reconstruct reflectance maps: i) ground truth, ii) our direct , iii) our indirect approach, iv) an approach that follows our indirect one, but does not use a CNN for sparse interpolation but an RBF reconstruction as described in Eq.|
|||Joint embeddings of shapes and images via cnn image purification.|
||7 instances in total. (in cvpr2016)|
|597|Wang_Attribute_Recognition_by_ICCV_2017_paper|This RNN based model explores explicitly a sequential prediction constraint that differs significantly from the existing CNN based concurrent prediction policy [23, 11, 21].|
|||Deep CNN models have been exploited for joint multi-attribute feature and classifier learning [52, 21, 41, 50, 23, 9], and shown to benefit from learning attribute co-occurrence dependency.|
|||As input to the LSTM encoder, we utilise a deep CNN initialised by ImageNet (e.g.|
|||To avoid noise back propagation from the RNN to CNN, we do not train the CNN image feature representation network together with the JRL RNN encoder-decoder.|
|||They include (I) two conventional discriminative attribute methods: (1) We adopted CNN features (FC7 output of the AlexNet) with the SVM attribute model [19], replacing its original Ensemble of Localized Features (ELF) [12, 19]; (2) MRFr2 [8] is a graph based attribute recognition method that exploits the context of neighbouring images by Markov random field for mining the visual appearance proximity relations between different images to support attribute reasoning; (II) Three deep learning attribute recognition methods: (3) Attributes Convolutional Network (ACN) [41] trains jointly a CNN model for all attributes, and sharing weights and transfer knowledge among different attributes; (4) DeepSAR [21] is a deep model that that treats attribute classes individually by training multiple attribute-specific AlexNet models [17]; (5) DeepMAR [21] , unlike DeepSAR, considers additionally inter-attribute correlation by jointly learning all attributes in a single AlexNet model [17], so to capture the concurrent attribute relationships, similar to [41]; (III) One multi-person image annotation recurrent model: (6) Contextual CNNRNN (CTX CNN-RNN) [24] is a CNN-RNN based sequential prediction model designed to encode the scene context and inter-person social relations for modelling multiple people in an image2; (IV) One generic multi-label image classification model: (7) Semantically Regularised CNN-RNN (SR CNN-RNN) [27] is a state-of-the-art multi-label image classification model that exploits the groundtruth attribute labels for strongly supervised deep learning and richer image embedding.|
|||For that, we built a stripped-down JRL model by removing the encoder and  537  directly using the CNN FC features for the decoder input.|
|||Person attribute recognition with a jointly-trained holistic cnn model.|
||7 instances in total. (in iccv2017)|
|598|cvpr18-Seeing Voices and Hearing Faces  Cross-Modal Biometric Matching|We make the following contributions: (i) we introduce CNN architectures for both binary and multi-way cross-modal face and audio matching; (ii) we compare dynamic testing (where video information is available, but the audio is not from the same video) with static testing (where only a single still image is available); and (iii) we use human testing as a baseline to calibrate the difficulty of the task.|
|||We show that a CNN can indeed be trained to solve this task in both the static and dynamic scenarios, and is even well above chance on 10-way classification of the face given the voice.|
|||The CNN matches human performance on easy examples (e.g.|
|||We make the following contributions: first, we introduce a CNN architecture that ingests face images and voice spectrograms, and is able to infer the correspondence between them.|
|||From left to right: (1) The static 3-stream CNN architecture consisting of two face sub-networks and one voice network, (2) a 5-stream dynamic-fusion architecture with two extra streams as dynamic feature subnetworks, and finally (3) the N-way classification architecture which can deal with any number of face inputs at test time due to the concept of query pooling (see Sec.|
|||This shortcoming is common to many CNN architectures, where it is difficult to change the number of inputs at test time.|
|||Conclusion  In this paper, we have introduced the novel task of crossmodal matching between faces and voices, and proposed a corresponding CNN architecture to address it.|
||7 instances in total. (in cvpr2018)|
|599|cvpr18-Functional Map of the World|These chips were then passed through a CNN architecture to classify and remove any undesirable cloud-covered images.|
|||Two of the methods presented involve fusing metadata into a CNN architecture in an attempt to enable the types of reasoning discussed in the introduction.|
|||The CNN used as the base model in our various baseline methods is DenseNet-161 [14], with 48 feature maps (k=48).|
|||We initialize our base CNN models using the pretrained ImageNet weights, which we found to improve performance during initial tests.|
|||66177   CNN-I A standard CNN approach using only images, where DenseNet is fine-tuned after ImageNet.|
|||The CNN is trained on all images across all temporal sequences of train + val.|
|||The images presented here show the extracted and resized images that are passed to the CNN approaches.|
||7 instances in total. (in cvpr2018)|
|600|Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper|In this paper, we describe a method that is able to learn a CNN which matches previous state of the art in terms of accuracy, while requiring 5 times fewer model evaluations during the architecture search.|
|||This cell structure is then stacked a certain number of times, depending on the size of the training set, and the desired running time of the final CNN (see Section 3 for details).|
|||[41] extended this by using a more structured search space, in which the CNN was defined in terms of a series of stacked  4 The code and checkpoint for the PNAS model trained on ImageNet can be downloaded from the TensorFlow models repository at http://github.com/tensorflow/ models/.|
|||This technique has been applied to neural net structure search in [25], but they used a flat CNN search space, rather than our hierarchical cell-based space.|
|||The overall CNN construction process is identical to [41], except we only use one cell type (we do not distinguish between Normal and Reduction cells, but instead emulate a Reduction cell by using a Normal cell with stride 2), and the cell search space is slightly smaller (since we use fewer operators and combiners).|
|||In [35] a fixed-length binary string encoding of CNN architecture  6 5 symbols per block, times 5 blocks, times 2 for Normal and Reduction cells.|
|||6 Discussion and Future Work  The main contribution of this work is to show how we can accelerate the search for good CNN structures by using progressive search through the space of increasingly complex graphs, combined with a learned prediction function to efficiently identify the most promising models to explore.|
||7 instances in total. (in eccv2018)|
|601|CDC_ Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos|3D CNN [60, 47] has been shown that it can learn spatiotemporal abstraction of high-level semantics directly from raw videos but loses granularity in time, which is important for precise localization, as mentioned above.|
|||; 3D CNN architecture called C3D [60] trained on a large-scale sports video dataset [27] ; improved Dense Trajectory Feature (iDTF) [63, 64] consisting of HOG, HOF, MBH features extracted along dense trajectories with camera motion influences eliminated; key frame selection [13]; ConvNets adapted for using motion flow as input [50, 10, 66]; feature encoding with Fisher Vector (FV) [40, 38] and VLAD [23, 71].|
|||[47] proposed an end-to-end Segment-based 3D CNN framework (S-CNN), which outperformed other RNN-based methods by capturing spatio-temporal information simultaneously.|
|||[78] originally proposed de-convolutional networks for image decomposition, and later Zeiler and Fergus [77] repurposed de-convolutional filter to map CNN activations back to the input to visualize where the activations come from.|
|||methods  Single-frame CNN [51] Two-stream CNN [50]  LSTM [7]  MultiLSTM [72]  C3D + LinearInterp  Conv & De-conv  CDC (fix 3D ConvNets)  CDC  mAP 34.7 36.2 39.3 41.3 37.0 41.7 37.4 44.4  Table 1.|
|||In Table 1, we first compare our CDC network (denoted by CDC) with some state-of-the-art models (results are quoted from [72]): (1) Single-frame CNN: the frame-level 16-layer VGG CNN model [51]; (2) Twostream CNN: the frame-level two-stream CNN model proposed in [50], which has one stream for pixel and one stream for optical flow; (3) LSTM: the basic per-frame labeling LSTM model of 512 hidden units [7] on the top of VGG CNN FC7 layer; (4) MultiLSTM: a LSTM model developed by Yeung et al.|
|||Twostream CNN models appearance and motion information separately.|
||7 instances in total. (in cvpr2017)|
|602|Oquab_Is_Object_Localization_2015_CVPR_paper|How can we modify the structure of the CNN to learn from such difficult data?|
|||We build on the successful CNN architecture [30] and the follow-up state-of-the-art results for object classification and detection [6, 22, 37, 43, 44], but introduce the following modifications.|
|||Interestingly, we find that this modified CNN architecture, while trained to output image-level labels only, localizes objects or their distinctive parts in training images, as illustrated in Figure 1.|
|||In this paper we set out to answer this question and analyze the developed weakly supervised CNN pipeline on two object recognition datasets containing complex cluttered scenes with multiple objects.|
|||However, most of the current CNN architectures assume in training a single prominent object in the image with limited background clutter [16, 30, 35, 44, 60] or require fully annotated object locations in the image [22, 37, 44].|
|||In order to provide quantitative evaluation of the localization power of our CNN architecture, we introduce a simple metric based on precisionrecall using the per-class response maps.|
|||We have implemented a simple extension of our method that aggregates CNN scores within selective search [53] object proposals.|
||7 instances in total. (in cvpr2015)|
|603|Wang_CNN-RNN_A_Unified_CVPR_2016_paper|For the CNN part, to avoid problems like overfitting, previous methods normally assume all classifiers share the same image features [36].|
|||The image embedding vectors are generated by a deep CNN while each label has its own label embedding vector.|
|||It contains two parts: The CNN part extracts semantic representations from images; the RNN part models image/label relationship and label dependency.|
|||Experiments  In our experiments, the CNN module uses the 16 layers VGG network [29] pretrained on ImageNet 2012 classification challenge dataset [5] using Caffe deep learning framework [16].|
|||tion on the shared CNN features on PASCAL VOC for multi-label classification.|
|||proposal information to fine-tune the CNN features pretrained on ImageNet 1000 dataset, and ac hives better performance than the methods that do not exploit region information.|
|||The proposed framework combines the advantages of the joint image/label embedding and label co-occurrence models by employing CNN and RNN to model the label co-occurrence dependency in a joint image/label embedding space.|
||7 instances in total. (in cvpr2016)|
|604|Bansal_Marr_Revisited_2D-3D_CVPR_2016_paper|Our Contributions: Our contributions include: (a) A skipnetwork architecture that achieves state-of-the-art performance on surface normal estimation; (b) A CNN architecture for CAD retrieval combining image and predicted surface normals.|
|||Most similar to our surface normal prediction approach is recent work that trains a CNN to directly predict depth [27], jointly predicts surface normals, depth, and object labels [9], or combines CNN features with the global room layout via a predicted 3D box [46].|
|||veloped, such as learning a mapping from CNN features to a 3D light-field embedding space for view-invariant shape retrieval [25] and retrieval using AlexNet [22] pool5 features [2].|
|||Our 2.5D 3D approach differs in its development of a CNN that jointly models appearance and predicted surface normals for viewpoint prediction and CAD retrieval.|
|||Eigen and Fergus [9] trained a feed-forward coarse-to-fine multi-scale CNN architecture.|
|||In light of the above, we seek to better leverage the rich feature representation learned by a CNN trained on largescale data tasks, such as object classification over ImageNet.|
|||Let cj p(I) correspond to the outputs of pre-trained CNN layer j at pixel location p given input image I.|
||7 instances in total. (in cvpr2016)|
|605|Unified Embedding and Metric Learning for Zero-Exemplar Event Detection|We choose to freeze CNN and Doc2Vec modules to speed up training.|
|||Then, using ResNet [35], we extract pool5 CNN features for the sampled frames.|
|||We experiment different features from different CNN models: ResNet (prob, fc1000), VGG [37] (fc6, fc7), GoogLeNet [47] (pool5, fc1024), and Places365 [48] (fc6, fc7,fc8) except we find ResNet pool5 to be the best.|
|||We only use ResNet pool5 and we dont fuse multiple CNN features.|
|||EvenNet dataset [14] is accompanied by 500-category CNN classifier.|
|||Clearly, CNN features and video exemplars in the training set can improve the model accuracy, but our method improves against VideoStory when trained on the same dataset and using the same features.|
|||Nonetheless, our method improves over them using only object-centric CNN feature representations.|
||7 instances in total. (in cvpr2017)|
|606|Lu__Online_Video_ICCV_2017_paper|These association features are a representation of the detected objects that captures both spatial and temporal information, since it is a CNN feature filtered by LSTM.|
|||Image(cid:173)based Object Detection  The vast development of convolutional neural networks has prompted research into designing different CNN models for object detection.|
|||[10] propose a CNN based framework followed by simple object tracking for detecting object from video.|
|||We can see that our association network is able to recover miss ing detections caused by CNN detector (Person-5 in frame 35 and Person-6 in frame 30), as well as reducing spuriousdetections (one false positive has been deleted in frame 35).|
|||The weakness of our approaches is that our LSTM modules are post-hoc: CNN features are not updated in re 2350  Methods  MDP (K=5) [28] MDP (K=9) [28]  CDT [11] a LST M  26.7 26.7 39.9 38.6  MOTA  MOTP  MT  ML  53.0 51.7 47.4 46.8  73.7 73.6 74.8 74.2  12.0 12.0 20.9 14.9  FP  3,386 3,290 914 788  FN  13,415 13,491 12,856 13,253  IDS  111 133 95 154  Table 4.|
|||Our future work includes adding a feedback loop to the pre-trained CNN so that the weights of the feature extractor can also be updated online guided by examining forward/backward temporal context provided by the RNN modules.|
|||Learning by tracking: Siamese cnn for robust target association.|
||7 instances in total. (in iccv2017)|
|607|Kulkarni_ReconNet_Non-Iterative_Reconstruction_CVPR_2016_paper|b) We introduce a novel class of CNN architectures called ReconNet which takes in CS measurements of an image block as input and outputs the reconstructed image block.|
|||Hence, we cannot use any of the standard CNN architectures that have  been proposed so far.|
|||Motivated by this, we introduce a novel class of CNN architectures for the CS recovery problem at any arbitrary measurement rate.|
|||Hence, the typical CNN architectures which can map images to rich hierarchical visual features are not applicable to our problem of interest.|
|||In [6], initial estimates of the high-resolution images are first obtained from low-resolution input images using bicubic interpolation, and then a 3-layered CNN is trained with the initial estimates as inputs and the ground-truth of the desired outputs as labels.|
|||Due to the aforementioned reasons, we relinquish the idea of obtaining initial estimates of the reconstructions, and instead propose a novel class of CNN architectures called ReconNet which can directly map CS measurements to image blocks.|
|||It is mainly because unlike traditional CS algorithms, our algorithm being CNN based relies on much simpler convolution operations, for which very fast implementations exist.|
||7 instances in total. (in cvpr2016)|
|608|See the Forest for the Trees_ Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification|[23] build a CNN to extract features of each frame and then apply an RNN to exploit the temporal information.|
|||In each stream, we first employ a CNN to extract  43234749  Figure 3: The structure of the temporal attention model.|
|||We denote the CNN as f (x) and the feature map of the fc7 layer as f (x)f c7.|
|||CaffeNet is adopted for CNN and LSTM for RNN.|
|||CNN refers to use a CNN to extract features of each frame and measure the similarity by the Euclidean distance.|
|||[31] take a similar deep neural network architecture, i.e., a CNN followed by an RNN.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||7 instances in total. (in cvpr2017)|
|609|Zhou_Sparseness_Meets_Deepness_CVPR_2016_paper|The second approach uses a CNN architecture for body part detection [10, 17, 45, 31] and then typically enforces the 2D spatial relationship between body parts as a subsequent processing step.|
|||The proposed approach only requires readily available annotated 2D imagery (e.g., the inthe-wild PennAction dataset [53]) to train a CNN part detector and a separate 3D motion capture dataset (e.g., the CMU MoCap database) for the pose dictionary.|
|||For each joint j, the mapping hj is approximated by a CNN learned from training data.|
|||CNN-based joint uncertainty regression  A CNN is used to learn the mapping Y 7 hj(; Y ), where Y denotes an input image and hj(; Y ) represents a heat map for joint j.|
|||The heat map resolution is reduced to 32  32 to decrease the CNN model size which allows a large batch size in training and prevents overfitting.|
|||The CNN architecture used is similar to the SpatialNet model proposed elsewhere [31] but without any spatial fu 44969  sion or temporal pooling.|
|||For the proposed approach, the CNN was trained using the annotated training images from the PennAction dataset, while the pose dictionary was learned with publicly available Mo Cap data1.|
||7 instances in total. (in cvpr2016)|
|610|Yang_Learning-Based_Cloth_Material_ICCV_2017_paper|The five-layer CNN structure.|
|||We applied a five-layer CNN (shown in Fig.|
|||To demonstrate the effectiveness of our CNN design, we visualize the activation of the fifth convolution layer (the conv5 layer).|
|||Learned CNN conv5-layer activation visualization.|
|||The remeshing scheme affects the wrinkle formulation of the simulated cloth, while the texture affects the visual feature that the CNN can extract.|
|||For the second baseline, we fix our CNN part of the network but train the LSTM part.|
|||The second test aims to validate the effectiveness of our CNN sub-network.|
||7 instances in total. (in iccv2017)|
|611|cvpr18-Guided Proofreading of Automatic Segmentations for Connectomics|While we train models based on traditional CNN architectures, we propose an error correction approach with human interaction that works with many classifiers.|
|||[28] use a contracting/expanding CNN path architecture to enable precise boundary localization with small amounts of training data.|
|||p=.5  Dense  Softmax  Figure 2: Our guided proofreading classifiers use a traditional CNN architecture of four convolutional layers, each followed by max pooling and dropout regularization.|
|||Our final CNN configuration for split error detection has four convolutional layers, each followed by max pooling with dropout regularization to prevent overfitting due to limited training data (Fig.|
|||However, we can reuse the same trained CNN for this task.|
|||Our CNN configuration results in 171,474 learnable parameters.|
|||We test the CNN by generating a balanced set of 8,780 correct and 8,780 error patches using unseen data of the left cylinder dataset.|
||7 instances in total. (in cvpr2018)|
|612|cvpr18-ClusterNet  Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information|In this work, we experimentally verify the failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a heatmap-based fully convolutional neural network (CNN), and propose a novel two-stage spatio-temporal CNN which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in WAMI object detection.|
|||These methods include connecting detections across frames using tracking methods or optical flow, using a sliding window  4004  or out-of-the-box detector to perform detection then classify this result using some temporal information, as well as combining the outputs of a single frame CNN and optical flow input to a CNN.|
|||is the only one of these methods which does not rely on single-frame detections, instead opting for a sliding window to first generate its object proposals before using a 3D CNN for classification.|
|||Our work proposes to generate all object proposals simultaneously using a multiframe, two-stage CNN for videos in WAMI in a more computationally efficient manner than background subtraction or sliding-windows, effectively combining both spatial and temporal information in a deep-learning-based algorithm.|
|||The advantage of using a 2D CNN over a 3D CNN as in [3, 6] is the preservation of the temporal relationship between frames.|
|||Instead of a sliding temporal convolution, our method uses the following equation,  N  kh  kw  i=1  h  X  n=1  X  j=1  X  f m x,y =  Vn(i, j)  Kn(kh  i, kw  j)i + bm (1) where Vn is the nth video frame temporally in the stack and Kn is the convolutional kernel for frame n of size (kh, kw), to produce our feature map values f m  RM , where M is the set of feature maps, n  N is a frame in the set of temporal frames input to the network, and bm is a learned bias for the feature map m. This formulation differs from both the standard 2D single-frame CNN and 3D CNNs by allowing us to choose which frame n we want to maximize via the backpropagation of the Euclidean or cross-entropy loss between the output scoremap and the ground truth heatmap for that desired frame.|
|||FoveaNet: Predicting Object Locations  The FoveaNet stage of our two-stage CNN works on the principle of the effective receptive field of neurons in ClusterNet.|
||7 instances in total. (in cvpr2018)|
|613|Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper|As illustrated in Figure 1 (a) and (b), a Convolutional Neural Network (CNN) or Region-based CNN (R-CNN) is usually exploited to encode an image and a decoder of Recurrent Neural Network (RNN) w/ or w/o attention mechanism is utilized to generate the sentence, one word at each time step.|
|||Regardless of these different versions of CNN plus  2  T. Yao, Y. Pan, Y. Li, and T. Mei  CNN  LSTM  Sentence  (a)  Attention/Mean Pooling  LSTM  Sentence  (b)  Trees  Dog  Man  Sungl asses  Fr isbees  Fr isbee  Cone  Grass  Shorts  R-CNN  GCN  Attention/Mean Pooling  LSTM  Sentence  (c)  .|
|||Visual representations generated by image encoder in (a) CNN plus LSTM, (b) R-CNN plus LSTM, and (c) our GCN-LSTM for image captioning.|
|||With the prevalence of deep learning [17] in computer vision, the dominant paradigm in modern image captioning is sequence learning methods [7,34,37,38,39,40] which utilize CNN plus RNN model to generate novel sentences with flexible syntactical structures.|
|||Recently, in [35,39], semantic attributes are shown to clearly boost image captioning when injected into CNN plus RNN model and such attributes can be further leveraged as semantic attention [40] to enhance image captioning.|
|||We compared the following state-of-the-art methods: (1) LSTM [34] is the standard CNN plus RNN model which only injects  1 https://github.com/tylin/coco-caption  Exploring Visual Relationship for Image Captioning  11  Table 1.|
|||(4) LSTMA [39] integrates semantic attributes into CNN plus RNN captioning model for boosting image captioning.|
||7 instances in total. (in eccv2018)|
|614|cvpr18-Knowledge Aided Consistency for Weakly Supervised Phrase Grounding|To leverage external knowledge from pre-trained CNN feature extractor, a Knowledge Based Pooling (KBP) gate is proposed to select query-related proposals.|
|||To leverage rich image features and available fixed category classifiers, we apply KBP to encode knowledge provided by CNN and weight each proposals importance in visual and language consistency.|
|||Knowledge Based Pooling (KBP)  We apply a pre-trained CNN to extract visual feature vi for a proposal ri, and predict a probability distribution pi for its own task, which provides useful cues to filter out unrelated proposals.|
|||For each proposals distribution pi, we select the most probable class with the highest  34044  C N N  category  tree  people  car  table ...  pi  0.02  0.84  0.07  0.01 ...  Pre-trained CNN prediction: people  Query: A smiling woman  # k"  = similarity (woman, people) = 0.72   Figure 3.|
|||A pre-trained CNN always predicts a probability distribution for its own task.|
|||We leverage the most probable category predicted by CNN and calculate the word similarity between noun words in the query as knowledge kq i  probability.|
|||A pre-trained CNN is employed to extract visual feature vi  Rdv for each proposal ri, and global visual feature v  Rdv for input image x.|
||7 instances in total. (in cvpr2018)|
|615|Li_Depth_and_Surface_2015_CVPR_paper|Multi-scale deep  features are extracted by a deep CNN network, and a regressor is trained.|
|||trained a large deep CNN on the ImageNet data set and achieved a performance leap.|
|||Recently, more and more work shows that pre-trained CNN features can be transferred to new classification or recognition problems and boost remarkable performance [20, 21].|
|||Our work here is the first one showing that pre-trained deep CNN feature can be transferred to depth and surface normal estimation.|
|||A deep CNN is then learned to encode the relationship between input patches and the corresponding depth.|
|||We then refine the predictions from the CNN by inference of a hierarchical CRF (not shown here; see text for details).|
|||An overall CNN architecture is presented in Fig.|
||7 instances in total. (in cvpr2015)|
|616|Xinjing_Cheng_Depth_Estimation_via_ECCV_2018_paper|[14] propose to directly learn the image-dependent affinity through a deep CNN with spatial propagation networks (SPN), yielding better results   equal contribution  2  X. Cheng, P. Wang and R. Yang  Fig.|
|||Single view depth estimation via CNN and CRF.|
|||Learning affinity matrix with deep CNN for diffusion or spatial propagation receives high interests in recent years due to its theoretical supports and guarantees [39].|
|||[40] trained a deep CNN to directly predict the entities of an affinity matrix, which demonstrated good performance on image segmentation.|
|||[13] propose to treat sparse depth map as additional input to a ResNet [4] based depth predictor, producing superior results than the depth output from CNN with solely image input.|
|||3 Our Approach  We formulate the problem as an anisotropic diffusion process and the diffusion tensor is learned through a deep CNN directly from the given image, which guides the refinement of the output.|
|||It shows the accuracy of sparse depth points cannot preserved, and some pixels could have very large displacement (0.2m), indicating that directly training a CNN for depth prediction does not preserve the value of real sparse depths provided.|
||7 instances in total. (in eccv2018)|
|617|cvpr18-Deep Diffeomorphic Transformer Networks|Current CNN architectures, however, typically employ small pooling regions (e.g., 2  2 or 3  3 pixels), thereby limiting their invariance to large transformations of the input data.|
|||From the users standpoint, using an ST-layer is transparent as the latter works architecturally similarly to a conv-layer and can be inserted at any point of a CNN or a more complicated network (e.g., recurrent neural networks [50]).|
|||We trained the 7 networks to classify MNIST digits that had been distorted in two  4407  Model  MNIST Distortion RTS 0.945 FCN 0.974 CNN 0.986 ST-Affine FCN 0.992 ST-Affine CNN 0.980 ST-CPAB FCN ST-CPAB CNN 0.982 ST-Affine+CPAB CNN 0.996  CPAB 0.918 0.952 0.964 0.974 0.980 0.989 0.993  Training time [s]  280 360 400 420  11587 12067 13058  Table 2: Classification accuracy on the two distorted MNIST datasets.|
|||The bestperforming model for both datasets is the CNN that has both an affine and a CPAB layer.|
|||The secondbest model in the RTS case is the ST-Affine CNN and for the CPAB dataset it is the ST-CPAB CNN; this is unsurprising as we have tailored the datasets to fit the different transformations.|
|||The transformations learned by the ST-Affine CNN and ST-CPAB CNN have a similar effect on the images, since they either zoom in on the digits or expand them.|
|||4408  ModelRTS MNIST datasetCPAB MNIST datasetOriginalST-AffineST-CPABST-Affine+CPABModel  Dataset  No transformer ST-Affine ST-AffineDiff ST-Homography ST-TPS ST-CPAB ST-Affine+CPAB  No transformer ST-Affine ST-AffineDiff ST-Homography ST-TPS ST-CPAB ST-Affine+CPAB  No transformer ST-Affine ST-AffineDiff ST-Homography ST-TPS ST-CPAB ST-Affine+CPAB  MNIST 0.991 0.993 0.994 0.993 0.996 0.994 0.996  Fashion MNIST  0.922 0.919 0.920 0.919 0.918 0.917 0.913  CIFAR10  CIFAR100  0.870 0.891 0.892 0.891 0.893 0.895 0.889  LFW 0.788 0.840 0.842 0.843 0.851 0.878 0.893  0.642 0.653 0.654 0.653 0.656 0.659 0.652  CelebA 0.712 0.734 0.740 0.742 0.751 0.756 0.772  Table 3: Classification performance of the CNN models trained, with or without transformations layers, on the datasets from Table 1.|
||7 instances in total. (in cvpr2018)|
|618|Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper|It was extended in [17] by feeding in the per-pixel predicted labels using a CNN classifier to the next stage of the same CNN classifier.|
|||It starts with the super-segmentation of the image followed by the extraction of visual features for each super-pixel; [19] used the Multi-scale CNN [2] to extract per pixel features that are then averaged over super-pixels.|
|||The CNN architecture has three convolutional stages with 8  8  16 conv  2  2 maxpool  7764 conv  22 maxpool  77256 conv configuration, each max-pooling is non-overlapping.|
|||Stanford Background  We report our results with CNN features extracted from the original scale only, because multi-scale CNN features overfit, perhaps due to small training data, as observed in [19].|
|||SIFT Flow  We report our results using multi-scale CNN features at three scales (1,1/2 and 1/4), as in [19].|
|||Daimler Urban  We report our results using multi-scale CNN features with balanced training in Table 3.|
|||Here, CNN refers to direct perpixel classification resulting from the multi-scale CNN.|
||7 instances in total. (in cvpr2015)|
|619|Light Field Reconstruction Using Deep Convolutional Network on EPI|We then apply a CNN to restore the angular detail of the EPI damaged by the undersampling.|
|||[30] proposed several CNN architectures, one of which was developed for the EPI slices; however, the network is designed for material recognition, which is different with the EPI restoration task.|
|||In our paper, we model the operation f with a CNN to learn a mapping between the blurred low angular resolution EPI and the blurred high angular resolution EPI.|
|||Then, we apply a CNN to restore the detail of the EPI in the angular domain (see Fig.|
|||It should be noted that the CNN is trained to restore the angular detail that is damaged by the undersampling of the light field rather than the spatial detail suppressed by the EPI blur.|
|||Detail restoration based on CNN  For CNN based image restoration, Dong et al.|
|||4.3.1 CNN architecture  where c1, c2 and c3 are scale parameters, and  is a shape parameter.|
||7 instances in total. (in cvpr2017)|
|620|Mang_YE_Robust_Anchor_Embedding_ECCV_2018_paper|Therefore, the easily collected anchor sequences provide abundant training samples to initialize the CNN model to obtain discriminative feature representations, which ensures the later label estimation performance from unlabeled sequences.|
|||2: (1) Anchor Initialization, several anchor sequences are randomly selected for CNN initialization to get discriminative feature representations for better label estimation (Section 3.2).|
|||Meanwhile, the selected anchor sequences are consequently used for later label estimation of unlabelled image  Anchor 1Anchor 2Anchor 3Anchor 4Anchor 5Several anchor sequences for CNN initializationUnlabeled sequences CNNafdkNNAnchorsa0.200.380.170.25Anchor EmbeddingadbfTop-k: Embedding WeightbafdTop-k: Similarity ScoreAnchor 6Label PredictionLabeled SequencesaLearnt RepresentationbbdfaeUpdateInitializeCombineaefdcb2 Label EstimationAnchorsUnlabeled31??|
|||With the initialized CNN representation, we could extract the feature representations of the unlabelled sequences X = {Xi | i = 1, 2,    , n} and anchor sequences A for label estimation.|
|||RACE transforms the high-dimensional CNN representation into low-dimensional embedding weight vector to reduce the computational cost.|
|||We use ResNet-50 [10] pre-trained on ImageNet as our basic CNN model.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||7 instances in total. (in eccv2018)|
|621|cvpr18-Teaching Categories to Human Learners With Visual Explanations|(11)  Here, f k j (x) denotes the feature value at pixel location j and output channel k for the CNN f .|
|||In practice, weight regularization applied during the training of the CNN ensures that we do not have explanations that are uniformly high, and the classification training objective ensures that there are some non-zero entries in the feature maps to correctly predict the class label.|
|||We use a ResNet18 [14] trained from scratch on each dataset as our backbone CNN and extract features from the output of the second residual block, resulting in a downsampling factor of 8.|
|||[36], we propose to use the features from our finetuned CNN from the previous section.|
|||Our EXPLAIN model results in superior learners in two of the three datasets with CNN generated hypothesis spaces.|
|||The Chinese Characters dataset represents an interesting failure case for EXPLAIN when using the CNN generated hypothesis space, see Fig.|
|||Average accuracy during teaching for the Chinese Characters dataset using the CNN generated hypothesis space, where random guessing is 33%.|
||7 instances in total. (in cvpr2018)|
|622|Dong_Li_Recurrent_Tubelet_Proposal_ECCV_2018_paper|The RTR capitalizes on a multi-channel architecture, where in each channel, a tubelet proposal is fed into a CNN plus LSTM to recurrently recognize action in the tubelet.|
|||Inspired by the recent advances of CNN based image object detection methods [4, 6, 26], previous action detection approaches first detect either framelevel [22, 39] or clip-level proposals [11] independently.|
|||Many progresses have been achieved by leveraging the recent advances of CNN, including discriminative feature learning (e.g., Fusion-CNN [15], C3D [35], FVVAE [23], P3D [24]) and effective architecture design (e.g., Two-Stream CNN [29], SR-CNN [37]).|
|||As shown in Figure 3(a), two consecutive frames, It and It+1, are first fed into a shared CNN to extract features.|
|||For RTP networks, we adopt the pre-trained VGG-16 [30] as our basic CNN model, and build RTP at the top of its last  Recurrent Tubelet Proposal and Recognition Networks for Action Detection  9  convolutional layer.|
|||T-CNN [11] improves them by utilizing 3D CNN to model short-term temporal information.|
|||Since AVA is a very recent dataset, there are very few studies on it and we only compare with [8], which implements Multi-Region Two-Stream CNN [22] with ResNet-101.|
||7 instances in total. (in eccv2018)|
|623|Attend to You_ Personalized Image Captioning With Context Sequence Memory Networks|Third, we propose to exploit a CNN to jointly represent nearby ordered memory slots for better context understanding.|
|||The unique updates of CSMN include (i) exploiting memory as a repository for multiple context information, (ii) appending previously generated words into memory to capture long-term information without, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots.|
|||We then apply a CNN to the attended output of memory Mot.|
|||As will be shown in our experiments, using a CNN significantly boosts the captioning performance.|
|||It is mainly due to that the CNN allows us to obtain a set of powerful representations by fusing multiple heterogeneous cells with different filters.|
|||The first baseline is (ShowTell) of [28], which is a multi-modal CNN and LSTM model.|
|||To validate the contribution of each component, we exclude one of key components from our model as follows: (i) without the memory CNN in section 4.2 denoted by (-NoCNN-), (ii) without user context memory denoted by (-NoUC-), and (iii) without feedback of previously generated words to output memory by (-NoWO-).|
||7 instances in total. (in cvpr2017)|
|624|Yao_Feng_Joint_3D_Face_ECCV_2018_paper|Recent works also explore the usage of CNN to predict 3DMM parameters directly.|
|||[15, 57, 31, 36] propose end-to-end CNN architectures to directly estimate the 3DMM shape parameters.|
|||Recently, [28] propose to straightforwardly map the image pixels to full 3D facial structure via volumetric CNN regression.|
|||[40] use multi-constraints to train a CNN which estimates the 3DMM parameters and then provides a very dense 3D alignment.|
|||Then we elaborate the CNN architecture and the loss function designed specially for learning the mapping from unconstrained RGB image to its 3D structure.|
|||Thus our position map records a dense set of points from 3D face with its semantic meaning, we are able to simultaneously obtain the 3D facial structure and dense alignment result by using a CNN to regress the position map directly from unconstrained 2D images.|
|||Jackson, A.S., Bulat, A., Argyriou, V., Tzimiropoulos, G.: Large pose 3d face reconstruction from a single image via direct volumetric cnn regression.|
||7 instances in total. (in eccv2018)|
|625|Oriented Response Networks|We set up a baseline CNN with four convolutional layers and multiple 3x3 filters, Fig.|
|||TIPooling network is constructed by duplicating the baseline CNN eight times to capture different augmented rotated versions of inputs, and a transform-invariant pooling layer before the output layer.|
|||ORNs are built by upgrading each convolution layer in the baseline CNN to Oriented Response Convolution layer using Active Rotating Filters (ARFs) with 4 or 8 orientation channels.|
|||Compared with the data augmentation strategy (baseline CNN on rot+), ORN (on rot) not only reduces network parameters and training cost but also achieves significant lower error rate (1.37% vs 2.19%).|
|||This validates our previous viewpoint: the conventional CNN used in STN lacks the capability to precisely estimate rotation parameters.|
|||7 that both CNN and STN have large within-class differences as the same digit with different angles produce various radii.|
|||Moreover, features generated by CNN and STN have apparently 180o symmetrical distribution, which means that they can barely tell the difference between upside-down 6 and 9.|
||7 instances in total. (in cvpr2017)|
|626|Busto_Open_Set_Domain_ICCV_2017_paper|Since CNN features show some robustness to domain changes [11], several domain adaptation approaches based on CNNs have been proposed [39, 31, 45, 48].|
|||Office dataset  We evaluate and compare our approach on the Office dataset [34], which is the standard benchmark for domain adaptation with CNN features.|
|||Unsupervised domain adaptation We firstly compare the accuracy of our method in the unsupervised set-up with state-of-the-art domain adaptation techniques embedded in the training of CNN models.|
|||BP [13] trains a CNN for domain adaptation by a gradient reversal layer and minimises the domain loss jointly with the classification loss.|
|||Our approach ATI outperforms the baseline and the CNN approach [46].|
|||As in the unsupervised case, the improvement compared to the CNN approach is larger for the open set (+4.8%) than for the closed set (+2.2%).|
|||We proposed new open set protocols for existing datasets and evaluated both CNN methods as well as standard unsupervised domain adaptation approaches.|
||7 instances in total. (in iccv2017)|
|627|Image Super-Resolution via Deep Recursive Residual Network|Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters.|
|||This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks.|
|||PSNR of recent CNN models for scale factor 3 on Set5 [1].|
|||As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) [2] predicts the nonlinear LR-HR mapping via a fully convolutional network, and significantly outperforms classical non-DL methods.|
|||[25] observe that the prior models [2, 32] increase LR images resolution via bicubic interpolation be 13147  fore CNN learning, which increases the computational cost.|
|||One commonality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN [2] uses 3 convolutional layers.|
|||1 shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR [2, 13, 14, 17, 25, 32] versus the number of parameters, denoted as k. Compared to the prior CNN models, DRRN achieves the best performance with fewer parameters.|
||7 instances in total. (in cvpr2017)|
|628|Yantao_Shen_Person_Re-identification_with_ECCV_2018_paper|[1] introduced a model called Cross-Input Neighbourhood Difference CNN model, which compares image features in each patch of one input image to the other images patch.|
|||Then the original global image and the transformed part images are fed into a CNN simultaneously for prediction.|
|||In recent years, extending CNN to graph data structure received increased attention [6, 18, 23, 45, 66, 13, 33], Bruna et al.|
|||We will first pretrain the base Siamese CNN model, we adopt an initial learning rate of 0.01 on all three datasets and reduce the learning rate by 10 times after 50 epochs.|
|||We treat the siamese CNN model that directly estimates pairwise similarities from initial node features introduced in Section 3.1 as the base model.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Crf learning with cnn features for image segmentation.|
||7 instances in total. (in eccv2018)|
|629|cvpr18-Non-Linear Temporal Subspace Representations for Activity Recognition|In the popular two-stream CNN architecture for action recognition [43, 14], the final classifier scores are fused using a linear SVM.|
|||Along these lines, the popular two-stream CNN model [43] for action recognition has been extended using more powerful CNN architectures incorporating intermediate feature fusion in [14, 12, 46, 53], however typically the features are pooled independently of their temporal order during the final sequence classification.|
|||Towards this end, we use (i) the JHMDB and MPII Cooking activities datasets, where frames are encoded using a VGG two-stream CNN model, (ii) HMDB dataset, for which we use features from a ResNet-152 CNN model, (iii) UTKinect actions dataset, using non-linear features from 3D skeleton sequences corresponding to human actions, and (iv) hand-crafted bag-of-words features from the MPII dataset.|
|||Dimensionality Reduction: To explore the possibility of dimensionality reduction (using PCA) of the CNN features and understand how well the kernelization results be (does the lower-dimensional features still capture the temporal cues?|
|||Comparisons to rank pooling using a homogeneous kernel linearization of CNN features via a Chi-Squared kernel as in [17].|
|||Effect of Moving Average (MA) and signed-square root (SSR) of CNN features before KRP-FS on HMDB split-1.|
|||P-cnn: Posebased cnn features for action recognition.|
||7 instances in total. (in cvpr2018)|
|630|cvpr18-Frustum PointNets for 3D Object Detection From RGB-D Data|[7] extends [31] by replacing SVM with 3D CNN on voxelized 3D grids.|
|||[29, 14] convert a point cloud of the entire scene into a volumetric grid and use 3D volumetric CNN for object proposal and classification.|
|||[16, 25, 32, 7] design more efficient 3D CNN or neural network architectures that exploit sparsity in point cloud.|
|||However, these CNN based methods still require quantitization of point clouds with certain voxel resolution.|
|||We first leverage a 2D CNN object detector to propose 2D regions and classify their content.|
|||However, both CNN baselines get far worse results compared to our main method.|
|||To understand why CNN baselines underperform, we vi sualize a typical 2D mask prediction in Fig.|
||7 instances in total. (in cvpr2018)|
|631|Gan_DevNet_A_Deep_2015_CVPR_paper|Taking key frames of videos as input, we first detect the event of interest at the video level by aggregating the CNN features of the key frames.|
|||However, whether and how the CNN architecture could be exploited for the video event detection and recounting problems has never been studied before.|
|||DevNet Framework  In the proposed DevNet, the CNN architecture is similar to the network described in [17] except that it is much deeper.|
|||The CNN contains nine convolutional layers and three fully-connected layers.|
|||Thus, we first use the large ImageNet dataset [7] to pre-train the CNN for parameter initialization.|
|||DevNet Pre-training  Our experiments start with a deep CNN trained on the ILSVRC-2014 dataset [16] which includes 1.2M training images categorized into 1000 classes.|
|||Based on the proposed DevNet, the CNN pretrained on large-scale image datasets, e.g, ImageNet, can be successfully transferred to the video domain.|
||7 instances in total. (in cvpr2015)|
|632|Chen_Liu_FloorNet_A_Unified_ECCV_2018_paper|The proposed neural architecture, dubbed FloorNet, effectively processes the data through three neural network branches: 1) PointNet with 3D points, exploiting 3D information; 2) CNN with a 2D point density image in a top-down view, enhancing local spatial reasoning; and 3) CNN with RGB images, utilizing full image information.|
|||The second branch uses a CNN with a 2D point density image in a top-down floorplan view, enhancing the local spatial reasoning.|
|||The third branch uses a CNN with RGB images, utilizing the full image information.|
|||3D deep learning: The success of CNN on 2D images has inspired research on 3D feature learning via DNNs.|
|||Volumetric CNNs [41, 20, 27] are straightforward extensions of CNN to a 3D domain, but there are two main challenges: 1) data sparsity and 2) computational cost of 3D convolutions.|
|||The third branch produces deep image features by a dilated residual network trained on the semantic segmentation task [44] as well as a stacked hourglass CNN trained on the room layout estimation [24].|
|||The top is OctNet [29], a state-of-the-art 3D CNN architecture.|
||7 instances in total. (in eccv2018)|
|633|Sharma_Scalable_Nonlinear_Embeddings_ICCV_2015_paper|(iii) We also demonstrate scalability by training with order of millions of training pairs of 4096 dimensional state-of-the-art CNN features [19, 46] and compare with five existing competitive baselines.|
|||Thus, we use CNN features using the MatConvNet [46] library, with the 16 layer model [42] which is pre-trained on the ImageNet dataset [9].|
|||To validate the baseline implementation we used the extracted 4096 dimensional CNN features with linear SVM to perform the classification task for the different datasets.|
|||As a reference, we take the full (2 normalized) 4096 dimensional CNN features (denoted No proj in the figures) with Euclidean distance.|
|||The CNN features for the test images are transformed by the respective methods and are compared with Euclidean distance.|
|||It is also interesting to note the performance of the method after compression compared to using the full 4096 dimensional CNN feature without any compression.|
|||Note the variations in the appearance of the retrieval results, which can be attributed to the combination of the strong CNN based appearance features and discriminatively learned embeddings.|
||7 instances in total. (in iccv2015)|
|634|Liu_Deep_Supervised_Hashing_CVPR_2016_paper|Specifically, we devise a CNN architecture that takes pairs of images (similar/dissimilar) as training inputs and encourages the output of each image to approximate discrete values (e.g.|
|||Inspired by the robustness of CNN features, we propose a binary code learning framework by exploiting the CNN structure, named Deep Supervised Hashing (DSH).|
|||In our method, first we devise a CNN model which takes image pairs along with labels indicating whether the two images are similar as training inputs, and produces binary codes as outputs, as shown in Figure 1.|
|||[31, 14, 30] enforce the network to learn binary-like outputs that preserve the semantic relations of image-triplets; [29] trains a CNN to fit the binary codes computed from the pairwise similarity matrix; [15] trains the model with a binary-like hidden layer as features for image classification tasks.|
|||To this end, we propose to use the CNN illustrated in Figure 1 to learn discriminative image representations and compact binary codes simultaneously, which can break out the limitations of both handcrafted features and linear models.|
|||Our method first trains the CNN using image pairs and the corresponding similarity labels.|
|||Then the CNN outputs are quantized to generate binary codes for new-coming images.|
||7 instances in total. (in cvpr2016)|
|635|Ouyang_DeepID-Net_Deformable_Deep_2015_CVPR_paper|Since training deep models is a nonconvex optimization problem with millions of parameters, the choice of a good initial point is a crucial but unsolved problem, especially when deep CNN goes deeper [44, 38, 22].|
|||Our new CNN layer is motivated by three observations.|
|||A new scheme for pretraining the deep CNN model.|
|||In  existing deep CNN models, max pooling and average pooling are useful in handling deformation but cannot learn the deformation penalty and geometric models of object parts.|
|||Similarly, we can consider the input of a convolutional layer in CNN as features and consider the filters of that convolutional layer as part filters.|
|||The composition of object parts is naturally implemented by CNN with hierarchical layers.|
|||Table 2 shows the experimental results on VOC-2007 testing data, which include approaches using hand-crafted features [15, 33, 47, 46, 11], deep CNN features [14, 19], and CNN features with deformation learning [16].|
||7 instances in total. (in cvpr2015)|
|636|cvpr18-End-to-End Deep Kronecker-Product Matching for Person Re-Identification|Most state-of-the-art CNN based approaches  The first two authors contribute equally to this work.|
|||State-ofthe-art CNN networks either utilize global average pooling (e.g., ResNet [10]) or direct vectorization (e.g., VGG network [29]) to convert the topmost feature maps into feature vectors.|
|||[6] trained CNN with triplet samples and minimized feature distances between the same person while maximized the distances among different people.|
|||[20] first introduced the multi-scale features into CNN training for person Re-ID, which directly downsamples image into different scales and fed them into shallow sub-networks.|
|||Back to the CNN representations where feature maps have height H and width W , the feature vector at each location is indexed by row and column, e.g., x(i, j) and y(p, q).|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Multi-scale triplet cnn for person re-identification.|
||7 instances in total. (in cvpr2018)|
|637|Gadot_PatchBatch_A_Batch_CVPR_2016_paper|We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images.|
|||This poses a design restriction on our method that is not shared with recent CNN approaches, which are optimized for accuracy at the cost of significant computation time.|
|||When using PM, instead of utilizing image-based patches as inputs, we use our own features, which were computed using a CNN architecture as mentioned before.|
|||In the FlowNet pipeline [15], a CNN was presented that conducts almost the entire optical-flow computation inside the neural network.|
|||In a recent work on stereo matching [42], a CNN architecture compares two candidate stereo patches, followed by extensive post-processing.|
|||spected several CNN architectures which are able to produce a patch-similarity score.|
|||On one hand the CNN can be modified, pruned, or compressed [33] in order to control  Descriptor computation ANN (PatchMatch) Connected component analysis Interpolation (EpicFlow) Total  27s 6.5s 0.5s 16s 50s  2.5s 6.5s 0.5s 16s 25.5s  Table 8.|
||7 instances in total. (in cvpr2016)|
|638|YuKang_Gan_Monocular_Depth_Estimation_ECCV_2018_paper|They adopt a depth CNN and a pose CNN to estimate monocular depth and camera motion simultaneously.|
|||The proposed network adopts an encoder-decoder architecture, where the input image is irst transformed and encoded as absolute feature maps by a deep CNN feature extractor.|
|||3.1 Ainity Layer While the relationships between neighboring pixels, namely ainities, are very important cues for inferring depth, they cannot be explicitly represented in a vanilla CNN model.|
|||The proposed ainity layer is integrated in the CNN model and works complementarily with the absolute features, which signiicantly helps depth estimation.|
|||5 Conclusions  In this work, we propose a novel ainity layer to model the relationship between neighboring pixels, and integrate this layer into CNN to combine absolute and relative features for depth estimation.|
|||Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for ine-grained visual recognition.|
||7 instances in total. (in eccv2018)|
|639|Tsung-Yu_Lin_Second-order_Democratic_Aggregation_ECCV_2018_paper|In recent CNN approaches [31,33,28] which perform end-to-end learning, the complexity becomes a prohibitive factor for typical d  1024 due to a costly backpropagation step which involves SVD or solving a Lyapunov equation [33] in every iteration of the CNN fine-tuning process; thus adding several hours of computations to training.|
|||More recent approaches form co-occurrence patterns from CNN feature vectors similar in spirit to Region Covariance Descriptors.|
|||In [34], the authors combine two CNN streams of feature vectors via outer product and demonstrate that such a setup is robust for the task of the fine-grained image recognition.|
|||A recent approach [49] extracts feature vectors at two separate locations in feature maps and performs an outer product to form a CNN co-occurrence layer.|
|||Recent work on Deep filter banks [9], denoted as FV+FC+CNN, which combines fullyconnected CNN features and Fisher Vector approach, scores 82.1% accuracy.|
|||Arandjelovi c, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: NetVLAD: CNN ar chitecture for weakly supervised place recognition.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear CNN Models for Fine-grained  Visual Recognition.|
||7 instances in total. (in eccv2018)|
|640|cvpr18-End-to-End Flow Correlation Tracking With Spatial-Temporal Attention|Inspired by the success of convolution neural networks (CNN) in object recognition, the visual tracking community has been focused on the deep trackers that exploit the strength of CNN in recent years.|
|||Inspired by the success of CNN in object classification [21, 12], detection [33] and segmentation [23] tasks, researchers in tracking community have started to focus on the deep trackers that exploit the strength of CNN.|
|||Since DCF provides an excellent framework for recent tracking research, the popular trend is the combination of DCF framework and CNN features.|
|||DeepSRDCF [5] exploits shallow CNN features in a spatially regularized DCF framework.|
|||In above mentioned methods, the chosen CNN features are always pre-trained in different tasks and individual components in tracking systems are learned separately.|
|||Similarly, UCT [48] treats feature extractor and tracking process both as convolution operation and trains them jointly, enabling learned CNN features tightly coupled to tracking process.|
|||The response of the filters on sample x is given by  R(x) =  d  Xl=1  l(x)  fl  (1)  where l(x) and fl is l-th channel of extracted CNN features and desired filters, respectively,  denotes circular correlation operation.|
||7 instances in total. (in cvpr2018)|
|641|Tulsiani_Pose_Induction_for_ICCV_2015_paper|Each output unit corresponds to a particular object class, euler angle and angular bin this CNN system shares most parameters across classes but has some classspecific parameters and disjoint output units.|
|||Let f (x; Wc) denote the pose prediction function for image x and classspecific CNN weights Wc, then f (xi, Wci ) computes the probability distribution over angular bins for instance i the CNN is trained to minimize the softmax loss corresponding to the true pose label (i, i, i) and f (xi, Wci ).|
|||This implicitly enforces the CNN based pose estimation system to exploit similarities across object classes and learn common representations that may be useful to predict pose across object classes.|
|||We train a VGG net [31] based CNN architecture with Na  N output units in the last layer the units corresponds to a particular euler angle and angular bin are shared across all classes.|
|||Let f (x; W ) denote the pose prediction function for image x and CNN weights W , then CNN is trained to minimize the softmax loss corresponding to the true pose label (i, i, i) and f (xi, W ).|
|||We then finetuned the CNN systems, after initializing weights using a pretrained model for Imagenet [9] classification, corresponding to the two approaches described above using pose annotations for the remaining 16 classes obtained via PASCAL3D+ or PASCAL VOC keypoint labels.|
|||Our feature representation for an instance is motivated by the observation that each channel in a higher-layer of a CNN can be reasoned as encoding a spatial likelihood of abstract parts.|
||7 instances in total. (in iccv2015)|
|642|cvpr18-CSRNet  Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes|The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations.|
|||Unlike the latest works such as [4, 5] which use the deep CNN for ancillary, we focus on designing a CNN-based density map generator.|
|||[39] propose a CNN which uses the high-level prior information to boost the density prediction performance.|
|||Similar idea is shown in Onoro and Sastre [20] where a scale-aware, multi-column counting model called Hydra CNN is presented for object density estimation.|
|||[5] present a Contextual Pyramid CNN, which uses CNN networks to estimate context at various levels for achieving lower count error and better quality density maps.|
|||Proposed Solution  The fundamental idea of the proposed design is to deploy a deeper CNN for capturing high-level features with larger receptive fields and generating high-quality density maps without brutally expanding network complexity.|
|||By taking advantage of the regular CNN network (without branch structures), CSRNet is easy to implement and fast to deploy.|
||7 instances in total. (in cvpr2018)|
|643|Top-Down Visual Saliency Guided by Captions|Captioning Models: Captioning models based on a combination of CNN and LSTM networks have shown impressive performance both for image and video captioning [6, 23, 24, 28].|
|||, vm) = (x)  (1)  where typically () is a CNN pre-trained for image classification.|
|||Although spatial pooling in the CNN discards the spatial location of detected visual concepts, the different gates of the LSTM enable it to focus on certain concepts depending on the LSTM hidden state.|
|||The CNN performs spatial average pooling to get a feature vector vi for the ith frame whose kth element is vik = Pa,b fk(a, b).|
|||We accomplish this by rearranging the grid of descriptors produced by the last convolutional layer of the CNN into a temporal sequence V = (v1, .|
|||Model details We implemented our model in TensorFlow [1] using InceptionV3 [21] pretrained on ImageNet [8] as CNN feature extractor.|
|||Note that the CNN was trained on ImageNet and was not finetuned during the training of the captioning model.|
||7 instances in total. (in cvpr2017)|
|644|Clement_Godard_Deep_Burst_Denoising_ECCV_2018_paper|Finally, we explore other applications of multi-frame image enhancement and show that our CNN architecture generalizes well to image super-resolution.|
|||Third, we extend the CNN with a parallel recurrent network that integrates the information of all frames in the burst.|
|||We in turn also make use of a deep model and base our CNN architecture on current  state of the art single-frame methods [36,49,27].|
|||[10], a single, feed-forward CNN is used to super-resolve an input image.|
|||Yin, W., Kann, K., Yu, M., Sch utze, H.: Comparative study of cnn and rnn for natural language  processing.|
|||Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
|||Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image restoration.|
||7 instances in total. (in eccv2018)|
|645|Venugopalan_Sequence_to_Sequence_ICCV_2015_paper|The input visual sequence to the model is comprised of RGB and/or optical flow CNN outputs.|
|||The main contribution of this work is to propose a novel model, S2VT, which learns to directly map a sequence of  14534  CNN Action  pretrainedCNN Object  pretrainedFlow imagesRaw FramesAmaniscuttingabottle<eos>LSTMsCNN OutputsOur LSTM network is connected to a CNN for RGB frames or aCNN for optical flow images.|
|||The flow images are also passed through a CNN and provided as input to the LSTM.|
|||Flow CNN models have been shown to be beneficial for activity recognition [31, 8].|
|||Their technique extracts CNN features for frames in the video and then mean-pools the results to get a single feature vector representing the entire video.|
|||In addition to CNN outputs from raw image (RGB) frames, we also incorporate optical flow measures as input sequences to our architecture.|
|||We then use a CNN [9] initialized with weights trained on the UCF101 video dataset to classify optical flow images into 101 activity classes.|
||7 instances in total. (in iccv2015)|
|646|Deep Learning Human Mind for Automated Visual Classification|As for automated object categorization, our human braindriven approach obtains competitive performance, comparable to those achieved by powerful CNN models and it is also able to generalize over different visual datasets.|
|||More specifically, the first-layer features of a CNN appear to be generalizable across different datasets, as they are similar to Gabor filters and color blobs, while the last-layer features are very specific to a particular dataset or task.|
|||(all features that can be modeled with first CNN layers) and conception, which involves higher cognitive processes that have never been exploited.|
|||The first approach is to train a CNN to map images to corresponding EEG feature vectors.|
|||Typically, the first layers of CNN attempt to learn the general (global) features of the images, which are common between many tasks, thus we initialize the weights of these layers using pre-trained models, and then learn the weights of last layers from scratch in an end to end setting.|
|||Approach 1: we stacked a regression layer to a common deep network and then trained, end to end, the resulting module; Approach 2: We extracted deep features using a common off-the-shelf deep network and then train separately the regressor  the pre-trained AlexNet CNN [13], and modified it by replacing the softmax classification layer with a regression layer (containing as many neurons as the dimensionality of the EEG feature vectors), using Euclidean loss as objective function.|
|||The second approach consists of extracting image features using pre-trained CNN models and then employ regression methods to map image features to EEG feature vectors.|
||7 instances in total. (in cvpr2017)|
|647|Cho_Weakly-_and_Self-Supervised_ICCV_2017_paper|In particular, we review recent image processing research based on deep CNN models, and then review image retargeting approaches.|
|||Deep CNN Model for Low-Level Vision Tasks Recently, there have been many works related to CNN for computer vision problems.|
|||Interestingly, researchers have shed little light on image retargeting based on a deep CNN approach.|
|||In this paper, we apply deep CNN for image retargeting to reflect more semantic information by utilizing many images for training.|
|||As far as we are aware, this is the first work that seriously addresses the content-aware image retargeting through the weaklyand self-supervised deep CNN model.|
|||As reported in [23, 47], activations from the first few convolutional layers of CNN provide low-level structural information.|
|||Unsupervised CNN for In single view depth estimation: Geometry to the rescue.|
||7 instances in total. (in iccv2017)|
|648|Rezatofighi_DeepSetNet_Predicting_Sets_ICCV_2017_paper|Right: In contrast, we propose to train a separate CNN to learn a parameter vector w, which is used to predict the set cardinality of a particular output.|
|||(10)  To estimate the most likely set Y  with cardinality m, we use the first CNN with the parameters  which predicts p(y1,    , yM |x+, ), where M is the maximum cardinalm.|
|||Therefore, the most likely set Y  with cardinality m is obtained by ordering the probabilities of the set elements y1,    , yM as the output of the first CNN and choosing m elements with highest probability values.|
|||A major advantage of this approach is that we can use any state-of-the-art classifier/detector as the first CNN () to further improve its performance.|
|||To generate the predicted labels for a particular image, we perform a forward pass of the CNN and choose top-k labels according to their scores similar to [15, 47].|
|||We also investigated failure cases where either the cardinality CNN or the classifier fails to make a correct prediction.|
|||We use the multi-scale deep CNN approach (MS-CNN) [3] trained on the KITTI dataset [13] for our  Table 2: Count mean absolute error on UCSD dataset.|
||7 instances in total. (in iccv2017)|
|649|cvpr18-CosFace  Large Margin Cosine Loss for Deep Face Recognition|A deep CNN is able to extract clean highlevel features, making itself possible to achieve superior performance with a relatively simple classification architecture: usually, a multilayer perceptron networks followed by  5265  a softmax loss [35, 32].|
|||Recently, face recognition has achieved significant progress thanks to the great success of deep CNN models [18, 15, 34, 9].|
|||In DeepFace [35] and DeepID [32], face recognition is treated as a multiclass classification problem and deep CNN models are first introduced to learn features on large multi-identities datasets.|
|||FaceNet [29] uses triplet loss to learn an Euclidean space embedding and a deep CNN is then trained on nearly 200 million face images, leading to the state-ofthe-art performance.|
|||For the fair comparison, the CNN architecture used in our work is similar to [23], which has 64 convolutional layers and is based on residual units[9].|
|||The CNN models are trained with SGD algorithm, with the batch size of 64 on 8 GPUs.|
|||Following the experimental setting in [23], we train a model with the guidance of the proposed LMCL on the CAISAWebFace[46] using the same 64-layer CNN architecture described in [23].|
||7 instances in total. (in cvpr2018)|
|650|Edouard_Oyallon_Compressing_the_Input_ECCV_2018_paper|We demonstrate that cascading a CNN with this representation performs on par with ImageNet classification models, commonly used in downstream tasks, such as the ResNet-50.|
|||Effective reduction of the spatial dimension and total signal size for CNN processing is difficult.|
|||This suggests that reducing the input size to subsequent CNN layers calls for a careful design.|
|||In  2  E. Oyallon, E. Belilovsky, S. Zagoruyko, M. Valko  this work, we (a) analyze a generic method that, without learning, reduces input size as well as resolution and (b) show that it retains enough information and structure that permits applying a CNN to obtain competitive performance on classification and detection.|
|||As in [16,33], we evaluate the inference time of the CNN from the encoding.|
|||We compare the inference speed of the learned CNN between the different models and for the detection models the inference speed of feature extraction.|
|||4 Conclusion  We consider the problem of compressing an input image while retaining the information and structure necessary to allow a typical CNN to be applied.|
||7 instances in total. (in eccv2018)|
|651|Counting Everyday Objects in Everyday Scenes|More recent work constructs CNN-based models for crowd counting [45, 33] and penguin counting [3] using lower level convolutional features from shallower CNN models.|
|||The input to the glance, aso-sub and seq-sub models are fc7 features from a VGG-16 [42] CNN model.|
|||Glancing (glance)  Our glance approach repurposes a generic CNN architecture for counting by training a multi-layered perceptron  Figure 3: Canonical counting scale: Consider images with grids 2  2 (left) and 6  6 (right).|
|||Given such a partition P of the image I and associated CNN features X = {xi,    , xn}, we now explain our models based on this approach: aso-sub : Our naive aso-sub model treats each cell independently to regress to the real-valued ground truth.|
|||glance: We explore the following choices of features: (1) vanilla classification fc7 features noft, (2) detection fine tuned fc7 features ft, (3) fc7 features from a CNN trained to perform Salient Object Subitizing sos [46] and (4) flattened conv-3 features from a CNN trained for classification aso-sub, seq-sub: We examine three choices of grid sizes (Sec.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
|||Deeper lstm and normalized cnn visual question answering model.|
||7 instances in total. (in cvpr2017)|
|652|Richer Convolutional Features for Edge Detection|So why dont we make full use the CNN features we have now?|
|||Unlike previous CNN methods, the proposed novel network uses the CNN features of all the conv layers to perform the pixelwise prediction in an image-to-image fashion, and thus is able to obtain accurate representations for objects or object parts in different scales.|
|||Concretely speaking, we attempt to utilize the CNN features from all the conv layers in a unified framework that can be potentially generalized to other vision tasks.|
|||By carefully designing a universal strategy to combine hierarchical CNN features, our system performs very well in edge detection.|
|||The aforementioned CNN-based models have advanced the state-of-the-art significantly, but all of them lost some useful hierarchical CNN features when classifying pixels to edge or non-edge class.|
|||To fix this case, we propose a fully convolutional network to combine features from each CNN layer efficiently.|
|||Conclusion  In this paper, we propose a novel CNN architecture, RCF, that makes full use of semantic and fine detail features to carry out edge detection.|
||7 instances in total. (in cvpr2017)|
|653|Modolo_Joint_Calibration_of_2015_CVPR_paper|We experiment with EE-SVM trained on state-of-the-art CNN descriptors.|
|||We train EE-SVM on state-of-the-art CNN descriptors [23] and present experiments on 10 classes of the ILSVRC 2014 dataset [13] and  on all 20 classes of PASCAL VOC 2007 [14] in sec.|
|||The sole purpose is to compare performance to EE-SVM on CNN features, as previous EE-SVM works typically use weaker HOG features [17, 9, 11].|
|||We attribute this phenomenon to the CNN features, which are more easily linearly separable [23, 3133].|
|||These results highlight two points: (1) joint calibration improves over independent calibration by +2% mAP, confirming what observed on ILSVRC2014; (2) CNN features bring a huge improvement over HOG to EE-SVM models (doubling mAP in this case).|
|||This confirms recent findings [23] about the benefits of CNN features for object detection.|
|||[32] A. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, CNN features off-the-shelf: An astounding baseline for recognition, in DeepVision workshop at CVPR, 2014.|
||7 instances in total. (in cvpr2015)|
|654|Object Detection in Videos With Tubelet Proposal Networks|[6] proposed the R-CNN to decompose the object detection problem into multiple stages including region proposal generation, CNN finetuning, and region classification.|
|||To accelerate the training process of R-CNN, Fast R-CNN [5] was proposed to avoid time-consumingly feeding each image patch from bounding box proposals into CNN to obtain feature representations.|
|||The input image is fed into a CNN and forward propagated to generate visual feature maps.|
|||In this way, CNN only needs to forward propagate once for each input image and saves much computational time.|
|||For N static object proposals in the same starting frame, the bottom CNN only needs to conduct an one-time forward propagation to obtain the visual feature maps, and thus enables efficient generation of hundreds of tubelet proposals (see Figure 4 (b)).|
|||As shown in Figure 2, the proposed classification subnetwork contains a CNN that processes input images to obtain feature maps.|
|||Base CNN model training  We choose GoogLeNet with Batch Normalization (BN) [12] as our base CNN models for both our TPN and CNN LSTM models without sharing weights between them.|
||7 instances in total. (in cvpr2017)|
|655|Walker_Dense_Optical_Flow_ICCV_2015_paper|Our CNN model leverages the data in tens of thousands of realistic videos to train our model.|
|||Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios.|
|||First, we present a CNN model for motion prediction.|
|||Given a static image, our CNN model predicts expected motion in terms of optical flow.|
|||Second, our CNN model gives state of the art performance on prediction compared to contemporary approaches.|
|||Finally, we also present a proof of concept extension of the CNN model which makes long-range prediction about future motion.|
|||Our preliminary results indicate that this new CNN model might indeed be promising even for the task of long-range prediction.|
||7 instances in total. (in iccv2015)|
|656|Sadeghian_Tracking_the_Untrackable_ICCV_2017_paper|, t. We use a CNN for our appearance feature extractor.|
|||The CNN accepts the raw content within each bounding box and passes it through its layers until it finally produces a 500-dimensional feature vector (A (which we wish to determine whether it corresponds to the true appearance trajectory of target i or not) through the same CNN that maps it to an H-dimensional vector j .|
|||j  Note that we use a 16-layer VGGNet as our CNN in Figure 3.|
|||We then train this CNN for the re-identification task for which the details can be found in Section 4.3.|
|||We construct a Siamese CNN using the same pre-trained CNN used as our appearance feature extractor in Section 3.2.|
|||We train our Siamese CNN on positive and negative samples extracted from two MOT2D and CUHK03 datasets [39, 42].|
|||Learning by tracking: Siamese cnn for robust target association.|
||7 instances in total. (in iccv2017)|
|657|cvpr18-A Bi-Directional Message Passing Model for Salient Object Detection|Some early deep saliency models utilize CNN features to predict the saliency scores of image segments like superpixels [15] or object proposals [27].|
|||compute saliency value of each superpixel by extracting its contextual CNN features.|
|||propose to embed low-level spatial features into the feature maps and then combine them with CNN features to predict saliency maps.|
|||Some attempts [37, 8, 14] have been conducted to exploit multi-level CNN features for salient object detection.|
|||propose a feature aggregating framework, in which the multi-level CNN features are integrated into different resolutions to predict saliency maps.|
|||Existing CNN models [26, 7] learn features of objects by stacking multiple convolutional and pooling layers.|
|||Gated In Proceedings of  bi-directional cnn for object detection.|
||7 instances in total. (in cvpr2018)|
|658|Pau_Rodriguez_Lopez_Attend_and_Rectify_ECCV_2018_paper|The original CNN is augmented with N attention modules at N different depths.|
|||Based on all these properties, we propose a novel attention mechanism that learns to attend low-level features from a standard CNN architecture through a set of replicable Attention Modules and gating mechanisms (see Section 3).|
|||proposed to use a deep CNN and a Recurrent Neural Network (RNN) to accumulate high multi-resolution glimpses of an image to make a final prediction [26].|
|||The overall architecture consists of an attention canvas generator, which extracts patches of different regions and scales from the original image; a VGG-16 [27] CNN is then used to extract features from the patches, which are aggregated with a long short-term memory [9] that attends to non-overlapping regions of the patches.|
|||Similarly, in [39], they proposed the Multi-Attention CNN (MA-CNN) to learn to localize informative patches from the output of a VGG-19 and use them to train an ensemble of part classifiers.|
|||The proposed attention mechanism is used to augment a CNN with five 3  3 convolutional layers and two fullyconnected layers in the end.|
|||The proposed model learns to attend the most informative parts of the CNN feature maps at different depth levels and combines them with a gating function to update the output distribution.|
||7 instances in total. (in eccv2018)|
|659|Zhu_A_Key_Volume_CVPR_2016_paper|And we believe the improvement of volume level classifier will also benefit sequential models as they are built upon volume level CNN features.|
|||Those volumes are convolved through the shared CNN module, and then passed to N logistic regressors (sigmoid) to get a score matrix S. Formally, we represent the score matrix as:  S = {Sk,n}, k = 1, ..., K, n = 1, ..., N,  (1)  where k is the volume index, K is the total number of volumes; n is the class label, N is the total number of classes.|
|||In this work, we first assign video-level labels to all volumes, and train the baseline convolutional network in Figure 3 with large learning rate until the loss stops to decrease; then we use this 1 stage pre-trained network to initialize the CNN module in our deep framework and start running the iterative loop.|
|||Baseline CNN and key-volume mining CNN correspond to the top row and bottom row of Figure 3 respectively.|
|||We use GoogLeNet with batch normalization [10] as CNN modules shown in Figure 3.|
|||This comparison shows, even without key volume mining, unsupervised key volume proposal still helps CNN based action classification.|
|||We use GoogLeNet with batch normalization [10] as our CNN module.|
||7 instances in total. (in cvpr2016)|
|660|cvpr18-Pose-Guided Photorealistic Face Rotation|Moreover, an identity preserving constraint is implemented by Light CNN [29].|
|||Particularly, we model the synthesizer as a CNN with u-net [15] architecture.|
|||Agent 1 is a conditional discriminator Dii implemented by a CNN framework, where ii is the parameter.|
|||We choose a pre-trained Light CNN [29] as Dip and fix the parameters during training procedure.|
|||The results of Light CNN are used as the baseline of our method.|
|||Once more, the results of Light CNN are used as the baseline of our method.|
|||A light cnn for deep face  representation with noisy labels.|
||7 instances in total. (in cvpr2018)|
|661|cvpr18-FeaStNet  Feature-Steered Graph Convolutions for 3D Shape Analysis|The geodesic CNN model of Masci et al .|
|||[2] proposed the anisotropic CNN model which further extends the geodesic CNN model by using an anisotropic patchextraction method, exploiting the maximum curvature directions to orient patches.|
|||The convolutional layers in a conventional CNN multiply together activations of the preceding feature map and learned filter weights, and sum the results to obtain the output as a linear function of the input, after which a point-wise non-linearity In spatial transformer networks [10] and dyis applied.|
|||Reformulating convolutional CNN layers  A convolutional CNN layer maps D input feature maps to E output feature maps.|
|||Multiplying the Mahalanobis distances by a large constant will recover the hard-assignments used in the standard CNN model of Eq.|
|||Complexity analysis  The weight matrices Wm are common between a conventional CNN and our approach, and contain M DE parameters.|
|||a conventional CNN are the vectors um, vm, which contain 2M D parameters.|
||7 instances in total. (in cvpr2018)|
|662|Fang_From_Captions_to_2015_CVPR_paper|Each location in this response map corresponds to the response obtained by applying the original CNN to overlapping shifted regions of the input image (thereby effectively scanning different locations in the image for possible objects).|
|||We use a cross entropy loss and optimize the CNN end-to-end for this task with stochastic gradient descent.|
|||2We denote the CNN from [21] as AlexNet and the 16-layer CNN from [42] as VGG for subsequent discussion.|
|||Unlike these approaches, in this work we detect words by applying a CNN to image regions [13] and integrating the information with MIL [49].|
|||Generating Word Scores for a Test Image  Given a novel test image i, we up-sample and forward propagate the image through the CNN to obtain pw i as described above.|
|||Image model: We map images to semantic vectors using the same CNN (AlexNet / VGG) as used for detecting words in Section 3.|
|||The second (Classification) is the result of a whole image classifier which uses features from AlexNet or VGG CNN [21, 42].|
||7 instances in total. (in cvpr2015)|
|663|Papon_Semantic_Pose_Using_ICCV_2015_paper|[1] take a different approach, instead using a multi-scale CNN to classify the full image, and then use superpixels to aggregate and smooth prediction outputs.|
|||1, uses a relatively complex CNN architecture to solve our three sub-tasks; class-, pose-, and position-estimation of objects, concurrently.|
|||[5] using a CNN which classifies boundingbox proposals in a room-centric embedding.|
|||The first, baseline, model is a standard CNN network closely resembling the successful model of Krizhevsky et al.|
|||[10] merge  779  Task  Model  n o i t a c i f i s s a l C  Standard CNN  Standard CNN +  Recomb.|
|||ACC  76.07  77.63  78.33  76.42  bathtub  30.49  23.63  41.82  45.02  Mean AUC  bathtub  40.30  40.13  39.68  39.70  ACC  51.04  19.25  23.94  19.65  20.93  bathtub  30.30  bed  68.90  71.69  71.24  69.82  bed  40.83  38.49  43.29  43.03  bed  52.71  chair  88.55  90.67  90.70  87.63  chair  41.40  41.88  40.52  39.70  chair  70.98  desk  56.29  57.80  54.56  53.58  desk  33.32  33.15  32.47  37.87  desk  26.76  dresser  monitor  nightstand  71.87  73.96  70.81  72.48  80.30  85.30  81.70  78.95  57.24  50.46  57.51  52.22  dresser  monitor  nightstand  29.95  30.66  31.90  35.13  41.29  39.63  42.75  44.65  22.83  23.35  24.55  22.95  dresser  monitor  nightstand  33.64  19.05  39.32  sofa  69.94  71.35  74.94  75.44  sofa  53.32  54.27  53.68  55.79  sofa  58.21  table  80.47  82.43  82.18  81.30  table  38.92  36.31  35.06  33.84  table  39.54  51.44  14.29  55.05  70.93  18.05  34.59  27.45  51.13  55.88  40.68  c i t e h t n y S  2 v U Y N  n Standard CNN o i t a m  Standard CNN +  Recomb.|
|||Performance of the four CNN architectures on synthetic (top) and NYUv2 (bottom) datasets for classification and pose estimation.|
||7 instances in total. (in iccv2015)|
|664|Wegner_Cataloging_Public_Objects_CVPR_2016_paper|The trick is to do late fusion of category labels: the outputs of state-of-the-art CNN detectors and clas 6015  Up  East  North  ex,ey  lat,lng  lat(c)  lng(c)  Equator  (a)  (b)  Figure 2.|
|||We define these terms below:  Aerial View Potential: We define the aerial view potential to be the detection score evaluated at the appropriate region:  (t, av(t); ) = CNN (X(av(t)), Pav(t); )  (4)  where X(av(t)) is the aerial image,  encodes the weights of the aerial view detection CNN, and Pav(t) transforms  6017  between pixel location and geographic coordinates.See supplementary material for details.|
|||Street View Potential: Similarly, we define the potential function for a street view image s  sv(t) as  (t, s; ) = CNN (X(s), Psv(t, c(s)); )  (5)  where X(s) is a street view image,  encodes the weights of the street view detection CNN, and Psv(t, c) is defined in Equation 2.|
|||6018  region is then fed through a CNN feature extractor.|
|||We train one CNN model per viewpoint and zoom level using a log-logistic loss via stochastic gradient descent.|
|||A (probably more elegant) alternative to simple feature concatenation would be to explicitly encode the three panorama zoom levels in a single CNN architecture.|
|||det2geo finds the locations of urban trees (on public land), with the help of probabilistic CRF-based fusion on CNN detector scores across views.|
||7 instances in total. (in cvpr2016)|
|665|cvpr18-Diversity Regularized Spatiotemporal Attention for Video-Based Person Re-Identification|[1] input a pair of cropped pedestrian images to a specifically designed CNN with a binary verification loss function for person re-identification.|
|||[44] jointly train the pedestrian detection and person re-identification in a single CNN model.|
|||2, we adopt the ResNet-50 CNN architecture [14] as our base model for extracting features from each sampled image.|
|||The CNN has a convolutional layer in front (named conv1), followed by four residual blocks.|
|||Once finished, we fix the CNN model and train the set of multiple spatial attention models with average temporal pooling and OIM loss function.|
|||Finally, the whole network, except the CNN model, is trained jointly.|
|||We finally fine-tune the whole network, including the CNN model, to each video dataset independently.|
||7 instances in total. (in cvpr2018)|
|666|Kang_Object_Detection_From_CVPR_2016_paper|Regions with CNN features (RCNN)).|
|||In this work, we propose a multi-stage framework based on deep CNN detection and tracking for object detection in videos.|
|||[10] proposed a multi-stage pipeline called Regions with Convolutional Neural Networks (R-CNN) for training deep CNN to classify region proposals for object detection.|
|||It decomposes the detection problem into several stages including bounding-box proposal, CNN pre-training, CNN fine-tuning, SVM training, and bounding box regression.|
|||[30] proposed the GoogLeNet with a 22-layer structure and inception modules to replace the CNN in the R-CNN, which won the ILSVRC 2014 object detection task.|
|||[22] trained a multi-domain CNN for learning generic representations for tracking objects.|
|||When tracking a new target, a new network is created by combining the shared layers in the pretrained CNN with a new binary classification layer, which is online updated.|
||7 instances in total. (in cvpr2016)|
|667|cvpr18-Flow Guided Recurrent Neural Encoder for Video Salient Object Detection|Recently, with the thriving application of deep CNN in salient object detection of static images, there are also attempts to extend CNN to video salient object detection [36, 16].|
|||In recent years, the profound deep CNN has pushed the research on salient object detection into a new phase and has become the dominant research direction in this field.|
|||Deep CNN based methods can be further divided into two categories, including region based deep feature learning [19, 42, 32] and end-to-end fully convolutional network based methods [20, 10, 18, 33, 17].|
|||[16] proposes to incorporate deep CNN feature in a spatio-temporal CRF framework for temporal consistency enhancement, it still suffers from the deficiency of multi-stage pipeline and its high-computational costs.|
|||The most representative work is FlowNet [7] which shows that CNN can be applied to highly effective optical flow inference.|
|||Based on the above consideration, we propose to combine a ConvLSTM [39] with CNN based FlowNet to jointly learn the flow map and refine in reverse order.|
|||Visual saliency detection based on multi scale deep cnn features.|
||7 instances in total. (in cvpr2018)|
|668|Wang_Mining_Discriminative_Triplets_CVPR_2016_paper|In this experiment, in order to fairly compare to both the traditional methods without using extra data/annotation and the more recent ones which finetune ImageNet pre-trained Convolutional Neural Networks (CNN), we evaluate our approach using both HOG and CNN features as the low-level representations of the patches.|
|||When extracting CNN features, we directly use the off-the-shelf ImageNet pre-trained CNN model as a general feature extractor without any finetuning.|
|||Our baselines include LLC [41] as HOG-based baseline and AlexNet [26] as CNN baselines.|
|||[43] finetunes a CNN with the help of another 10,000 images of cars without finegrained labels; the best result of [28] is achieved by finetuning a two-stream CNN architecture; [24] integrates segmentation, graph-based alignment, finetuned R-CNN [17] and SVM to produce its best result.|
|||When using HOG as low-level patch representation, our approach not only greatly outperforms the HOG-based baseline (LLC) (by more than 15%), but it even outperforms the CNN baseline of finetuned AlexNet by a fairly noticeable margin (more than 2%)  a significant achievement since we are only using HOG and geometric constraints without any extra data or annotations.|
|||When using offthe-shelf CNN features, our method with geometric constraints outperforms B-CNN [28] which uses two streams of VGGNet-16, and obtained quite comparable results to the state-of-the-art [24].|
|||Bilinear CNN models for fine-grained visual recognition.|
||7 instances in total. (in cvpr2016)|
|669|Eunbyung_Park_Meta-Tracker_Fast_and_ECCV_2018_paper|By reformulating the correlation filter as a convolutional layer, it can be effectively integrated into an CNN framework [2].|
|||CREST used PCA to reduce the number of channels of extracted CNN features, from 512 to 64.|
|||Putting it all together, F (xj, ) in our proposed meta-training framework for CREST, now takes an input a cropped image xj from an input frame Ij, pass it through a CNN feature extractor followed by dimensionality reduction (1x1 convolution with the weight 0d ).|
|||MDNet is based on a binary CNN classifier consisting of a few of convolutional layers and fully connected layers.|
|||Then, the patches go through a CNN classifier, and the loss function L is a simple cross entropy loss  PN Label shuffling.|
|||This might lead a deep CNN classifier to memorize all object instances in the dataset and classify newly seen objects as backgrounds.|
|||A large part of the computation comes from extracting CNN features for every patch.|
||7 instances in total. (in eccv2018)|
|670|Uta_Buchler_Improving_Spatiotemporal_Self-Supervision_ECCV_2018_paper|However, can we not find permutations that are of higher utility for improving a CNN representation than the random set?|
|||Moreover the effect of the permutations on the CNN changes during training since the state of the network evolves.|
|||Therefore, wrapped around the standard back-propagation training of the CNN, we have a reinforcement learning algorithm that acts by proposing permutations for the CNN training.|
|||4  U. B uchler, B. Brattoli, B. Ommer  3.1 Self-Supervised Spatiotemporal Representation Learning  Subsequently, we learn a CNN feature representation (CaffeNet [21] architecture up to pool5) for images and individual frames of a video using spatiotemporal self-supervision (see Fig.|
|||However, permutations should be selected conditioned on the state of the network that is being trained to sample new permutations according to their utility for learning the CNN representation.|
|||5 also shows, that each of the three models has a substantial gain when the CNN is trained using the policy.|
|||(A): with Policy, (B): without policy  network randomly and the CNN model from an intermediate checkpoint (average validation error 72.3%).|
||7 instances in total. (in eccv2018)|
|671|Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper|For an image classification problem, [7,8] used an LSTM [9] caption generation model that generates textual justifications for a CNN model.|
|||The attention map j is applied to the CNN output yielding context vectors yj t .|
|||The controller following the CNN has 5 fully connected layers (i.e., #hidden dims: 1164, 100, 50, 10, respectively), which predict the acceleration and the change of  1 The number of video intervals (not full videos), where the provided action descriptions (not  explanations) are identical (common actions e.g., the car slows down).|
|||Using other more expressive networks may give a performance boost over our base CNN configuration, but these explorations are out of our scope.|
|||For a fair comparison, we use an identical CNN for all models.|
|||Specifically, our model without the entropy regularization term (last row) performs the best compared to CNN based approaches [3] and [11].|
|||Note that in our implementation S2VT uses our CNN and does not  12  J. Kim, A. Rohrbach, T. Darrell, J.|
||7 instances in total. (in eccv2018)|
|672|Bai_GIFT_A_Real-Time_CVPR_2016_paper|In on-line processing, once a user submits a query shape, GIFT can react and present the retrieved shapes within one second (the off-line preprocessing operations, such as CNN model training and inverted file establishment, are excluded).|
|||The CNN used here takes depth images as input, and the loss function is exerted on the classification error for projections.|
|||However, in our specific scenario, the outputs of different layers of CNN are usually at different abstraction resolutions.|
|||q  q  and F (2)  For example, two different layers of CNN lead to two different similarities S(1) and S(2) by Eq.|
|||unique models per category from the subset, in which 80 models are used for training the CNN model and the rest for testing the retrieval performance.|
|||As for the feature extractor, we collected 54, 728 unrelated models from ModelNet [39] divided into 461 categories to train a CNN model.|
|||We train CNN on an independent TSB dataset [36], and then use the trained CNN to extract view features for the shapes in all the three testing datasets.|
||7 instances in total. (in cvpr2016)|
|673|Rahmani_Learning_Action_Recognition_ICCV_2017_paper|First, we propose a deep CNN model which transfers the depth appearance of human body-parts to a shared view-invariant space.|
|||Learning this deep CNN requires a large dataset containing a variety of human bodyparts in different actions observed from many viewpoints.|
|||[23] proposed to generate synthetic depth training data of human poses in order to train a deep CNN model that transfers human poses, acquired from different views, to a view-invariant space.|
|||To overcome this problem, we learn a single deep CNN model for all human body-parts to transfer them to a shared viewinvariant space.|
|||However, learning such a deep CNN requires a large training dataset containing a large number of human body-parts, performing a variety of actions observed from many viewpoints.|
|||We initialize the convolution layers of the proposed CNN with the model trained on depth images of full human body from [23].|
|||In order to make our method robust to viewpoint changes, we introduced a deep CNN which transfers visual appearance of human body-parts acquired from different unknown views to a view-invariant space.|
||7 instances in total. (in iccv2017)|
|674|Chen_Deep_Domain_Adaptation_2015_CVPR_paper|Our method instead uses a supervised double path CNN with shared layers.|
|||Fine-grained attributes classification results for Streetdata-c.  Street-data-c CNN DDAN-U  Type-T1 Type-T1-b Color-T1 Color-T1-b Pattern-T1 Pattern-T1-b  41.53 48.31 5.08 5.93 70.34 71.19  48.32 55.12 15.34 18.87 72.45 75.90  Figure 4.|
|||We compare the result of our enhanced RCNN-NIN detector with two baselines: (1) Deformable Part-based Model(DPM) [10] and (2) RCNN using a traditional CNN model [26] pretrained on ImageNet [6] as feature generator.|
|||(2) CNN-FC2, where we use the FC2 layer of a CNN model [26] as the features for training a classifier using street domain images.|
|||(3) CNN-FT, i.e., CNN fine-tuning, which keeps the original shop domain objective function, and then feeds the network with the street domain training samples.|
|||Street-data-a CNN CNN-FC2 CNN-FT DDAN-S DDAN-U 25.44 Type-T1 29.68 Type-T1-b 16.23 Color-T1 20.18 Color-T1-b Pattern-T1 73.11 Pattern-T1-b 73.39  33.42 38.92 27.39 32.30 74.13 74.90  22.12 25.67 10.02 14.43 60.91 63.20  31.02 36.20 25.21 30.46 75.31 76.01  32.53 37.9 22.87 27.21 76.2 76.6  tricks which are commonly used in the ImageNet challenges (e.g., multiple models ensembles, data augmentation, etc.)|
|||We can see consistent improvement of DDAN-U over directly applying the source domain CNN model with big margin.|
||7 instances in total. (in cvpr2015)|
|675|cvpr18-Learning From Noisy Web Data With Category-Level Supervision|It can be seen from Figure 1 that our network consists of a classification network in the top flow and variational autoencder (VAE) in the bottom flow, which share common modules (i.e., CNN and encoder) and jointly leverage category-level information.|
|||More recently, several CNN approaches were proposed for webly supervised learning [49, 42, 6, 57, 34, 8].|
|||Suppose we have a noisy training set I = {Ii|n  i=1} with Ii being the i-th image and n being the number of training images, the visual feature of Ii (i.e., output of CNN in Figure 1) is denoted as xi and its associated label (resp., predicted label variable) is denoted as ci (resp., yi).|
|||In semantic VAE, as opposed to impos ing non-discriminative generic prior p1 (z), we replace the regularizer KL[q2 (z|xi)||p1 (z)] in (3) with softmax loss  log p(yi = ci|zi) and obtain the following loss function:  l(xi, ci) =  log p(yi = ci|zi)  log p1 (xi|zi),  (4)  in which  = {0, 1, 2} including the CNN parameters 0.|
|||For the CNN model, we adopt Inception-V3 [43], which outputs 2048-dim visual feature.|
|||1) For basic CNN, we train Inception-V3 without consider ing label noise, which is referred to as CNN in Table 1.|
|||For fair comparison, we extract visual features from Inception-v3 retrained on our training sets, in which case feature-based methods can achieve at least comparable results with CNN in Table 1.|
||7 instances in total. (in cvpr2018)|
|676|cvpr18-Deep Extreme Cut  From Extreme Points to Object Segmentation|Example results of DEXTR: The user provides the extreme clicks for an object, and the CNN produces the segmented masks.|
|||Closer to our approach, [3] employs point-level supervision in the form of a single click to train a CNN for semantic segmentation and [26] uses central points of an imaginary bounding box to weakly supervise object detection.|
|||iFCN [42] guides a CNN from positive and negative points acquired from the ground-truth masks.|
|||Architecture of DEXTR: Both the RGB image and the labeled extreme points are processed by the CNN to produce the segmented mask.|
|||The output of the CNN is a probability map representing whether a pixel belongs to the object that we want to segment or not.|
|||The CNN is trained to minimize the standard cross entropy loss, which takes into account that different classes occur with different frequency in a dataset:  L = X  wyj C (yj, yj),  j  1, ..., |Y |  (1)  jY  where wyj depends on the label yj of pixel j.|
|||Interestingly, OHEM is a crucial component for improving performance: without it the network does not focus on the difficult examples (only 11% of objects of the second training split are hard examples), and fails to improve on the erroneous region indicated by the  We have presented DEXTR, a CNN architecture for semi-automatic segmentation that turns extreme clicking annotations into accurate object masks; by having the four extreme locations represented as a heatmap extra input channel to the network.|
||7 instances in total. (in cvpr2018)|
|677|cvpr18-GVCNN  Group-View Convolutional Neural Networks for 3D Shape Recognition|Concretely, we first use an expanded CNN to extract a view level descriptor.|
|||The Group-View CNN framework for 3D shape recognition.|
|||More specifically, we first use an expanded CNN to extract a view level descriptor.|
|||We then present our proposed group-view CNN architecture in Sec.3.|
|||And the deeper CNN will have the content information which could  (a) 8 Views  (b) 12 Views  Figure 4.|
|||In experiments, our GVCNN is compared with the Multi-view CNN by Su et al.|
||6 instances in total. (in cvpr2018)|
|678|cvpr18-MoNet  Moments Embedding Network|Bilinear CNN (BCNN) only has 2nd order information and does not use matrix normalization.|
|||Bilinear CNN (BCNN) was first proposed by Lin et al.|
|||The main contributions of this work are three-fold:   We unify the G2DeNet and bilinear pooling CNN us ing the empirical moment matrix and decouple the Gaussian embedding from bilinear pooling.|
|||However, both of these works focus on a conventional pipeline and did not bring moments to modern CNN architectures.|
|||Experiments  Aligned with other bilinear CNN based papers, we also evaluate the proposed MoNet with three widely used finegrained classification datasets.|
|||Bilinear cnn models for fine-grained visual recognition.|
||6 instances in total. (in cvpr2018)|
|679|Zhu_Soft_Proposal_Networks_ICCV_2017_paper|Recent research [6] demonstrates that the convolutional filters in CNN can be seen as object detectors and their feature maps can be aggregated to produce Class Activation Map (CAM) [36], which specifies  11841  Spotlightproposal mapgeneratecoupleSoftProposal(cid:1847)(cid:1848)the spatial distribution of discriminative patterns for different image classes.|
|||Last but not least, the proposal iteratively evolves along with CNN filters updating.|
|||Visualization of Class Activation Maps (CAM) [36] for generic CNN and the proposed SPN.|
|||Despite the SP module can be inserted after any CNN layer, we apply it after the last convolutional layer where the deep features are most informative.|
|||Since the M flows along with gradients , inserting one SP module after the top convolutional layer can effect all CNN filters.|
|||Experiment  We upgrade state-of-the-art CNN architectures, e.g., VGG16 and GoogLeNet, to SPNs, and evaluate them on popular benchmarks.|
||6 instances in total. (in iccv2017)|
|680|Owens_Visually_Indicated_Sounds_CVPR_2016_paper|For each frame t, we construct an input feature vector xt by concatenating CNN features for the spacetime image at frame t and the color image from the first frame3:  xt = [(Ft), (I1)],  (2)  where  are CNN features obtained from layer fc7 of the AlexNet architecture [27] (its penultimate layer), and Ft is the spacetime image at time t. In our experiments (Section 6), we either initialized the CNN from scratch and trained it jointly with the RNN, or we initialized the CNN with weights from a network trained for ImageNet classification.|
|||Sound prediction model We use a recurrent neural network (RNN) with long short-term memory units (LSTM) [18] that takes CNN features as input.|
|||To compensate for the difference between the video and audio sampling rates, we replicate each CNN feature vector k times, where k = T /N  (we use k = 3).|
|||We train the RNN and CNN jointly using stochastic gradient descent with Caffe [24, 12].|
|||We computed fc7 features from a CNN pretrained on ImageNet [27] for the center frame of each sequence, which by construction is the frame where the impact sound occurs.|
|||3.85  4.73  4.30  3.85  7.92  8.39  7.19  7.74  9.32  r  0.47  0.33  0.37  0.47  0.28  0.18  0.21  0.20  0.02  0.50  0.25  0.00  (a) Model evaluation  (b) Predicted sound confusions  (c) CNN feature confusions  Figure 5: (a) We measured the rate at which subjects chose an algorithms synthesized sound over the actual sound.|
||6 instances in total. (in cvpr2016)|
|681|Ke_Is_Rotation_a_2014_CVPR_paper|The goal of CNN is to learn a hierarchy of feature representations.|
|||The learning of CNN is based on Stochastic Gradient Descent (SGD), which includes two main operations: Forward and BackPropagation.|
|||1), the Forward operation amounts to convolve the times series with CNN filters and average the outputs in each layer of the convolutional neural network.|
|||Assuming we have K layers CNN and a signal x0.|
|||In all the experiments, we used four layers CNN (including the input layer and the single layer perception for regression output).|
|||In this setup, we reserved a validation set consisting of 10 images per object that are in the same class as the test object, and stop the training when the CNN converges.|
||6 instances in total. (in cvpr2014)|
|682|Zepeda_Exemplar_SVMs_as_2015_CVPR_paper|One drawback with these types of approaches is their large complexity, as a CNN architecture consists of many tens of millions of coefficients.|
|||Their approach consists of using the activation coefficients from a fully connected layer of a deep CNN architecture as an image feature for retrieval.|
|||Results for VLAD, BoW, Fisher and CNN encodings and their RE-SVM-1 and RE-SVM-2 variants.|
|||Classifier CNN [15]  + RE-SVM-1  NCM K-NN 3 K-NN 5 K-NN 10  51.8 60.7 65.7 68.9  55.5 62.2 66.5 69.8  Table 3.|
|||Results (mAP) on the Pascal VOC image classification task when using CNN [15] as a base feature.|
|||Hybrid Multi-Layer Deep CNN / Aggregator Feature for Image Classification.|
||6 instances in total. (in cvpr2015)|
|683|Tian_Text_Flow_A_ICCV_2015_paper|The CNN structure is similar to the one that is implemented for the handwritten digit recognition [12].|
|||The CNN is trained by using the image patches that are obtained by applying the character detector on the training images.|
|||The CNN models used in the min-cost flow network are trained using the character candidate samples detected from the training images.|
|||To verify that the good performance is attributed to the min-cost flow model instead of the CNN classifier, a baseline scene text detection system is implemented for comparison.|
|||flow model contributes much more to the good performance than the CNN classifier.|
|||In addition, the good performance is also partially due to the cascade boosting model which gives a high recall of character candidates as well as the CNN employed in the min-cost flow network which provides reliable character confidence.|
||6 instances in total. (in iccv2015)|
|684|Lluis_Gomez_Single_Shot_Scene_ECCV_2018_paper|The novelty of the proposed model consists in the usage of a single shot CNN architecture that predicts at the same time bounding boxes and a compact text representation of the words in them.|
|||In this way, the text based image retrieval task can be casted as a simple nearest neighbor search of the query text representation over the outputs of the CNN over the entire image database.|
|||[12] scene text segmentation was performed by a text proposals mechanism that was later refined by a CNN that regressed the correct position of bounding boxes.|
|||Afterwards, those bounding boxes were inputed to a CNN that classified them in terms of a predefined vocabulary.|
|||[19] proposed a pipeline that included a CNN to obtain text region proposals followed by a region feature encoding module that is the input to an LSTM to detect text.|
|||In this way, the scene text retrieval of a given query word is performed with a simple nearest neighbor search of the query PHOC representation over the outputs of the CNN in the entire image database.|
||6 instances in total. (in eccv2018)|
|685|Vinyals_Show_and_Tell_2015_CVPR_paper|NIC, our model, is based end-to-end on a neural network consisting of a vision CNN followed by a language generating RNN.|
|||Hence, it is natural to use a CNN as an image encoder, by first pre-training it for an image classification task and using the last hidden layer as an input to the RNN decoder that generates sentences (see Fig.|
|||Our particular choice of CNN uses a novel approach to batch normalization and yields the current best performance on the ILSVRC 2014 classification competition [12].|
|||LSTM model combined with a CNN image embedder (as defined in [12]) and word embeddings.|
|||all the parameters of the LSTM, the top layer of the image embedder CNN and word embeddings We.|
|||The most obvious way to not overfit is to initialize the weights of the CNN component of our system to a pretrained model (e.g., on ImageNet).|
||6 instances in total. (in cvpr2015)|
|686|Dai_Convolutional_Feature_Masking_2015_CVPR_paper|The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions.|
|||The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition.|
|||The R-CNN methods [8, 10] for semantic segmentation extract two types of CNN features one is region features [8] extracted from proposal bounding boxes [22]; the other is segment features extracted from the raw image content masked by the segments [10].|
|||Top: the methods of Regions with CNN features (R-CNN) [8] and Simultaneous Detection and Segmentation (SDS) [10] that operate on the raw image domain.|
|||In the breakthrough object detection paper of R-CNN [8], the CNN features are also used like holistic features, but are extracted from sub-images which are the crops of raw images.|
|||Both methods are not based on CNN features.|
||6 instances in total. (in cvpr2015)|
|687|Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper|2, the HP-net consists of two parts, one is the Main Net (M-net) that is a plain CNN architecture, the other is the Attentive Feature Net (AF-net) including multiple branches of multidirectional attention (MDA) modules applied to different semantic feature levels.|
|||In principle, any kind of CNN structure can be applied to construct the HP-net.|
|||ACN [24] and DeepMar [13] are CNN models that achieved good performances by joint training the multiple attributes.|
|||The Market-1501 [38] dataset is also evaluated with the metric learning WARCA-L [11], a novel Siamese LSTM architecture LOMO+CN [27], the Siamese CNN with learnable gate S-CNN [26], and the bag of words model BoW-best [38].|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Person attribute recogIn CVPR,  nition with a jointly-trained holistic cnn model.|
||6 instances in total. (in iccv2017)|
|688|He_Delving_Into_Salient_ICCV_2017_paper|While this CNN is trained for salient object detection, the weights of its adaptive weight layer are dynamically determined by an auxiliary subitizing network, allowing the layer to encode subitizing knowledge.|
|||In this way, a single deep CNN architecture has a dynamical representation space, and it is able to deal with various input contextual information by dynamically predicting weight in the adaptive weight layer.|
|||[30] first apply a CNN to extract local patch features to obtain intermediate saliency results, and another CNN to globally integrate the initial saliency map with object proposals.|
|||[17] use a pre-trained CNN as a feature extractor.|
|||These methods, however, apply CNN in a sliding window fashion, resulting in a high computational cost.|
|||Weight prediction in CNN has been explored in [16] and [24] for zero-shot learning and visual question answering, respectively.|
||6 instances in total. (in iccv2017)|
|689|Srinivasan_Learning_to_Synthesize_ICCV_2017_paper|We propose a CNN framework that factors the light field synthesis problem into estimating depths for each ray in the light field, rendering a Lambertian approximation to the light field, and refining this approximation by predicting occluded rays and non-Lambertian effects (incorrect rays that are refined, in this case red rays that should be the color of the background instead of the flower, are marked with blue arrows).|
|||The CNN parameters are learned end-to-end by minimizing the sum of the reconstruction error of the Lambertian approximate light field, the reconstruction error of the predicted light field, and regularization losses for the predicted depths, for all training tuples:  min d,o  X  (cid:2)||Lr  L||1 + || L  L||1  S  +cc(D) + tvtv(D)(cid:3)  (3)  Given an image from a single viewpoint, our goal is to synthesize views from a densely-sampled grid around the input view.|
|||We stack all sub-aperture images along one dimension and use a 3D CNN so each filter has access to every 2D view.|
|||This 3D CNN predicts a residual that, when added to the approximate Lambertian light field, best predicts the training example true light fields.|
|||We also tested a CNN that directly regresses from an input image to an output light field, and found that our model outperforms this network with a mean L1 error of 0.026 versus 0.031 across all views.|
|||Unsupervised CNN for single view depth estimation: geometry to the rescue.|
||6 instances in total. (in iccv2017)|
|690|Weixuan_Chen_DeepPhys_Video-Based_Physiological_ECCV_2018_paper|In contemporary techniques, both appearance and motion representations can be learned in parallel (two-stream methods [27]), in cascade (CNN connected with RNN [8, 4]) or in a mixed way (3D CNN [33]).|
|||8  W. Chen and D. McDuff  4.2 Convolutional Neural Network  The foundation of our learning model is a VGG-style CNN for estimating the physiological signal derivative from the motion representation, as shown in Fig.|
|||Different from classical CNN models deigned for object recognition, we used average pooling layers instead of max pooling.|
|||Table 1 shows that our motion-only CNN, stacked CNN and CAN all outperform the prior methods for HR measurement over task two to task six, both in terms of MAE and SNR.|
|||However, for heart rate measurement the Stacked CNN and CAN still outperform all the previous methods.|
|||For breathing rate measurement, though motion-only CNN and the stacked CNN have accuracies similar or inferior to Tarassenko et al.|
||6 instances in total. (in eccv2018)|
|691|Rex_Yue_Wu_BusterNet_Detecting_Copy-Move_ECCV_2018_paper|More precisely, it takes input image X, extracts features using CNN Feature Extractor, upsamples the feature maps to the original image size using Mask Decoder, and applies Binary Classifier to fulfill the auxiliary task, i.e.|
|||Any convolutional neural network (CNN) can serve as CNN Feature Extractor.|
|||The resulting CNN feature f X m is of size 16  16  512, whose resolution is much lower than that is required by the manipulation mask.|
|||2) takes an input image X, extracts features using CNN Feature Extractor, computes feature similarity via Self-Correlation module, collects useful statistics via Percentile Pooling, upsamples feature maps to the original image size using Mask Decoder, and applies Binary Classifier to fulfill the auxiliary task, i.e.|
|||6  Y. Wu, W. Abd-Almageed, & P. Natarajan  Like Mani-Det branch, Simi-Det branch starts with feature representation via CNN Feature Extractor.|
|||To train BusterNet, we initialize all parameters from random weights except for using a pretrained VGG16 on ImageNet for CNN Feature Extractor in Simi-Det.|
||6 instances in total. (in eccv2018)|
|692|Malinowski_Ask_Your_Neurons_ICCV_2015_paper|It combines a CNN with a LSTM into an end-to-end architecture that predict answers conditioning on a question and an image.|
|||We encode the image x using a CNN and provide it at every time step as input to the LSTM.|
|||Implementation We use default hyper-parameters of LSTM [5] and CNN [11].|
|||All CNN models are first pretrained on the ImageNet dataset [25], and next we randomly initialize and train the last layer together with the LSTM network on the task.|
|||In a pilot study, we have found that GoogleNet architecture [11, 29] consistently outperforms the AlexNet architecture [11, 16] as a CNN model for our task and model.|
|||We observe that indoor scene statistics, spatial reasoning, and small objects are not well captured by the global CNN representation, but the true limitations of this representation can only be explored on larger datasets.|
||6 instances in total. (in iccv2015)|
|693|Szeto_Click_Here_Human-Localized_ICCV_2017_paper|To the best of our knowledge, this is the first work to integrate such human guidance into a CNN at inference time.|
|||First, we propose a novel CNN that integrates two types of informationan image and information about a single keypointto output viewpoint predictions; this model is designed to be incorporated into a hybrid-intelligence viewpoint estimation pipeline.|
|||Even better performance can be achieved by supervising the CNN training stage with intermediate representations [28, 14].|
|||Click-Here CNN for Viewpoint Estimation  Our goal is to estimate three discrete angles that describe the rotation of the camera about a target object, where we are given a tight crop of the object, the location of a visible keypoint in the image, and the keypoint class (e.g.|
|||the image and object class are available at test time, by fine-tuning popular CNN architectures such as AlexNet [13] and VGGNet [21].|
|||We believe that for viewpoint estimation, this information can be used to produce features that complement the global image features extracted from popular CNN architectures.|
||6 instances in total. (in iccv2017)|
|694|cvpr18-Real-Time Rotation-Invariant Face Detection With Progressive Calibration Networks|The detailed CNN structures of three stages in our proposed PCN method.|
|||For fairer comparison, we also implement a Cascade CNN [13] face detector using the same networks as our PCN, trained with data augmentation.|
|||Besides, we insert STN [9] in the Cascade CNN for more extensive comparison.|
|||2) Divide-and-Conquer: we implement an upright face detector based on Cascade CNN and run this detector four times on the images rotated by 0, 90, 90, 180, to form a rotation-invariant face detector.|
|||Besides, our PCN performs much better than the baseline Cascade CNN with almost no extra time cost benefited from the efficient calibration process.|
|||43272301  ,80!489;08%7:0!489;0#,90 ,  :5;/0 ,3/ 436:07#49,943#4:907(,8.,/0(,8.,/0( $%($$( ' ,8907# ( ' ,8907# ( ' ! 4:78 ,80!489;08%7:0!489;0#,90 -  /43;/0 ,3/ 436:07#49,943#4:907(,8.,/0(,8.,/0( $%($$( ' ,8907# ( ' ,8907# ( ' ! 4:78 ,80!489;08%7:0!489;0#,90 .  019;/0 ,3/ 436:07#49,943#4:907(,8.,/0(,8.,/0( $%($$( ' ,8907# ( ' ,8907# ( ' ! 4:78 ,80!489;08%7:0!489;0#,90 /  79;/0 ,3/ 436:07#49,943#4:907(,8.,/0(,8.,/0( $%($$( ' ,8907# ( ' ,8907# ( ' ! 4:78 ,80!489;08%7:0!489;0#,90#49,943#;/0 ,3/ 436:07#49,943#4:907(,8.,/0(,8907# ( ' ,8907# ( ' ! 4:78 Method  Divide-and-Conquer Rotation Router [18] Cascade CNN [13] Cascade CNN [13] + STN [9] SSD500 [14] (VGG16) Faster R-CNN [17] (VGGM) Faster R-CNN [17] (VGG16) R-FCN [2] (ResNet-50) PCN (ours)  Recall rate at 100 FP on FDDB  Speed  Up 85.5 85.4 85.0 85.8 86.3 84.2 87.0 87.1 87.8  Down 85.2 84.7 84.2 85.0 86.5 82.5 86.5 86.6 87.5  Left 85.5 84.6 84.7 84.9 85.5 81.9 85.2 85.9 87.1  Right 85.6 84.5 85.8 86.2 86.1 82.1 86.1 86.0 87.3  Ave 85.5 84.8 84.9 85.5 86.1 82.7 86.2 86.4 87.4  CPU 15FPS 12FPS 31FPS 16FPS 1FPS 1FPS 0.5FPS 0.8FPS 29FPS  GPU 20FPS 15FPS 67FPS 30FPS 20FPS 20FPS 10FPS 15FPS 63FPS  Model Size  2.2M 2.5M 4.2M 4.7M 95M 350M 547M 123M 4.2M  Table 1.|
||6 instances in total. (in cvpr2018)|
|695|Tang_Towards_a_Unified_ICCV_2017_paper|Specifically, the Leaf-node is modeled as  Su(u) = leaf  u   fu(wu, I; )  (8)  u  where leaf is the primitive filter; fu(wu, I; ) is CNN features extracted at location wu of image I with  being the collection of the CNNs parameters.|
|||(12) can be considered as a CNN, formed by stacking a convolution layer [18] with weights leaf and the CNN feature extractor fu(wu, I; ).|
|||We compare the CompNet with i) a part-based model [27], ii) state-of-the-art methods using HOG [31] and CNN [22] features, iii) CNN and Maxout networks with similar architectures with our CompNet, i.e., the same layer numbers, kernel sizes and channel numbers.|
|||Note extra datasets, including the Chars74K, are exploited to train the CNN used in [22].|
|||We follow the regionbased CNN (R-CNN) frameworks [11, 10], which generate object proposals [33] from each input image, later refined by a regression, and utilize the region of interest (RoI) pooling layer [11] to extract feature maps of fixed size, i.e., 6  6  256, for each proposal from the conv5 features.|
|||Specially, we respectively use and(k4c256)-or(c128)and(k3c512)-and(k1c21) for structure modeling and fc(c21) as Object-level data potentials to obtain the classification  3Extra data are used to train the CNN feature extractor.|
||6 instances in total. (in iccv2017)|
|696|Ramanathan_Learning_Temporal_Embeddings_ICCV_2015_paper|In practice, the embedding function can be a CNN built from the frame pixels, or any underlying image or video representation.|
|||However, following the recent success of ImageNet trained CNN features for complex event videos [41, 44], we choose to learn an embedding on top of the fully connected fc6 layer feature representation obtained by passing the frame through a standard CNN [19] architecture.|
||| fc6 and fc7: Features extracted from the ReLU layers following the corresponding fully connected layers of a standard CNN model [19] pre-trained on ImageNet.|
|||A similar observation was made in [41, 44], where simple CNN features trained from ImageNet consistently provided the best results.|
|||Additionally, the Imagenet pre-trained CNN features seem to perform better than most previous representations, which is also consistent with previous work [41, 44].|
|||[44] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and Exploiting image-trained cnn arclassification.|
||6 instances in total. (in iccv2015)|
|697|Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper|The original CNN consists of two parts: 1) the input layers, five convolution layers and maxpooling layers, and 2) two fully connection layers FC1 and FC2, and the output layers which produces a distribution over the 1,000 class labels.|
|||Furthermore, we change the original loss function to Equation (1),  loss(v, ~t+, ~t) = X max[0,   ~t+M~v + ~tM~v],  (1)  where ~t+ is the semantic vector representation of the query and ~v is the output of FC2 in CNN network for the thumbnail in the given clicked query-thumbnail pairs, ~t is the semantic vector representations of other text which is independent of v, M is the matrix of trainable parameters in the projection layer,  is the parameter of margin (set to 0.1).|
|||Before training, we init the CNN with the parameters trained well in [21].|
|||In the training process, we firstly trained the parameters of M while holding both the above layers in CNN and the text representation fixed.|
|||In the later stages, the derivative of the loss function was back-propagated into all CNN layers to fine-tune the output.|
|||In the future, we will try to simultaneously finetune more parallel projection layers Mk for different tasks in one CNN network.|
||6 instances in total. (in cvpr2015)|
|698|Narihira_Direct_Intrinsics_Learning_ICCV_2015_paper|Section 2 describes the details of our CNN architecture and learning objectives for direct intrinsics.|
|||Direct Intrinsics  We circumvent the data availability roadblock by training on purely synthetic images and testing on both real  We break the full account of our system into specification of the CNN architecture, description of the training  2993  - MSCR  MSCR+HC  Figure 3.|
|||Left: Motivated by the multiscale architecture used by Eigen and Fergus [7] for predicting depth from RGB, we adapt a similar network structure for direct prediction of albedo and shading from RGB and term it Multiscale CNN Regression (MSCR).|
|||As Figure 3 shows, we adopt a Multiscale CNN Regression (MSCR) architecture with important differences from [7]:   Instead of fully connected layers in scale 1, we use a 11 convolution layer following the upsampling layer.|
|||Learning  Given an image I, we denote our dense prediction of  albedo A and shading S maps as:  (A, S) = F (I, )  (3)  where  consists of all CNN parameters to be learned.|
|||We develop a two-level feed-forward CNN architecture based on a successful previous model for RGB to depth prediction, where the coarse level architecture predicts the  global context and the finer network uses the output of the coarse network to predict the finer resolution result.|
||6 instances in total. (in iccv2015)|
|699|HU_Jian-Fang_Deep_Bilinear_Learning_ECCV_2018_paper|Bilinear pooling has been introduced to combine features extracted by two CNN models [21, 9, 10].|
|||There are two merits of using such a composition: 1) it enables efficient training of trajectory-based CNN as we dont need to train a CNN for each trajectory-based patch sequence; and 2) it captures the dynamics of patch appearances along each trajectory.|
|||In total, our cube descriptor contains five temporal feature maps, with two from RGB AHSs (1channel CNN and 16-channel CNN), two from depth AHSs ((1-channel CNN and 16-channel CNN), and one from the skeleton AHSs (RNN), each of which characterizes actions at different AHS lengths from a specific modality.|
|||Note that for constructing the temporal feature for the AHS of a specific modality and temporal length, we use the output of the final layer of CNN (or RNN for skeleton AHSs), whose size is the same as the number of action  5 The input of K-channel CNN is K gray images concatenated along the channel  dimension.|
|||Thus, it is a CNN whose input size is 224  224  K.  Deep Bilinear Learning for RGB-D Action Recognition  7  Fig.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition.|
||6 instances in total. (in eccv2018)|
|700|Chen_Monocular_3D_Object_CVPR_2016_paper|Our method first aims to generate a set of candidate class-specific object proposals, which are then run through a standard CNN pipeline to obtain highquality object detections.|
|||The most recent line of work aims to learn how to propose promising object candidates using either ensembles of binary segmentation models [27], parametric energies [29] or window classifiers based on CNN features [18].|
|||The method is efficient since it explores an exhaustive set of windows via integral images over the CNN responses.|
|||Since our input is a single monocular image, our ground-plane is assumed to be orthogonal to  2148  Conv  layers  Box proposal  FCs  FCs  ROI  pooling  ROI  pooling  Context region  FC  FC  FC  C o n c a t e n a t i o n  Softmax  classification  Box   regression  Orientation  regression  Figure 2: CNN architecture adopted from [10] used to score our proposals for object detection and orientation estimation.|
|||To compute instance-level segmentation we exploit the approach of [51, 50], which uses a CNN to create both instance-level pixel labeling as well as ordering in depth.|
|||Comparison with Baselines: As strong baselines, we also use our CNN scoring on top of three other proposals methods, 3DOP [10], EdgeBoxes (EB) [55], and Selective Search (SS) [45], where we re-train the network accordingly.|
||6 instances in total. (in cvpr2016)|
|701|Jianbo_Jiao_Look_Deeper_into_ECCV_2018_paper|Eigen and Fergus [4] design a deep CNN that takes RGB, depth, surface normal as input to predict the semantic labels.|
|||Owing to the power of CNN models, other methods [41, 49, 50] are proposed to better leverage depth for semantic labeling recently.|
|||[51] use a CNN following by a hierarchical CRF to jointly predict semantic labels and depth.|
|||Another sharing approach [18] applying dense connections between each layer in a CNN is proposed for recognition tasks.|
|||3.2 Network Architecture  The proposed synergy network is a multi-task deep CNN that mainly consists of four parts: the depth prediction sub-network, semantic labeling sub-network, knowledge sharing unit/connection, and the attention-driven loss.|
|||Garg, R., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation:  Geometry to the rescue.|
||6 instances in total. (in eccv2018)|
|702|cvpr18-DeblurGAN  Blind Motion Deblurring Using Conditional Adversarial Networks|[36] use CNN to estimate blur kernel, Chakrabarti [6] predicts complex Fourier coefficients of motion kernel to perform non-blind deblurring in Fourier space whereas Gong [9] use fully convolutional network to move for motion flow estimation.|
|||All of these approaches use CNN to estimate the unknown blur function.|
|||Recently, a kernel-free end-to-end approaches by Noorozi [27] and Nah [25] that uses multi-scale CNN to directly deblur the image.|
|||Perceptual loss is a simple L2-loss, but based on the difference of the generated and target image CNN feature maps.|
|||Network architecture  Generator CNN architecture is shown in Figure 3.|
|||It can handle blur caused by camera shake and object movement, does not suffer from usual artifacts in kernel estimation methods and at the same time has more than 6x fewer parameters comparing to Multi-scale CNN , which heavily speeds up the inference.|
||6 instances in total. (in cvpr2018)|
|703|Workman_Wide-Area_Image_Geolocalization_ICCV_2015_paper|Our work makes the following main contributions: (1) an extensive evaluation of off-the-shelf CNN network architectures and target label spaces for the problem of crossview localization; (2) cross-view training for learning a joint semantic feature space from different image sources; (3) a massive new dataset with multi-scale aerial imagery; (4) state-of-the-art performance on two smaller-scale evaluation benchmarks for cross-view geolocalization; and (5) extensive qualitative evaluation, including visualizations,  3961  Figure 2: Existing CNNs trained on ground-level imagery provide high-level semantic representations which can be location dependent.|
|||[26] apply a siamese CNN architecture for learning a joint feature representation between ground-level images and 45 oblique aerial imagery.|
|||The end result of cross-view training is a CNN that is able to extract semantically meaningful features from aerial images without manually specifying semantic labels.|
|||Localization using Off(cid:173)The(cid:173)Shelf CNN Fea(cid:173)  tures  As a baseline to our cross-view training approach, we evaluated the localization performance of off-the-shelf CNN features on Charleston.|
|||Figure 6 shows a comparison of our multi-scale approach versus our single-scale approach and a recent  1http://cs.uky.edu/ scott/  3965  1  2  3  4  5  Localization Threshold (% of Candidate Images)  Figure 5: Comparison of several off-the-shelf CNN features in terms of localization accuracy on the Charleston dataset.|
|||The result is a coarse-resolution false-color image that summarizes the semantic information extracted by a particular CNN from the aerial images.|
||6 instances in total. (in iccv2015)|
|704|Singh_Online_Real-Time_Multiple_ICCV_2017_paper|[28] use a supervised region proposal generation approach [30], and eliminate the need for multi-stage training [6] by using a single end-to-end trainable CNN for action classification and bounding box regression.|
|||Gkioxari and Malik [7], in particular, have built on [6] and [37] to tackle spatial action localisation in temporally trimmed videos only, using Selective-Search region proposals, fine-tuned CNN features and a set of one-vs-rest SVMs.|
|||[60], for example, accelerate the twostream CNN architecture of [37], performing action classification at 400 frames per second.|
|||As in prior work in action localisation [33, 7, 53], we use a two-stream CNN approach [37] in which optical flow and appearance are processed in two parallel, distinct streams.|
|||The architecture unifies a number of functionalities in single CNN which are, in other action and object detectors, performed by separate components [7, 53, 30, 33], namely: (i) region proposal generation, (ii) bounding box prediction and (iii) estimation of class-specific confidence scores for the predicted boxes.|
|||Real-time capabilities can be achieved by either not using optical flow (using only appearance (A) stream on one GPU) or by computing real-time optical flow [16] on a CPU in parallel with two CNN forward passes on two GPUs.|
||6 instances in total. (in iccv2017)|
|705|cvpr18-Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition|[16] transformed the skeletons into a series of color images and fed them into the CNN architecture to classify action category.|
|||[20] employed a two-stream CNN architecture to combine the position and velocity information of human joints.|
|||For t = 1, 2, ..., T , we concatenate gt in the time axis and obtain a feature map G for the input video:  G = concat[g1, g2, .., gT ]  (3)  where G is a 3D tensor, which is finally sent into a conventional CNN for action recognition.|
|||Then, Sa is processed by a CNN of 3 convolutional layers with the kernel size 3  3 and a fully connected layer (fc1), while Sb is passed through fc2.|
|||Year 2015 2015 2016 2016 2017 2017 2017 2017 2017 2017 2017 2017  Method Dynamic Skeletons [43] HBRNN-L [24] Part-aware LSTM [22] ST-LSTM+Trust Gate [51] STA-LSTM [13] LieNet-3Blocks [21] Two-Stream RNN [19] Clips+CNN+MTLN [15] VA-LSTM [23] View invariant [16] Two-Stream CNN [20] LSTM-CNN [52]  Ours-CNN Ours-GCNN Ours-DPRL Ours-DPRL+GCNN1 Ours-DPRL+GCNN2 Ours-DPRL+GCNN  CS 60.2 59.1 62.9 69.2 73.4 61.4 71.3 79.6 79.2 80.0 83.2 82.9  79.7 81.1 82.3  82.5 82.8 83.5  CV 65.2 64.0 70.3 77.7 81.2 67.0 79.5 84.8 87.7 87.2 89.3 91.0  84.9 87.0 87.7  88.1 88.9 89.8  4.2.|
|||This is because [52] combines 3 LSTM and 7 CNN to reach the higher performance, while our model only requires training the two CNN-based models and is easier to implement.|
||6 instances in total. (in cvpr2018)|
|706|cvpr18-Learning From Millions of 3D Scans for Large-Scale 3D Face Recognition|We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities.|
|||Although this modality in face recognition is gaining popularity [2, 4, 8, 17, 18, 32, 35], literature survey shows that there is no deep CNN designed specifically for 3D face recognition.|
|||Through extensive experiments, we show how existing methods and CNN models perform on this large scale dataset.|
|||The literature contains a variety of  state-of-the-art deep CNN architectures for 2D face recognition [23, 41, 49, 52].|
|||(3) Deep 3D Face Recognition Network (FR3DNet): We propose the first ever deep CNN designed specifically for 3D face recognition and trained on 3.1M 3D faces.|
|||A purpose designed 3D face recognition CNN is proposed and trained from scratch on this dataset.|
||6 instances in total. (in cvpr2018)|
|707|Kolkin_Training_Deep_Networks_ICCV_2017_paper|As a secondary contribution, we propose a memory-efficient CNN architecture which is capable of producing high resolution pixel-wise predictions, taking full advantage of the spatial information provided by our proposed loss.|
|||Zoomout features are extracted from a CNN by upsampling and downsampling the features computed by each convolutional filter to be the same spatial resolution, then concatenating the features computed at all layers of the CNN.|
|||As the base CNN we use VGG16 [29] for saliency, portraits, and distractors.|
|||The base CNN is kept fixed (not fine-tuned) in the first two stages.|
|||In the third stage we set the learning rate to 1e-5 for 14 epochs, fine-tuning the weights of the base CNN as well.|
|||Visual saliency detection based on multiscale deep cnn features.|
||6 instances in total. (in iccv2017)|
|708|cvpr18-Facial Expression Recognition by De-Expression Residue Learning|There are existing work that combine CNN and cGANs for many applications, including face generation [7], object reconstruction from edge maps [11], and object attributes manipulation [24].|
|||For each local CNN model, the cost function is noted as lossi, i  [1, 2, 3, 4].|
|||Note that all these methods except the IACNN [21], CNN baseline and our DeRL method exploited temporal information extracted from image sequences.|
|||As we can see in Table.5, our proposed method outperforms the CNN baseline when both training and testing are done on BP4D+.|
|||The experiment result shows that the performance on cross-database validation is lower than the same-database validation, nevertheless, it is still comparable with the CNN baseline.|
|||Without exploiting the temporal information, the DeRL method outperforms the baseline CNN methods, the state-of-the-art image-based methods, and even most of the state-of-the-art sequencebased methods that utilize the spatio-temporal information.|
||6 instances in total. (in cvpr2018)|
|709|Multi-Level Attention Networks for Visual Question Answering|Specifically, these works extract global image representation from a pre-trained CNN model and extract question representation from a RNN model.|
|||Specifically, a CNN-based recognizer is trained for each concept, and the distribution over the semantic output layer in CNN constitutes the high-level representation of an image.|
|||Local region representations are first extracted from convolutional layers in CNN and further fed into a bidirectional RNN model by a pre-defined order.|
|||Component (C) is designed to incorporate information from differentlevel layers in the CNN by joint attention learning.|
|||We use the fine-tuned CNN model for concept detection from  4712  sual feature for each region, which overcomes the scale inconsistency problem in multi-modal feature pooling.|
|||We take the feature map of the last convolutional layer in the CNN model as our visual representation, which can preserve complete spatial information of each region.|
||6 instances in total. (in cvpr2017)|
|710|Cao_Pedestrian_Detection_Inspired_CVPR_2016_paper|Recently, the methods based on CNN have achieved very good performance [6, 20, 30, 31, 34].|
|||Though the methods based CNN can achieve the best performance, it needs the relatively expensive device (i.e., GPU).|
|||For example, by combining the simple local features (e.g, ACF [9], Checkerboards [37], and LDCF [21]) and very deep CNN features (e.g., VGG [29] and AlexNet [18]) , Cai et al.|
|||Though the proposed non-neighboring and neighboring features are much simpler than those in CNN and Checkerboards, they result in better detection results.|
|||According to whether using CNN or not, Fig.|
|||The log-average miss rates and frames per second of the methods without CNN are visualized in Fig.|
||6 instances in total. (in cvpr2016)|
|711|End-To-End Representation Learning for Correlation Filter Based Tracking|However, in the aforementioned works, the CF is simply applied on top of pre-trained CNN features, without any deep integration of the two methods.|
|||The key step in achieving such integration is to interpret the CF as a differentiable CNN layer, so that errors can be propagated through the CF back to the CNN features.|
|||Moreover, we demonstrate the practical utility of our approach in training CNN architectures endto-end.|
|||Both inputs are processed by a CNN f with learnable parameters .|
|||Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [13, 6, 22, 3] with CNN features, as proposed in previous work [21, 7].|
|||Learning by tracking: Siamese CNN for robust target association.|
||6 instances in total. (in cvpr2017)|
|712|Yuan_Temporal_Action_Localization_CVPR_2016_paper|The FV from motions and average pooled CNN features from scenes are fused to give the detection labels on video segments.|
|||The CNN [19] has been dominating in image classification [15, 29], and its intermediate features generated by hidden layers have been widely used in object detection, segmentation and saliency applications.|
|||[28] proposed a two-stream CNN for action recognition in videos.|
|||[13] proposed various CNN fusion models for large-scale video classification.|
|||[39] proposed the latent concept descriptors (LCD), combined with Fisher / VLAD encoding on differ ent layers of CNN intermediates, which gives comparative performance with IDT features.|
|||Future Studies  ing technique based on CNN as enhancement of trajectories, and achieve best performance in action classification in THUMOS15.|
||6 instances in total. (in cvpr2016)|
|713|Jung_Joint_Fine-Tuning_in_ICCV_2015_paper|Deep Learning(cid:173)Based Method  Typically, a CNN uses a single image, but CNN can also be used for temporal recognition problems, such as action recognition [15].|
|||In this 3D CNN method, the filters are shared along the time axis.|
|||Deep Temporal Appearance Network  In this paper, a CNN is used for capturing temporal changes of appearance.|
|||Conventional CNN uses still images as input, and 3D CNN was presented recently for dealing with image sequences.|
|||As mentioned in Section 2, the 3D CNN method shares the 3D filters along the time axis [15].|
|||Figure 3 demonstrates the filters learned by a single frame-based CNN in the first convolutional layers using the CK+ database.|
||6 instances in total. (in iccv2015)|
|714|cvpr18-Density-Aware Single Image De-Raining Using a Multi-Stream Dense Network|Unlike using two priors to characterize different rain-density conditions [26], the rain-density label estimated from a CNN classifier is used for guiding the de-raining process.|
|||This is mainly due to the fact that high-level features (deeper part)  697  of a CNN tend to pay more attention to localize the discriminative objects in the input image [46].|
|||Motivated by the observation that CNN feature-based loss can better improve the semantic edge information [15, 17] and to further enhance the visual quality of the estimated de-rained image [41], we also leverage a weighted combination of pixelwise Euclidean loss and the feature-based loss.|
|||The loss for training the multi-stream densely connected network is as follows  L = LE,r + LE,d + F LF ,  (4) where LE,d represents the per-pixel Euclidean loss function to reconstruct the de-rained image and LF is the featurebased loss for the de-rained image, defined as  LF =  1  CW H  kF (x)c,w,h  F (x)c,w,hk2 2,  (5)  where F represents a non-linear CNN transformation and x is the recovered de-rained image.|
|||Input  DSC [21] (ICCV15) GMM [19] (CVPR16) CNN [6] (TIP17)  JORDER [36] (CVPR17) DDN [7] (CVPR17)  JBO [47] (ICCV17)  DID-MDN  Test1  0.7781/21.15  0.7896/21.44  0.8352/22.75  0.8422/22.07  0.8622/24.32  0.8978/ 27.33  0.8522/23.05  0.9087/ 27.95  Test2  0.7695/19.31  0.7825/20.08  0.8105/20.66  0.8289/19.73  0.8405/22.26  0.8851/25.63  0.8356/22.45  0.9092/ 26.0745  Heavy  Medium  Light  Figure 5: Samples synthetic images in three different conditions.|
|||DSC  GMM CNN (GPU)  JORDER (GPU) DDN (GPU)  JBO (CPU) DID-MDN (GPU)  512X512  189.3s  674.8s  2.8s  600.6s  0.3s  1.4s  0.2s  work (DID-MDN) for jointly rain-density estimation and deraining.|
||6 instances in total. (in cvpr2018)|
|715|XU_YANG_Shuffle-Then-Assemble_Learning_Object-Agnostic_ECCV_2018_paper|A plausible way is to append additional conv-layers to the original base CNN (e.g., VGG16 [44] or ResNet-150 [19]) to remove the object-sensitive responses inherited from image classification pre-training dataset (e.g., ImageNet [8]).|
|||The reason resides in the base CNN feature map.|
|||We directly use RoI features which extracted from the base CNN for  relationship prediction task.|
|||For example, in the  Shuffle-Then-Assemble  13  Table 3: Computed overlap ratios (%) of two kinds of feature maps  Dataset OA Base CNN Dataset OA Base CNN  VRD 50.27  42.45  VG 48.50  41.32  second row, STAs feature maps put more attention on peoples feet, which would provide cues for predicting the right relationship stand on.|
|||We compare the ratios computed by OA feature and Base CNN feature on both VRD and VG datasets.|
|||Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image  captioning.|
||6 instances in total. (in eccv2018)|
|716|Zhang_Weakly_Supervised_Semantic_2015_CVPR_paper|We look on each image as a document, and consider each component of the learned CNN features d as a word wj (j=1,...,4096).|
|||Superpixel-Level Potential Similar with image-level potential, we also extract 4096+K dimensional features for each superpixel by simultaneously employing the CNN appearance model and latent semantic concept model, which helps to narrow the semantic gap and to alleviate the impact of noisy training image-level labels.|
|||Let xp = [ap; cp] be the feature vector concatenating the CNN feature and latent semantic concept distribution extracted from the superpixels.|
|||The model parameters a for CNN features and c for LSC features can be learned via iteratively solving the optimization problem in an alternating manner: 1) Fix  Figure 4. level labels in a flip-flop manner.|
|||We employ the publicly available implementation Caffe [9] to compute the CNN features.|
|||We represent each superpixel by its CNN feature and latent semantic concept distribution, which are extracted in the same way as the image global features .|
||6 instances in total. (in cvpr2015)|
|717|Keisuke_Tateno_Distortion-Aware_Convolutional_Filters_ECCV_2018_paper|Deformation of the Convolutional Unit Approaches to deform the shape of the convolutional operator to improve the receptive field of a CNN have been recently ex 4  K. tateno and N. Navab and F. Tombari  Receptive field of the kernels  Input feature map   Output feature map   Fig.|
|||propose a method to learn specific convolution kernel along each horizontal scanline so to adapt a CNN trained on perspective images to the equirectangular domain [29].|
|||3 Distortion-aware CNN for depth prediction  In this section, we formulate the proposed distortion-aware convolution operator.|
|||3.3 CNN architecture for dense prediction task  In general, the distortion-aware convolution operator can be applied to any type of CNN architecture by replacing the standard convolutional operator.|
|||Finally we show the generalization of our distortion-aware convolution to a different task (i.e., panoramic style transfer) and a different CNN architecture (i.e., VGG).|
|||This is due to the limited field of view of each image of the cube map, which limits the receptive field of the CNN on such regions.|
||6 instances in total. (in eccv2018)|
|718|Zoumpourlis_Non-Linear_Convolution_Filters_ICCV_2017_paper|This fact has been ignored by most of the CNN implementations so far, as they have settled to the linear type of convolution filters, trying to apply quantitative rather than qualitative changes in their functionalities.|
|||For this reason, we restrain ourselves to testing such filters only in the first convolutional layer of a CNN model.|
|||To evaluate the impact of applying the Volterra-based convolution on each dataset, we tested two versions of the general CNN model.|
|||Figure 1: Structure of the proposed CNN model (left) and a typical convolutional block (right).|
|||The exploration of CNN architectures that are optimized for using non-linear convolution filters, is an open problem  4767  e s n o p s e R  0.08  0.06  0.04  0.02  0  -0.02  e s n o p s e R  0.8  0.6  0.4  0.2  0  0.1  0.05  0  e s n o p s e R  -0.05  e s n o p s e R  0.3  0.2  0.1  0  -0.1  -0.2  y  y  y  y  1  2  3  4  0.5  1  1.5  2  0.5  1  1.5  2  0.5  1  1.5  2  Patch norm  Patch norm  Patch norm  e s n o p s e R  0.5  0.4  0.3  0.2  0.1  0  0.12  0.1  e s n o p s e R  0.08  0.06  0.04  0.02  0  0.5  1  1.5  2  0.5  1  1.5  2  0.5  1  1.5  2  Patch norm  Patch norm  Patch norm  Figure 5: Various cases of responses.|
|||Based on the research results of neuroscience that prove the existence of non-linearities in the response profiles of complex visual cells, we have proposed a non-linear convolution scheme that can be used in CNN architectures.|
||6 instances in total. (in iccv2017)|
|719|Non-Local Deep Features for Salient Object Detection|To achieve state-of-the-art performance, the top performing CNN models require nontrivial steps such as generating object proposals, applying post-processing, enforcing smoothness through the use of superpixels or defining complex network architectures, all the while making predictions far slower than real-time.|
|||In this paper, we show that the overarching objectives of state-of-the-art CNN models (enforcing spatial coherence of the predicted saliency map and using both the local and global features in the optimization) can be achieved with a much simplified non-local deep feature (NLDF) model.|
|||Although some traditional AI methods such as SVM [41] perform well, deep learning methods, specifically CNN models, have raised the bar and imposed themselves as the unavoidable standard.|
|||Since superpixels can be inaccurate, some methods [43] use several object proposals which they combine afterwards while others use more than one CNN stream [26, 50].|
|||Our CNN model ensures that the output has the right size while capturing the local and global context as well as features at various resolutions.|
|||To achieve this goal, we have implemented a novel grid-like CNN network containing 5 columns and 4 rows.|
||6 instances in total. (in cvpr2017)|
|720|Papandreou_Modeling_Local_and_2015_CVPR_paper|[32], however, recent works on combining convolutional networks [39], or sliding window detectors with CNN features [13, 33, 44] still lag substantially behind the current state-of-the-art techniques [11, 15, 30] that use region proposals delivered, e.g.|
|||Aspect ratio variability and search Sliding window detectors have typically substantially lower performance than region-based systems, even when trained with CNN features [13,33,44]; e.g.|
|||Detection mean Average Precision (%) on the PASCAL VOC 2007 test set, using the proposed CNN sliding window detector that performs explicit position, scale, and aspect ratio search.|
|||We compare to the RCNN architecture of [11] End-to-End (E2E) trained DPMs of [44], the DPM with CNN features (CNN-DPM) of [33] and the Max-Pooled (MP) DPM of [13].|
|||Compared to the best sliding window detectors that employ CNN features (Rows 4-6), we have a substantially better performance: we score 11.7 points higher than the best mAP result of 46.9% reported in [44] which can be attributed to our explicit search over aspect ratios, and the use of a more powerful DCNN classifier.|
|||Deformable part models with cnn features.|
||6 instances in total. (in cvpr2015)|
|721|Simon_Generalized_Orderless_Pooling_ICCV_2017_paper|Generalized orderless pooling performs implicit salient matching  Marcel Simon1, Yang Gao2, Trevor Darrell2, Joachim Denzler1, Erik Rodner3  1 Computer Vision Group, University of Jena, Germany  2 EECS, UC Berkeley, USA  3 Corporate Research and Technology, Carl Zeiss AG  {marcel.simon, joachim.denzler}@uni-jena.de {yg, trevor}@eecs.berkeley.edu  Abstract  Most recent CNN architectures use average pooling as a final feature encoding step.|
|||While bilinear pooling shows the largest benefits in fine-grained applications, average pooling is the most commonly chosen final pooling step in literally all state-of-the-art CNN architectures.|
|||In a common CNN like VGG16, f corresponds to the first part of a CNN up to the last convolutional layer, g are two fully connected layers and C is the final classifier.|
|||Special cases Average pooling is a common final feature encoding step in most state-of-the-art CNN architectures like ResNet [15] or Inception [27].|
|||The combination [19] of CNN feature maps and bilinear pooling [28, 5] is one of the current state-of-the-art approaches in the fine-grained area.|
|||Bilinear CNN models for fine-grained visual recognition.|
||6 instances in total. (in iccv2017)|
|722|cvpr18-Viewpoint-Aware Attentive Multi-View Inference for Vehicle Re-Identification|Learning F () for extracting vehicles single-view features is first addressed by training a deep CNN using vehicles attribute labels.|
|||3.2.1 Vehicle Feature Learning  The F Net is built with a deep CNN module for learning vehicles intrinsic features containing vehicles model, color and type information.|
|||Baselines  Single-view feat  Single-view feat + LReid  Multi-view feat  Multi-view feat + LReid Regular objective for Gf  41.59 No auxiliary classifiers for D 33.43 Regular CNN for Gf and Gr 42.89 34.96 50.13  l2 loss VAMI  mAP 28.64 32.59 41.94 50.13  r=1 63.52 66.21 71.51 77.03  74.26 69.54 72.95 67.92 77.03  r=5 78.69 80.63 85.69 90.82  86.51 79.34 84.66 82.60 90.82  r=20 87.13 89.86 93.66 97.16  92.55 88.46 92.82 91.48 97.16  .|
|||Regular CNN for Gf and Gr.|
|||To evaluate the advantage of the designed residual transformation module, we compare it with a regular CNN without identity mapping.|
|||The Siamese-Visual simply adopts a pairwise deep CNN for distance metric learning but does not include vehicles se mantic attributes learning.|
||6 instances in total. (in cvpr2018)|
|723|ChestX-ray8_ Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases|3, So far, all image captioning and VQA techniques in computer vision strongly depend on the ImageNet pre-trained deep CNN models which already perform very well in a large number of object classes and serves a good baseline for further model fine-tuning.|
|||In details, we tailor Deep Convolutional Neural Network (DCNN) architectures for weakly-supervised object localization, by considering large image capacity, various multi-label CNN losses and different pooling strategies.|
|||For the pathology classification and localization task, we randomly shuffled the entire dataset into three  2102  subgroups for CNN fine-tuning via Stochastic Gradient Descent (SGD): i.e.|
|||CNN Setting: Our multi-label CNN architecture is implemented using Caffe framework [21].|
|||Furthermore, the total training iterations are customized for different CNN models to prevent over-fitting.|
|||Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learnings.|
||6 instances in total. (in cvpr2017)|
|724|Yi_Learning_to_Assign_CVPR_2016_paper|We minimize a loss function  2 2  ,  (1)  Pi Li over the parameters W of a CNN, with L (pi) =(cid:13)(cid:13)g(p1 i ))(cid:13)(cid:13)  i ))  g(p2  i , fW(p1  i , fW(p2  i , p2 where Li = L (pi), the pairs pi = {p1 i } are pairs of image patches from the training set, fW(p i ) denotes the orientation computed for image patch p i using a CNN with i ) is the descriptor for patch p parameters W, and g(p i and orientation  i .|
|||Directly predicting an angle with a CNN did not work well in our early experiments, probably because the periodicity of fW(p i ) in Eq.|
|||To alleviate the problem of periodicity, similarly to how manifolds are embed in [30, 31], we train a CNN fW(.)|
|||to predict two values, which can be seen as a scaled cosine and sine, and compute an angle by taking:  fW(p  i ) = arctan2( f (1)  W (p  i ), f (2)  W (p  i )) ,  (2)  109  W (p  where f (1) i ) and f (2) by the CNN for patch p quadrant inverse tangent function2.|
|||W (p  i ) are the two values returned i , and arctan2(y, x) is the four This function is not defined at the origin, which turned out to be a problem only happening in rare occasions at the first iteration, after random initialization of the CNN parameters W. To prevent this, we use the following approximation for its gradient:  arctan2(y, x) =(cid:18)  y  x2 + y2 + o  ,  x  x2 + y2 + o(cid:19) ,  (3)  where o is a very small value.|
|||g,i ,i  g,i ,i  To implement the CNN fW(.|
||6 instances in total. (in cvpr2016)|
|725|Meyers_Im2Calories_Towards_an_ICCV_2015_paper|To train a classifier for this problem, we took the GoogLeNet CNN model from [34], which had been pretrained on ImageNet, removed the final 1000-way softmax, and replaced it with a single logistic node.|
|||We took the GoogLeNet CNN model from [34], which had been pre-trained on ImageNet, removed the final 1000way softmax, replaced it with a 101-way softmax, and finetuned the model on the Food101 dataset [3].|
|||To tackle the segmentation problem, we use the DeepLab system from [6].10 This model uses a CNN to provide the unary potentials of a CRF, and a fully connected graph to perform edge-sensitive label smoothing (as in bilateral filtering).|
|||We first predict the distance of every pixel from the camera, using the same CNN architecture as in [12] applied to a single RGB image.|
|||We see that the CNN is able to predict the depth map fairly well, albeit at a low spatial resolution of 75 x 55.|
|||We see that for most of the meals, our CNN volume predictor is quite accurate.|
||6 instances in total. (in iccv2015)|
|726|Pinheiro_From_Image-Level_to_2015_CVPR_paper|Instead, a CNN learns the features: the model is trained through a cost function which casts the problem of segmentation into the problem of finding pixellevel labels from image-level labels.|
|||In a recent yet unpublished work [18], the authors adapt an Imagenet-trained CNN to the Pascal VOC classification task.|
|||As shown in Figure 2, our CNN is quite standard, with 10 levels of convolutions and (optional) pooling.|
|||Note that this aggregation method is equivalent as applying a traditional fully-connected classification CNN with a mini-batch.|
|||Indeed, each value in the ho  wo output plane corresponds to the output of the CNN fed with a sub-patch centered around the correspond pixel in the input plane.|
|||As shown in [15, 19], one can efficiently retrieve the label of all pixels of the image using a CNN model, by simply shifting the input image in both spatial directions, and forwarding it again through the network.|
||6 instances in total. (in cvpr2015)|
|727|Tsai_Learning_Robust_Visual-Semantic_ICCV_2017_paper|[9] constructed a deep model that took visual embeddings extracted by CNN [19] and word embeddings as input, and trained the model with the objective that the visual and word embeddings of the same class should be well aligned under linear transformations.|
|||For example, DeViSE [9] designed fv() as a CNN model followed by a linear transformation matrix.|
|||We choose GoogLeNet [43] as the CNN model in DeViSE, CMT, and our architecture.|
|||(a) Original CNN features (b) Reconstructed features (c) Visual codes for AwA dataset in ReViSE under transductive zero-shot setting.|
|||Conclusion  h  Figure 5 further shows the t-SNE [26] visualization for the original CNN features, the reconstructed visual features rv( v(te)), and the visual codes  v(te) on AwA dataset with glo attributes under transductive zero-shot setting.|
|||For example, leopard images (green dots) are near humpback whale images (light purple dots) in the original CNN feature space.|
||6 instances in total. (in iccv2017)|
|728|Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper|On top of the CNN features, a Long Short-Term Memory (LSTM) unit models shortrange temporal dynamics.|
|||[90] in CNN feature transferability.|
|||we maximize log 2(c  t is the true class at step t.  t |s1:t; g, h, c), where c  maximizes its total reward R = PT  The parameters {g, w} of CNN and RTA are learned so that the agent t=1 rt, where rt has defined in Eq.|
|||We show the top-1 re-identification accuracy on DPI-T when the bottom CNN layers are frozen (left) and slowly  12  N. Karianakis, Z. Liu, Y. Chen and S. Soatto  Table 1.|
|||A 3D CNN with average pooling over time [8] and the gait energy volume [70] are evaluated in multi-shot mode.|
|||Following the official protocol, we use the Training IDs to perform RGB-toDepth transfer for our CNN embedding.|
||6 instances in total. (in eccv2018)|
|729|cvpr18-Learning Steerable Filters for Rotation Equivariant CNNs|Contribution  We propose a rotation-equivariant CNN architecture which shares weights over filter orientations to improve generalization and to reduce sample complexity.|
|||Compared to a conventional CNN which independently learns filters in  orientations in a rotation-invariant recognition task, a corresponding SFCNN consumes  times less parameters to extract the same representation.|
|||Right: Rotational generalization capabilities of a conventional CNN and a SFCNN with  = 16 using different data augmentation strategies.|
|||Specifically, we take the the first 12000 samples of the conventional MNIST dataset to train a SFCNN with  = 16 as well as a conventional CNN of comparable size using either no augmentation, augmentation by rotations which are multiples of either  2 or augmentation by rotations which are densely sampled from [0, 2).|
|||One can see that, lacking rotational equivariance, the conventional CNN does not generalize well over orientations.|
|||Conclusion  We have developed a rotation-equivariant CNN whose filters are learned such that they are steerable.|
||6 instances in total. (in cvpr2018)|
|730|Molchanov_Online_Detection_and_CVPR_2016_paper|Building on the recent success of CNN classifiers for gesture recognition, we propose a network that employs a recurrent three dimensional (3D)-CNN with connectionist temporal classification (CTC) [10].|
|||Similarly, to employ the pretrained CNN with two-channel inputs (e.g., optical flow), we remove the third channel of each kernel and rescale the first two by a factor of 1.5.|
|||Method  Modality  Accuracy  HOG+HOG2 [27] Spatial stream CNN [34] iDT-HOG [39] C3D [37] Ours  HOG+HOG2 [27] SNV [42] C3D [37] Ours  color color color color color  depth depth depth depth  iDT-HOF [39] Temporal stream CNN [34] iDT-MBH [39] Ours  opt flow opt flow opt flow opt flow  HOG+HOG2 [27] Two-stream CNNs [34] iDT [39] Ours  color + depth color + opt flow color + opt flow all  Human  color  24.5% 54.6% 59.1% 69.3% 74.1%  36.3% 70.7% 78.8% 80.3%  61.8% 68.0% 76.8% 77.8%  36.9% 65.6% 73.4% 83.8%  88.4%  Table 3: Comparison of 2D-CNN and 3D-CNN trained with different architectures on depth or color data.|
|||Clip-wise C3D [37]  R3DCNN  Modality  Color Depth  fc  69.3% 78.8%  fc  rnn  73.0% 74.1% 79.9% 80.1%  network is absent, i.e., No RNN in Table 3, the CNN is directly connected to the softmax layer, and the network is trained with a negative log-likelihood cost function.|
|||Notice also that adding global temporal modeling via RNN into the classifier generally improves accuracy, and the best accuracy for all sensors is obtained with the CTC cost function, regardless of the type of CNN employed.|
|||This technique has been shown to provide little or no improvement when training from a CNN from scratch [11].|
||6 instances in total. (in cvpr2016)|
|731|L2-Net_ Deep Learning of Discriminative Patch Descriptor in Euclidean Space|Moreover, the generalization of the CNN based descriptors to other datasets(e.g., Oxford dataset [18]) does not show overwhelming superiority to handcrafted descriptors.|
|||The proposed L2-Net is a CNN based model without metric learning layers, and it outputs 128 dimensional descriptors, which can be directly matched by L2 distance.|
|||The proposed network is very powerful although not very deep, it achieves state-of-the-art performance on several standard benchmark datasets, receiving significant improvement over previous descriptors and even surpassing those CNN models with metric learning layers.|
|||Since the purpose of this paper is descriptor learning, below we give a brief review of descriptor learning methods in the literatures, ranging from traditional methods to the recently proposed CNN based methods.|
|||It significantly improves previous results, showing a great potential of CNN in descriptor learning.|
|||One should note that CNN models with specific learned metric are not suitable for evaluation on the Oxford dataset, as the nearest neighbor search can not be well performed using similarity score.|
||6 instances in total. (in cvpr2017)|
|732|Li_Jiang_GAL_Geometric_Adversarial_ECCV_2018_paper|Features from the 2D CNN serve as a condition to enforce predicted 3D point cloud with respect to the semantic class of the input.|
|||2)  contains a PointNet [16] to extract features of the generated and ground-truth point clouds, and a CNN takes Iin as input to extract semantic features of the object.|
|||The CNN part of the discriminator is a pre-trained classification network to extract 2D semantic features, which are then concatenated with feature produced by PointNet [16] for identifying real and fake samples.|
|||Discriminator Architecture Our discriminator D contains a CNN part to extract semantic features from the input image and a PointNet part to extract features from point cloud as shown in Fig.|
|||The backbone of the CNN part is VGG16 [18].|
|||The features from CNN and PointNet are concatenated together for final discrimination.|
||6 instances in total. (in eccv2018)|
|733|STD2P_ RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling|[5] combined the strengths of conditional random field (CRF) with CNN to refine the prediction, and thus achieved more accurate results.|
|||[27] proposed a CNN based semantic 3D mapping system for indoor scenes.|
|||They applied a SLAM system to build correspondences, and mapped semantic labels predicted from CNN to 3D point cloud data.|
|||We further aim at facilitating training from partially annotated input sequences, so that existing datasets  24838  their correspondences inside a CNN architecture.|
|||Methods  Mutex Constraints [9] RGBD R-CNN [15] Bayesian SegNet [18] Multi-Scale CNN [11] CRF-RNN [40] DeepLab [5] DeepLab-LFOV [6] BI (1000) [12] BI (3000) [12] E2S2 [4] FCN [26]  Ours (superpixel) Ours (superpixel+) Ours (full model)  Methods  Mutex Constraints [9] RGBD R-CNN [15] Bayesian SegNet [18] Multi-Scale CNN [11] CRF-RNN [40] DeepLab [5] DeepLab-LFOV [6] BI (1000) [12] BI (3000) [12] E2S2 [4] FCN [26]  Ours (superpixel) Ours (superpixel+) Ours (full model)  l l a w  65.6 68.0   70.3 67.9 70.2 62.8 61.7 56.9 69.9  70.9 72.4 72.7  n i a t r u c  47.6 29.1   34.8 32.9 36.0 29.3 30.8 38.0 32.5  34.5 42.9 40.7  r o o fl  79.2 81.3   81.5 83.0 85.2 66.8 68.1 67.8 79.4  83.4 84.3 85.7  r e s s e r d  27.6 34.8   33.2 34.3 36.9 20.3 22.9 34.8 31.0  41.6 35.9 44.1  t e n i b a c  51.9 44.9   49.6 53.1 55.3 44.2 45.2 50.0 50.3  52.6 52.0 55.4  w o l l i p  42.5 34.4   34.7 40.2 41.4 21.7 19.5 31.5 37.5  37.7 40.8 42.0  d e b  66.7 65.0   64.6 66.8 68.9 47.7 50.6 59.5 66.0  68.5 71.5 73.6  r o r r i  m  30.2 16.4   20.8 23.7 32.5 13.0 13.9 31.7 22.4  20.1 27.7 34.5  r i a h c  41.0 47.9   51.4 57.8 60.5 35.8 38.9 43.8 47.5  54.1 54.3 58.5  t a m r o o fl  32.7 28.0   24.0 15.0 16.0 18.2 16.1 25.3 13.6  15.9 31.9 35.6  a f o s  55.7 47.9   50.6 57.8 59.8 35.9 40.3 44.3 53.2  56.0 58.8 60.1  s e h t o l c  12.6 4.7   18.7 20.2 17.8 14.1 13.7 14.2 18.3  20.1 19.3 22.2  e l b a t  36.5 29.9   35.9 43.4 44.5 10.9 26.2 31.3 32.8  40.4 37.9 42.7  g n i l i e c  56.7 60.5   60.9 55.1 58.4 44.7 42.5 39.7 59.1  56.8 55.6 55.9  r o o d  20.3 20.3   24.6 19.4 25.4 18.3 20.9 24.6 22.1  25.5 28.2 30.2  s k o o b  8.9 6.4   29.5 22.1 20.5 10.9 21.3 26.7 27.3  28.8 28.2 29.8  w o d n i w  33.2 32.6   38.1 45.5 47.8 21.5 36.0 37.9 39.0  38.4 41.9 42.1  e g d i r f  21.6 14.5   31.2 30.6 45.1 21.5 16.6 27.1 27.0  23.8 38.3 41.7  f l e h s k o o b  32.6 18.1   36.0 41.5 42.6 35.9 34.4 32.7 36.1  40.9 38.5 41.9  v t  19.2 31.0   41.1 49.4 48.0 30.4 30.9 35.2 41.9  51.8 46.9 52.5  e r u t c i p  44.6 40.3   48.8 49.3 47.9 41.5 40.8 46.1 50.5  51.5 52.3 52.9  r e p a p  28.0 14.3   18.2 21.8 21.0 18.8 14.9 17.8 15.9  19.1 17.6 21.1  r e t n u o c  53.6 51.3   52.6 58.3 57.7 30.9 31.6 45.0 54.2  54.8 58.2 59.7  l e w o t  28.6 16.3   25.6 32.1 41.5 22.3 23.3 21.0 26.1  26.6 31.2 34.4  s d n i l b  49.1 42.0   47.6 47.8 52.4 47.4 48.3 51.8 45.8  47.3 49.7 46.7 n i a t r u c r e w o h s  22.9 4.2   23.0 6.4 9.4 17.7 17.8 19.9 14.1  29.3 11.0 15.5  s e v l e h s  9.1 3.5   7.6 7.3 9.1 8.5 7.9 9.1 8.6  7.5 8.1 9.4  d r a o b e t i h w  1.0 14.2   13.9 14.8 14.3 12.4 9.9 36.9 12.9  4.7 28.2 29.2  k s e d  10.8 11.3   13.2 15.5 20.7 12.8 9.3 15.8 11.9  11.3 14.3 13.5  x o b  1.6 2.1   7.4 5.8 8.0 5.5 3.3 7.4 6.5  6.8 6.5 7.8  d n a t s t h g i n  30.6 27.2   31.4 37.7 41.8 15.8 15.8 17.6 30.1  37.4 34.1 42.2  n o s r e p  9.6 0.2   57.9 55.3 67.0 45.9 44.7 35.0 57.6  66.1 66.7 60.7  t e l i o t  48.4 55.1   57.2 57.9 69.7 56.5 53.8 31.8 61.3  56.1 62.8 62.7  k n i s  41.8 37.5   45.4 47.7 46.8 32.2 32.1 36.3 44.8  46.3 47.8 47.4  p m a l  28.1 34.8   36.9 40.0 40.1 24.7 22.8 14.8 32.1  34.5 35.1 38.6  b u t h t a b  27.6 38.2   39.1 44.7 45.1 17.1 19.0 26.0 39.2  26.7 26.4 28.5  g a b  0 0.2   4.9 6.6 2.1 0.1 0.1 9.9 4.8  5.8 8.8 7.3  t c u r t s  r e h t o  9.8 7.1   14.6 18.0 20.7 12.2 12.3 14.5 15.2  12.7 19.3 18.8  i n r u f  r e h t o  7.6 6.1   9.5 12.9 12.4 6.7 5.3 9.3 7.7  12.3 10.9 15.1  s p o r p  r e h t o  24.5 23.1   29.5 33.8 33.5 21.9 23.2 20.9 30.0  30.6 29.2 31.4  .|
|||f  48.5 47.0   51.4 51.0 52.5 54.7 41.9 43.0 44.2 49.5  52.9 54.0 55.7  Methods  Mutex Constraints [9] RGBD R-CNN [15] Bayesian SegNet [18] Multi-Scale CNN [11] CRF-RNN [40] DeepLab [5] DeepLab-LFOV [6] BI (1000) [12] BI (3000) [12] E2S2 [4] FCN [26]  Ours (superpixel) Ours (superpixel+) Ours (full model)  better performance than recently proposed methods based on superpixels and CNN[12, 4].|
||6 instances in total. (in cvpr2017)|
|734|cvpr18-SurfConv  Bridging 3D and 2D Convolution for RGBD Images|Comparing to the vanilla CNN model (i.e.|
|||The smaller size of KITTI allows us to thoroughly explore different settings of SurfConv levels, influence index , as well as CNN model capacity.|
|||We study the effect of CNN model capacity across different SurfConv levels.|
|||Finetuning from an ImageNet pre-trained CNN using different importance index value  and different SurfConv levels, on the KITTI dataset.|
|||achieve significant improvement over the single-level baseline in all image-wise and surface-wise metrics, while using exactly the same CNN model (F in Eq.|
|||Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation.|
||6 instances in total. (in cvpr2018)|
|735|cvpr18-AdaDepth  Unsupervised Content Congruent Adaptation for Depth Estimation|To further improve the prediction quality, hierarchical graphical models like CRF have been combined with the CNN based super-pixel depth estimation [27].|
|||But training deep CNN models on such diverse synthetic images does not generalize directly for natural RGB scenes.|
|||Considering a deep CNN model as a transfer function from an input image to the corresponding depth, the base model can be divided into two transformations: Ms, that transforms an image to latent representation, and Ts, that transforms latent representation to the final depth prediction.|
|||The base CNN model is first trained with full supervision from the available synthetic image-depth pairs i.e.|
|||Content Congruency  In practice, a deep CNN exhibits complex output and latent feature distribution with multiple modes.|
|||Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||6 instances in total. (in cvpr2018)|
|736|Dai_Deformable_Convolutional_Networks_ICCV_2017_paper|The limitation originates from the fixed geometric structures of CNN modules: a convolution unit samples the input feature map at fixed locations; a pooling layer reduces the spatial resolution at a fixed ratio; a RoI (region-of-interest) pooling layer separates a RoI into fixed spatial bins, etc.|
|||For one example, the receptive field sizes of all activation units in the same CNN layer are the same.|
|||This is undesirable for high level CNN layers that encode the semantics over spatial locations.|
|||766  To integrate deformable ConvNets with the state-of-theart CNN architectures, we note that these architectures consist of two stages.|
|||Some works learn invariant CNN representations with respect to different types of transformations such as [45], scattering networks [2], convolutional jungles [28], and TI-pooling [29].|
|||Object detection via a multiregion & semantic segmentation-aware cnn model.|
||6 instances in total. (in iccv2017)|
|737|Gkioxari_Contextual_Action_Recognition_ICCV_2015_paper|  A using a CNN trained with stochastic gradient descent (SGD).|
|||[23] use a CNN on ground-truth boxes for the task of action classification, but observe a small gain in performance compared to previous methods.|
|||[24] modify the CNN architecture [17], which divides the image into equal sized regions and combines their scores via a final max pooling layer to classify the whole image.|
|||7  7 for the 16-layer CNN by [28]) specific to each ROI.|
|||[12] use part detectors for head, torso, legs and train a CNN on the part regions and the ground-truth box.|
|||[12] detect parts and train a CNN jointly on the whole and the parts.|
||6 instances in total. (in iccv2015)|
|738|SyncSpecCNN_ Synchronized Spectral CNN for 3D Shape Segmentation|SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation  Li Yi1  Hao Su1  Xingwen Guo2  Leonidas Guibas1  1Stanford University  2The University of Hong Kong  Abstract  In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs.|
|||To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases.|
|||Conventional image CNN can be viewed as a graph CNN on 2D regular grids of pixels, with RGB values as the vertex function.|
|||There have been some previous works studying graph CNN on more general graphs instead of 2D regular grids [3, 6, 8, 5], and [19, 1, 2] have a special focus on 3D shape graphs like human bodies.|
|||In addition we design an additional baseline using a 3D volumetric CNN architecture, denoted as Voxel CNN, which generalizes VoxNet [20] for segmentation tasks.|
|||In comparison to [2], the state of the art in the family of graph CNNs, our approach introduces spectral dilated kernel parametrization, which increases the effectiveness of spectral CNN framework.|
||6 instances in total. (in cvpr2017)|
|739|Zhai_Detecting_Vanishing_Points_CVPR_2016_paper|We propose to use a CNN to extract global image context from a single image.|
|||We use this as a foundation to create a CNN that simultaneously generates a categorical distribution for each horizonline parameter.|
|||The output of our CNN is visualized as an overlaid heatmap, with red indicating more likely locations.|
|||In the case of sampling with global image context, the offset PDF, p(o|I) (blue curve), is fit from the CNN categorical probability distribution outputs (hollow bins).|
|||To evaluate the impact of global context extraction, we considered three alternatives: our proposed approach (CNN), replacing the CNN with a random forest (using the Python sklearn library with 25 trees) applied to a GIST [23] descriptor (GISTRF), and omitting context entirely (NONE).|
|||2 show that both components play important roles in the algorithm and that CNN provides better global context information than GISTRF.|
||6 instances in total. (in cvpr2016)|
|740|Kodirov_Unsupervised_Domain_Adaptation_ICCV_2015_paper|For the AwA dataset, we extract the OverFeat implementation of the CNN features (4,096D) [31].|
|||For the CUB dataset, we extract the same CNN features as for AwA.|
|||These 11 models differ in various aspects: (1) Features: Most reported results on the dataset-provided lowlevel features, although more recently the CNN features have been used [27, 4, 11, 2].|
|||Notations  F: features; L: low-level features; SI: side information; C: CNN features; A: attribute space; W: semantic word vector space; H: WordNet hierarchy; CS: class similarity.|
|||Note that [2] obtained better result using the CNN features (47.1% vs. 40.6%) but its result on low-level feature is much weaker than ours (19.0% vs. 28.1%).|
|||It is worth pointing out that [2] employ combined low-level and CNN features, and use more than two semantic spaces.|
||6 instances in total. (in iccv2015)|
|741|Krause_Fine-Grained_Recognition_Without_2015_CVPR_paper|We do this with a simple, yet effective heuristic: as a proxy for difference in pose we measure the cosine distance between fourth-layer convolutional (conv4) features around each bounding box, using a CNN pretrained on ILSVRC 2012 [23, 28, 39].|
|||We note that PD+DCoP, without bounding boxes during test time and without fine-tuning, is already able to out-perform a fine-tuned CNN when given the ground truth bounding box, highlighting the importance of reasoning at the level of parts and validating the effectiveness of our approach.|
|||+ft means that the CNN used to extract features after detection was fine-tuned for classification.|
|||Impact of CNN choice on variants of our method, mea suggesting that improvements to the detection model will not impact classification results much on CUB-2011.|
|||Impact of CNN Architecture.|
|||We anticipate that improving PB R-CNN and PN-DCN with better CNN architectures will again result in performance higher than our best approach due to their additional supervision.|
||6 instances in total. (in cvpr2015)|
|742|cvpr18-Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos|We argue that a simpler modification on existing CNN can overcome this challenge.|
|||In this way, CP introduces no image boundary while utilizing existing CNN layers.|
|||This module is fast, effective, and generally applicable to almost all existing CNN architectures.|
|||Panel (a) shows our static model: (1) the pre-process to project an equirectangular image I to a cubemap image I, (2) the CNN with Cube Padding (CP) to extract a saliency feature Ms, (3) the post-process to convert Ms into an equirectangular saliency map OS .|
|||To sum up, Cube Padding (CP) has following advantages: (1) applicable to most kinds of layers in CNN (2) the CP generated features are trainable to learn 360 spatial correlation across multiple cube faces, (3) CP preserves the receptive field of neurons across 360 content without the need for additional resolution.|
|||Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||6 instances in total. (in cvpr2018)|
|743|Borrowing Treasures From the Wealthy_ Deep Transfer Learning Through Selective Joint Fine-Tuning|Gabor filters [28] form an example of a linear filter bank, and the complete set of kernels from certain layers of a pretrained CNN form an example of a nonlinear filter bank.|
|||It has been shown in [27] that low-level features computed in the bottom layers of a CNN encode very rich information, which can completely reconstruct the original image.|
|||We use the 152-layer residual network with identity mappings [13] as the CNN architecture in our experiments.|
|||A deep CNN can extract low/middle/high level features at different convolutional layers [48].|
|||Bilinear cnn models for fine-grained visual recognition.|
|||Harvesting discriminative meta objects with deep cnn features for scene classification.|
||6 instances in total. (in cvpr2017)|
|744|Xiong_Recognize_Complex_Events_2015_CVPR_paper|Subsequently, a 4096-dimension CNN feature is derived for each candidate, which is then fed to SVMs to predict whether it belongs to specific object classes or not.|
|||Training Algorithms  At the training stage, the CNN of the first channel was pre-trained on ImageNet [12].|
|||For the CNN of the second channel, the weights were randomly initialized from a zero-mean normal distribution.|
|||We also compared it with Gist [24], Spatial Pyramid Matching (SPM) [13], ObjectBank [17], and CNN [12].|
|||We compare the accuracy of FCNN with the original fine-tuned CNN on these categories.|
|||For 40 out of 60 classes, our method outperforms CNN [12].|
||6 instances in total. (in cvpr2015)|
|745|cvpr18-Erase or Fill  Deep Joint Recurrent Rain Removal and Reconstruction in Videos|Then, a small twolayer CNN is built to estimate t.|
|||(9)  Single Frame CNN Extractor (CNN Extractor).|
|||The residual learning architecture [13, 32] is used for single frame CNN feature extraction.|
|||5, residual blocks are stacked to build a CNN network.|
|||We first employ a CNN to extract features of t-th frame Ot.|
|||The CNN architecture for single frame CNN feature extraction.|
||6 instances in total. (in cvpr2018)|
|746|Deep Semantic Feature Matching|We introduce a novel method for semantic matching with pre-trained CNN features which is based on convolutional feature pyramids and activation guided feature selection.|
|||[20, 33] reported that pre-trained CNN features perform similarly or even worse compared to hand-engineered features such as SIFT [34] or HOG [12, 22].|
|||[33] studied the capabilities of deep features for semantic alignment by investigating a SIFT Flow version with CNN features of a pre-trained classification network.|
|||Our approach is in line of the first mentioned research direction and uses pre-trained CNN features extracted from a classification network without any additional data or training.|
|||Proposed approach  In this chapter we present our semantic flow algorithm which is based on pre-trained CNN features and sparse MRF matching.|
|||Conclusion  We have presented a semantic matching algorithm using standard pre-trained CNN features without additional data or training.|
||6 instances in total. (in cvpr2017)|
|747|Bharath_Bhushan_Damodaran_DeepJDOT_Deep_Joint_ECCV_2018_paper|In summary, we learn jointly the embedding between the two domains and the classifier in a single CNN framework.|
|||As the deeper layers of a CNN encode both spatial and semantic information, we believe them to be more apt to describe the image content for both domains, rather than the original features that are affected by a number of factors such as illumination, pose or relative position of objects.|
|||With fixed CNN parameters (g, f ) and for every randomly drawn minibatch  (of m samples), obtain the coupling  min  (s,t)  m  Xi,j=1  ij (cid:16)kg(xs  i )  g(xt  j)k2 + tLt(cid:16)ys  i , f (g(xt  j))(cid:17)(cid:17)  (8)  using the network simplex flow algorithm.|
|||The color MNIST-M images can be easily identified by a human, however it is challenging for the CNN trained on MNIST, which is only grayscale.|
|||The poor embeddings of the target samples with StochJDOT shows the necessity of computing the ground metric (cost function) of optimal transport in the deep CNN layers.|
|||It is noted that the performance of our method depends on the capacity of the source model: if a larger CNN is used, the performance of our method could be improved further.|
||6 instances in total. (in eccv2018)|
|748|Emotion Recognition in Context|Proposed CNN Model  We propose an end-to-end model, shown in Figure 6, for simultaneously estimating the discrete categories and the continuous dimensions.|
|||Experiments and Discussion  We trained different configurations of the CNN model showed in Figure 6, with different inputs and different loss functions and evaluated the models with the testing set.|
|||The first 3 columns are results obtained with a combined loss function (Lcomb) , with CNN architectures that take as inputs just the body (B, first column), just the image (I, second columns), and the combined body and image (B + I, third column).|
|||Suffering  Average  CNN Inputs and Loss  I Lcomb 20.43  B + I  22.94  17.74  26.01  19.31  18.58  B + I Ldisc 20.03  20.04  18.95  52.59  80.48  69.17  52.81  49.06  78.48  65.42  49.32  45.38  68.82  19.71  11.30  33.25  16.93  7.30  1.87  7.88  6.60  3.59  6.04  5.15  4.94  6.28  16.85  14.60  2.98  5.35  58.99  86.27  81.09  55.21  48.65  49.23  78.54  21.96  15.25  33.57  21.25  10.31  3.08  9.01  70.83  20.92  11.11  33.16  16.25  7.67  1.84  8.42  16.28  10.04  9.56  7.81  16.39  11.77  11.29  8.94  19.29  20.13  16.44  10.00  17.60  8.33  4.91  7.26  18.21  15.35  4.17  7.42  B  20.63  21.98  18.83  54.31  82.17  74.33  54.78  48.65  74.16  21.95  11.68  33.49  18.03  9.53  2.26  8.69  12.32  8.13  11.62  7.93  5.86  9.44  18.75  15.73  6.02  10.06  25.44  22.48  28.33  24.18  Table 2: Average precision obtained per each category for the different CNN input configurations: Body(B), Image(I), Body+Image(B+I).|
|||Dimension  Valence Arousal Dominance  Average  CNN Inputs and Loss  I Lcomb 0.9  1.9  0.8  1.2  B + I  0.9  1.2  0.9  1.0  B + I Lcont  1.0  1.5  0.8  1.1  B  0.9  1.1  1.0  1.0  Table 3: Mean error rate obtained per each dimension for the different CNN input configurations: Body(B), Image(I), Body+Image(B+I).|
|||We also proposed a CNN model for the task of emotion estimation in context.|
||6 instances in total. (in cvpr2017)|
|749|Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper|We utilize CNN [31] to encode audio and visual inputs, adapt LSTM [26] to capture temporal dependencies, and apply Fully Connected (FC) network to make the final predictions.|
|||[48] propose an end-to-end Segment-based 3D CNN method (S-CNN), Zhao et al.|
|||Here, dv denotes the number of CNN visual feature maps, k is the vectorized spatial dimension of each feature map, and da denotes the dimension of audio features.|
|||6.1 Visual and Audio Representations  It has been suggested that CNN features learned from a large-scale dataset (e.g.|
|||So, we adopt pre-trained CNN models to extract features for visual segments and their corresponding audio segments.|
|||: Cnn architectures for large-scale audio classification.|
||6 instances in total. (in eccv2018)|
|750|Paisitkriangkrai_Learning_to_Rank_2015_CVPR_paper|In the deep CNN architecture, convolutional layers are placed alternatively between maxpooling and contrast normalization layers [18].|
|||For region covariance and CNN features, we apply the Gaussian RBF kernel (x, x(cid:48)) = exp((cid:107)x  x(cid:48)(cid:107)/2).|
|||On VIPeR, CUHK01 and CUHK03 data sets, we observe that both SIFT/LAB and LBP/RGB significantly outperform covariance descriptor and CNN features.|
|||We observe that CNN features perform poorer than hand-crafted lowlevel features in our experiments.|
|||Experi Figure 1: Performance comparison of base metrics with different visual features: SIFT/LAB, LBP/RGB, covariance descriptor and CNN features.|
|||We compare the performance of both algorithms with  51015202530405060708090100RankRecognition rate (%)iLIDS    OursTop (50.34%)      OursTriplet (48.81%)  SIFT/LAB (43.05%)  LBP/RGB (43.73%)  Covariance (37.97%)  CNN (42.71%)51015202530405060708090100RankRecognition rate (%)3DPeS    OursTop (53.33%)      OursTriplet (52.92%)  SIFT/LAB (45.73%)  LBP/RGB (40.42%)  Covariance (42.19%)  CNN (44.38%)510152025010203040506070RankRecognition rate (%)PRID 2011    OursTop (17.90%)      OursTriplet (16.80%)  SIFT/LAB (11.10%)  LBP/RGB (6.50%)  Covariance (9.60%)  CNN (6.30%)5101520250102030405060708090100RankRecognition rate (%)VIPeR    OursTop (45.89%)      OursTriplet (45.70%)  SIFT/LAB (34.27%)  LBP/RGB (34.05%)  Covariance (26.36%)  CNN (22.47%)5101520250102030405060708090100RankRecognition rate (%)CUHK01    OursTop (53.40%)      OursTriplet (52.97%)  SIFT/LAB (37.13%)  LBP/RGB (33.20%)  Covariance (23.15%)  CNN (25.67%)5101520250102030405060708090100RankRecognition rate (%)CUHK03    OursTop (62.10%)      OursTriplet (60.50%)  SIFT/LAB (39.00%)  LBP/RGB (53.60%)  Covariance (19.40%)  CNN (28.10%)51015202530405060708090100RankRecognition rate (%)iLIDS    Ours (Nonlin.|
||6 instances in total. (in cvpr2015)|
|751|cvpr18-Mask-Guided Contrastive Attention Model for Person Re-Identification|The contrastive attention maps are then added  to CNN features to generate body-aware and backgroundaware features, respectively.|
|||In this way, the CNN model can learn the appearance feature from the RGB channels and learn the body shape feature from the mask channel.|
|||Even in the worst case, i.e., the mask is totally wrong, the CNN model still can learn features from the RGB channels.|
|||Taking the masks as additional inputs can enhance the CNN in two aspects: 1) Mask contains human shape feature and is robust to illumination and clothing colors.|
|||2) Mask can provide apparent hints for CNN to distinguish human body and background regions in original RGB image.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||6 instances in total. (in cvpr2018)|
|752|Neural Aggregation Network for Video Face Recognition|The first one is a feature embedding module which serves as a frame-level feature extractor using a deep CNN model.|
|||It is built upon a modern deep CNN model for frame feature embedding, and becomes more powerful for video face recognition by adaptively aggregating all frames in the video into a compact vector representation.|
|||To leverage modern deep CNN networks with high-end performances, in this paper we adopt the GoogLeNet [34] with the Batch Normalization (BN) technique [16].|
|||Despite the huge success of applying deep CNN in image-based face recognition task, little attention has been drawn to CNN feature aggregation to our knowledge.|
|||After training, the CNN is fixed and we focus on analyzing the effectiveness of the neural aggregation module.|
|||Unconstrained face verification using deep cnn features.|
||6 instances in total. (in cvpr2017)|
|753|LSTM Self-Supervision for Detailed Behavior Analysis|We address challenges (i)-(iii) jointly by combining a CNN for individual postures with an LSTM for behavior sequences which train themselves without the need of annotated grasping sequences.|
|||Given weak initial candidate detections of grasping paws obtained using motion information, a CNN is trained to separate paws from clutter.|
|||Moreover, due to the absence of posture annotations we will also utilize this CNN model as an implicitly learned, initial representation of posture.|
|||Therefore, the CNN for individual postures is directly linked to a recurrent network (LSTM) for behavior, indirectly optimizing the posture representation using the surrogate task of behavior learning through sequence ordering.|
|||In addition to these positive samples we add hard negatives randomly sampled from locations around the positives to then train a CNN (AlexNet [24], trained with stochastic gradient descend with cross-entropy loss) to separate both classes.|
|||A CNN for pose representation is interlinked with an LSTM for behavior and an FCN hand detector.|
||6 instances in total. (in cvpr2017)|
|754|Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper|LRCN processes the (possibly) variable-length visual input (left) with a CNN (middle-left), whose outputs are fed into a stack of recurrent sequence models (LSTMs, middle-right), which finally produce a variable-length prediction (right).|
|||To date, CNN models for video processing have successfully considered learning of 3-D spatio-temporal filters over raw sequence data [13, 2], and learning of frame-to-frame representations which incorporate instantaneous optic flow or trajectory-based models aggregated over fixed windows or video shot segments [16, 33].|
|||which is time-invariant and independent at each timestep has the important advantage of making the expensive convolutional inference and training parallelizable over all timesteps of the input, facilitating the use of fast contemporary CNN implementations whose efficiency relies on independent batch processing, and end-to-end optimization of the visual and sequential model parameters V and W .|
|||We explore two variants of the LRCN architecture: one in which the LSTM is placed after the first fully connected layer of the CNN (LRCN-fc6) and another in which the LSTM is placed after the second fully connected layer of the CNN (LRCN-fc7).|
|||The CNN base of the LRCN is a hybrid of the Caffe [14] reference model, a minor variant of AlexNet [22], and the network used by Zeiler & Fergus [48].|
|||The strength of our temporal model (and integration of the temporal and visual models) can be more directly measured against the ConvNet [18] result, which uses the same base CNN architecture [22] pretrained on the same data.|
||6 instances in total. (in cvpr2015)|
|755|Du_RPAN_An_End-To-End_ICCV_2017_paper|At the t-th step, the video frame is fed into CNN to generate the convolutional feature cube Ct. Then, with guidance of the previous hidden state ht1 of LSTM, our pose attention mechanism learns several human-part-related features FP from Ct. As attention parameters are partially shared on the semantic-related human joints belonging to the same body part, our human-part-related features encode robust body-structure-information to discriminate complex actions.|
|||First, the current video frame is fed into CNN to generate a convolutional feature cube.|
|||For the t-th video frame (t = 1, ..., T ), we denote the convolutional cube from CNN as Ct  RK1K2dc , which consists of dc feature maps with size of K1  K2.|
|||We evaluate RPAN, based on two widely-used CNNs in action recognition, i.e., Good-Practice CNN [46] and TSN [47].|
|||Hence, we perform our RPAN, based on two widely-used CNNs in the research of action recognition, Good-Practice CNN (built on VGG16) [46] and TSN (built on BN-Inception) [47].|
|||For Good-Practice CNN [46], the convolutional feature cube is generated from the convolutional layer in the temporal-stream (the conv5 3 layer, 7  10  512 / 8  15  512 for Sub-JHMDB / PennAction).|
||6 instances in total. (in iccv2017)|
|756|cvpr18-Super SloMo  High Quality Estimation of Multiple Intermediate Frames for Video Interpolation|Specifically, we first use a flow computation CNN to estimate the bi-directional optical flow between the two input images, which is then linearly fused to approximate the required intermediate optical flow in order to warp input images.|
|||[19] consider the frame interpolation as a local convolution over the two input frames and use a CNN to learn a spatially-adaptive convolution kernel for each pixel.|
|||[15] develop a CNN model for frame interpolation that has an explicit sub-network for motion estimation.|
|||The last row shows that the refinement from our flow interpolation CNN is mainly around the motion boundaries (the whiter a pixel, the bigger the refinement).|
|||Conclusion  We have proposed an end-to-end trainable CNN that can produce as many intermediate video frames as needed between two input images.|
|||We then use a flow interpolation CNN to refine the approximated flow fields and predict soft visibility maps for interpolation.|
||6 instances in total. (in cvpr2018)|
|757|Peng_Learning_Deep_Object_ICCV_2015_paper|Introduction  Deep CNN models achieve state-of-the-art performance on object detection, but are heavily dependent on largescale training data.|
|||Extensions to detection have included sliding-window CNN [19] and Regions-CNN (RCNN) [5].|
|||Thus while the CNN representation is invariant to slight translation, rotations and deformations, it remains unclear to what extent are CNN representation to large 3D rotations.|
|||Results, shown in Table 3, point to important and surprising conclusions regarding the representational power of the CNN features.|
|||While this invariance to large and complex pose changes may be explained by the fact the CNN model was itself trained with both views of the object present, and subsequently fine-tuned with both views  1282  BG TX  RR-RR  Real RGB Real RGB  W-RR White  Real RGB  W-UG White  Unif.|
|||The CNN used in the top table is trained on ImageNet-1K classification, the CNN in the bottom table is also finetuned on PASCAL 2007 detection.|
||6 instances in total. (in iccv2015)|
|758|Wenhao_Jiang_Recurrent_Fusion_Network_ECCV_2018_paper|Usually, a pre-trained CNN for image classification is leveraged to extract image representations.|
|||In this framework, a CNN pre-trained on an image classification task is used as the encoder, while a RNN is used as the decoder to translate the information from the encoder into natural sentences.|
|||3.1 Encoder  Under the encoder-decoder framework for image captioning, a CNN pre-trained for an image classification task is usually employed as the encoder to extract the global representation and subregion representations of the input image.|
|||The global representation and subregion representations extracted from the m-th CNN are denoted as a(m) and A(m) = {a(m)  }, respectively.|
|||Please note that the encoder of Up-Down model is not a CNN pre-trained on ImageNet dataset [51].|
|||Gu, J., Wang, G., Chen, T.: Recurrent highway networks with language cnn for  image captioning.|
||6 instances in total. (in eccv2018)|
|759|IRINA_ Iris Recognition (Even) in Inaccurately Segmented Data|Both were used to train a CNN that extracts high-level texture information and distinguishes between the corresponding / noncorresponding patches.|
|||The CNN input is 4242 image patches and its architecture (Fig.|
|||The responses of the CNN enable to obtain the posterior probabilities that two iris patches regard the same biologi According to this formulation, obtaining the deformation model between a pair of images is equivalent to infer the random variables in the MRF that minimize its energy:  sparse sampling strategy reduces over 50% the number of labels without significant decreases in the method performance (leftmost part of Fig.|
|||2.2.1 Unary Costs  Let (i, j) : N2  [0, 1] be the CNN response for one pair  of patches, expressing the likelihood p!(i, j) | Ci,j" that  the ith patch of one sample corresponds to the jth patch of its counterpart.|
|||Note the significantly best performance (and smallest variance) for the CASIAIrisV3-Lamp among all sets, due to the learning data that fed the CNN (same set, yet with disjoint instances).|
|||Left plot: decision environment of the responses given by the CNN to distinguish between corresponding Ci,j and noncorresponding  Ci,j iris patches (CASIA-IrisV3-Lamp set).|
||6 instances in total. (in cvpr2017)|
|760|cvpr18-Learning Convolutional Networks for Content-Weighted Image Compression|Illustration of the CNN architecture for content-weighted image compression.|
|||Inspired by the binarized CNN [34, 18, 4], we introduce a proxy function to approximate the binary operation in backward propagation.|
|||To encode c, we modify the coding schedule, redefine the context, and use CNN for probability prediction.|
|||The CNN for convolutional entropy encoder.|
|||Instead, we introduce a CNN model for probability prediction.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||6 instances in total. (in cvpr2018)|
|761|Piotr_Koniusz_Museum_Exhibit_Identification_ECCV_2018_paper|To achieve robust baselines, we build on a recent approach that aligns per-class scatter matrices of the source and target CNN streams.|
|||Indeed, fine-tuning of CNN is a powerful domain adaptation and transfer learning tool by itself [16, 17].|
|||For the pipeline in Figure 1, we use two CNN streams of the VGG16 network [14] which correspond to the source and target domains.|
|||However, we first project class-specific vector representations from the last fc layers of the source and target CNN streams to the common space via Nystr om projections for tractability and then we combine them with the JBLD or AIRM distance to exploit the (semi)definite positive nature of scatter matrices.|
|||5 Experiments  Below we detail our CNN setup, discuss the Open MIC dataset and our evaluations.|
|||Alex VGG16 GoogLe Net Net 88.92 89.70 91.33 91.20  S+T 82.4 88.66 84.5 89.45 So JBLD 85.6 90.80 AIRM 85.2 90.72  DLID  DaNN  DeCAF6 S+T  [6] 51.9 [38] 80.7 [39] 53.6 [7] 56.5 [7] 80.5 Source+Target CNN [7] 82.5 Dom.|
||6 instances in total. (in eccv2018)|
|762|cvpr18-Deep Material-Aware Cross-Spectral Stereo Matching|The symmetric CNN in (b) prevents the STN learning disparity.|
|||[41] also proposed a symmetric filter CNN for recognition but their filters are radial symmetric while ours are reflection symmetric.|
|||1922  Method  Common  Only RGB as DPN input Averaging RGB as STN Asymmetric CNN in STN  No material awareness  Ignore light sources  Ignore glass  Ignore glossy surfaces  Smoothing w/o confidence  Full proposed method  0.66 0.52 0.53 0.51 0.54 0.56 0.63 0.53 0.53  Light Glass Glossy Vegetation 1.12 0.80 0.88 1.08 0.81 0.74 0.71 0.69 0.69  1.10 0.78 0.88 1.57 0.71 1.08 1.23 1.20 0.70  0.89 0.74 0.82 1.05 0.74 0.97 0.71 0.71 0.65  0.92 0.76 0.77 0.69 0.76 0.75 0.79 0.85 0.72  Skin Clothing Bag Mean 1.06 1.61 0.89 1.30 0.89 1.13 1.01 1.00 0.90 1.37 0.88 1.06 0.90 1.12 0.87 1.06 0.80 1.15  1.24 1.21 1.17 1.22 1.17 1.02 1.09 1.12 1.15  0.95 1.04 0.94 0.90 1.10 0.86 0.94 0.81 0.80  Table 2.|
|||Ablation Study: We have tested three network structure choices: Only RGB as DPN input, Averaging RGB as STN averaging R, G and B channels as pseudo-NIR, and Asymmetric CNN in STN.|
|||A symmetric CNN is utilized to separate geometric and spectral differences.|
|||Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||6 instances in total. (in cvpr2018)|
|763|cvpr18-Visual Question Answering With Memory-Augmented Networks|The attention mechanism [1] typically consists of a language parser and a number of CNN feature vectors representing spatially distributed regions.|
|||The co-attention mechanism computes weights for each CNN feature vector as well as each textural word  embedding (see Figure 3 the highlighted weights in different colors).|
|||We use the pre-trained VGGNet-16 [32] and ResNet-101 [12] to extract CNN features.|
|||Interestingly, the performance gains become larger with deeper CNN features and with more answers in both the multiple-choice and open-end tasks.|
|||To validate our design choices, we start with the CNN+LSTM baseline, which uses LSTM and CNN (VGG) as question and image embeddings as in [2].|
|||Accuracy and loss with or without external memory using CNN (VGGNet) image embedding with top 3000 answers for training on the train+val sets of the VQA benchmark.|
||6 instances in total. (in cvpr2018)|
|764|Nimisha_T_M_Unsupervised_Class-Specific_Deblurring_ECCV_2018_paper|Also, the underlying idea in all these networks is to use a pair of GANs to learn the domains, but usually training GANs is highly unstable [34, 8] and thus using two GANs   Blur  I(cid:269)putGe(cid:269)erator   Discri(cid:268)i(cid:269)ator CNN Gradie(cid:269)t ModuleUnsupervised Deblurring  5  simultaneously escalates in stability issues in the network.|
|||Instead of using a second GAN to learn the blur domain, we use a CNN network for reblurring the output of GAN and a gradient module to constrain the solution space.|
|||We enforce the generator to produce result (x) that when reblurred using the CNN module will furnish back the input.|
|||(b) Reblurring CNN module architecture  Module  Generator  Discriminator  Layers  conv conv conv  conv  Kernel Size 5 64  Features  5  5  128 128  5  256  conv  5  256  d/o (0.2) d/o (0.2)  conv conv conv conv conv conv conv conv conv conv  fc  128 128  5 64  4 64  128  256  512  4  512  5  5  5 64  5 3  4  4  4  (a)  Module  CNN  Layers  conv conv conv conv conv tanh  Kernel Size  Features  5 64  5 64  5 3  5 64  5 64 (b)  with slight modification in the feature layers.|
|||The reblurring CNN architecture is a simple 5-layer convolutional module provided in Table 1(b).|
|||Rengarajan, V., Balaji, Y., Rajagopalan, A.: Unrolling the shutter: Cnn to correct motion distortions.|
||6 instances in total. (in eccv2018)|
|765|Zhu_Saliency_Pattern_Detection_ICCV_2017_paper|To address this issue, we propose using SSP for regularization by integrating the global semantic information derived from a CNN model.|
|||(a) A CNN model is trained to generate a binary proposal map.|
|||(a) A binary proposal map generated by the CNN model.|
|||The second term refers to the loss towards the segmentation of CNN model.|
|||We further introduce an adaptive ranking method for SSP refinement via learning a CNN model.|
|||we plan to learn SSP from the feature maps of CNN models directly rather than from the hand-crafted features.|
||6 instances in total. (in iccv2017)|
|766|Zhou_Multi-Label_Learning_of_ICCV_2017_paper|Experiments with CNN features  Recently, several approaches using CNN features have achieved the state-of-the-art performance for pedestrian detection [3, 33, 26, 4].|
|||The proposed multi-label learning approach also applies to CNN features.|
|||RPN+BF [33] also adopts a similar framework in which a set of decision trees are learned to form a full-body detector using CNN features from the RPN.|
|||Table 3 shows the results of different approaches using CNN features.|
|||Results of different approaches using CNN features.|
|||The proposed approach is applied to channel features and CNN features and shows promising performance for detecting partially occluded pedestrians, especially the heavily occluded ones.|
||6 instances in total. (in iccv2017)|
|767|Shitala_Prasad_Using_Object_Information_ECCV_2018_paper|They trained a CNN on a synthetic dataset [8].|
|||Object-based CNN (TO-CNN).|
|||A CNN which is modified from another VGG-16 network is added on the Object VGG-16 net.|
|||This CNN is called Text VGG-16 net.|
|||3.3 Training and Implementation Details  The Object CNN and the Text CNN are initialized by VGG-16 pre-trained ImageNet classification model [38].|
|||Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic segmentation-aware cnn model.|
||6 instances in total. (in eccv2018)|
|768|Gebru_Fine-Grained_Recognition_in_ICCV_2017_paper|Classification Loss  We start with a CNN following the architecture of [36], taking {xS, yS} and {xT , yT } as inputs.|
|||Model  Adapt Attr Consist Acc (%)  Source CNN Source CNN w/att Source CNN w/att+ACL DC [49] DC [49] w/att+ACL  X  X  X  X  X  X  X  9.28 10.80 14.37 14.98 19.05  Table 2.|
|||Model  Adapt Attr Consist Acc (%)  S+T CNN S+T CNN w/att+ACL DC [49] DC [49] w/att+ACL  X  X  X  X  X  X  4.12 7.45 12.34 19.11  Table 3.|
|||Model  Adapt Attr Consist Acc (%)  Source CNN Source CNN w/att Source CNN w/att+ACL DC [49] DC [49] w/att+ACL  X  X  X  X  X  X  X  60.9 59.5 61.2 61.1 62.4  Table 4.|
|||Make: Chrysler  Model: Pt-Cruiser  Body Type: Wagon  Trims: Base, Limited, Touring  Manufacturing Year: 2006-2009  Model  Adapt Attr Consist Acc (%)  S+T CNN S+T CNN w/att+ACL DC [49] DC [49] w/att+ACL  X  X  X  X  X  X  45.5 45.3 47.0 51.8  Table 5.|
|||Conclusion  We have presented a multi-task CNN architecture for semi-supervised domain adaptation.|
||6 instances in total. (in iccv2017)|
|769|Altwaijry_Learning_to_Match_CVPR_2016_paper|shed some light on feature localization within a CNN, and determine that features in later stages of the CNN correspond to features finer than the receptive fields they cover.|
|||Matching Results  We compare our CNN models with a variety of baselines on the aerial dataset.|
|||The third set of baselines are vanilla CNN models used in a siamese fashion (without fine-tuning).|
|||We note that while VLAD surpassed the performance of these two CNN approaches, both VLAD and Fisher Vectors require training with our dataset.|
|||Our Hybrid CNN outperforms all the baselines.|
|||A variant of the Hybrid CNN was trained without the conv5 and pool5 layers, with a 1  1 convolution layer after conv4 to reduce the dimensionality of its output.|
||6 instances in total. (in cvpr2016)|
|770|Luo_ThiNet_A_Filter_ICCV_2017_paper|In this paper, we propose a unified framework, namely ThiNet (stands for Thin Net), to prune the unimportant filters to simultaneously accelerate and compress CNN models in both training and test stages with minor performance degradation.|
|||The ResNet-50 model [11] has less redundancy compared with classic CNN models.|
|||Beyond pruning, there are also other strategies to obtain small CNN models.|
|||Given an input image, we first apply the CNN model in the forward run to find the input and output of layer i + 1.|
|||ResNet(cid:173)50 on ImageNet  We also explore the performance of ThiNet on the recently proposed powerful CNN architecture: ResNet [11].|
|||Conclusion  In this paper, we proposed a unified framework, namely ThiNet, for CNN model acceleration and compression.|
||6 instances in total. (in iccv2017)|
|771|cvpr18-Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet |Representative video datasets, such as UCF101 [21] and HMDB-51 [17], can be used to provide realistic videos with sizes around 10 K, but even though they are still used as standard benchmarks, such datasets are obviously too small to be used for optimizing CNN representations from scratch.|
|||In this study, we examine various 3D CNN architectures from relatively shallow to very deep ones using the Kinetics and other popular video datasets (UCF-101, HMDB-51, and ActivityNet) in order to provide us insights for answering the above question.|
|||The 3D CNN architectures tested in this study are based on residual networks (ResNets) [10] and their extended versions [11, 12, 30, 31] because they have simple and effective structures.|
|||Method  Top-1 Top-5 Average  ResNeXt-101  ResNeXt-101 (64f)  CNN+LSTM [16]  Two-stream CNN [16]  C3D w/ BN [16]  RGB-I3D [3]  Two-stream I3D [3]      57.0  61.0  56.1  68.4  71.6      79.0  81.3  79.5  88.0  90.0  74.5  78.4  68.0  71.2  67.8  78.2  80.8  tures with ResNeXt-101 make further improvements based on higher accuracies of two-stream I3D.|
|||Here, we can see that ResNeXt101 achieved higher accuracies compared with C3D [23], P3D [19], two-stream CNN [20], and TDD [27].|
|||We can also see  Method  Dim UCF-101 HMDB-51  ResNeXt-101  ResNeXt-101 (64f)  C3D [23]  P3D [19]  Two-stream I3D [3]  Two-stream CNN [20]  TDD [27]  ST Multiplier Net [7]  TSN [29]  3D  3D  3D  3D  3D  2D  2D  2D  2D  90.7  94.5  82.3  88.6  98.0  88.0  90.3  94.2  94.2  63.8  70.2      80.7  59.4  63.2  68.9  69.4  that two-stream I3D [3], which utilizes simple two-stream 3D architectures pretrained on Kinetics, achieved the best accuracies.|
||6 instances in total. (in cvpr2018)|
|772|cvpr18-InLoc  Indoor Visual Localization With Dense Matching and View Synthesis|Given a shortlist of potentially relevant database images, we apply two progressively more discriminative geometric verification steps: (i) We use dense matching of CNN descriptors that capture spatial configurations of higher-level structures (rather than individual local features) to obtain the correspondences required for camera pose estimation.|
|||To overcome this problem, we use multi-scale dense CNN features for both image description and feature matching.|
|||Examples in figure 4 demonstrate that our dense CNN matching (4th column) obtains better matches in indoor environments when compared to matching standard local features (3rd column), even for less-textured areas.|
|||A binary representation (instead of floats) of features in the intermediate CNN layers significantly reduces memory requirements.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Bilinear cnn models for fine-grained visual recognition.|
||6 instances in total. (in cvpr2018)|
|773|Li_Leveraging_Weak_Semantic_ICCV_2017_paper|[5] fine-tune a CNN that are pre-trained  3648  on ImageNet for video event classification and evidence recounting.|
|||For each shot t, a deep CNN [16, 28] pre-trained on ImageNet is used to output a 1000-way vector pt i, which is a probability distribution over 1000 ImageNet categories.|
|||The middle frame of each shot is selected as its representative, whose feature is extracted by applying a very deep CNN architecture (from fc6 layer of VGG-19 [28]).|
|||evaluate several recent proposed very deep CNN architectures such as VGG-16, VGG-19 [28] and M2048 [4] etc., for fine-tuning.|
|||Note that, their method leverages semantic relevance from three aspects i.e., object, scene, and low-level CNN feature, each of which corresponds to a different source domain.|
|||In the meanwhile, our method is expected to be further improved by considering motion features for video shot representation, while we only use static CNN feature for model  simplicity in this work.|
||6 instances in total. (in iccv2017)|
|774|Woojae_Kim_Deep_Video_Quality_ECCV_2018_paper|Step 1: The CNN model is regressed onto a subjective score by the average pooling.|
|||A deep CNN with 33 filters is used for step 1 inspired by the recent CNN based work [3] for IQA.|
|||In step 2, the proposed CNAN is trained using the pre-trained CNN model in step 1 and regressed onto the subjective video score as shown in Fig.|
|||Once each feature is derived from the previous CNN independently, they are fed into the CNAN.|
|||Then, the spatio-temporal sensitivity map st is obtained from the CNN model of step 1 as  r|, where f t  t = |f t  d  f t  st = CN Ns1( I t  d, et  s, f t  d, et  t; s1),  (2)  where CN Ns1 is the CNN model of step 1 with parameters s1.|
|||5 Conclusion  In this paper, we proposed a novel FR-VQA framework using a CNN and a CNAN.|
||6 instances in total. (in eccv2018)|
|775|ArtTrack_ Articulated Multi-Person Tracking in the Wild|We build on recent CNN detectors [13] that are effective in localizing body joints in cluttered scenes and explore different mechanisms for assembling the joints into multiple person configurations.|
|||Left: Heatmaps for the chin (=root part) used to condition the CNN on the location of the person in the back (top) and in the front (bottom).|
|||Setting  Head Sho Elb Wri Hip Knee Ank AP CNN graph  BU-full, label 90.0 84.9 71.1 58.4 69.7 64.7 54.7 70.5 0.18 91.2 86.0 72.9 61.5 70.4 65.4 55.5 71.9 0.18 BU-full 91.1 86.5 70.7 58.1 69.7 64.7 53.8 70.6 0.18 BU-sparse  3.06 0.38 0.22  TD/BU + SP 92.2 86.1 72.8 63.0 74.0 66.2 58.4 73.3 0.947  0.08  Table 1.|
|||For each algorithm we also report run time CNN of the proposal generation and graph of the graph partitioning stages.|
|||On average we expect similar image area to be processed during proposal generation stage in both TD/BU and BU-sparse, and expect the runtimes CNN to be comparable for both models.|
|||Second, in the case of TD/BU model the graph is sparse and a large portion of the computation is performed by the feed-forward CNN introduced in Sec.|
||6 instances in total. (in cvpr2017)|
|776|cvpr18-Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes|Comparison with the state-of-the-art CNN based face hallucination methods.|
|||(d) Result of VDSR [11], which is a CNN based generic super-resolution method.|
|||[24] use a conditional CNN to generate faces based on attributes.|
|||Because CNN filters in the first layers mainly extract low-level features while filters in higher layers extract image patterns or semantic information [30], we concatenate the attribute information with the extracted feature maps on the third layer.|
|||Kim et al.s method [11] is a generic CNN based super-resolution method.|
|||[11] present a deep CNN for generic purpose super-resolution known as VDSR.|
||6 instances in total. (in cvpr2018)|
|777|Wang_Designing_Deep_Networks_2015_CVPR_paper|A final CNN considers these predictions as well as evidence from vanishing points to yield a final prediction.|
|||Similarly, instead of hand-designing an optimizable model to reason about ambiguities, we learn a CNN to arbitrate between conflicting evidence.|
|||Additionally, since there have been no published CNN results, we adapt the coarse network of the Eigen et al.|
|||This coarse network nearly matches the full systems performance on depth, and as a single feed-forward CNN with no intermediate representations or designed structures, it is a good baseline.|
|||(Higher Better) (Lower Better) Mean Median 11.25  22.5  30  42.0 61.2 68.2 26.9 39.2 62.0 71.1 23.7 35.2 40.5 54.1 58.9 27.7 49.0 58.7 33.5 39.2 52.9 57.8 36.3 35.3 16.4 36.6 48.2  14.8 15.5 17.9 23.1 19.2 31.2  Our Network Stacked CNN [7] UNFOLD [10] Discr.|
|||Table 2: Ablative Analysis  Mean Median 11.25  22.5  30  42.0 61.2 68.2 26.9 Full 34.6 57.8 66.0 28.8 Full w/o Global 40.2 60.1 67.5 27.3 Fusion (+VP) 37.5 59.4 67.4 Fusion (+Edge) 27.8 Fusion (+Layout) 27.7 38.8 59.9 67.4 37.4 59.2 67.1 27.9 Fusion 25.6 46.4 56.2 34.0 Local Global 30.9 31.4 52.3 60.5  14.8 17.7 15.6 16.4 16.0 16.6 25.1 20.8  Coarse CNN [8] 30.1  24.7  24.1 46.4 57.9  classifies a big patch on the wall near the picture frame.|
||6 instances in total. (in cvpr2015)|
|778|Incremental Kernel Null Space Discriminant Analysis for Novelty Detection|In step 3, to obtain D2, a matrix prod (a) The CNN feature space.|
|||Then we train a CNN network (i.e., Alexnet [13]) for feature extraction.|
|||Note that only the training set is utilized in the CNN training process.|
|||Results and Discussions  of  images  in  The  learned CNN features  the FounderType-200 dataset are visualized in Figure 5a, from which we can see that large variance exists between samples within the same class.|
|||We can also observe from the performance comparisons that our method along with other KDA approaches outperforms the DNN classifier in the original CNN feature space.|
|||This suggests that the original CNN features is less capable of handling novelty detection tasks without proper transformations.|
||6 instances in total. (in cvpr2017)|
|779|cvpr18-CLEAR  Cumulative LEARning for One-Shot One-Class Image Recognition|[15], who proposed a Siamese CNN architecture  that takes as an input two images and outputs the probability that both of them come from the same class.|
|||[4], where they used Bayesian approach to update weights of hidden layers in the pretrained Deep CNN to create a classifier from just a single image input (instead of regular abundance of examples required to train such network).|
|||[4] on Deep CNN hidden layers transformation (described in 2.1) is also very promising.|
|||When the amount of training data might be not enough for some classification task, two common techniques are to use a network such as [18, 31] pretrained on a large labelled dataset (frequently ImageNet [28]) and fine-tune the weights of the CNN to suit this novel recognition task, or to use the CNN as a representation (or feature extractor) and train a classifier such as a Support Vector Machine (SVM) on those extracted features, as in work by Athiwaratkun and Kang [2].|
|||Unfortunately, when the number of visual categories changes while training, the architecture of the CNN (including the number of output neurons) must change and the training process started all over again.|
|||We use the Caffe [12] GoogLeNet [31] CNN as a feature extractor.|
||6 instances in total. (in cvpr2018)|
|780|cvpr18-Multi-Cell Detection and Classification Using a Generative Convolutional Model|The SVM and CNN algorithms operate on extracted image patches of detected cells, where the cells were detected via thresholding, filtering detections by size (i.e., discarding objects that were smaller or larger than typical cells).|
|||The methods, from left to right, are: SVM on patches extracted from images via thresholding, CSC without statistical priors, CNN on patches extracted from images via thresholding, and our method.|
|||Additionally, we implemented a CNN similar to that described in [3].|
|||Note that the CNN requires much more training data than our method, which requires only a few training images.|
|||Both the SVM and CNN classifiers perform considerably  worse than our proposed method, with the SVM producing errors up to 32%.|
|||The CNN achieves slightly better performance than the SVM and standard CSC methods, but errors still reach up to 29%.|
||6 instances in total. (in cvpr2018)|
|781|cvpr18-Context Encoding for Semantic Segmentation|Recent work has made great progress in generalizing traditional encoders in a CNN framework [1, 56].|
|||introduces an Encoding Layer that integrates the entire dictionary learning and residual encoding pipeline into a single CNN layer to capture orderless representations.|
|||Context Encoding Module  We refer to the new CNN module as Context Encoding Module and the components of the module are illustrated in Figure 2.|
|||Given an input image, we first use a pre-trained CNN to extract dense convolutional featuremaps.|
|||In addition, the proposed Context Encoding Module as a simple CNN unit is compatible with all existing FCN-based approaches.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
||6 instances in total. (in cvpr2018)|
|782|Point to Set Similarity Based Deep Feature Learning for Person Re-Identification|In order to learn the feature representations from multiple perspectives, we construct an effective part-based deep CNN to extract discriminative features from different body parts of each person.|
|||Compared with the existing P2P distance based metrics, our method considers the P2S information and is more effective at improving the ranking performance; 2) An effective part-based deep CNN is constructed to extract discriminative and stable feature representations of different body parts for person Re-ID.|
|||[34] proposed a domain guided dropout algorithm to improve the performance of deep CNN to extract robust feature representation for person Re-ID.|
|||Deep Architecture  The proposed P2S metric is combined with our proposed part-based deep CNN to implement an end-to-end framework for both feature learning and fusion.|
|||Results  Table 1 lists the results on the 3DPeS dataset, in which our P2P method gets the second best performance, contributed by the part-based deep CNN architecture, and our P2S method achieves the best performance in all Top 1 to Top 20 accuracies.|
|||Conclusion  In this paper, we propose a novel person re-identification method by point to set (P2S) similarity comparison in a part-based deep CNN to perform integrated feature learning and fusion.|
||6 instances in total. (in cvpr2017)|
|783|cvpr18-Revisiting Oxford and Paris  Large-Scale Image Retrieval Benchmarking|Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods.|
|||sion performs similarity propagation by starting from the querys nearest neighbors according to the CNN global descriptor.|
|||Of course this experiment is expensive and we perform it to merely show a possible direction to improve CNN global descriptors.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations.|
|||Particular object retrieval In ICLR,  with integral max-pooling of CNN activations.|
||6 instances in total. (in cvpr2018)|
|784|Haroon_Idrees_Composition_Loss_for_ECCV_2018_paper|[29] train a CNN alternatively to predict density map and count in a patch, and then average the density map for all the overlapping patches to obtain density map for the entire image.|
|||The global and local contexts are obtained by learning to classify the input image patches into various density levels, later fused with the output of a multi-column CNN to obtain the final density map.|
|||[24], image patches are relayed to the appropriate CNN using a switching mechanism learnt during training.|
|||The independent CNN regressors are designed to have different receptive fields while the switch classifier is trained to relay the crowd scene patch to the best CNN regressor.|
|||3 Deep CNN with Composition Loss  In this section, we present the motivation for decomposing the loss of three interrelated problems of counting, density map estimation and localization, followed by details about the deep Convolutional Neural Network which can enable training and estimation of the three tasks simultaneously.|
|||It is also interesting to note that the various connections between density levels and base CNN also serve to provide intermediate supervision which aid in training the filters of base CNN towards counting and density estimation early on in the network.|
||6 instances in total. (in eccv2018)|
|785|Maire_Affinity_CNN_Learning_CVPR_2016_paper|Rather than directly focus on image segmentation, recent CNN architectures [14, 3, 28, 4] target edge detection.|
|||Pure CNN approaches for depth from a single image do focus on directly constructing the desired output [9, 8].|
|||We engineer a system for simultaneous segmentation and figure/ground organization by directly connecting a CNN to  n3  affinity C N N  nk  nk          image  pixel-centric grouping/ordering  assemble  figure-ground  pairwise complex affinity  n1  angular  nn  embedding  +i  Figure 1.|
|||Training the CNN with a target appropriate for the inference procedure eliminates the need for hand-designed intermediate stages such as edge detection.|
||| We train a CNN to directly predict all data-dependent  terms in the model.|
|||Our CNN replaces hand-designed features.|
||6 instances in total. (in cvpr2016)|
|786|Cao_Look_and_Think_ICCV_2015_paper|Visualization, Detection, and Localization  Feedback is often related with visualization of CNN and object localization since both of them aim to project the high-level semantic information back to image representations.|
|||We show that the proposed Feedback CNN has the potential to unify recognition and detection into a single network architecture in this experiment, instead of using separate ones to perform different tasks respectively.|
|||We   Resize image to size 224 224, run CNN model and  predict top 5 class labels.|
|||We foresee the potential of Feedback CNN to further improve various computer vision and machine learning tasks, such as fine-grained recognition, object detection, and multi-tasks learning.|
|||However, instead of simulating the human vision system, we attribute the improvement of Feedback CNN to the efficiency in utilizing computation resources.|
||5 instances in total. (in iccv2015)|
|787|Zhou_Learning_Deep_Features_CVPR_2016_paper|A simple modification of the global average pooling layer combined with our class activation mapping (CAM) technique allows the classification-trained CNN to both classify the image and localize class-specific image regions in a single forward-pass e.g., the toothbrush for brushing teeth and the chainsaw for cutting trees.|
|||Cinbis et al [2] and Pinheiro et al [18] combine multiple-instance learning with CNN features to localize objects.|
|||Deep Features for Generic Localization  The responses from the higher-level layers of CNN (e.g., fc6, fc7 from AlexNet) have been shown to be very effective generic features with state-of-the-art performance on a variety of image datasets [3, 20, 35].|
|||Note that in this case, we do not have train and test splits  we just use our CNN for visual pattern discovery.|
|||Thus we could infer that the CNN actually learns a bag of words, where each word is a discriminative class-specific unit.|
||5 instances in total. (in cvpr2016)|
|788|Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper|Further, our method requires no precomputation, in contrast to existing spectral graph CNN approaches [3,4].|
|||It has been used to benchmark related graph CNN approaches [3,11] as well as pointnet++ [1].|
|||In addition, our performance surpasses that of the Network in Network model [19], which is a strong regular image CNN model.|
|||Yi, L., Su, H., Guo, X., Guibas, L.: Syncspeccnn: Synchronized spectral cnn for 3d shape  segmentation.|
|||Yi, L., Su, H., Guo, X., Guibas, L.: Syncspeccnn: Synchronized spectral cnn for 3d shape  segmentation.|
||5 instances in total. (in eccv2018)|
|789|FC4_ Fully Convolutional Color Constancy With Confidence-Weighted Pooling|In patchbased CNNs, these two types of patches are treated equally, even though patches that are ambiguous for color constancy provide little or no value, and furthermore inject noise into both CNN training and inference.|
|||Also, as their CNN is not fully convolutional, test images need to be resized to predefined dimensions, which may introduce spatial distortions of image content.|
|||Unlike previous patch-based methods, such as [7], which treat each Ri independently over an image and use a CNN to learn g, we instead consider all of the local patches within the same image jointly so that their relative importance for estimating the global illumination color can be well explored.|
|||In addition, an FCN can take an input of any size, which avoids distortions of semantic information that may occur with CNN methods that employ resizing [31].|
|||Based on this idea, we developed a novel CNN architecture for color constancy that learns this distinction among image patches, as well as how to use this information for training and inference.|
||5 instances in total. (in cvpr2017)|
|790|Schroff_FaceNet_A_Unified_2015_CVPR_paper|[23] employ a deep network to warp faces into a canonical frontal view and then learn CNN that classifies each face as belonging to a known identity.|
|||Our network consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding.|
|||Deep Convolutional Networks  In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop [8, 11] and AdaGrad [5].|
|||Effect of CNN Model  We now discuss the performance of our four selected models in more detail.|
|||This sets it apart from other methods [15, 17] who use the CNN bottleneck layer, or require additional post-processing such as concatenation of multiple models and PCA, as well as SVM classification.|
||5 instances in total. (in cvpr2015)|
|791|cvpr18-Occluded Pedestrian Detection Through Guided Attention in CNNs|We start with interpreting CNN channel features of a pedestrian detector, and we find that different channels activate responses for different body parts respectively.|
|||Contributions  In summary, our contributions are as follows: (1) We provide an analysis to understand the relation between body regions and different CNN channel features of a pedestrian detector, and find many of them are localizable and interpretable.|
|||Related Work  Since we use a FasterRCNN detector as our base pedestrian detector, and an attention network as an add-on to handle occlusion, we review recent work on CNN based pedestrian detectors, occlusion handling for pedestrians and attention mechanisms respectively.|
|||In principle, our attention net can be added on top of any CNN based method.|
|||This attention net can be added as an additional component to any CNN based detector.|
||5 instances in total. (in cvpr2018)|
|792|cvpr18-Memory Based Online Learning of Deep Representations From Video Streams|The method exploits CNN based face detectors and descriptors together with a novel incremental memory based learning mechanism that collects descriptors and distills them based on their redundancy with respect to the current representation.|
|||Quantitative comparison with other state-of-the-art multi-object tracking methods on the Music video dataset  APINK  BRUNOMARS  DARLING  Mode Method Offline mTLD [59] Offline ADMM [60] IHTLS [61] Offline Pre-Trained [48] Offline mTLD2 [59] Offline Offline Siamese [48] Offline Triplet [48] Offline SymTriplet [48] Ours-dpm Online Ours-tiny Online  IDS  MOTA  MOTP  71.2 76.1 76.1 75.5 76.3 76.3 76.3 76.3 61 65.4  -2.2 72.4 74.9 54.0 77.4 79.0 78.9 80.0 21.8 55.1  31 179 173 100 173 124 140 78 121 191  GIRLSALOUD  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online  IDS  MOTA  MOTP  71.0 87.1 87.2 87.7 88.2 87.8 87.8 87.8 61 66.1  -1.1 46.6 51.8 42.7 46.7 51.6 51.7 51.6 -2.7 49.3  9 487 396 138 322 112 80 64 51 339  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  TARA  Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online  IDS  MOTA  MOTP  65.3 85.7 85.8 88.0 87.9 87.8 87.8 87.8 61 65.5  -8.7 50.6 52.7 48.3 52.6 56.7 56.6 56.8 4.5 48.8  35 428 375 151 278 126 126 105 78 420  HELLOBUBBLE  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online  IDS  MOTA  MOTP  66.5 69.9 69.9 68.5 70.5 70.6 70.5 70.5 59.0 69.9 WESTLIFE  -3.5 47.6 52.0 36.6 52.6 56.3 56.2 56.5 4.0 51.4  7 115 109 71 139 105 82 69 170 88  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  24 412 381 115 278 214 187 169 64 449  Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online PUSSYCATDOLLS Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online  IDS  MOTA  MOTP  69.9 88.4 88.4 88.5 89.3 88.9 88.9 88.9 63.7 66.0  -22.0 53.0 62.7 42.7 59.8 69.5 69.2 70.5 2.2 62.1  IDS  MOTA  MOTP  71.3 63.5 63.5 64.9 64.9 64.9 64.9 64.9 61.1 62.7  3.1 63.2 70.3 65.1 68.3 70.3 69.9 70.2 -13.5 30.7  24 287 248 128 296 107 99 82 55 83  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online  IDS  MOTA  MOTP  67.9 63.8 63.8 72.4 72.6 72.5 72.5 72.4 68 76.4  1.4 29.4 35.3 57.3 56.0 58.4 59.0 59.2 15 39.5  130 251 218 143 251 106 94 75 124 270  Method mTLD ADMM IHTLS Pre-Trained mTLD2 Siamese Triplet SymTriplet Ours-dpm Ours-tiny  Mode Offline Offline Offline Offline Offline Offline Offline Offline Online Online  IDS  MOTA  MOTP  56.9 87.5 87.5 88.2 88.1 88.0 88.0 88.1 61.5 66.1  -34.7 62.4 60.9 57.0 58.1 64.1 64.5 68.6 -0.2 58.9  20 223 113 85 177 74 89 57 47 76  tracklets are used as face pairs (Siamese) or face triplets (Triplet and SymTriplet) to fine-tune a CNN initial face feature descriptor based on the AlexNet architecture trained on the CASIA-WebFace (Pre-trained).|
|||Our main observation is that with modern CNN based face detector and descriptor, the state-of-the-art offline trackers do not have expected advantages over the simpler online ones.|
|||Considering that CNN descriptor fine-tuning takes around 1 hour per sequence on a modern GPU, our method perform favorably in those applications operating on real time streaming data.|
|||Conclusion  In this paper we exploited deep CNN based face detection and descriptors coupled with a novel memory based learning mechanism that learns face identities from video sequences unsupervisedly, exploiting the temporal coherence of video frames.|
||5 instances in total. (in cvpr2018)|
|793|Chaojian_Yu_Hierarchical_Bilinear_Pooling_ECCV_2018_paper|These models regard CNN as part detector and obtain great improvement in fine-grained recognition.|
|||Suppose an image I is filtered by a CNN and the  Hierarchical Bilinear Pooling for Fine-Grained Visual Recognition  5  output feature map of a convolution layer is X  Rhwc with height h, width w and channels c, we denote a c dimensional descriptor at a spatial location on X as x = [x1, x2,    , xc]T .|
|||Therefore, to capture finer grained part feature, we develop a cross-layer bilinear pooling approach that treats each convolution layer in a CNN as part attributes extractor.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition.|
|||Sochor, J., Herout, A., Havel, J.: Boxcars: 3d boxes as cnn input for improved finegrained vehicle recognition.|
||5 instances in total. (in eccv2018)|
|794|Spindle Net_ Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion|It is the first time human body structure information is considered in a CNN framework to facilitate feature learning.|
|||With the great success of CNN features, a lot of recent ReID approaches [7, 13, 22, 24, 25, 28, 29] are designed based on CNN structure.|
|||[7] introduced a multi-channel CNN to learn body features from the input image.|
|||In FEN-C3,  0  i  i  i  i  1080  CNN INPUT FEN-C1 FEN-P1 ROI pooling FEN-P2 FEN-C2 FFN-1 CNN FEN-C3 Feature Extraction Net (FEN) Feature Fusion Net (FFN) ROI pooling Legs Arms CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN CNN FFN-2 FFN-3 FFN-4 WholeFusion BodyFusion UpperBody LowerBody RPN D = 4.52  D = 1.03  D = 0.11  D = 0.31  (a)  (b)  (c)  (d)  (e)  (f)  Figure 4.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||5 instances in total. (in cvpr2017)|
|795|Cheng_Focusing_Attention_Towards_ICCV_2017_paper|[24] proposed an end-to-end neural network that combines CNN and RNN.|
|||A ResNet-based CNN architecture for robust text feature extraction.|
|||Implementation Details  Network: For the encoder, we construct a 32-layer ResNet [10]-based CNN as described in Tab.|
|||Then, the extracted sequence of features from CNN are fed into a BLSTM (256 hidden units) network.|
|||Joint trained CNN and attention based RNN are applied to recognize the words.|
||5 instances in total. (in iccv2017)|
|796|George_Papandreou_PersonLab_Person_Pose_ECCV_2018_paper|We develop a box-free fully convolutional system whose computational cost is essentially independent of the number of people present in the scene and only depends on the cost of the CNN feature extraction backbone.|
|||Our PersonLab system consists of a CNN model that predicts: (1) keypoint heatmaps, (2) short-range offsets, (3) mid-range pairwise offsets, (4) person segmentation maps, and (5) long-range offsets.|
|||We perform offset refinement at the resolution of CNN output activations (before upsamling to the original image resolution), making the process very fast.|
|||3 we illustrate the long-range offsets corresponding to the Nose keypoint as computed by our trained CNN for an example image.|
|||Model training details We report experimental results with models that use either ResNet-101 or ResNet-152 CNN backbones [70] pretrained on the Imagenet classification task [71].|
||5 instances in total. (in eccv2018)|
|797|Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper|2  W. Liu, S. Liao, W. Hu, X. Liang and X. Chen  Beyond early studies focusing on hand-craft features, RCNN [17] firstly introduced CNN into object detection.|
|||However, its speed is limited by repeated CNN feature extraction and evaluation.|
|||DeepParts [37] uses the LDCF detector [29] to generate proposals and then trains an ensemble of CNN for detecting different parts.|
|||Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic In: Proceedings of the IEEE International  segmentation-aware cnn model.|
|||Lee, H., Eum, S., Kwon, H.: Me r-cnn: multi-expert region-based cnn for object  detection.|
||5 instances in total. (in eccv2018)|
|798|Xiaoming_Li_Learning_Warped_Guidance_ECCV_2018_paper|As to face images, several CNN architectures have been developed for face hallucination [5, 8, 17, 59], and the adversarial learning is also introduced to enhance the visual quality [52,53].|
|||It is worth noting that the success of such kernel-free end-to-end approaches depends on both the modeling capability of CNN and the sufficient sampling on clean images and degradation parameters, making it difficult to design and train.|
|||[18] present a CNN model to learn multi-scale guidance, while Gu et al.|
|||Huang, H., He, R., Sun, Z., Tan, T.: Wavelet-SRNet: A wavelet-based cnn for multi-scale face super resolution.|
|||Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||5 instances in total. (in eccv2018)|
|799|cvpr18-Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking|STRCF performs favorably in terms of accuracy, robustness and speed in comparison with the stateof-the-art CF-based and CNN trackers.|
|||And more discriminative features are widely used, such as HOG [8], color names (CN) [14] and deep CNN features [28, 34].|
|||Then we extract HOG, CN [14] and CNN features for the image region.|
|||[13], BACF [20], ECO-HC [9], SRDCFDecon [10], Staple [1], Staple+CA[30], SAMF+AT [3], DSST [11], SAMF [24], MEEM [41] and KCF [19]) and using CNN features (i.e.|
|||One can see that DeepSTRCF achieves a mean OP of 84.2% and performs much better than the SRDCF with CNN features (i.e.|
||5 instances in total. (in cvpr2018)|
|800|cvpr18-Scalable and Effective Deep CCA via Soft Decorrelation|As for the supervised CNN classifier, it was recently shown that decorrelation losses can be beneficial for maximizing model capacity and reducing overfitting [5].|
|||CNN Classifier and DeCov loss Using CNN with a classification loss (e.g., cross entropy) for object recognition is perhaps the most popular application of deep learning in computer vision.|
|||Therefore, SDL can be applied to each layer of a CNN classifier to boost the model performance.|
|||Person re-identification In this experiment, a CNN classifier is applied to solve a more challenging recognition problem.|
|||As case studies, SDL was shown to outperform alternative decorrelation losses in FAE latent factor disentanglement and CNN object and instance recognition.|
||5 instances in total. (in cvpr2018)|
|801|Tan_Learning_Graph_Structure_2015_CVPR_paper|Here, we adopt two kinds of features to represent PASCAL2007, i.e., the dense SIFT features (denoted by PASCAL2007DSIFT) of 3000 dimensions, and deep CNN (convolutional neural network) features [19].|
|||BR [23] 0.054  HLLM-L1 [28]  LSG21 [4]  CL-TREE  0.024  0.059  PLST [32] MMOC [44]  0.055  0.064  LEAD [43]  0.0484  PLEM [22]  0.064  we use CNN features only.|
|||In our experiment, we extract CNN features via Caffe tool box,6 which represent each image by 4096 features.|
|||On PASCAL07, CGM with CNN features achieves significantly better results than others.|
|||Interestingly, this comparison also demonstrates the superiority of CNN local features on multi-label image classification tasks.|
||5 instances in total. (in cvpr2015)|
|802|cvpr18-Encoding Crowd Interaction With Deep Neural Network for Pedestrian Trajectory Prediction|Behavior CNN represents historical trajectories of all pedestrians with a position displacement map in the image space, and then a CNN is adapted to associate each pedestrian with its neighbors for future trajectory prediction.|
|||Further, different from Social LSTM and Behavior CNN that directly predict the coordinates, we propose to predict the location displacement between between next and current frame, which further validates the effectiveness of residual learning in computer vision [8][9].|
|||Specifically, Behaviour-CNN [25] employs a 2D map to encode the history walking path and use a CNN to model the interactions between different pedestrians, yet it doesnt consider the effect of pedestrians in a more distant future.|
|||Further, compared with Behavior CNN [25] and Social LSTM [1], the number of hidden nodes in our framework is very small.|
|||The performance of Social LSTM [1], Behavior CNN [25], SRGP [7], and SF [23] on these datasets are directly adapted from the corresponding papers.|
||5 instances in total. (in cvpr2018)|
|803|Yanbei_Chen_Semi-Supervised_Deep_Learning_ECCV_2018_paper|Illustration of the memory-assisted semi-supervised deep learning framework that integrates a deep CNN with an external memory module trained concurrently.|
|||During training, given (a) sparse labelled and abundant unlabelled training data, mini-batches of labelled/unlabelled data are feed-forward into (b) the deep CNN to obtain the up-to-date feature representation x and probabilistic prediction p for each sample.|
|||Output: A deep CNN model for classification.|
|||We adopt the same 10-layers CNN architecture as [14].|
|||Data projection in 2-D space is attained by tSNE [17] based on the feature representation extracted on the same sets of data using the CNN at different training stages.|
||5 instances in total. (in eccv2018)|
|804|Eddy_Ilg_Occlusions_Motion_and_ECCV_2018_paper|[52] were the first to present a Siamese CNN for matching patches.|
|||This shows that the chicken-and-egg problem of occlusion estimation is much easier to handle with a CNN than with classical approaches [44, 24, 17, 27] and that CNNs can perform very well at occlusion reasoning.|
|||Our CNN on the other hand is able to estimate most of the fine details.|
|||Although we do not train on Sintel, from the results of Table 6, our CNN outperforms their method by a large margin.|
|||To mitigate this problem we train a CNN to reason about disparities in occluded regions (see the architecture from Figure 1(d)).|
||5 instances in total. (in eccv2018)|
|805|Qian_Recurrent_Color_Constancy_ICCV_2017_paper|The convolutional LSTM can be divided into CNN and LSTM, which we will describe separately in the following paragraphs.|
|||Let us assume a sequence of CNN representations of input frames as input and a vector describing the last (shot) frame color as output.|
|||SS is generated only from the shot frame, then fed to an AlexNet-based CNN (Figure 2(f)) [6] followed by a LSTM for SS (Figure 2(g)).|
|||The reason we switched to the AlexNet-based CNN here instead of VGGNet is that we achieve competitive performance with less computation.|
|||As the objective function, general multioutput CNN regression and related CNN-based color constancy algorithms use the Euclidean loss [6, 35].|
||5 instances in total. (in iccv2017)|
|806|Kwitt_One-Shot_Learning_of_CVPR_2016_paper|Obviously, this also affects the representation of an image in feature space, e.g., as activations at a certain CNN layer, and consequently impacts scene recognition performance.|
|||1 78  While the majority of approaches to scene recognition either rely on variants of Bag-of-Words [6, 13], Fisher vectors [22], or outputs of certain CNN layers [8, 31, 3], several works have also advocated more abstract, semantic (attribute) representations [21, 23, 10].|
|||This clearly highlights that limited variability with respect to the transient state of a scene can severely impact recognition accuracy, even when using one of the state-of-the-art CNN representations.|
|||Even in case of Places, our experiments suggest that the image data does not sufficiently cover variability in transient attributes so that the CNN can learn features that exhibit the required degree of invariance.|
|||The CNN is based on the AlexNet architecture of [9].|
||5 instances in total. (in cvpr2016)|
|807|cvpr18-ClcNet  Improving the Efficiency of Convolutional Neural Network Using Channel Local Convolutions|A new CNN model named clcNet is then constructed using CLC blocks, which shows significantly higher computational efficiency and fewer parameters compared to stateof-the-art networks, when being tested using the ImageNet1K dataset.|
|||More recently, due to the pervasive use of mobile and wearable devices, and the rise of emerging applications such as self-driving car, CNN models have been implemented in resource constrained environments, such as mobile and embedded platforms.|
|||The computational and memory efficiency of CNN is crucial for its successful deployment on these platforms, since they generally have very strict resource requirements.|
|||Prior Work  The research on CNN efficiency improvement can be dated back to early days of convolutional network research.|
|||Another popular CNN model is Residual Newtwork [9], in which the residual block can be considered as the sparsification of regular convolution by the summation of an identity function and a residual  function.|
||5 instances in total. (in cvpr2018)|
|808|cvpr18-Discriminative Learning of Latent Features for Zero-Shot Recognition|Unfortunately, this issue has been thus far neglected in ZSL and almost all the methods follow the same paradigm: 1) extracting image features by hand-crafting or using pre-trained CNN models; and 2) utilizing the humandesigned attributes as the semantic representations.|
|||Firstly, the image features (x) either crafted manually or from a pre-trained CNN model may be not representative enough for zero-short recognition task.|
|||Though the features from a pre-trained CNN model are learned, yet restricted to a fixed set of images (e.g., ImageNet [22]), which is not optimal for a particular ZSL task.|
|||Implementation Details  The FNets are initialized using two different CNN models pre-trained on ImageNet, i.e., GoogLeNet [24] and VGG19 [23] respectively, to learn, (x).|
|||The comparison results using two different CNN models are shown in Table 1.|
||5 instances in total. (in cvpr2018)|
|809|Melih_Engin_DeepKSPD_Learning_Kernel-matrix-based_ECCV_2018_paper|In the recent work of Bilinear CNN [21, 13], an outer product layer is applied to combine the activation features maps from two CNNs, and this produces clear improvement in fine-grained visual recognition.|
|||For example, instead of directly computing a kernel as usual, the authors of [17] utilises Taylor series to approximate a kernel function via explicit feature maps, which allows them to generalise the Bilinear CNN framework to consider higher-order feature interaction.|
|||In the literature, the Bilinear CNN models use the setting of element-wise signed square-rooting plus L2 normalisation after the image representation stage as the post-processing stage.|
|||B-CNN denotes the bilinear CNN method [21] and Improved BCNN [12] is an extension of B-CNN where matrix square-rooting is applied.|
|||Lin, T., Roy Chowdhury, A., Maji, S.: Bilinear CNN models for fine-grained visual recognition.|
||5 instances in total. (in eccv2018)|
|810|Semantic Compositional Networks for Visual Captioning|Similar to the conventional CNN-LSTM-based image captioning framework, a CNN is used to extract the visual feature vector, which is then fed into a LSTM for generating the image caption (for simplicity, in this discussion we refer to images, but the method is also applicable to video).|
|||The differences of the various methods mainly lie in the types of CNN architectures and language models.|
|||In order to effectively represent the spatiotemporal visual content of a video, we use a two-dimensional (2D) and a threedimensional (3D) CNN to extract visual features of video frames/clips.|
|||We then perform a mean pooling process [47] over all 2D CNN features and 3D CNN features, to generate two feature vectors (one from 2D CNN features and the other from 3D CNN features).|
|||For video representation, in addition to using the 2D ResNet-152 to extract features on each video frame, we also utilize a 3D CNN (C3D) [43] to extract features on each video.|
||5 instances in total. (in cvpr2017)|
|811|cvpr18-Recurrent Scene Parsing With Perspective Understanding in the Loop|We train this recurrent adaptive pooling CNN architec ture end-to-end and evaluate its performance on several scene parsing datasets.|
|||To leverage features conveying finer granularity lower in the CNN hierarchy, it has proven useful to combine features across multiple layers (see e.g., FCN [31], LRR [13] and RefineNet [27]).|
|||As pixel-wise labelling tasks are essentially a structured prediction problem, there has also been a related line of work that aims to embed unrolled conditional random fields into differentiable CNN architectures to allow for more tractable learning and inference (e.g., [20, 39]).|
|||To achieve this, our recurrent refinement module takes as input feature maps extracted from a feed-forward CNN model along with current segmentation predictions available from previous iterations of the recurrent module.|
|||Fusenet: Incorporating depth into semantic segmentation via fusionbased cnn architecture.|
||5 instances in total. (in cvpr2018)|
|812|Kiapour_Where_to_Buy_ICCV_2015_paper|Whole Image Retrieval  In this approach, we apply the widely used CNN model of Krizhevsky et al.|
|||For shop images, we compute CNN features on the entire image.|
|||Our hypothesis is that the cosine similarity on existing CNN features may be too general to capture the underlying differences between the street and shop domains.|
|||Therefore, we explore methods to learn the similarity measure between CNN features in the street and shop domains.|
|||We formulate the similarity learning task as a binary classification problem, in which positive/negative examples are pairs of CNN features from a query bounding box and a shop image selective-search based item proposal, for the same item/different items.|
||5 instances in total. (in iccv2015)|
|813|Cai_A_Probabilistic_Collaborative_CVPR_2016_paper|Coupled with the CNN features, it also leads to state-of-the-art classification results on a variety of challenging visual datasets.|
|||4.4.2 Evaluation of different classifiers with the BOW SIFT features and CNN feature  including softmax,  To verify that ProCRC is an effective classifier, we present a detailed comparison between ProCRC and several widelyused classifiers, linear SVM, kernel SVM with 2 kernel, CRC, SRC and CORC.|
|||Specifically, with the powerful CNN features, ProCRC obtains at least 1.5% performance gains over all the other classifiers.|
|||Compared with the other three methods which all use a specially designed CNN architecture for bird specie recognition, the improvement by ProCRC is obvious.|
|||Coupled with CNN features (e.g., VGG19), ProCRC demonstrated state-of-the-art performance on challenging visual datasets such as Stanford 40 Actions, CUB200-2011, Oxford 102 Flowers, and Caltech-256.|
||5 instances in total. (in cvpr2016)|
|814|Duong_Temporal_Non-Volume_Preserving_ICCV_2017_paper|ResNet style CNN layers)  Element-wise operator  Figure 4: Our proposed TNVP structure with two mapping units.|
|||Both transformations S and T can be easily formulated as compositions of CNN layers.|
|||Mapping function as CNN layers  In general, a bijection function between two highdimensional domains, i.e.|
|||From this property, the functions S and T are set up as a composition of CNN layers in ResNet (i.e.|
|||As mentioned in the previous section, since each mapping unit is set up as a composition of CNN layers, the bijection F with the form of Eqn.|
||5 instances in total. (in iccv2017)|
|815|Recurrent Modeling of Interaction Context for Collective Activity Recognition|Given tracklets of N i person, we feed each tracklet into spatial CNN and motion CNN respectively and concatenate their outputs, followed by a person level LSTM network to represent person dynamics.|
|||Two CNN networks are applied, and the spatial CNN (AlexNet [20]) is for original images and the motion CNN (GoogleNet [35]) is for flow images.|
|||Typically, the motion CNN is not needed to train when performing person level training.|
|||The outputs of spatial CNN, motion CNN and person level LSTM are concatenated, therefore the dimensionality of the input vector to each LSTM cell is 4096 + 1024 + 1024 = 6144 if context encoding step do not change its dimension.|
|||Class Moving Waiting Queuing Talking MPCA  [22] 92 69 76 99 84  [5] 90.0 82.9 95.4 94.9 90.8  [17] 95.9 66.4 96.8 99.5 89.7  [15] 87 75 92 99 88.3  Pooling Ours 94.2 94.4 63.6 50.3 100.0 100.0 99.5 99.5 85.8 89.4  and motion CNN to recognize action is inspired by [31, 12] and it is most similar to the work [17].|
||5 instances in total. (in cvpr2017)|
|816|Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper|In [31], a CNN was shown to learn a descriptor which distinguishes object poses from each other for RGB-D and RGB images.|
|||[10] trained a CNN to directly regress the 6D pose of a scene from an RGB image but with moderate accuracy.|
|||[2] by utilizing a CNN in the final pose optimization stage.|
|||We attribute this to the robust inlier-based pose optimization which is different from the general purpose CNN in [13] and the hand-crafted energy of [2].|
|||very recent PoseNet [10] work, which is a CNN that directly regresses the 6D camera pose.|
||5 instances in total. (in cvpr2016)|
|817|Zhang_PANDA_Pose_Aligned_2014_CVPR_paper|However, poselet detected parts may not always cover the whole image region and in  Specifically, we start from poselet patches, resize them to 64x64 pixels (Figure 3), randomly jitter each patch and flip it horizontally with probability 0.5 to improve generalization, and train a CNN for each poselet.|
|||DPD and DeCAF We used the publicly available implementations of [27] based on deformable part models and [8] based on CNN trained on ImageNet.|
|||We then used the same setup as PANDA  trained CNN classifiers for each of the 48 parts, combined them with the global model and trained SVM on top.|
|||We train a CNN on this 12x64x64 input on the full Attributes-25K dataset.|
|||The structure we used is similar to the CNN in Figure 2 and it consists of two convolution/normalization/pooling stages, followed by a fully connected layer with 512 hidden units followed by nine columns, each composed of one hidden layer with 128  Attribute DL-Pure DeCAF  Poselets150 L2  DLPoselets  PANDA  male 80.65 79.64 81.70 92.10 91.66  long hair  63.23 62.29 67.07 82.26 82.70  glasses 30.74 31.29 44.24 76.25 69.95  hat 57.21 55.17 54.01 65.55 74.22  tshirt 37.99 41.84 42.16 44.83 49.84  longsleeves  71.76 78.77 71.70 77.31 86.01  short 35.05 80.66 36.71 43.71 79.08  jeans 60.18 81.46 42.56 52.52 80.99  long pants Mean AP  86.17 96.32 87.41 87.82 96.37  58.11 67.49 58.62 69.15 78.98  Table 3: Relative performance of baselines and components of our system on the Berkeley Attributes of People test set.|
||5 instances in total. (in cvpr2014)|
|818|cvpr18-Revisiting Video Saliency  A Large-Scale Benchmark and a New Model|(b) CNN layers with attention module are used for learning intra-frame static features, where the attention module is learned with the supervision from static saliency data.|
|||Through the additional attention module, the CNN is enforced to generate a more explicit spatial saliency representation.|
|||6 in residual form:  X c = (1 + M )  X c.  (7)  With the residual connection, both the original CNN features and the enhanced features are combined and fed to the LSTM model.|
|||tion module, which makes our model explicitly learn static and dynamic saliency representations in CNN and LSTM separately.|
|||Predicting video saliency with object-to-motion CNN and two-layer convolutional LSTM.|
||5 instances in total. (in cvpr2018)|
|819|Yunpeng_Chen_Fast_Multi-fiber_Network_ECCV_2018_paper|State-of-the-art video understanding models, such as Res3D [7] and I3D [1] build their CNN models in this straightforward manner.|
|||The major advantage of adopting 2D CNN based methods is their computational efficiency.|
|||We use one of the most popular 2D CNN model, residual network (ResNet-18) [3], and the most computationally efficient ModelNet-v2 [26] as the backbone CNN in the following studies.|
|||As can be seen from the results, 3D based CNN models significantly improve the Top-1 accuracy upon 2D CNN based models.|
|||Compared with the 2D CNN based models that also only use RGB frames, our proposed model improves the accuracy by more than 15% (74.6% v.s.|
||5 instances in total. (in eccv2018)|
|820|Ilchae_Jung_Real-Time_MDNet_ECCV_2018_paper|This algorithm is inspired by an object detection network, R-CNN [14]; it samples candidate regions, which are passed through a CNN pretrained on a large-scale dataset and fine-tuned at the first frame in a test video.|
|||2.2 Representation Learning for Visual Tracking  MDNet [1] pretrains class-agnostic representations appropriate for visual tracking task by fine-tuning a CNN trained originally for image classification.|
|||3 Efficient Feature Extraction and Discriminative Feature  Learning  This section describes our CNN architecture with an improved RoiAlign layer, which accelerates feature extraction while maintaining quality of representations.|
|||A CNN feature corresponding to each RoI is extracted from the shared feature map using an adaptive RoIAlign operation.|
|||6 Conclusions  We presented a novel real-time visual tracking algorithm based on a CNN by learning discriminative representations of target in a multi-domain learning framework.|
||5 instances in total. (in eccv2018)|
|821|cvpr18-Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition|[33] via pooling the 3DMM parameters of the images of the same subject and using a CNN to regress the pooled parameters.|
|||Jourabloo and Liu [1719] first employed CNN to regress 3DMM parameters from 2D images for the purpose of large-pose face alignment.|
|||In [41], a cascaded CNN pipeline was proposed to exploit the intermediate reconstructed 3D face shapes for better face alignment.|
|||[16] proposed to represent 3D face shapes by 3D volumetric coordinates, and train a CNN to directly regress the coordinates from the input 2D face image.|
|||Large pose 3D face reconstruction from a single image via direct volumetric CNN regression.|
||5 instances in total. (in cvpr2018)|
|822|Zhang_S3FD_Single_Shot_ICCV_2017_paper|On the one hand, many works [21, 31, 54, 55, 58] have applied CNN as the feature extractor in the traditional face detection framewrok.|
|||Convnet [24] integrates CNN with 3D face model in an endto-end multi-task learning framework.|
|||1, the stride size and the receptive field of each detection layer are fixed, which are two base points when we design the anchor scales:   Effective receptive field: As pointed out in [29], a unit in the CNN has two types of receptive fields.|
|||Joint training of cascaded cnn for face detection.|
|||Cmsrcnn: contextual multi-scale region-based cnn for unconstrained face detection.|
||5 instances in total. (in iccv2017)|
|823|AGA_ Attribute-Guided Augmentation|Our experiments show that attribute-guided augmentation of highlevel CNN features considerably improves one-shot recognition performance on both problems.|
|||Finetuning a pre-trained CNN with millions of parameters to  17455  such inadequate datasets is clearly not a viable option.|
|||A one-shot classifier trained on CNN activations will also be prone to over-fitting due to the high dimensionality of the feature space.|
|||For Sem-FV [8], we use ImageNet CNN features extracted at one image scale.|
|||Finally, we combined our aug. FVs with the state-of-the-art semantic FV of [8] and Places CNN features [38] for one-shot classification.|
||5 instances in total. (in cvpr2017)|
|824|cvpr18-Resource Aware Person Re-Identification Across Multiple Resolutions|Features from the last layer of a CNN mostly encode semantic features, like object presence [15], but lose all information about the fine spatial details such as the pattern of ones facial hair or the particular shape of ones body.|
|||For example, a robot might need to make decisions within a time limit, or it may have a limited battery supply that precludes the running of a massive CNN on every frame.|
|||Deep supervision and skip connections  The idea of using multiple layers of a CNN has been explored before.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Multi-scale triplet cnn for person re-identification.|
||5 instances in total. (in cvpr2018)|
|825|Reed_Learning_Deep_Representations_CVPR_2016_paper|The text-based CNN can be viewed as a standard CNN for images, except that the image width is 1 pixel and the number of channels is equal to the alphabet size.|
|||However, the CNN models are extremely fast and scale well to long sequences such as character strings.|
|||To get the benefits of both recurrent models and CNNs, we propose to stack a recurrent network on top of a mid-level temporal CNN hidden layer.|
|||Long Short-Term Memory (LSTM)  As opposed to the CNN models,  the LSTM explicitly takes into account the temporal structure starting from words or characters.|
|||The CNN input size (sequence length) was set to 30 for word-level and 201 for character-level models; longer text inputs are cut off at this point and shorter ones are zeropadded.|
||5 instances in total. (in cvpr2016)|
|826|Hinami_Joint_Detection_and_ICCV_2017_paper|Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events.|
|||Unlike previous approaches that have trained anomaly detectors on low-level features, our anomaly detector is trained on more semantic spaces by using CNN outputs (i.e., deep features and classification scores) as features.|
|||Despite the great success of CNN approaches in many visual tasks, the application of CNN to abnormal event detection is yet to be explored.|
|||Normally CNN requires rich supervised information (positive/negative, ranking, etc.)|
|||Learning motion features with two-stream CNN [12] or 3DCNN [39] remains to be undertaken in future work.|
||5 instances in total. (in iccv2017)|
|827|cvpr18-End-to-End Learning of Motion Representation for Video Understanding|A currently successful example of applying optical flow to video understanding is the twostream model [33], where a CNN is trained on the optical flow data to learn action patterns.|
|||In the second stage, a CNN is trained on the extracted flow data.|
|||These approaches cast the optical flow estimation as an optimization problem with respect to the CNN parameters.|
|||A natural idea is to combine the flow CNN with the task-specific network to formulate an end-to-end model (see for example in [45]).|
|||Combining CNN models with trajectory-based handcrafted IDT features [37] can improve the final performances [38, 36, 5, 9].|
||5 instances in total. (in cvpr2018)|
|828|cvpr18-Analysis of Hand Segmentation in the Wild|To demonstrate the benefit of accurate hand maps, we train a CNN for hand-based activity recognition and achieve higher accuracy when a CNN was trained using hand maps produced by the fine-tuned RefineNet.|
|||Since, few actions like highfive, catching, replacing, and pushing rarely occur, we trained a CNN for 8 most frequent action classes which uses a single hand instance to classify which fine-level action is being performed.|
|||We trained the same CNN [21] in all of our experiments related to activity/action recognition, except that the last layer changes according to the number of classes.|
|||Activity and Action Recognition Results Coarse-level activity recognition: Our baseline [2] trained a Caffe [15] based CNN as a 4-way classifier (coarse level) for 4 activities: cards, chess, jenga and puzzle.|
|||Thus, we trained a CNN on 5) Using objects only, we achieved an accuracy of 55.1%.|
||5 instances in total. (in cvpr2018)|
|829|Improved Stereo Matching With Constant Highway Networks and Reflective Confidence Learning|More generally, assessing the correctness of a CNN output is also a subject that is under magnifying glass.|
|||The labels used for this loss are changing dynamically, based on the current success of the CNN on each training sample.|
|||4.2 we introduce a novel way to measure the correctness of the output of a CNN via reflective learning that outperforms other techniques in the literature for assessing confidence in stereo matching.|
|||A very recent work [31] was the first to leverage a CNN for stereo confidence measure.|
|||They incorporated conventional confidence features to the input of the CNN and trained the network especially for this purpose.|
||5 instances in total. (in cvpr2017)|
|830|Shading Annotations in the Wild|In summary, our contributions are:   a new large-scale dataset of shading annotations col lected via crowdsourcing,   a CNN trained to recognize shading effects using this  data, and a comparison to baseline methods, and   an example use of this model as a smooth shading prior  to improve intrinsic image decomposition.|
|||Learning to Predict Shading Features  We demonstrate the utility of our shading annotation data by training a CNN to make per-pixel predictions of different types of shading features.|
|||We compare our CNN predictions to seven baseline algorithms: (1) Constant Reflectance (i.e., the shading channel is the luminance channel of the input image itself), (2) [Shen et al.|
|||Using this dataset, we trained a CNN to achieve competitive performance against a number of baselines in per-pixel classification of shading effects in images.|
|||Our CNN for classifying pixels into different shading categories could be extended in a number of ways.|
||5 instances in total. (in cvpr2017)|
|831|cvpr18-Unsupervised Cross-Dataset Person Re-Identification by Transfer Learning of Spatial-Temporal Patterns|The network adopts a siamese scheme including two ImageNet pre-trained CNN modules, which share same weight parameters and extract visual features from the input images Si and Sj .|
|||The CNN module is achieved from the ResNet-50 network [8] by removing its final fully-connected (FC) layer.|
|||While deploying this classifier to perform Re-ID, given two images Si and Sj as input, the CNN modules extract their visual feature vectors ~vi and ~vj as shown in Fig.|
|||3, the triplets network takes the three images, Si,Sj ,and Sk as input, and shares the CNN modules with C to extract visual features.|
|||After training the triplets network, the CNN modules which are shared with the classifier C get incrementally optimized.|
||5 instances in total. (in cvpr2018)|
|832|Daniel_Worrall_CubeNet_Equivariance_to_ECCV_2018_paper|To the best of our knowledge, this is the first 3D rotation equivariant CNN for voxel representations.|
|||We introduce a CNN architecture, which is linearly equivariant (a generalization of invariance defined in the next section) to 3D rotations about patch centers.|
|||Applying a standard translation equivariant CNN to this representation is then equivariant to rotations and scalings about the image center.|
|||For 3D rotation equivariant methods, Cohen & Welling introduce the Spherical CNN [10], which operates on images projected onto the sphere, while Kondor [28] and Thomas et al.|
|||We then show how to apply these groups in a group equivariant CNN using Cayley tables to build three different 3D rotation equivariant CNNs.|
||5 instances in total. (in eccv2018)|
|833|Yu_Liu_Transductive_Centroid_Projection_ECCV_2018_paper|To this end, a question arises: how to utilize unlabelled data in a stable training process, such as a CNN modle with softmax classification loss function, without any recursion and avoid the inter-class conflict?|
|||1(b), the projection matrix W of the classification layer in original CNN is replaced by the joint matrix of W and ad hoc centroids C. In this manner, labelled data and unlabelled data function the  Labeled dataModelUnlabeled dataClusteringID: 1ID: 2ID: 3ID: n......Warm up trainingSelf-trainingLabeled dataUnlabeled dataModelflfuC,C[,]SoftMax Lossad hoc centroidsTranductivecentroid projection(a)Unstable Self-training strategy for semi-supervised learning(b)TCP enable training unlabelleddata with inornateclassification pipelineTransductive Centroid Projection: A deep semi-supervised method  3  same during training.|
|||In this work, we mainly pay attention to a basic question, namely how to wisely train a simple CNN model by fully leveraging both labelled and unlabelled data, without self-training or transfer learning.|
|||We first train the CNN with DL which is used to extract features of DU, and then obtain the pseudo ground truth by a cluster algorithm.|
|||7 Conclusion  By observing the latent space learned by softmax loss in CNN, we propose a semisupervised method named TCP which can be steadily embedded in CNN and followed by any classification loss functions.|
||5 instances in total. (in eccv2018)|
|834|WILDCAT_ Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation|When computing local features with deep models, the most naive approach is to rescale each region into a fixedsize vector adapted to the CNN architecture, as done in early works for detection, e.g.|
|||In Table 3, we compare WILDCAT results for scene categorization with recent global image representations used for image classification: deep features [71, 28], and global image representation with deep features computed on image regions: MOP CNN [25] and Compact Bilinear Pooling [18].|
|||Method CaffeNet Places [71] MOP CNN [25] Negative parts [47] GAP GoogLeNet [70] WELDON [13] Compact Bilinear Pooling [18] ResNet-101 (*) [28] SPLeaP [35] WILDCAT  90.2   88.3 94.3   91.9   94.4  68.2 68.9 77.1 66.6 78.0 76.2 78.0 73.5 84.0  Table 3.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Object detection via a multiIn  region and semantic segmentation-aware cnn model.|
||5 instances in total. (in cvpr2017)|
|835|cvpr18-Mean-Variance Loss for Deep Age Estimation From a Face|For a batch of training data, each image is passed through a CNN for feature extraction.|
|||2 gives the overview of our approach, in which the proposed mean-variance loss, together with the softmax loss, is embedded into a CNN for end-to-end learning.|
|||The output of the last FC  and f (xi)  RNM denote the output of a CNN ahead of the layer (z  RNK ), and a typical softmax (p  RNK ) proba bility can be computed using  z = f (xi) T  ,  pi, j =  ezi, j k=1 ezi,k  K  ,  (1)  where   RKM is the parameter of the last FC layer, and zi, j is one element of z; j  {1, 2, ..., K} denotes the class labels (here it denotes the age); So pi denotes the estimated age distribution for sample i over all the K classes, and pi, j denotes the probability that sample i belongs to class j.|
|||The variance loss component in our mean-variance loss penalizes the dispersion of an estimated  5287  softmax input CNN network for feature extraction joint loss ... ... convolution Fully connected ... ... train dataset 0 1 2 3 ... 10 11 12 13 14 15 16 17 18 ... 97 98 99 100 softmax loss  (cid:1865)(cid:1866)= (cid:1862)  mean  loss   (cid:1861)(cid:1866)=  (cid:4666)(cid:1862)(cid:1865)(cid:1866)(cid:4667)2 variance loss calculate probability, mean and variance predicted age = (cid:1862)=14 with its mean getting closer to the ground-truth age.|
|||Ordinal regression with multiple output CNN for age estimation.|
||5 instances in total. (in cvpr2018)|
|836|SGM-Nets_ Semi-Global Matching With Neural Networks|Deep learning using a CNN offers a promising way for our purpose.|
|||However, it isnt straightforward to involve the CNN for the task, i.e.|
|||Some papers which learn CRF parameters with CNN have been proposed [41, 19, 34].|
|||Our method fully learns SGM penalties with CNN in order to improve disparity maps.|
|||L x1 and c(x, d) represents the previous pixel (x0  r) and a pixel-wise matching cost, which is given by for instance ZNCC (Zero Mean Normalized Cross-Correlation), Census [37], or CNN based methods [38, 21, 30, 3].|
||5 instances in total. (in cvpr2017)|
|837|cvpr18-Multi-Level Fusion Based 3D Object Detection From Monocular Images|A region proposal network (RPN) is utilized to generate 2D proposals in the image, as RPN provides strong objectness confidence regions of interest (RoIs) with CNN features and it can share weights with the down-stream detection network [31, 36, 37, 19].|
|||It also utilizes deep CNN features to estimate the pose and dimension of a 3D object, constituting the complete detection framework.|
|||Now we have two streams for 3D localization, one is from CNN features with RoI max pooling and the other is from point cloud with RoI mean pooling.|
|||extend the existing proposal-driven 2D detectors with the help of deep CNN features.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||5 instances in total. (in cvpr2018)|
|838|Tong_Image_Super-Resolution_Using_ICCV_2017_paper|Recent works [11, 12] have successfully used very deep convolutional neural networks (CNN) to perform single image super-resolution (SISR), and significant improvements over shallow CNN structures [2] have been observed.|
|||However, it is challenging to effectively train a very deep CNN due to the vanishinggradient problem.|
|||The reuse of feature maps from bottom layers is helpful for reducing feature redundancy, thus learning more compact CNN models.|
|||Our main contributions are:   We demonstrate that the deep CNN framework with the denseNet as basic blocks can achieve good reconstruction performance and that the fusion of features at different levels through dense skip connections can further boost the reconstruction performance for SISR.|
|||On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN [2] with 3-layer CNN and an increase of about 0.5 dB over VDSR [11] with 20-layer CNN.|
||5 instances in total. (in iccv2017)|
|839|Felsen_What_Will_Happen_ICCV_2017_paper|Our hypothesis about why this modifications helps is that forcing the CNN to predict player locations results in representations that capture the important feature of player configurations and are thus more likely to generalize than other nuisance factors that the CNN can latch onto given the small size of the training set.|
|||These numbers suggest that humans and our system  3347  Method  Error (1s in Future)  Error (2s in Future)  Distance (%)  Angle (o)  Distance (%)  Angle (o)  Mean Median Mean Median Mean Median Mean Median  Last Position  11.7  10.4    20.0  18.3    Ball Velocity  100  100  89.3  88.7  100  100  88.8  85.9  CNN + LSTM  11.4  CNN (Early Fusion)  10.8  8.6  8.3  61.8  46.6  17.1  14.1  53.1  38.1  60.2  44.1  16.8  13.8  54.3  38.3  Table 3: The early fusion CNN outperforms Last Position and Ball Velocity baseline methods and a late fusion CNN based approach in predicting (basket)ball position 1s and 2s in the future.|
|||We experiment with two different CNN training strategies: (a) early fusion of the temporal information by concatenating 5 images into a 15 channel image that was fed into a CNN or, (b) late fusion by using a LSTM on the output of CNN feature representation of the 5 images.|
|||We tested two different neural networks (a) Overhead CNN that took as inputs the image representation of the overhead view (see Section 5.2) along with the hand designed features described above and (b) Image CNN that took as input raw RGB images.|
|||The Overhead CNN outperforms the Image CNN suggesting that extracting features relevant for forecasting from raw visuals is a hard problem.|
||5 instances in total. (in iccv2017)|
|840|cvpr18-Compressed Video Action Recognition|It is 4.6 times faster than state-of-the-art 3D CNN model Res3D [40], and 2.7 times faster than ResNet-152 [12].|
|||RNNs [6, 47], temporal CNNs [21], or other feature aggregation techniques [11, 44] on top of CNN features have also been explored.|
|||CoViAR is fast in both preprocessing and CNN computation.|
|||Table 3 compares the CNN computational cost of our method with state-of-the-art 2D and 3D CNN architectures.|
|||For CNN time, we consider both settings where (i) we can forward multiple CNNs at the same time, and (ii) we do it sequentially.|
||5 instances in total. (in cvpr2018)|
|841|cvpr18-Boosting Domain Adaptation by Discovering Latent Domains|Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture: (i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution.|
|||However, to address the additional challenges of discovering and handling multiple latent domains, we propose a novel architecture which is able to (i) learn a set of assignment variables which associate source samples to a latent domain and (ii) exploit this information for aligning the distributions of the internal CNN feature representations and learn a robust target classifier (Fig.2).|
|||DA-layers are described in [7] which, embedded into an arbitrary CNN architecture, are able to align source and target representation distributions.|
|||As a practical example, in the typical case in which mDA-layers are used in a CNN to normalize convolutional activations, the network would predict a single set of probabilities for each input image, which would then be given as input to all mDA-layers and broadcasted across all spatial locations and feature channels corresponding to that image.|
|||We apply our approach to three different CNN architectures: the MNIST network described in [12], AlexNet [25] and ResNet [20].|
||5 instances in total. (in cvpr2018)|
|842|Singh_Track_and_Transfer_CVPR_2016_paper|Finally, recent work uses videos for semi-supervised object detection with bounding box annotations as initialization [21], or trains a CNN for feature learning using tracking as supervision and fine-tuning the learned representation with bounding box annotations for detection [38].|
|||We use the state-of-the-art Regions with CNN (R-CNN) system [12].|
|||Briefly, R-CNN computes CNN features over selective search [36] proposals, trains a one-vs-all linear SVM (with GT boxes as positives and proposals that have less than 0.3 intersection-over-union overlap (IOU) with any GT box as negatives) to classify each region, and then performs bounding box regression to refine the objects detected location.|
|||Fine-tuning CNN features has not previously been demonstrated in the weaklysupervised detection setting, likely due to existing methods producing too many false detections in the training data.|
|||We compare with state-of-the-art weakly-supervised detection methods [33, 34, 1, 37, 4] that use the same AlexNet CNN features pre-trained on ILSVRC 2012.|
||5 instances in total. (in cvpr2016)|
|843|cvpr18-Low-Latency Video Semantic Segmentation|One is to apply Conditional Random Fields (CRF) or their variants on top of the CNN models to increase localization accuracy [4, 18, 30].|
|||[8] proposed a spatial-temporal LSTM on per-frame CNN features.|
|||Based on F t  At runtime, to initialize the entire procedure, the framework will feed the first frame I 0 through the entire CNN and obtain both low-level and high-level features.|
|||At each time step t, the lower-part of the CNN Sl first computes the low-level features F t F k high-level features F t using spatially variant convolution.|
|||CamVid Dataset  We also evaluated our method on CamVid, another video segmentation dataset, and compard it with multiple previous methods that have reported performances on CamVid, which range from traditional methods to various CNN or RNN based methods.|
||5 instances in total. (in cvpr2018)|
|844|cvpr18-Learning a Complete Image Indexing Pipeline|The SUBIC encoder operates on the feature vector x produced by a CNN to enable learning of (relaxed) block-structured codes ( b) b.|
|||The input to the network is the feature vector x consisting of activation coefficients obtained by running a given image I through a CNN feature extractor.|
|||We refer to this feature extractor as the base CNN of our system.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||The CNN News Footage Datasets : Enabling Supervision in Image Retrieval.|
||5 instances in total. (in cvpr2018)|
|845|AnchorNet_ A Weakly Supervised Network to Learn Geometry-Sensitive Features for Semantic Matching|It is unclear why CNN representations, which perform well for many challenging vision tasks, including object detection [16] and segmentation [36], image captioning [57],  15277  (a)  (b)  (c)  Figure 2: Example responses of anchor filters discovered by the AnchorNet.|
|||While this allows to use CNNs as excellent object and keypoint detectors, it defeats the purpose of using CNN features as generic descriptors for discovering correspondences in an unsupervised manner, as matching requires.|
|||[37] studied the application of CNN features pre-trained on large classification datasets for finding correspondences between object instances.|
|||They found that CNN features perform on par with hand-crafted alternatives such as SIFT for the weakly-supervised keypoint transfer problems, and can outperform them when keypoint supervision is available.|
|||As noted by [10], such CNN feature vectors are analogous to hand-crafted dense descriptors like HoG and Dense-SIFT and can often be used as a plug-andplay replacement for the latter in applications.|
||5 instances in total. (in cvpr2017)|
|846|Tracking by Natural Language Specification|In these encoder-decoder models, the encoder typically is a CNN [32], while the decoder is composed of LSTMcells [13], sequentially predicting the words in the caption.|
|||We employ a deep CNN to extract the visual feature map of an input frame.|
|||Model II takes the visual patch corresponding to the target identified from the first frame as input to the Visual Specification Network, which employs a CNN to dynamically generate the visual filters and convolves an input frame with the filters.|
|||However, instead of employing the targets language specification to generate the convolutional filters, following [34] we adopt a CNN to generate the visual features of the target as our filters, namely:  vvisual t  = CN N (B),  (4)  t=0  where B = ft=0(xlanguage ) is the image patch that corresponds to the location retrieved in the first frame by the lingual specification network.|
|||We use the VGG-16 [32] as our CNN architecture for both the input frame and the visual target B.|
||5 instances in total. (in cvpr2017)|
|847|Shankar_DEEP-CARVING_Discovering_Visual_2015_CVPR_paper|However, in a weakly supervised scenario, widely used CNN training procedures do not learn a robust model for predicting multiple attribute labels simultaneously.|
|||We compared attribute prediction performances on the SUN Attributes Dataset (with weak supervision) using the state-of-the-art ensemble of low-level features proposed in [29] and the deep CNN architecture proposed in [20], and found that under a weakly supervised scenario, deep CNNs outperformed the low-level features  Sigmoid Cross Entropy Loss Input ImageL1L2L3L4L5L6L7L8Analyse the convolutional layers to provide pseudo labels to the multi label loss layerRETRAINDEEP-CARVINGSigmoid Cross Entropy Loss Input ImageL1L2L3L4L5L6L7L8for attribute prediction in scenic images (details provided in Section 4).|
|||We use this as the base CNN architecture for all our experiments.|
|||The CNN Architecture:  Inspired by its huge success [13, 17, 44], we use AlexNet [20] as our base deep CNN architecture (Fig 3) for all our purposes.|
|||Al though inversion of CNN features [35] for different input images might be more appropriate for analysing the filters and feature map responses, the marked differences in the filters of the first convolutional layer give a fair indication of how the net might be getting biased.|
||5 instances in total. (in cvpr2015)|
|848|Walch_Image-Based_Localization_Using_ICCV_2017_paper|The CNN then learns to predict the corresponding tile for a given image, thus pro viding the approximate position from which a photo was taken.|
|||An extension of the approach repeatedly evaluates the CNN with a fraction of its neurons randomly disabled, resulting in multiple different pose estimates that can be used to predict pose uncertainty [23].|
|||Conclusion  In this paper, we address the challenge of image-based localization of a camera or an autonomous system with a novel deep learning architecture that combines a CNN with  LSTM units.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Fusenet: Incorporating depth into semantic segmentation via fusionbased cnn architecture.|
||5 instances in total. (in iccv2017)|
|849|cvpr18-A Perceptual Measure for Deep Single Image Camera Calibration|Image calibration network  In this section, we present our CNN architecture for single image calibration and compare it to the state-of-the-art estimation method.|
|||In the following, we discuss how we generate our camera calibration dataset, how the CNN model was trained and what are the probable cues it looks for to perform camera calibration.|
|||Furthermore, when no clear vanishing line is detected in the image, the CNN model tends to focus on boundaries between sky and land, as the horizon typically lies on or below this boundary.|
|||Finally, we leverage the user study results to define a distance function based on human perception, which is used to compare our CNN to previous approaches.|
|||While the trained CNN works very robustly in a large number of realistic scenarios, extreme pitch angles (e.g.|
||5 instances in total. (in cvpr2018)|
|850|Ming_Sun_Multi-Attention_Multi-Class_Constraint_ECCV_2018_paper|The work in [51] exploit unified CNN framework with spatially weighted representation by the Fisher vector [30].|
|||Therefore, the methods enabling CNN to attend loosely defined regions for general objects have emerged as a promising direction.|
|||The methods BCNN and RAN share similar multi-branch ideas with the OSME in our method, where B-CNN connects two CNN features with outer product, and RAN combines the trunk CNN feature with an additional attention mask.|
|||6 Conclusion  In this paper, we propose a novel CNN with the multi-attention multi-class constraint (MAMC) for fine-grained image recognition.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear CNN models for fine-grained  visual recognition.|
||5 instances in total. (in eccv2018)|
|851|Markus_Oberweger_Making_Deep_Heatmaps_ECCV_2018_paper|Unfortunately, as the results of our experiments show, predicting these 2D projections using a regular CNN or a Convolutional Pose Machine is highly sensitive to partial occlusions, even when these methods are trained with partially occluded examples.|
|||[26] already reformulated a Hough Forest as a CNN by predicting classification and regression for patches of the input image.|
|||Specifically, a CPM is a carefully designed CNN that predicts dense heatmaps by sequentially refining  6  Oberweger et al .|
|||We consider two networks: A standard CNN trained to predict the 2D projections of 3D points as a vector [5], and a CPM [27] with 3 stages trained to predict a heatmap for each of the same 2D projections.|
|||Osherov, E., Lindenbaum, M.: Increasing CNN Robustness to Occlusions by Re ducing Filter Support.|
||5 instances in total. (in eccv2018)|
|852|cvpr18-Scale-Recurrent Network for Deep Image Deblurring|Our Encoder-decoder ResBlock network, on the contrary, amplifies the merit of various CNN structures and yields the feasibility in training.|
|||Related Work  In this section, we briefly review image deblurring meth ods and recent CNN structures for image processing.|
|||Multi-scale CNN [25] and cascaded refinement network (CRN) [4] (Fig.|
|||Encoder(cid:173)decoder with ResBlocks  The modified network is expressed as  Encoder-decoder Network Encoder-decoder network [24, 28] refers to the symmetric CNN structures that first progressively transform input data into feature maps with smaller spatial sizes and more channels (in encoder), and then transform them back to the shape of the input (in decoder).|
|||used CNN to predict kernel direction.|
||5 instances in total. (in cvpr2018)|
|853|cvpr18-A Twofold Siamese Network for Real-Time Object Tracking|Ensemble Trackers  Our proposed SA-Siam is composed of two separately trained branches focusing on different types of CNN features.|
|||It constructs trackers using correlation filters (CFs) with CNN features from each layer, and then uses an adaptive Hedge method to hedge these CNN trackers.|
|||BranchOut[12] employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers.|
|||A common insight of these ensemble trackers is that it is possible to make a strong tracker by utilizing different layers of CNN features.|
|||We directly use a CNN pretrained in the image classification task as S-Net and fix all its parameters during training and testing.|
||5 instances in total. (in cvpr2018)|
|854|cvpr18-Intrinsic Image Transformation via Scale Space Decomposition|Direct Intrinsics [37] is a successful early example of this type, using a (back then seemingly bold) multilayer CNN architecture to transform an image directly into shading and albedo.|
|||Notably, all of the above work utilizes hierarchical features from a CNN network for multi-scale processing.|
|||[34, 40, 16, 28]) use multi-scale features produced by a CNN network.|
|||The row (Ours w/ FPN input) shows the result that takes CNN features as input following exactly the FPN network [34].|
|||The comparison shows our final model holds a slight but unclear advantage, meaning that the high level features of a CNN still well preserve much of the necessary semantic information for our pixelto-pixel transformation network.|
||5 instances in total. (in cvpr2018)|
|855|Na_A_Read-Write_Memory_ICCV_2017_paper|We experimentally confirm the following CNN design after thorough tests, by varying the dimensions, depths, strides of convolution layers.|
|||As done in the write network, we also leverage a CNN to implement the read network.|
|||Among  682  v , f w v , sr  hyperparameters of the RWMN, the following three combinations have significant effects on the performance of the model; i) conv-filter/stride sizes of the write network (f w v , sw c ), ii) conv-filter/stride sizes of the read network (f r v, f r c ), and iii) number of read/write CNN layers r, w.|
|||First, as the number of CNN layers in read/write network increases, the capacity of memory interaction may increase as well; yet the performance becomes worsen.|
|||It is hinted by our results that the two-layer CNN is the best for training performance, while the one-layer CNN is the best for validation.|
||5 instances in total. (in iccv2017)|
|856|Li_A_Data-Driven_Metric_ICCV_2015_paper|The CNN takes two ESMs (S1 and S2) and one GSM (G) as input, which are resized to the resolution of 128  128.|
|||A softmax function is applied to the output of CNN so as to infer a binary label, which equals to 1 if S1 outperforms S2, and 0 otherwise.|
|||The structure of CNN is shown in Fig.|
|||The last three layers are full connection layers, and the CNN will output a 2D feature vector.|
|||In the experiments, we optimize the CNN parameters with 80 feed-forward and backpropagation iterations, and it takes about 21.6s per iteration on a GPU platform (NVIDIA GTX 980).|
||5 instances in total. (in iccv2015)|
|857|cvpr18-Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation|We use an auto-encoding variational CNN to characterize the anatomical prior, and an encoder-decoder CNN to provide fast segmentation of medical images in unsupervised settings.|
|||In our work, we use CNN architectures to learn anatomical priors and segment medical images.|
|||This is in stark contrast with typical computer vision problems that have led to many popular CNN architectures, where object location, shape, and appearance can be unpredictable.|
|||Implementation  A CNN can be seen as a hierarchical function, a set of concatenated functions, or layers.|
|||Results  At test time, a new subject only needs to be affinely registered to a template, after which the proposed CNN model evaluates a segmentation estimate.|
||5 instances in total. (in cvpr2018)|
|858|Huaizu_Jiang_Self-Supervised_Relative_Depth_ECCV_2018_paper|Our hypothesis is that to excel at relative depth estimation, the CNN will benefit by learning to recognize such structures.|
|||3.2 Predicting Relative Depth From a Single Image  While a CNN for predicting depth from a single image is a core component of our system, we are primarily interested in relative depth prediction as a proxy task, rather than an end in itself.|
|||We therefore select standard CNN architectures and focus on quantifying the power of the depth task for pre-training and domain adaptation, compared to using the same networks with labeled data.|
|||Since the relative depth (i.e., the depth percentile) is estimated over the entire image, it is essential to feed the entire image to the CNN to make a prediction.|
|||: Unsupervised CNN for single view depth  estimation: Geometry to the rescue.|
||5 instances in total. (in eccv2018)|
|859|cvpr18-Variational Autoencoders for Deforming 3D Mesh Models|In recent years, effort has been made to generalize image-based CNN models to analyze 3D shapes.|
|||[31] use multi-view CNN models for 3D object classification.|
|||[35] propose a method that converts 3D shapes into a panoramic view, and propose a variant of CNN to learn the representation from such views.|
|||Maturana and Scherer [28] combine the volumetric occupancy grid representation with a 3D CNN to recognize 3D objects in real time.|
|||[36] propose to learn CNN models using geometry images [12], which is then extended to synthesize 3D models [37].|
||5 instances in total. (in cvpr2018)|
|860|cvpr18-PIXOR  Real-Time 3D Object Detection From Point Clouds|Overfeat [30] slides a CNN on different positions and scales and predicts a bounding box per class at each time.|
|||RCNN first extracts the whole-image feature map with an ImageNet [5] pretrained CNN and then predicts a confidence score as well as box position per proposal via a RoI-pooling operation on the whole-image feature map [13].|
|||Faster-RCNN [28] further proposes to learn to generate region proposals with a CNN and share the feature representation with detection, which leads to further gain in both performance and speed.|
|||Vehicles are then detected by applying a 2D CNN on the depth map.|
|||It combines CNN features extracted from multiple views (front view, birds eye view as well as camera view) to do 3D object detection.|
||5 instances in total. (in cvpr2018)|
|861|Wen_Latent_Factor_Guided_CVPR_2016_paper|In this paper, we explore the use of deep CNNs in AIFR and propose a latent factor guided CNN (LF-CNN) framework to learn the age-invariant deep face features.|
|||FaceNet [29] achieves 99.63% verification accuracy on LFW with a deep CNN trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.|
|||For fair comparison, we also train a baseline CNN model with the same networks as LF-CNNs, and learn all the parameters by SGD.|
|||This confirms that directly applying the deep CNN model to address the AIFR problem is indeed not a good choice.|
|||So it is desirable to design a new deep CNN model to address the AIFR problem.|
||5 instances in total. (in cvpr2016)|
|862|Thomas_Seeing_Behind_the_CVPR_2016_paper|Using this dataset, we examined the effectiveness of a variety of features (low and high-level, including CNN features) at identifying the photographer.|
||| Deep Convolutional Networks:   CaffeNet: This pre-trained CNN [18] is a clone of the winner of the ILSVRC2012 challenge [22].|
||| PhotographerNET: We trained a CNN with the same architecture as the previous networks to identify the author of photographs from our dataset.|
|||The best feature overall is in bold, and the best one per CNN in italics.|
|||Because a validation set is useful when training a CNN to determine when learning has peaked, we created a validation set by randomly sampling 10% of the images from the training set and excluding them from the training set for our CNN only.|
||5 instances in total. (in cvpr2016)|
|863|cvpr18-Low-Shot Learning With Large-Scale Diffusion|While early works on semisupervised labelling [14] were using ad-hoc semantic global descriptors like GIST [27], we extract activation maps from a CNN trained on images from a set of base classes that are independent from the novel classes on which the evaluation is performed.|
|||Complexity  For the complexity evaluation, we distinguish two In the off-line stage, (i) the CNN is trained on stages.|
|||We run the CNN on all images, and extract a 2048-dim vector from the 49th layer, just before the last fully connected layer.|
|||Particular object retrieval In ICLR,  with integral max-pooling of CNN activations.|
|||Efficient diffusion on region manifolds: Recovering small objects with compact CNN representations.|
||5 instances in total. (in cvpr2018)|
|864|Convolutional Neural Network Architecture for Geometric Matching|However, these methods differ from ours as they: (i) require class labels; (ii) dont use CNN features; (iii) jointly align a large set of images, while we align image pairs; and (iv) dont use a trainable CNN architecture for alignment as we do.|
|||Feature extraction  The first stage of the pipeline is feature extraction, for which we use a standard CNN architecture.|
|||A CNN without fully connected layers takes an input image and produces a feature map f  Rhwd, which can be interpreted as a h  w dense spatial grid of d-dimensional local descriptors.|
|||Matching network  The image features produced by the feature extraction networks should be combined into a single tensor as input to the regressor network to estimate the geometric transforma 6149  Figure 3: Correlation map computation with CNN features.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||5 instances in total. (in cvpr2017)|
|865|Liu_The_Treasure_Beneath_2015_CVPR_paper|For all methods, we adopt the pretrained Alex net [1] provided in the caffe [19] package to extract CNN activations.|
|||The time can be break down into two parts, time spend on extracting CNN features and time spend on performing pooling.|
|||However, it still requires more CNN extraction time than ours since beside the same convolutional feature extraction step as required by our method it also requires additional convolution operations to extract the fully connected layer activations.|
|||Our discovery suggests that if used appropriately, convolutional layers of a pretrained CNN contains very useful information and can be turned into a powerful image representation.|
|||[3] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, CNN features off-the-shelf: an astounding baseline for recognition, Proc.|
||5 instances in total. (in cvpr2015)|
|866|Yoon_Pixel-Level_Matching_for_ICCV_2017_paper|Indeed, CNN generally requires training with abundant and representative datasets for specific classes.|
|||Finally, the localization of the target object using a CNN is also a challenging issue, especially in relation to pixel-level accuracy.|
|||Most CNN structures encode category-level semantic information using features from deep layers generally exploited for object classification.|
|||Our CNN pixel-level matching structure is designed and pre-trained using the Caffe toolbox.|
|||We conduct a comparison with one hand-crafted feature based method (TD: Thermal-infrared based drivable region detection [32]) and three different CNN based approaches (AlexNet [11], CN24 [3], and FCN [15]).|
||5 instances in total. (in iccv2017)|
|867|Zhu_segDeepM_Exploiting_Segmentation_2015_CVPR_paper|For each box we extract the last feature layer of the CNN network [15], that is fine-tuned on the PASCAL dataset as proposed in [9].|
|||We run the image through the CNN [15] trained on the ImageNet dataset and fine-tuned on PASCALs data [9].|
|||We then warp the image in each enlarged box to 227 227 and fine-tune the original ImageNet-trained CNN using these images and labels.|
|||For our contextual features ctx(x, p) we extract the f c7 layer features of the expanded CNN by running the warped image in the enlarged window through the network.|
|||In particular, we first extract the CNN features and regress to a corrected set of boxes.|
||5 instances in total. (in cvpr2015)|
|868|Kernel Square-Loss Exemplar Machines for Image Retrieval|Note that ESVM features can be derived from arbitrary base features (e.g., CNN activations) of the exemplar and the images in the generic negative pool.|
|||We manually rotate by 90 degrees some images that are not in their natural orientation to compensate for the fact that CNN features are not rotation invariant [1, 4, 14, 20, 31].|
|||The first CNN features we use consist of the activation coefficients of the previous-to-last layer of the AlexNet architecture [22], based on a publicly available pre-trained model [19].|
|||In this table, we present our results for VLAD [10], sum-pooling of convolutional features (SPoC) [3], activation coefficients from the previous-to-last CNN layer (AlexNet) [22] and activation of NetVLAD layer [1].|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||5 instances in total. (in cvpr2017)|
|869|cvpr18-2D 3D Pose Estimation and Action Recognition Using Multitask Deep Learning|We present our contributions as follows: First, the proposed pose estimation method achieves  15137  Multitask CNN t=1t=T...RGB imagest=0AggregationAction: "Baseball pitch"AppearencerecognitionPoserecognition2D/3D estimated poses...Visualfeatures...t=0t=1t=TProbabilitymaps...t=0t=1t=Tstate-of-the-art results on 3D pose estimation and the most accurate results among regression methods for 2D pose estimation.|
|||The problem of human pose estimation has been intensively studied in the last years, from Pictorial Structures [2, 17, 37] to more recent CNN approaches [34, 25, 38, 20, 41, 54, 5, 51, 52, 36].|
|||The input image is fed through a CNN composed by one entry flow and K prediction blocks.|
|||Monocular 3d human pose estimation using transfer learning and improved CNN supervision.|
|||Monocap: Monocular human motion capture using a CNN coupled with a geometric prior.|
||5 instances in total. (in cvpr2018)|
|870|cvpr18-GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB|For training our CNN we propose a novel approach for the synthetic generation of training data that is based on a geometrically consistent image-to-image translation network.|
|||Finally, using annotated RGB images produced by our GAN, we train a CNN that jointly regresses image-space 2D and root-relative 3D hand joint positions.|
|||Given a live monocular RGB-only video stream we use a CNN hand joint regressor, the RegNet, to predict 2D joint heatmaps and 3D joint positions (Sec.|
|||The output images of the GeoConGANthe GANerated imagesare better suited to train a CNN that will work on real imagery.|
|||Such discrepancy between real and synthetic images limits the generalization ability of a CNN trained only on the latter.|
||5 instances in total. (in cvpr2018)|
|871|cvpr18-Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks|Based on large scale unlabeled video data, a CNN model is trained with adversarial learning.|
|||[27] estimate high-level structure (human body pose), and learn a LSTM and an analogy-based encoderdecoder CNN to generate future frames based on the current frame and the estimated high-level structure.|
|||Hence, we define a content loss function as a complement to the adversarial loss, to further ensure that the content of the generated  1In the generator, we can also use a 2D CNN to encode an image, but we duplicate the input image to a video to better fit our 3D U-net like architecture of G1.|
|||These features are extracted from a welltrained CNN model and previous experiments suggest they capture semantics of visual contents [13].|
|||The discriminator D2 of the Refine-Net is also a CNN with 3D convolutions and shares the same structure as D1 in the BaseNet.|
||5 instances in total. (in cvpr2018)|
|872|cvpr18-CleanNet  Transfer Learning for Scalable Image Classifier Training With Label Noise|[41] proposed attention in random sample groups but did not compare with standard CNN classifiers, and thus is less practical.|
|||In Section 3.3 we integrate CleanNet and the CNN into one framework for image classifier learning from noisy data.|
|||The first one is random sampling a subset from all images in class c and extract features using a pre-trained CNN fv()  / $  )  .|
|||We compare with a widely used classification filtering method: we train a CNN model on noisy data and predict top-K classes for each training image.|
|||CleanNet and all the baselines depend on a CNN to extract image features.|
||5 instances in total. (in cvpr2018)|
|873|cvpr18-A Closer Look at Spatiotemporal Convolutions for Action Recognition|Furthermore, an image-based 2D CNN (ResNet-152 [25]) operating on individual frames of the video achieves performance remarkably close to the state-of-the-art on the challenging Sports-1M benchmark.|
|||Another influential approach to CNN-based video modeling is represented by the two-stream framework introduced by Simonyan and Zisserman [29], who proposed to fuse deep features extracted from optical flow with the more traditional deep CNN activations computed from color RGB input.|
|||This type of CNN architecture is illustrated in Figure 1(a).|
|||3.2. f(cid:173)R2D: 2D convolutions over frames  Another 2D CNN approach involves processing independently the L frames via a series of 2D convolutional residual block.|
|||This type of CNN architectures is illustrated in Figure 1(b).|
||5 instances in total. (in cvpr2018)|
|874|cvpr18-Im2Struct  Recovering 3D Shape Structure From a Single RGB Image|The structure masking network is a two-scale CNN which is trained to produce a contour mask for the object of interest.|
|||The structure recovery network first fuses the feature map of the masking network and the CNN feature of the original image, and decode the fused feature recursively into a box structure.|
|||The structure recovery network fuses the features extracted in the structure masking network and the CNN features of the original input image and feed them into a recursive neural network (RvNN) for 3D structure decoding [13].|
|||For the task of 3D modeling from 2D images, discriminative models are learned to map an input image directly to the output 3D representation, either by a deep CNN for one-shot generation or a recurrent model for progressive generation [2].|
|||Another channel is the CNN feature of the original image extracted by a VGG-16.|
||5 instances in total. (in cvpr2018)|
|875|Ionescu_Unmasking_the_Abnormal_ICCV_2017_paper|To build our appearance features, we consider a pre-trained CNN architecture able to process the frames as fast as possible, namely VGG-f [3].|
|||We hereby note that better anomaly detection performance can probably be achieved by employing deeper CNN architectures, such as VGG-verydeep [21], GoogLeNet [23] or ResNet [8].|
|||Hence, we simply use the pre-trained CNN to extract deep features as follows.|
|||Regarding the CNN features, we show that slightly better results can be obtained with the conv5 features rather than the fc6 or fc7 features.|
|||Since the CNN features are more robust to illumination variations, we obtain a framelevel AUC of 86.5%.|
||5 instances in total. (in iccv2017)|
|876|Deep Temporal Linear Encoding Networks|Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences.|
|||FV encoding [31] and VLAD [1, 12] have lately been integrated as a layer in ConvNet architectures, and CNN encoded features have produced superior results for several challenging tasks.|
|||The feature maps are matrices {S1, S2, ..., SK} of size S  Rhwc, where h, w and c denote the height, width, and number of channels of the CNN feature maps.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||5 instances in total. (in cvpr2017)|
|877|Learning and Refining of Privileged Information-Based RNNs for Action Recognition From Depth Sequences|Unfortunately, these conventional end-toend manners (CNN+RNN) are difficult to be applied to action recognition from depth sequences due to the fact that: 1) Color and texture are precluded in depth maps, which weaken the discriminative power of the representation captured by the CNN model.|
|||Compared to the widely used CNN encoder for RGB data [30, 37], our encoder is more compact and effective for depth sequences.|
|||Our approach is an end-to-end trainable network that jointly learns the parameters of the CNN and the RNN.|
|||Instead, the CNN encoder is trained from the scratch in the learning stage.|
|||DisturbLabel: Regularizing CNN on the Loss Layer.|
||5 instances in total. (in cvpr2017)|
|878|Pauline_Luc_Predicting_Future_Instance_ECCV_2018_paper|Our approach offers several advantages: (i) we handle cases in which the model output has a variable size, as in object detection and instance segmentation, (ii) we do not require labeled video sequences for training, as the interme Predicting Future Instance Segmentation  3  diate CNN feature maps can be computed directly from unlabeled data, and (iii) we support models that are able to produce multiple scene interpretations, such as surface normals, object bounding boxes, and human part labels [10], without having to design appropriate encoders and loss functions for all these tasks to drive the future prediction.|
|||Our contributions are the following:   the introduction of the new task of future instance segmentation, which is  semantically richer than previously studied anticipated recognition tasks,   a self-supervised approach based on predicting high dimensional CNN features of future frames, which can support many anticipated recognition tasks,  experimental results that show that our feature learning approach improves over strong baselines, relying on optical flow and repurposed instance segmentation architectures.|
|||[18] were the first to predict high level CNN features of future video frames to anticipate actions and object appearances in video.|
|||were predicting the activations of much more compact fully connected CNN layers.|
|||3 Predicting Features for Future Instance Segmentation  In this section we briefly review the Mask R-CNN instance segmentation framework, and then present how we can use it for anticipated recognition by predicting internal CNN features of future frames.|
||5 instances in total. (in eccv2018)|
|879|cvpr18-Multi-Level Factorisation Net for Person Re-Identification|Related CNN Architectures Instead of constructing each block with holistic modules as in [18, 37, 9], a split transform-merge strategy [40] is used to construct the modularised block architecture in ResNeXt [44].|
|||Images of men  We proposed MLFN, a novel CNN architecture that learns to discover and dynamically identify discriminative latent factors in input images for person Re-ID.|
|||Multi-scale triplet cnn for person re-identification.|
|||Person re-identification using cnn features learned from combination of attributes.|
|||Exploiting the complementary strengths of multi-layer cnn features for image retrieval.|
||5 instances in total. (in cvpr2018)|
|880|Wang_SORT_Second-Order_Response_ICCV_2017_paper|Hence, a CNN can be considered as a composite function, which is trained by back-propagating error signals defined by the difference between supervision and prediction at the top layer.|
|||An intriguing property of the CNN lies in its transfer  ability.|
|||Another family of multi-branch networks follow the bilinear CNN model [35], which constructs two separate streams to model the co-occurrence of local features.|
|||For example, the bilinear CNN [35] computes the outer-product of neural responses from two individual networks to capture feature co-occurrence at the same spatial positions.|
|||Training a bilinear CNN is often slow, even in the improved versions [9][33].|
||5 instances in total. (in iccv2017)|
|881|Liu_RankIQA_Learning_From_ICCV_2017_paper|We then use fine-tuning to transfer the knowledge represented in the trained Siamese Network to a traditional CNN that estimates absolute image quality from single images.|
|||The classical approach trains a deep CNN regressor directly on the ground-truth.|
|||[14] achieved spectacular results with a CNN in the ImageNet competition, that they achieved wide attention and adoption in the broader computer vision community.|
|||We call this learning from rankings approach RankIQA, and with it we learn to rank image in terms of quality using Siamese Networks, and then we transfer knowledge learned from ranked images to a traditional CNN fine-tuned on IQA data in order to improve the accuracy of IQA.|
|||In [12] the authors design a multi-task CNN to learn the type of distortions and image quality simultaneously.|
||5 instances in total. (in iccv2017)|
|882|Xie_InterActive_Inter-Layer_Activeness_CVPR_2016_paper|A CNN is composed of several stacked layers, in each of which responses from the previous layer are convoluted and activated by a differentiable function.|
|||Hence, a CNN can be considered as a composite function, and is trained by back-propagating error signals defined by the difference between supervised and predicted labels at the top level.|
|||A discussion of how different CNN configurations impact deep feature performance is available in [4].|
|||The Activeness of Network Connections  Let a deep CNN be a mathematical function h(cid:0)X(0); (cid:1),  in which X(0) denotes the input image and  the weights over neuron connections.|
|||DisturbLabel: Regularizing CNN on the Loss Layer.|
||5 instances in total. (in cvpr2016)|
|883|Shen_Deep_Binaries_Encoding_ICCV_2017_paper|RPN, CNN and LSTM are utilized to form the image encoding function f () (the upper network), encoding image regions from dominant to minor.|
|||Recent works in region-based CNN [15, 58] show great potential in detecting semantically meaningful areas  ak = (ck + dk)/2,  (2)  where ck  (0, 1) denotes the confidence score determined by the RPN and dk  (0, 1) refers to the normalized proportion of the k-th detected proposal in an image.|
|||The benchmark CNN architecture AlexNet [32] is involved here, from which a feature representation of 4096-D is obtained.|
|||Additionally, the CNN feature of the holistic image is also appended to the end of the RNN input sequence, making a total of K + 1 semantic regions for encoding.|
|||As we expected, the MAP scores drop dramatically with simple image CNN (TVDB-I1, TVDB-I2) and bag-of-words features (TVDB-T1), but are still acceptable compared to some existing methods.|
||5 instances in total. (in iccv2017)|
|884|Wei-Chih_Hung_Learning_to_Blend_ECCV_2018_paper|Recently, many CNN based methods for image editing tasks have been proposed with impressive results, such as image filtering [29,24,22], enhancement [1,14,50,9], inpainting [38,51], composition [57,45,44,49], colorization [15,23,54,56], and image translation [16,58,3,28,59].|
|||Most of these approaches consist of a CNN model that directly transforms a single input photo to the desired output.|
|||Recently, many methods [18,21,30,31,33,39] are proposed based on CNN models.|
|||Therefore, we consider the evaluation function conditional on the foreground image and build it with a CNN model that takes both the blending photo and the foreground image as input.|
|||We denote the proposed CNN as quality network since it indicates the aesthetic quality of the blending result.|
||5 instances in total. (in eccv2018)|
|885|Yang_WIDER_FACE_A_CVPR_2016_paper|For negative samples, we randomly cropped  2More details of Multi-scale Cascade CNN can be found in the project page: http://mmlab.ie.cuhk.edu.hk/projects/ WIDERFace/.|
|||If the proposed window is a false positive, the CNN outputs a vector of [1, 1, 1, 1].|
|||We evaluate the multi-scale cascade CNN and baseline  (c) WIDER Hard  Figure 8.|
|||8, the multi-scale cascade CNN obtains 8.5% AP improvement on the WIDER Hard subset compared to the retrained Faceness, suggesting its superior capability in handling faces with different scales.|
|||For the WIDER Medium subset, the multi-scale cascade CNN outperforms other baseline methods with a considerable margin.|
||5 instances in total. (in cvpr2016)|
|886|Yang_Du_Interaction-aware_Spatio-temporal_Pyramid_ECCV_2018_paper|In action recognition, many existing methods [13,14] often use CNN or RNN based architectures to extract the channel-level or frame-level local features, which are subsequently modeled by combining LSTM with soft attention.|
|||This layer can be plugged into a general CNN to form an  4  Yang Du et al.|
|||We aim to adding an attention layer into the general CNN after one convolutional layer to emphasize the features of the key local regions and further improve the performance of the network.|
|||So, as long as the feature maps of a CNN constructing the spatio-temporal pyramid are selected, the parameters are determined and are irrelevant to K. Based on this fact, we can expediently use frames of different numbers to train and test our networks.|
|||An interaction-aware spatio-temporal pyramid attention layer is inserted to CNN to aggregate K group of feature maps by K frames.|
||5 instances in total. (in eccv2018)|
|887|Carlos_Esteves_Learning_SO3_Equivariant_ECCV_2018_paper|2 Related work  We will start describing related work on group equivariance, in particular equivariance on the sphere, then delve into CNN representations for 3D data.|
|||In 3D deep learning, the most natural adaptation of 2D methods was to use a voxel-grid representation of the 3D object and amend the 2D CNN framework to use collections of 3D filters for cascaded processing in the place of conventional 2D filters.|
|||[8] observe significant overfitting when attempting to train the aforementioned end-to-end and choose to amend the technique using subvolume classification as an auxiliary task, and also propose an alternate 3D CNN which learns to project the volumetric representation to a 2D representation, then processed using a conventional 2D CNN architecture.|
|||Yet, the Spherical CNN outperforms it, even with orders of magnitude fewer parameters and faster training.|
|||Spherical CNN accuracy on rotated ModelNet40.|
||5 instances in total. (in eccv2018)|
|888|Haque_Recurrent_Attention_Models_CVPR_2016_paper|The CNN was pretrained on augmented training examples before RAM training.|
|||(6) A threedimensional CNN operates on 3D point clouds.|
|||(8) A 3D CNN with average pooling [9] over time [74] .|
|||The input to both the 3D CNN and 3D RAM are 3D point clouds.|
|||The 3D CNN does not perform such data augmentation and instead operates on the entire point cloud.|
||5 instances in total. (in cvpr2016)|
|889|Justin_Liang_End-to-End_Deep_Structured_ECCV_2018_paper|In particular, we exploit a CNN to predict semantic segmentation, semantic edge information as well as crosswalk directions.|
|||This 4-channel image is then fed  Structured Models for Drawing Crosswalks  5  to a multi-task CNN that is trained to produce semantic segmentation, semantic contour detection as well as angles defining the crosswalk direction.|
|||The second entry in Table 1 shows the results of using only the output of the CNN models semantic segmentation branch for the final prediction.|
|||Speed: The CNN forward pass runs at 50 ms per image.|
|||Here the CNN outputs both an inverse distance transform and predicted segmentation.|
||5 instances in total. (in eccv2018)|
|890|Discretely Coding Semantic Rank Orders for Supervised Image Hashing|Specifically, the pre-trained VGG-19 CNN model [36] on ImageNet [31] is fine-tuned with a Euclidean loss layer with the supervision from B obtained by Algorithm 1.|
|||Retrieval result comparison (MAP, precision of top 100 samples, NDCG of top rank 50 with Hamming distance ranking and training/test time cost) on SUN397 dataset using 4096-d CNN features from VGG19 fc7.|
|||Note that we only evaluate DSeRH-L on the CIFAR-10 dataset with GIST features since tiny images are not suitable for the CNN feature learning used in DSeRH-D.  All methods are evaluated with different code lengths (i.e., 16, 32, 64 and 128) under four possible evaluation metrics: Mean Average Precision (MAP), precision@top100, Normalized Discounted Cumulative Gain (NDCG@rank50), and precision-recall curve.|
|||Retrieval result comparison (MAP, precision of top 100 samples, NDCG of top rank 50 with Hamming distance ranking and training/test time cost) on ImageNet100 dataset using 4096-d CNN features from VGG19 fc7.|
|||DSeRH supported hash functions of both linear and deep CNN models.|
||5 instances in total. (in cvpr2017)|
|891|Zheng_Learning_View-Invariant_Features_ICCV_2017_paper|In this paper, we also employ CNN and LSTM networks to extract features for CVPI, but using optical flows as input and also incorporating 3D human skeleton data as a stream of the network.|
|||The flow-stream sub-networks consist of identical CNN and LSTM networks and share parameters.|
|||The CNN network consists of five convolutional layers, followed by max-pooling layers and dropout layers.|
|||The flow images are resized to 227  227 pixels before they are fed to CNN network.|
|||Each frame of optical flow features from CNN are the input to an LSTM node, which represents that particular time step.|
||5 instances in total. (in iccv2017)|
|892|Kafle_Answer-Type_Prediction_for_CVPR_2016_paper|Their LSTM model used a one-hot encoding of question words and CNN features from a pre-trained network.|
|||A linear transformation mapped the CNN features to the same dimensionality of the question words.|
|||In [15], a similar approach was taken, with the main difference being that they fed CNN features to the LSTM as the first word, followed by vectors encoding each word of the sentence, and then finally the last word was the CNN In a variant of this approach, [12] features once more.|
|||For the Bayesian model, we reduced the dimensionality of the CNN features using linear discriminant analysis to K  1 dimensions, where K is the number of possible answers.|
|||This may be because we and others used off-the-shelf CNN features that were tuned to recognize objects on ImageNet.|
||5 instances in total. (in cvpr2016)|
|893|Zhao_Saliency_Detection_by_2015_CVPR_paper|Deep CNN aims to mimic the functions of neocortex in human brain as a hierarchy of filters and nonlinear operations.|
|||Superpixel segmentation is firstly performed on images using the SLIC [2] method, and the input of global-context CNN is a superpixel-centered large context window including the full image.|
|||(6)  Then, the corresponding unnormalized saliency prediction score function is formulated as  gc,1xgc + fu(wT  gc,1xgc + )wT  lc,1xlc, (7)  1 ) = wT  f (xgc, xlc;  where fu() is defined as (cid:26) t  fu(t) =  0  While the CNN at the upper branch (global-context model) aims to robustly model saliency with few large errors, CNN at the lower branch are designed to look at details it focuses on a smaller context to refine the saliency prediction of the centered superpixel.|
|||Similar strategy in [16] can be directly used to finetune the contemporary CNN models for saliency detection.|
|||Despite the disparity in class labels, it is shown that deep CNN pre-trained for 1, 000-class classification can be generalized for fine-tuning the classification problem with fewer classes [16].|
||5 instances in total. (in cvpr2015)|
|894|Variational Bayesian Multiple Instance Learning With Gaussian Processes|[7] get region proposals from selective search, compute CNN and Fisher vectors on top of them, and use a multi-fold MIL approach for final prediction.|
|||[27] assign a heuristic score to region proposals and train one single CNN interchangeably, which lets them achieve similar performance levels with much less computational effort.|
|||From prior work, we deduce three observations: i) endto-end training improves performance, ii) scoring region proposals with high recall is of critical importance, iii) all three weakly supervised object detection methods build one module on a CNN pretrained on supervised external data.|
|||We perform region proposal generation and feature extraction jointly and recruit a pretrained CNN for this task.|
|||This indicates that while the CNN part of the pipeline is very powerful, a strong MIL method as the second part is still necessary for state of the art results.|
||5 instances in total. (in cvpr2017)|
|895|Mazaheri_Video_Fill_in_ICCV_2017_paper|Spatial Attention Model  We use a CNN (i.e., VGG-19 [2], more details are provided in Section 3.4) to extract visual features.|
|||First, we apply max-pooling over the raw CNN features (i.e the output from the last pooling layer of VGG-19 pre-trained network) from all video key-frames to obtain a spatial visual feature from the whole video:  quences can be embedded as:  q1 l = W1 q1 r = W1  x[x1, x2, ..., xt1]  x[xn, xn1, ..., xt+1],  Source	Sentence  Textual	 Encoder  Augmentation  (2)  Video	Frames  VGG19  Max	Pooling	over	 last	Pooling	layers  Augmented	  Textual	 Features [k	x	m]  Visual	 Features [k	x	m]  Trainable	 Weights  [k x	1]  Combined	  Representation  [k	x	m]  Spatial	Attention  [m]  q2 l = [l, W2 r = [r, W2 q2  x[x1, x2, ..., xt1], l]  x[xn, xn1, ..., xt+1], r],  (4)  F = tanh(Wf (f (f t))|f tF ),  (7)  where W2 two external memory vectors obtained by:  x is of the same size as W1  x and r, l  Rc are  r = u1 l = u1  l W rW  (5)  l |  |q1  r are two sequences while |q2  where W  Rhc encodes the LSTMs outputs to memory vectors.|
|||Ask Your Neurons [24] encodes the visual CNN feature and concatenate with each of words and pass them through left and right LSTMs one by one.|
|||We believe any other very deep CNN network like GoogLeNet [3] or ResNet [4] can produce similar results.|
|||For temporal attention in section 2.3, we use pre-trained 3D CNN (C3D) network [32] pre-trained on [38] and followed settings defined in [32].|
||5 instances in total. (in iccv2017)|
|896|Discriminative Bimodal Networks for Visual Localization and Detection With Natural Language Queries|A few pioneering works [21, 38] use recurrent neural language models [15, 39, 50] and deep image represen (a) Conditional captioning using RNNs  (cid:4666)(cid:2869)|(cid:4667)(cid:4666)(cid:2870)|(cid:4667)(cid:4666)(cid:2871)|(cid:4667)(cid:4666)(cid:2872)|(cid:4667)(cid:4666)(cid:2873)|(cid:4667)(cid:4666)(cid:3032)(cid:3031)|(cid:4667)  spots  end  with  black  white  dog            RNN  RNN  RNN  RNN  RNN  RNN  =   white dog with black spots  dog with a ball in its month    black leather chair   start    =  white =  (cid:4666)(cid:2869),  dog =  (cid:2870),  with =  (cid:2871),  Text   Network  black =  spots =  (cid:2873)(cid:4667) (cid:2872), (cid:4666)=(cid:883)|,(cid:4667) (cid:4666)=(cid:882)|,(cid:4667) (cid:4666)=(cid:882)|,(cid:4667)  (cid:883) (cid:882) (cid:882)  positive image region negative  image region  positive phrase negative  phrase  (b) Our discriminatively  trained CNN   Figure 1: Comparison between (a) image captioning model and (b) our discriminative architecture for visual localization.|
|||For textual representations, we develop a character-level CNN [60] for extracting phrase features.|
|||Recently, character-level CNN has also been demonstrated an effective way for paragraph categorization [60] and zero-shot image classification [44].|
|||Model learning  In DBNet, we drive the training of the proposed twopathway bimodal CNN with a binary classification objective.|
|||VGGNet is the default visual CNN for all methods.|
||5 instances in total. (in cvpr2017)|
|897|Chen_A_Deep_Visual_ICCV_2015_paper|Our work is closely related to a recent paper [35], in which CNN is leveraged to compute stereo matching costs.|
|||The architecture of our CNN leads to two orders of magnitude (100) speed-up in computing a dense disparity map compared to [35] with little sacrifice in reconstruction accuracy.|
|||We apply a 4-layer CNN model to extract features f (I), followed by an inner-product layer to calculate the matching score < f (I L), f (I R) > and an ensemble voting.|
|||It takes about 3 hours to train the CNN with KITTI training set.|
|||Regarding the running time, the CNN step takes about 1.0s on average, 734ms at the original resolution and 267ms at the coarse resolution.|
||5 instances in total. (in iccv2015)|
|898|cvpr18-Recurrent Slice Networks for 3D Segmentation of Point Clouds|[19] designed a multi-view CNN for object detection in point clouds.|
|||[30] designed a 2D CNN for 3D shape recognition by taking as inputs multi-view images.|
|||Through experiments, the performances of RSNets are compared with various state-of-the-art 3D segmentation methods including 3D volumes based methods [31], spectral CNN based method [35], and point clouds based methods [21, 23, 14].|
|||In [31], the data representation is voxelized 3D volumes and a 3D CNN is built for segmenting objects in the volumes.|
|||Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation.|
||5 instances in total. (in cvpr2018)|
|899|StyleBank_ An Explicit Representation for Neural Image Style Transfer|These CNN algorithms either apply an iterative optimization mechanism [12], or directly learn a feed-forward generator network [19, 37] to seek an image close to both the content image and the style image  all measured in the CNN (i.e., pre-trained VGG-16 [36]) feature domain.|
|||Notwithstanding their demonstrated success, the principles of CNN style transfer are vaguely understood.|
|||[12] successfully applies CNN (pre-trained VGG-16 networks) to neural style transfer and produces more impressive stylization results compared to classic texture transfer methods.|
|||This idea is further extended to portrait painting style transfer [35] and patch-based style transfer by combining Markov Random Field (MRF) and CNN [22].|
|||Learning frame models  using cnn filters.|
||5 instances in total. (in cvpr2017)|
|900|FASON_ First and Second Order Information Fusion Network for Texture Recognition|One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features.|
|||They defined a hypercolumn representation, where the descriptor of each pixel was constructed using activations of multiple CNN units above that pixel.|
|||combined multiple layers of CNN features after training to further improve performance on texture recognition [4].|
|||Given an input image I, we extract its deep convolutional features F  Rwhch from a CNN and calculate the bilinear feature B  Rchch as:  B(F ) =  w  h  X  X  i=1  j=1  Fi,jF T i,j  (1)  This formulation is related to orderless texture descriptors such as VLAD , Fisher Vectors and region covariance [31].|
|||Combining the improvements from first and second order information fusion with multi-layer feature fusion, we obtain a 2% improvement from a strong bilinear CNN baseline for both VGG-16 and VGG-19.|
||5 instances in total. (in cvpr2017)|
|901|Zhang_Online_Collaborative_Learning_CVPR_2016_paper|This idea has also  been applied in the latest work on visual learning [17, 14], where the CNN model is fine-tuned with image embeddings learned by collaborative filtering.|
|||Similar to CNN which has no label space reduction, it defines W = I and U as the SVM model; 3) WSABIE: an online large-scale vocabulary image annotation method [46].|
|||Except for CNN and IncSVM, the dimensionality r is a crucial parameter.|
|||Therefore, N-way classifiers such as CNN can still perform well.|
|||Besides similar observations as above, we can see that the performance of CNN and IncSVM considerably drop by using only one epoch.|
||5 instances in total. (in cvpr2016)|
|902|cvpr18-VITAL  VIsual Tracking via Adversarial Learning|Extensions include, but are not limited to, kernelized correlation filters [23], scale estimation [10], re-detection [37], spatial regularization [12, 14, 9], ADMM optimization [29], sparse representation [32, 42, 33], CNN feature integrations [36, 43, 64, 28] and end-to-end CNN predictions [57, 55, 50].|
|||Proposed Algorithm  We build VITAL upon the CNN tracking-by-detection framework, which consists of feature extraction and classification.|
|||Given an input frame, we first generate multiple candidate proposals and extract their CNN features.|
|||We feed the CNN features of the candidate proposals into the classifier to get the probability scores.|
|||MDNet improves CNN-SVM through an endto-end CNN network formulation.|
||5 instances in total. (in cvpr2018)|
|903|cvpr18-Deep Regression Forests for Age Estimation|This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free update rule derived from Variational Bounding.|
|||Recently, end-to-end learning with CNN has become very popular and has been shown to be useful for improving the performance of various computer vision tasks, such as image classification [35], semantic segmentation [38] and object detection [44, 13].|
|||With the rapid development of deep networks, more and more end-to-end CNN based age estimation methods [46, 41, 1] have been proposed to address this non-linear regression problem.|
|||Following the recent deep learning based age estimation method [46], we use the VGG-16 Net [51] as the CNN part of the proposed DRFs.|
|||Ordinal In  regression with multiple output cnn for age estimation.|
||5 instances in total. (in cvpr2018)|
|904|Shi_Real-Time_Single_Image_CVPR_2016_paper|To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space.|
|||[3] proposed to use multi-stage trainable nonlinear reaction diffusion (TNRD) as an alternative to CNN where the weights and the nonlinearity is trainable.|
|||integrating it into the CNN as part of the SR operation was not fully recognised and the option not explored.|
|||Evaluation performed on an extended bench mark data set with upscaling factor of 4 shows that we have a significant speed (> 10) and performance (+0.15dB on Images and +0.39dB on videos) boost compared to the previous CNN approach with more parameters [7] (5-3-3 vs 9-5-5).|
|||This makes our model the first CNN model that is capable of SR HD videos in real time on a single GPU.|
||5 instances in total. (in cvpr2016)|
|905|Zimmermann_Learning_to_Estimate_ICCV_2017_paper|The CNN architecture of Toshev and Szegedy [24] directly regresses 2D cartesian coordinates from color image input.|
|||[22] used a CNN for detection of hand keypoints in 2D, which is conditioned on a multi-resolution image pyramid.|
|||[12] train a CNN that directly regresses 3D coordinates given hand cropped depth maps.|
|||[13] utilizes a CNN that can synthesize a depth map from a given pose estimate.|
|||Monocular 3d Human Pose Estimation Using Transfer Learning and Improved CNN Supervision.|
||5 instances in total. (in iccv2017)|
|906|Xu_Jointly_Attentive_Spatial-Temporal_ICCV_2017_paper|The typical architecture is composed of two parts: a feature extracting network, usually a CNN or RNN, and multiple metric learning layers to make final prediction.|
|||A recent work [20] used CNN to obtain feature representation from the multiple frames of the video, then applied RNN to learn the interaction between them.|
|||As shown in Figure 2, each input (one frame from a video with optic flow involved) is passed through a CNN network to extract feature maps from the last convolutional layer.|
|||In addition, we also  Table 1. layer parameter of the CNN network  Layer  Type  Conv(size,  channel, pad, stride)  Max Pooling  Conv 1  c+t+p  Conv 2  c+t+p  Conv 3  c+t  55, 16, 4, 1  55, 32, 4, 1  55, 32, 4, 1  22  22  N/A  c: Convolutional layer; t: Tanh layer; p: Pooling layer  take identity classification loss into consideration, following the work [20].|
|||ASTPN extends the standard RNN-CNNs by decomposing pooling into two steps: a spatial-pooling on feature map from CNN and an attentive temporal-pooling on the output of RNN.|
||5 instances in total. (in iccv2017)|
|907|Ouyang_Joint_Deep_Learning_2013_ICCV_paper|The rst convolutional layer and its following average pooling layer use the standard CNN settings.|
|||We start with a 1-layer CNN using supervised training.|
|||A one-layer CNN (CNN-1layer in Fig.|
|||A two-layer CNN (CNN-2layer in Fig.|
|||Adding more convolutional and pooling layers on the top of the two-layer CNN does not improve the performance.|
||5 instances in total. (in iccv2013)|
|908|Temporal Convolutional Networks for Action Segmentation and Detection|[16] introduced a spatiotemporal CNN with a constrained segmental model which they applied to 50 Salads.|
|||We use the spatial CNN features of Lea et al.|
|||The camera is mounted on the users head and is  25.5 46.9 52.8 67.7 72.2  50 Salads (higher) F1@{10, 25, 50} Edit Acc 68.0 Spatial CNN [16] 35.0, 30.5, 22.7 71.1 Dilated TCN 55.8, 52.3, 44.3 71.3 ST-CNN [16] 61.7, 57.3, 47.2 Bi-LSTM 70.9 72.2, 68.4, 57.8 76.5, 73.8, 64.5 73.4 ED-TCN F1@{10, 25, 50} Edit Acc 50 Salads (mid) 54.9 32.3, 27.1, 18.9 Spatial CNN [16] 48.7 44.4, 38.9, 27.8 IDT+LM [25] Dilated TCN 52.2, 47.6, 37.4 59.3 59.4 55.9, 49.6, 37.1 ST-CNN [16] 55.7 62.6, 58.3, 47.0 Bi-LSTM 68.0, 63.9, 52.6 64.7 ED-TCN  24.8 45.8 43.1 45.9 55.6 59.8  Table 2.|
|||Receptive field experiments (left) ED-TCN: varying layer count L and filter durations d (right) Dilated TCN: varying layer count L and number of blocks B.  GTEA EgoNet+TDD [29] Spatial CNN [16] ST-CNN [16] Dilated TCN Bi-LSTM ED-TCN   F1@{10,25,50} Acc 64.4 54.1 60.6 58.3 55.5 64.0  41.8, 36.0, 25.1 58.7, 54.4, 41.9 58.8, 52.2, 42.2 66.5, 59.0, 43.6 72.2, 69.3, 56.0  Table 4.|
|||Unlike EgoNet and TDD, our approach uses simpler spatial CNN features which can be computed in real-time.|
||5 instances in total. (in cvpr2017)|
|909|cvpr18-Feature Space Transfer for Data Augmentation|1, objects subject to pose variation span low-dimensional manifolds of image space, or corresponding spaces of CNN features.|
|||The encoder maps the feature responses x of a CNN for an object image into a pair of appearance A(x) and pose P(x) parameters.|
|||While our work uses an encoder-decoder architecture, which is fairly common in the GAN-based image generation literature, we aim for a different goal of generating CNN feature responses.|
|||Motivation  In this work, we assume the availability of a training set with pose annotations, i.e., {(xn, pn, yn)}n, where xn  RD is the feature vector (e.g., a CNN activation at some layer) extracted from an image, pn is the corresponding pose value and yn a category label.|
|||Recognition accuracy depends on the network used to extract the feature vectors, denoted as CNN in Fig.|
||5 instances in total. (in cvpr2018)|
|910|3D Bounding Box Estimation Using Deep Learning and Geometry|2) A novel discrete-continuous CNN architecture called MultiBin regression for estimation of the objects orientation.|
|||Pavlakos et al [14], used CNN to localize the keypoints and they used the keypoints and their 3D coordinates from meshes to recover the pose.|
|||Correspondence Constraints  Using the regressed dimensions and orientations of the 3D box by CNN and 2D detection box we can solve for the translation T that minimizes the reprojection error with respect to the initial 2D detection box constraints in Equation 2.|
|||The CNN architecture of our parameter estimation module is shown in Figure 5.|
|||For each bin, the CNN network estimates both a confidence probability ci that the output angle lies inside the ith bin and the residual rotation correction that needs to be applied to the orientation of the center ray of that bin in order to obtain the output angle.|
||5 instances in total. (in cvpr2017)|
|911|Ciprian_Corneanu_Deep_Structure_Inference_ECCV_2018_paper|Finally, edges Ep is an empty set, since in our model an independent CNN is trained on each image patch Ij and we do not assign any edge among p. Given this assumption, probability distribution P (y, p|x, ) is given by:  P (y, p|x, ) = P (y|p, x, ) Y  P (pk|x, ).|
|||Each input image is cropped into a set of patches {Ii}P i=1 which is used for training an independent CNN for producing a probability vector pi for N AUs (p in Eq.|
|||The DSIN is end-to-end trainable and CNN features can be trained based on gradients back-propagated from structure inference in a multi-task learning fashion.|
|||Each i is a CNN computing N AUs probabilities through sigmoid function at last layer.|
|||When comparing our proposed model with ROI on BP4D our CNN trained just on face without class balancing has inferior results.|
||5 instances in total. (in eccv2018)|
|912|cvpr18-R-FCN-3000 at 30fps  Decoupling Detection and Classification|It is due to the powerful feature learning capabilities of deep CNN architectures.|
|||Indeed, the very first application of deep-learning for object detection [13] used Selective-Search [39] to obtain class-agnostic object proposals and classified them using a deep CNN fine-tuned AlexNet in this case.|
|||A region proposal network (RPN) is used for generating object proposals, which is a two layer CNN on top of the conv4 features.|
|||We show that careful design choices with respect to the CNN architecture, loss function and training protocol can yield a large-scale detector trained on the ImageNet classification set with significantly better accuracy compared to weakly supervised detectors.|
|||These two branches help detect the super-classes which are represented by each cluster k. For obtaining finegrained class information, we employ a two layer CNN on the conv5 feature map, as shown in Fig .|
||5 instances in total. (in cvpr2018)|
|913|cvpr18-TieNet  Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays|They conditioned the long short-term memory (LSTM) decoder on different parts of the input image during each decoding step, and the attention signal was determined by the previous hidden state and CNN features.|
|||The initial CNN component uses layers borrowed from ImageNet pre-trained models for image classification, e.g., ResNet-50 (from Conv1 to Res5c).|
|||The CNN component additionally includes a convolutional layer (transition layer) to manipulate the spatial grid size and feature dimension.|
|||The intuition behind our model is that the connection between the CNN and RNN network will benefit the training of both because the image activations can be adjusted for the text embedding task and salient image features could be extracted by pooling based on high text saliency.|
|||It may be because the gradients from RNN are backpropagated to the CNN part and the adjustment of image features from Transition layer will benefit the report generation task.|
||5 instances in total. (in cvpr2018)|
|914|Xiaokun_Wu_HandMap_Robust_Hand_ECCV_2018_paper|The stateof-the-art methods have been classified and discussed in a recent survey based on the HANDS 2017 challenge [19] with respect to different aspects, such as learning outcome (regression vs. detection), dimension of CNN (2D vs. 3D), learning model structure (hierarchical vs. non-hierarchical), etc.|
|||[11] exploit a hierarchical tree-like structured CNN to estimate hand joints from local poses.|
|||[14] train a 3D CNN based on projective distance fields from three canonical views to regress hand joint locations.|
|||[30] use a 3D CNN to estimate per-voxel likelihood for each joint, resulting in the best overall performance in the HANDS 2017 challenge compared with previous methods based on 2D CNNs and hand joint regression.|
|||Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns.|
||5 instances in total. (in eccv2018)|
|915|cvpr18-Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform|Specifically, we try to restore the visually ambiguous plant and building pairs using two different CNN models, each of which is specially trained on a plant dataset and a building dataset.|
|||In addition, semantic segmentation results on LR images are satisfactory given a contemporary CNN [32, 31, 28] that is fine-tuned on LR images.|
|||In contrast to these studies, we explore categorical priors in the form of segmentation probability maps in a CNN framework.|
|||Enforcing category-specific priors in CNN has been attempted in Xu et al.|
|||This method is computationally inefficient as we need to perform forward passes with several CNN models for a single input image.|
||5 instances in total. (in cvpr2018)|
|916|cvpr18-Weakly Supervised Instance Segmentation Using Class Peak Response|With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location.|
|||Most existing weakly supervised semantic segmentation methods consider convolutional filters in CNN as object detectors and aggregate the deep feature maps to extract class-aware visual evidence [47, 43].|
|||External localization network is therefore used to initialize object locations [26, 14, 23], and refining low-resolution CNN planes with pre-generated object segment proposal priors.|
|||Fully Convolutional Architecture  By simply removing the global pooling layer and adapting fully connected layers to 1x1 convolution layers, modern CNN classifiers can be seamlessly converted to fully convolutional networks (FCNs) [19] that naturally preserve spatial information throughout the forwarding.|
|||Experiment  We implement the proposed method using state-of-theart CNN architectures, including VGG16 and ResNet50, and evaluate it on several benchmarks.|
||5 instances in total. (in cvpr2018)|
|917|Locality-Sensitive Deconvolution Networks With Gated Fusion for RGB-D Indoor Semantic Segmentation|Instead of the handcrafted features, patch-wise CNN models [5] and RCNN models [11] are proposed to learn RGB-D features of the superpixels or region proposals.|
|||FCN adapts the CNN model designed for classification into an end-toend system for holistic scene segmentation.|
|||[5] concatenate the RGB and depth image as four-channel input for the CNN model (early fusion); Gupta et al.|
|||[11] leverage two CNN models to extract features from RGB and depth images independently, and then concatenate them to learn the final semantic classifier (middle fusion); Long et al.|
|||[19] also learn two independent CNN models but directly predict the score map of each modality, followed by score fusion with equal-weight sum (late fusion).|
||5 instances in total. (in cvpr2017)|
|918|Sangryul_Jeon_PARN_Pyramidal_Affine_ECCV_2018_paper|[15] developed a CNN architecture for geometry-invariant matching that estimates transformation parameters across semantically similar images and different modalities.|
|||PARN: Pyramidal Affine Regression Networks  3  In this paper, we present a novel CNN architecture, called pyramidal affine regression networks (PARN), that estimates locally-varying affine transformation fields across semantically similar images in a coarse-to-fine fashion, as shown in Fig.|
|||[12] proposed a CNN architecture for estimating a geometric model such as an affine transformation for semantic correspondence estimation.|
|||5 Conclusion  We presented a novel CNN architecture, called PARN, which estimates locallyvarying affine transformation fields across semantically similar images.|
|||In contrast to previous CNN based methods for geometric field estimations, our method yields locally-varying affine transformation fields that lie in the continuous solution space.|
||5 instances in total. (in eccv2018)|
|919|cvpr18-Thoracic Disease Identification and Localization With Limited Supervision|Following the R-CNN work [8], recent progresses has focused on processing all regions with only one shared CNN [11, 7], and on eliminating explicit region proposal methods by directly predicting the bounding boxes.|
|||first trained a CNN on image patches and then an image-level decision fusion model by patchlevel prediction histograms to generate the image-level labels [14].|
|||(b) The patch slicing layer resizes the convolutional features from the CNN using max-pooling or bilinear interpolation.|
|||Our framework can be easily extended to any other advanced CNN models.|
|||As the CNN gives c input feature maps with size of h  w, we down/up sample the input feature maps to P P through a patch slicing layer shown in Figure 3(b).|
||5 instances in total. (in cvpr2018)|
|920|cvpr18-PointGrid  A Deep Network for 3D Shape Understanding|Features obtained from multiple views are combined via a view pooling (which is the max-pooling) and then passed through another CNN to predict the final object label.|
|||They first render the 3D model with different views, each of which is fed through a shared CNN before unprojected back to 3D.|
|||Method  # shapes  test  total  Wu [56]  Yi  3D  [62] CNN Net [38]  PointKd-Net [27]  SpecCNN O-CNN PointGrid (ours)  [63]  [55]  mean IoU  airplane bag cap car chair earphone guitar knife lamp laptop motor mug pistol rocket skateboard table  341 14 11 158 704 14 159 80 286 83 51 38 44 12 31 848  2690 76 55 898 3758 69 787 392 1547 451 202 184 283 66 152 5271    81.4  63.2    73.5    74.4       74.8  81.0 78.4 77.7 75.7 87.6 61.9 92.0 85.4 82.5 95.7 70.6 91.9 85.9 53.1 69.8 75.3  79.4  75.1 72.8 73.3 70.0 87.2 63.5 88.4 79.6 74.4 93.9 58.7 91.8 76.4 51.2 65.3 77.1  83.7  83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6  77.2  79.9 71.2 80.9 68.8 88.0 72.4 88.9 86.4 79.8 94.9 55.8 86.5 79.3 50.4 71.1 80.2  84.7  81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1  85.9  85.5 87.1 84.77 77.0 91.1 85.1 91.9 87.4 83.3 95.4 56.9 96.2 81.6 53.5 74.1 84.4  86.4  85.7 82.5 81.8 77.9 92.1 82.4 92.7 85.8 84.2 95.3 65.2 93.4 81.7 56.9 73.5 84.6  used a higher resolution to represent an object (80 views of 224 224 image versus 16 16 16 4 in our PointGrid).|
|||PointNet [38] 3D CNN (323) 3D CNN (643) PointGrid (163)  Classification 8.98ms 27.50ms 49.12ms 14.91ms  Segmentation 28.37ms 64.32ms 136.54ms 48.94ms  use grid size of 16 16 16 where all the other volumetric methods use at least 32 32 32 grid.|
|||SyncSpecCNN: Synchronized spectral CNN for 3d shape segmentation.|
||5 instances in total. (in cvpr2018)|
|921|Seeing Invisible Poses_ Estimating 3D Body Pose From Egocentric Video|It greatly outperforms several alternative methods, including a CNN regression method modeled after the third-person DeepPose [5] approach retrained for our setting.|
|||In summary, our contributions are: (1) We tackle a new problem that estimates the wearers invisible pose from a single egocentric video; (2) We propose a novel global optimization method that leverages both learned dynamic and scene classifiers and the pose coupling over a long time span; and (3) We benchmark several methods, including hand crafted features and CNN learned features, for our task.|
|||We train a CNN classifier to categorize each image as representing a sittingor standinglike pose by fine tuning the last three layers of the fully connected network in AlexNet [26]; the learning rates of other layers are set to zero.|
|||We properly normalize all coordinates for the CNN sigmoid layer.|
|||We compute the local pose cost as one minus the class probability from the CNN output.|
||5 instances in total. (in cvpr2017)|
|922|cvpr18-A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation|Finally, a third component combines the images and uses an image classification CNN [9] to classify them.|
|||The most straightforward approach is to convert the point cloud to a uniform voxel grid and use CNN based methods for volumetric representations, such as the methods presented in [18, 27, 16, 2, 26].|
|||In MVCNN [23], a 3D shape model is rendered with different virtual cameras from fixed view points, and the resulting images are combined with a view pooling operation and classified with a CNN based architecture.|
|||Image recognition using CNNs Our method is related to image based CNN architectures, as we classify point clouds by first automatically extracting 2D images.|
|||Our key idea is to automatically transform the input point cloud to one or more depth images, which can be combined and classified by a CNN classification module.|
||5 instances in total. (in cvpr2018)|
|923|Joint Registration and Representation Learning for Unconstrained Face Identification|DeepFace and VGG-Face are based on common CNN architectures whereas FaceNet and DeepID use a specialized inception architecture.|
|||For example, CNN features in combination with triplet loss embedding are used in [4, 29].|
|||Five pose-specific CNN models are trained from facial data generated by 3D pose rendering in [1].|
|||Features from a bilinear CNN architecture are used in [4].|
|||Unconstrained face verification using deep cnn features.|
||5 instances in total. (in cvpr2017)|
|924|Zehao_Huang_Data-Driven_Sparse_Structure_ECCV_2018_paper|To address this problem, recently methods [50, 1, 43] proposed to apply group sparsity to retain a hardware friendly CNN structure.|
|||These scaling factors endow more flexibility to CNN with very few parameters.|
|||Several methods have been explored to learn CNN architectures without handcrafted design [2, 51, 32].|
|||3 Proposed Method  Notations Consider the weights of a convolutional layer l in a L layers CNN as a 4dimensional tensor Wl  RNlMlHlWl , where Nl is the number of output channels, Ml represents the number of input channels, Hl and Wl are the height and width of a 2-dimensional kernel.|
|||6  Z. Huang, N. Wang  MXNet implementation of APG  import mxnet as mx def apg_updater(weight, lr, grad, mom, gamma):  z = weight lr * grad z = soft_thresholding(z, lr * gamma) mom[:] = z weight + 0.9 * mom weight[:] = z + 0.9 * mom  def soft_thresholding(x, gamma):  y = mx.nd.maximum(0, mx.nd.abs(x) gamma) return mx.nd.sign(x) * y  In our framework, we add scaling factors to three different CNN micro-structures, including neurons, groups and blocks to yield flexible structure selection.|
||5 instances in total. (in eccv2018)|
|925|Yan_Exploiting_Multi-Grain_Ranking_ICCV_2017_paper|Apart from the pairwise related works, some researchers utilize triplet constraints to learn the similarity ranking in a CNN framework [24, 25, 26].|
|||In this paper we summarize the relationship of vehicle images as multiple grains and propose two multi-grain based ranking approaches integrated with multi-attribute classification in a multi-task CNN framework.|
|||All the images of a multi-grain list are extracted features in a shared CNN architecture.|
|||All images are extracted features in a shared CNN architecture supervised by multi-attribute labels.|
|||We add a softmax classification layer after fc7 layer to train attributes classification in VGG CNN M 1024 network [32].|
||5 instances in total. (in iccv2017)|
|926|Jin_End-To-End_Face_Detection_ICCV_2017_paper|We extract 4096-D feature vectors from the fc7 layer of a standard pre-trained face recognition CNN [26].|
|||In addition to these two images, we use a fixed reference set with G images (we typically set G = 50), and compute CNN feature vectors for each of these reference images.2 Let the CNN feature vectors for the reference images be R1, R2, ..., RG.|
|||If the 4,096 CNN features were statistically independent (but not identically distributed), then the distribution of rank-1 counts would be a binomial distribution (blue curve).|
|||However, the dependencies among the CNN features prevent the mismatched rank-1 counts distribution from being binomial, and so this approach is not possible.|
|||As shown in Table 1, we observe that our verifier has higher recall than three competing methods (all of which use the same base CNN representation) at low false positive rates.|
||5 instances in total. (in iccv2017)|
|927|cvpr18-Blind Predicting Similar Quality Map for Image Quality Assessment|Deep Image Quality Assessment  With the rise of CNN for detection and segmentation tasks [4, 13, 14], more and more researchers have started to apply the deep network into IQA.|
|||[8] constructed a shallow CNN only consisting of one convolutional layer to predict subjective scores.|
|||Most deep-IQA methods only employ CNN to extract discrimi 6374  Figure 2.|
|||BIECON [9] utilized a CNN to estimate a patch score map and utilized one hidden layer to regress the extracted patch-wise features into a subjective score.|
|||Previous works for deep NR-IQA [2, 8] lets f be a regression function using CNN with parameter .|
||5 instances in total. (in cvpr2018)|
|928|Weakly Supervised Action Learning With RNN Based Fine-To-Coarse Modeling|Combining deep learning and temporal modeling, the authors of [18] use a segmental CNN and a semi-Markov model to represent temporal transitions between actions.|
|||[13] integrate CNNs with hidden Markov models to learn sign language hand shapes based on a single frame CNN model from weakly annotated data.|
|||[21], aiming to learn a temporal order verification for human actions in an unsupervised way by training a CNN with correct vs. shuffled video snippets and thus capturing temporal information.|
|||They use an hybrid HMM model in combination with a CNN based visual food detector to align a sequence of instructions, e.g.|
|||Deep hand: How to train a CNN on 1 million hand images when your data is continuous and weakly labelled.|
||5 instances in total. (in cvpr2017)|
|929|cvpr18-An Analysis of Scale Invariance in Object Detection ­ SNIP|We also make minor modifications to the CNN architecture for classifying images of different scales.|
|||These are then up-sampled to 224x224 and provided as input to a CNN architecture trained on 224x224 size images, referred to as CNN-B (see Fig.|
|||These ranges were design decisions made during training, based on the consideration that after re-scaling, the resolution of the valid RoIs does not significantly differ from the resolution on which the backbone CNN was trained.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||5 instances in total. (in cvpr2018)|
|930|Brandon_RichardWebster_Visual_Psychophysics_for_ECCV_2018_paper|In order to compensate for Gaussian blur, Ding and Tao [52] perturb sequences of face images for the purpose of learning blurinsensitive features within a CNN model.|
|||For all experiments, we made use of the following face recognition algorithms: VGG-Face [59], FaceNet [60], OpenFace [61], a simple three-layer CNN trained via high-throughput search of random weights [62] (labeled slmsimple below), and OpenBR 1.1.0 [63], which makes use of handcrafted features.|
|||2, VGG-Face is once again the best performing algorithm, but remarkably, we see that the three-layer CNN trained via a random search for weights (labeled slmsimple) works just as well.|
|||(3) FaceNet uses a Multi-Task CNN [76] for facial landmark detection and alignment, while OpenFace uses dlib [77]  which FaceNet intentionally avoids due to its lower yield of faces for the training set.|
|||To look at the effect of CNN weight perturbations coupled with stimulus perturbations, we use VGG-Face as a case study.|
||5 instances in total. (in eccv2018)|
|931|Han_SCNet_Learning_Semantic_ICCV_2017_paper|MC-CNN [47] and its efficient extension [48] train CNN models to predict how well two image patches match and use this information to compute the stereo matching cost.|
|||[27] use CNN features pre-trained for ImageNet classification tasks (due to a lack of available datasets for learning semantic correspondence) with performance comparable to SIFT flow.|
|||Similarity function and geometry kernel  There are many possible choices for the function f that computes the appearance similarity of the two regions r and s making up match number m. Here we assume a trainable embedding function c (as will be shown later, c will be the output of a CNN in our case) that outputs a L2 normalized feature vector.|
|||In each case, SCNet takes as input two images IA and IB, and maps them onto feature maps FA and FB by CNN layers.|
|||Conclusion  We have introduced a novel model for learning semantic correspondence, and proposed the corresponding CNN architecture that uses object proposals as matching primitives and learns matching in terms of appearance and geometry.|
||5 instances in total. (in iccv2017)|
|932|Controlling Perceptual Factors in Neural Style Transfer|Patch-based methods have also been used with CNN features [16, 2], leading to improved texture representations and stylisation results.|
|||The method is based on a parametric texture model [14, 10, 19] defined by summary statistics on CNN responses [7] and appears to have several advantages over patch-based synthesis.|
|||The guidance channels are propagated to the CNN to produce guidance channels Tr l for each layer.|
|||We do this by applying Neural Style Transfer from the fine-scale style image to the coarse-scale style image, using only the Gram Matrices from lower layers in the CNN (e.g., only layer conv1 1 and conv2 1 in Fig.|
|||Since the receptive fields in a CNN have a fixed size, the stylisation outcome depends on the resolution of the input images: stylisation happens only up to the scale of the receptive fields in the output.|
||5 instances in total. (in cvpr2017)|
|933|Deep Roots_ Improving CNN Efficiency With Hierarchical Filter Groups|Improving CNN Efficiency with Hierarchical Filter Groups  Deep Roots:  Yani Ioannou1  Duncan Robertson2 Antonio Criminisi2  Roberto Cipolla1  1University of Cambridge, 2Microsoft Research  Abstract  We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root.|
|||We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets.|
|||Recently, learning a low-rank basis for filters was found to improve generalization while reducing the computational complexity and model size of a CNN with only full rank filters [9].|
|||We show that simple alterations to state-of-the-art CNN architectures can drastically reduce computational cost and model size without compromising accuracy.|
|||[2] propose a CNN architecture that is highly optimized for computational efficiency.|
||5 instances in total. (in cvpr2017)|
|934|Learning Cross-Modal Deep Representations for Robust Pedestrian Detection|thermal data) are used as a form of supervision for learning CNN features from RGB images.|
|||In [26], a CNN pre-trained with an unsupervised method based on convolutional sparse coding was presented.|
|||We first provide an overview of our approach and we describe in details the CNN architectures we design to reconstruct thermal data from RGB input and to transfer the learned crossmodal representations for the purpose of robust pedestrian detection.|
|||The first series of experiments aims to demonstrate the effectiveness of the proposed Cross-Modality Transfer CNN (CMT-CNN) framework.|
|||In fact, we believe that our unsupervised learning of cross-modal representations can be also integrated in other CNN architectures, to improve their robustness in coping with bad illumination conditions.|
||5 instances in total. (in cvpr2017)|
|935|Identifying First-Person Camera Wearers in Third-Person Videos|To handle the two modalities (motion and spatial-domain), we design a new two-stream Siamese CNN architecture where one stream captures temporal information using optical flow (i.e., motion) and the other captures spatial information (i.e., surrounding scene appearance), which we detail in Section 3.1.3.|
|||Contrastive loss: For the Siamese or semi-Siamese networks, we want firstand third-person frame representations generated by the CNN to be close only if they correspond to the same person.|
|||These included mapping of optical flow features from first-person to third-person view, direct matching of pre-trained CNN features, and learning an embedding space with traditional HOOF features.|
|||In addition to the above basic baselines, we tested two types of stronger baselines: (1) directly comparing standard video CNN features (two-stream [25] and C3D [27]) from firstand third-person videos, and (2) learning an embedding space with traditional HOOF (or motion magnitude).|
|||We found that a combination of three innovations achieved the best results: (1) a semi-Siamese structure, which takes into account different features of firstand third-person videos (as opposed to full Siamese), (2) a two-stream CNN structure which combines spatial and motion cues (as opposed to a single stream), and (3) a triplet loss which explicitly enlarges the margin between firstand third-person videos (as opposed to Siamese contrastive loss).|
||5 instances in total. (in cvpr2017)|
|936|Hyo_Jin_Kim_Hierarchy_of_Alternating_ECCV_2018_paper|Beyond the detailed innovations, our proposed algorithm is generalizable to other categorization tasks, and is applicable to any CNN architecture.|
|||In contrast to organizing multiple CNN models, there have been efforts to separate visual features of a single CNN in a tree structure [3,26,31,36,42].|
|||Our algorithm is applicable to a variety of CNN models and visual category recognition tasks.|
|||Arandjelovi c, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: NetVLAD: CNN ar chitecture for weakly supervised place recognition.|
|||Wu, R., Wang, B., Wang, W., Yu, Y.: Harvesting discriminative meta objects with  deep cnn features for scene classification.|
||5 instances in total. (in eccv2018)|
|937|cvpr18-Objects as Context for Detecting Their Semantic Parts|Our work is related to those that explicitly train CNN models to localize semantic parts using bounding-boxes [68, 14], as opposed to keypoints [1, 4] or segmentation masks [13, 17, 22, 24, 27, 43].|
|||based on a CNN that scores a set of region proposals [46] by processing them through several layers of different types.|
|||Intuitively, a CNN is a good framework to learn this regressor, as the activation maps of the network contain localized information about the parts of the object [5052].|
|||Partstacked cnn for fine-grained visual categorization.|
||4 instances in total. (in cvpr2018)|
|938|Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper|Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs.|
|||A wide range of architectures has been proposed since the successful implementation of a large-scale CNN [19].|
|||One can seamlessly integrate CBAM in any CNN architectures and jointly train the combined CBAM-enhanced networks.|
|||5 Conclusion  We have presented the convolutional block attention module (CBAM), a new approach to improve representation power of CNN networks.|
||4 instances in total. (in eccv2018)|
|939|Guney_Displets_Resolving_Stereo_2015_CVPR_paper|In combination, both smoothness terms are able to reduce the error by 16.6% for reflective and by 5.7% for all image regions considering the better performing CNN features.|
|||This figure shows the decrease in error and energy on all image regions vs. MP-PBP iterations using CNN (left) and SGM (right).|
|||6 shows the average energy and error over the images in validation set using CNN (left) and SGM (right) as input disparity maps.|
|||We thus require 265 seconds in total for processing a single image using the full model in combination with CNN based matching costs.|
||4 instances in total. (in cvpr2015)|
|940|Xu_Lan_Person_Search_by_ECCV_2018_paper|For the CNN architecture, we adopt the state-of-the-art ResNet-50 [14] as the backbone network (Fig.|
|||For all compared methods, we utilised the same person detection model and the same backbone identity matching network (except DeepMu that exploits a specially proposed CNN architecture) as the CLSA for fair comparison.|
|||This verifies our hypothesis that directly applying the CNN feature hierarchy may harm the model performance due to the intrinsic semantic discrepancy across different pyramid levels.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||4 instances in total. (in eccv2018)|
|941|Self-Supervised Video Representation Learning With Odd-One-Out Networks|Self-Supervised Video Representation Learning With Odd-One-Out Networks  ACRV, The Australian National University University of Oxford QUVA Lab, University of Amsterdam  Basura Fernando Hakan Bilen Efstratios Gavves Stephen Gould  Abstract  We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called odd-oneout learning.|
|||Here each image is transformed using a large number of transformations and a CNN is trained to recognize instances of transformed images.|
|||Mathematically, let us assume that given long video X we have sub-sample m subsequences of size W , denoted by { Xi} where i = 1    m. Therefore the CNN returns the conditional probability of action category y for subsequence Xi which is denoted by p(y| Xi).|
|||Exploiting image-trained CNN architectures for unconstrained video classification.|
||4 instances in total. (in cvpr2017)|
|942|Wilkinson_Neural_Ctrl-F_Segmentation-Free_ICCV_2017_paper|Given an input image, it is fed through the first CNN of the model and Dilated Text Proposals (DTP) are extracted.|
|||The proposals are fed through a second CNN and finally, each box coordinates are fine-tuned, given a wordness score, and a descriptor is extracted.|
|||The 34-layer pre-activation ResNet [11] was chosen as a CNN architecture due to its high performance and expressiveness compared to its small memory footprint.|
|||These are then fed through the rest of the CNN and  then used as input to three branches.|
||4 instances in total. (in iccv2017)|
|943|Nicholas_Rhinehart_R2P2_A_ReparameteRized_ECCV_2018_paper|We designed a CNN model that takes in M and outputs O  RHmapWmap6.|
|||RNN: The Linear and Field models reason with different contextual inputs: Linear uses the past, and CNN uses the feature map M .|
|||M is passed through a CNN similar to Fields.|
|||5: RNN and CNN Policy models.|
||4 instances in total. (in eccv2018)|
|944|Bai_Ensemble_Diffusion_for_ICCV_2017_paper|We follow its pipeline by training an 8-layer CNN as the view feature extractor, and apply Hausdorff matching to activations of the 7th fully-connected lay Methods  Vol.|
|||The two similarities fused are Volumetric CNN and GIFT.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations.|
||4 instances in total. (in iccv2017)|
|945|Su_Pose-Driven_Deep_Convolutional_ICCV_2017_paper|The global image and the new modified part image are then fed into our CNN together.|
|||t datasets have different sizes, it is not appropriate to directly use the CNN models pre-trained on the ImageNet dataset [7].|
|||Global+Part denotes CNN trained through two streams without FEN and FWN.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||4 instances in total. (in iccv2017)|
|946|cvpr18-Jointly Localizing and Describing Events for Dense Video Captioning|Sequence learning approaches utilize CNN plus RNN architecture to generate novel sentences with more flexible syntactical structures.|
|||Technically, given input video V = {vt}Tv  t=1, a 3D CNN is utilized to encode the frame sequence into a series of clip-level features {ft = F (vt : vt+)}Tf t=1 where  is the temporal resolution of each feature ft.|
|||To empirically verify the merit of our proposed dense video captioning system, we compare the following video captioning baselines:  (1) Long Short-Term Memory (LSTM) [33]: LSTM utilizes a CNN plus RNN framework to directly translate from video pixels to natural language descriptions.|
|||(3) Temporal Attention (TA) [34]: TA combines the frame representation from GoogleNet [29] and video clip representation from 3D CNN trained on hand-crafted descriptors.|
||4 instances in total. (in cvpr2018)|
|947|cvpr18-Learning Attentions  Residual Attentional Siamese Network for High Performance Online Visual Tracking|For example, CF2 [35] and DeepSRDCF [11] concatenated features from different layers of a pretrained CNN such as VGG [40] into correlation filter.|
|||For instance, CNN-SVM tracker [23] utilized CNN model with saliency map and SVM.|
|||DeepTrack [29] casted tracking as a classification problem and employed a candidate pool of multiple CNN as a data-driven model of different instances of the target object.|
|||The TSN tracker [45] proposed a CNN network encoding temporal and spatial information in the context of classification.|
||4 instances in total. (in cvpr2018)|
|948|Jiafan_Zhuang_Towards_Human-Level_License_ECCV_2018_paper|The sequential methods [21, 32, 7, 6] are proposed in recent years, which first conduct sliding window to extract the sequential features of a license plate, and then recognize the license plate from the extracted features by a CNN or RNN model.|
|||On the other hand, our proposed method possesses higher computational efficiency because semantic segmentation once recognizes all pixels of an input image and only several times (far less than the number of characters in a license plate) of calls to the CNN model are needed in the subsequent counting procedure.|
|||With the arrival of deep learning, character recognition in the LPR task is usually implemented by a CNN network [25].|
|||Here the models for feature extraction include sweep OCR [7], CNN [21], and FCN [32].|
||4 instances in total. (in eccv2018)|
|949|Yu_Semantic_Jitter_Dense_ICCV_2017_paper|Visual Comparison Predictor  Let xi  RNx denote an image with Nx pixels and let (xi)  RD denote its D-dimensional descriptor (e.g., Gist, color, CNN feature, or simply raw pixels).|
|||This deep learning to rank method combines a CNN optimized for a paired ranking loss [4] together with a spatial transformer network (STN) [14].|
|||As above, our approach trains this CNN with all pairs in {PA S SA}.|
|||3Pretrained CNN features with RankSVM proved inferior.|
||4 instances in total. (in iccv2017)|
|950|cvpr18-Learning Compressible 360° Video Isomers|Other works study visual features in 360 images such as detecting SIFT [18] or learning a CNN either from scratch [23, 14] or from an existing model trained on ordinary perspective images [39].|
|||One common approach is to improve predictive coding using either a feed-forward CNN [35, 34] or recurrent neural network (RNN) [43, 44, 20].|
|||Another approach is to allocate the bit rate dynamically using a CNN [30].|
|||Given the input video in cubemap format, we extract both motion and appearance features (details below) and feed them into a CNN that predicts the video size S for each .|
||4 instances in total. (in cvpr2018)|
|951|Wang_Multiple_Granularity_Descriptors_ICCV_2015_paper|Detection network is finetuned from a generic CNN using its associated grain labels, with original raw input image; description network is further refined from detection network with the same label set, but is instead fed with the image regions selected by ROIs from the detection network.|
|||fine-grained detection CNN is fed with labels at species level).|
|||We could feed it into detection CNN (which is refined from ImageNet with the target grain labels) to extract features (second half of Table 4).|
|||Or, alternatively let the detection CNN act as region of interest generator which picks up potential region hypothesis according to feature map scores.|
||4 instances in total. (in iccv2015)|
|952|Nguyen_Shadow_Detection_With_ICCV_2017_paper|First, they trained two Convolutional Neural Networks (CNNs): one CNN is trained to label shadow regions, the other CNN was trained to label shadow boundaries.|
|||[30] used a stacked CNN for large scale shadow detection.|
|||The stacked architecture refines the image level predictions of a FCN [17], pretrained for semantic labeling, with a patch-based CNN tuned on shadow data.|
|||The discriminator is a CNN with the following layers: conv R64, conv BR128, conv BR256,  4513  .|
||4 instances in total. (in iccv2017)|
|953|Varun_Jampani_Superpixel_Sampling_Networks_ECCV_2018_paper|The algorithm starts with deep image feature extraction using a CNN (line 1).|
|||The CNN for feature extraction is composed of a series of convolution layers interleaved with batch normalization [21] (BN) and ReLU activations.|
|||We use 3  3 convolution filters with the number of output channels set to 64 in each layer, except the last CNN layer which outputs k  5 channels.|
|||Method  We perform an additional experiment where we plug SSN into the downstream semantic segmentation network of [13], The network in [13] has bilateral inception layers that makes use of superpixels for longrange data-adaptive information propagation across intermediate CNN representations.|
||4 instances in total. (in eccv2018)|
|954|ViP-CNN_ Visual Phrase Guided Convolutional Neural Network|proposed Faster R-CNN by utilizing CNN to do region proposal [37].|
|||Combining the RNN and CNN becomes a standard pipeline on solving the Image Captioning problems [46, 21, 5, 10, 11, 25].|
|||Taking the output of the Conv4 3 as input, three convolutional layers are used for extracting CNN features.|
|||A language model based on word vectors of the object categories and a visual model based on the CNN feature of the object pair are then trained to recognize the interactions.|
||4 instances in total. (in cvpr2017)|
|955|cvpr18-Leveraging Unlabeled Data for Crowd Counting by Learning to Rank|As introduced in the review of [32], CNN-based approaches can be classified into different categories based on the properties of the CNN (see Table 1 for an overview of state-of-the-art networks and the properties they hold).|
|||Basic CNNs incorporate only basic CNN layers in their networks.|
|||The Basic CNN is trained from scratch on the training set.|
|||Learning to count with cnn boosting.|
||4 instances in total. (in cvpr2018)|
|956|Weakly Supervised Dense Video Captioning|Lexical based CNN model is of great advantages over the ImageNet based CNN model [39] in image/video captioning, since the ImageNet based CNN model only captures a limited number of object concepts, while the lexical based CNN model is able to capture all kinds of semantic concepts (nouns for objects and scenes, adjective for shape and attributes, verb for actions, etc).|
|||Multi-Instance	Multi-Label	Learning  to adopt/fine-tune the existing ImageNet CNN models with lexical output.|
|||For instance, [7] adopted a weakly supervised multiple instance learning (MIL) approach [27, 56] to train a CNN based word detector without the annotations of image-region to words correspondence; and [1] applied a multiple label learning (MLL) method to learn the CNN based mapping between visual inputs and multiple concept tags.|
|||It can process a 30-frame video clip in about 840ms on the TitanX GPU, including 570ms for CNN feature extraction, 90ms for region-sequence generation, and 180ms for language model.|
||4 instances in total. (in cvpr2017)|
|957|Pan_Jointly_Modeling_Embedding_CVPR_2016_paper|propose to use a 3-D CNN for modeling video clip dynamic temporal structure and an attention mechanism to select the most relevant temporal clips [34].|
|||We compare our LSTM-E approach with two 2-D CNN of AlexNet [14] and the 19-layer VGG [23] network both pre-trained on Imagenet ILSVRC12 dataset [22], and one 3D CNN of C3D [28] pre-trained on Sports-1M video dataset  4598  [11].|
|||(6) Soft-Attention (SA) [34]: SA combines the frame representation from GoogleNet [25] and video clip representation based on a 3-D CNN trained on Histograms of Oriented Gradients (HOG), Histograms of Optical Flow (HOF), and Motion Boundary Histogram (MBH) handcrafted descriptors.|
|||Compared to LSTM-E (Alex), LSTM-E (VGG) using a more powerful frame representation brought by a deeper CNN exhibits significantly better performance.|
||4 instances in total. (in cvpr2016)|
|958|Look Closer to See Better_ Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition|A visual attentionbased approach proposes a two-level domain-net on both objects and parts, where the part templates are learned by clustering scheme from the internal hidden representations in CNN [31].|
|||Picking deep filter responses [34] and multigrained descriptors [28] propose to learn a set of part detectors by analyzing filter responses from CNN that respond to specific patterns consistently in an unsupervised way.|
|||Classification and Ranking  The proposed recurrent attention CNN is optimized by two types of supervision, i.e., intra-scale classification loss and inter-scale pairwise ranking loss, for alternatively generating accurate region attention and learning more finegrained features.|
||| PN-CNN [2]: pose normalized CNN proposes to com pute local features by estimating the objects pose.|
||4 instances in total. (in cvpr2017)|
|959|Saining_Xie_Rethinking_Spatiotemporal_Feature_ECCV_2018_paper|This idea was successfully used in [24], who proposed a two-stream architecture where one CNN stream handles raw RGB input, and the other handles precomputed optical flow.|
|||Since then, many video classification methods follow the same multi-stream 2D CNN design, and have made improvements in terms of new representations [25, 26], different backbone architecture [2729, 17], fusion of the streams [3033] and exploiting richer temporal structures [3436].|
|||4.1 Replacing all 3D convolutions with 2D  In this section, we seek to determine how much value 3D convolution brings, motivated by the surprising success of 2D CNN approaches to video classification (see e.g., [36]).|
|||Model  Backbone Val Top-1 (%) Val Top-5 (%) Test Top-1 (%)  Pre-3D CNN + Avg [7] VGG-16 Multi-scale TRN [39] Inception Inception Inception Inception Inception  I2D I3D S3D  S3D-G   34.4 34.4 45.8 47.3 48.2   63.2 69.0 76.5 78.1 78.7  11.5 33.6   42.0  Table 3.|
||4 instances in total. (in eccv2018)|
|960|Papadopoulos_We_Dont_Need_CVPR_2016_paper|In most of our experiments, we use AlexNet [28] as the underlying CNN architecture.|
|||In sections 5.2, 5.3 we carry out a detailed analysis of our system in these settings, using AlexeNet as CNN architecture [28].|
|||For completeness, in section 5.4 we also present results when using the complete trainval set and the deeper VGG16 as CNN architecture [46].|
|||Hence, on both CNN architectures our verification-based training scheme produces high quality detectors, achieving 90% of the mAP of their fully supervised counterparts.|
||4 instances in total. (in cvpr2016)|
|961|Guoliang_Kang_Deep_Adversarial_Attention_ECCV_2018_paper|We train a source CNN which guides the attention alignment of the target CNN whose convolutional layers have the same architecture as the source network.|
|||The target CNN is trained with a mixture of real and synthetic images from both source and target domains.|
|||t1 is the parameters of the target-domain CNN at last training step t  1.|
|||Table 1: Classification accuracy (%) for MNIST  MNIST-M. CNN denotes the source and target network (Section 4.2).|
||4 instances in total. (in eccv2018)|
|962|Busta_Deep_TextSpotter_An_ICCV_2017_paper|The novelties include: training of both text detection and recognition in a single end-to-end pass, the structure of the recognition CNN and the geometry of its input layer that preserves the aspect of the text and adapts its resolution to the data.|
|||[10] train a character-centric CNN [14], which takes a 24  24 image patch and predicts a text/notext score, a character and a bigram class.|
|||Training  We pre-train the detection CNN using the SynthText dataset [6] (800, 000 synthetic scene images with multiple words per image) for 3 epochs, with weights initialized from ImageNet [22].|
|||The recognition CNN is pre-trained on the Synthetic Word dataset [7] (9 million synthetic cropped word images) for 3 epochs, with weights randomly initialized from the N (0, 1) distribution.|
||4 instances in total. (in iccv2017)|
|963|cvpr18-Tips and Tricks for Visual Question Answering  Learnings From the 2017 Challenge|The method is based on a ResNet CNN within a Faster R-CNN framework [29].|
|||In all cases, the CNN is pretrained and held fixed during the training of the VQA model.|
|||Those photographs are passed through a ResNet-101 CNN pretrained on ImageNet [13].|
|||Deeper lstm and normalized cnn visual question answering model.|
||4 instances in total. (in cvpr2018)|
|964|Ba_Predicting_Deep_Zero-Shot_ICCV_2015_paper|The last layer of the CNN is then passed through a linear projection to produce a set of image features g. The score of the class is produced via f g.|
|||It is common to boost the performance of a vision system by using the features from the fully connected layer of a CNN [4].|
|||Predicting a joint classifier  We can also take advantage of the CNN architecture by using features extracted from both the intermediate convolutional layers and the final fully connected layer.|
|||(3.3) that predicts the convolutional filters for CNN feature maps.|
||4 instances in total. (in iccv2015)|
|965|Mohammad_Tavakolian_Deep_Discriminative_Model_ECCV_2018_paper|[12] designed TwoStream CNN which includes the spatial and the temporal networks.|
|||[14] proposed a factorized spatiotemporal CNN and exploited different ways to decompose 3D convolutional kernels.|
|||However, mainstream CNN frameworks usually focus on appearances and short-term motions.|
|||Method  iDT+HSV [42] MoFAP [43] Two-Stream CNN [12] C3D (3 nets) [13] C3D (3 nets)+iDT [13] FSTCN (SCI Fusion) [14] TDD+FV [44] KVMF [41]  DDM DDM+SCSP  Average Accuracy (%)  87.9 88.3 88.0 85.2 90.4 88.1 90.3 93.1  91.5 94.3  To compare our approach with the benchmark, we obtain the average precession performance for each class and take the mean average precession (mAP) as indicated in Table 3.|
||4 instances in total. (in eccv2018)|
|966|Liu_Deep_Learning_Face_ICCV_2015_paper|(2) A novel fast feed-forward algorithm for CNN with locally shared filters is devised.|
|||[7] demonstrated that off-the-shelf features learned by CNN of ImageNet [13] can be effectively adapted to attribute classification.|
|||(1) i  To this end, we propose an interweaved operation, which is a fast feed-forward method for CNN with locally-shared filters.|
|||It is applicable to CNNs with local filters and compatible to all existing CNN operations.|
||4 instances in total. (in iccv2015)|
|967|Shen_DSOD_Learning_Deeply_ICCV_2017_paper|In the past several years, many innovative CNN network structures have been proposed.|
|||State-of-the-art CNN based object detection methods can be divided into two groups: (i) region proposal based methods and (ii) proposal-free methods.|
|||R-CNN requires high computational costs since each region is processed by the CNN network separately.|
|||We investigated all the stateof-the-art CNN based object detectors, and found that they could be divided into three categories.|
||4 instances in total. (in iccv2017)|
|968|Yongyi_Lu_Image_Generation_from_ECCV_2018_paper|The Sketchy Database [20] contributed a collection of sketch-photo pairs which were used to train a cross-domain CNN to embed them in the same space.|
|||The identity preserving features were extracted using the pretrained Light CNN [30] and compared using L2 norm.|
|||: Joint embeddings of shapes and images via cnn image purification.|
|||Wu, X., He, R., Sun, Z., Tan, T.: A light cnn for deep face representation with  noisy labels.|
||4 instances in total. (in eccv2018)|
|969|Luo_Pedestrian_Parsing_via_2013_ICCV_paper|Unlike CNN [11], whose weights are shared and locally connected, we find fully connecting adjacent layers in DDN can capture the global structures of humans and can improve the parsing results.|
|||autoencoder (bMAE) [16] and CNN [11] for data transformation on the HumanEva dataset.|
|||Instead, we cascade RoBM [24], PCA [25], and CNN [11] described in Sec.4.1 as our baseline.|
|||The RoBM and CNN are tuned on PPSS for fair comparison.|
||4 instances in total. (in iccv2013)|
|970|Wei_Tang_Deeply_Learned_Compositional_ECCV_2018_paper|v are CNN mappings with   u and   Part sharing and higher-order potentials.|
|||4 Experiments  4.1  Implementation details  The proposed DLCM is a general framework and can be instantiated with any compositional body structures and CNN modules.|
|||To instantiate a DLCM with three semantic levels, we need five hourglass modules, i.e., the five CNN blocks in Fig.|
|||The final prediction is the maximum activating location of the score map for a given joint predicted by the last CNN module.|
||4 instances in total. (in eccv2018)|
|971|Ahmet_Iscen_Local_Orthogonal-Group_Testing_ECCV_2018_paper|Nowadays, image search is dominated by CNN descriptors [11 15], which also use high dimensional non-sparse vectors to represent images.|
|||Tolias, G., Sicre, R., J egou, H.: Particular object retrieval with integral max pooling of cnn activations.|
|||Radenovi c, F., Tolias, G., Chum, O.: CNN image retrieval learns from bow: Un supervised fine-tuning with hard examples.|
|||Radenovi c, F., Tolias, G., Chum, O.: Fine-tuning cnn image retrieval with no  human annotation.|
||4 instances in total. (in eccv2018)|
|972|Real-Time Video Super-Resolution With Spatio-Temporal Networks and Motion Compensation|The common paradigm for CNN based approaches has been to upscale the LR image with bicubic interpolation before attempting to solve the SR problem [6, 22].|
|||However, increasing input image size through interpolation considerably impacts the computational burden for CNN processing.|
|||Sub(cid:173)pixel convolution SR  to be the result of low-pass filtering and downscaling by a  For a given LR image I LR  RHW which is assumed factor r the HR image I HR  RrHrW , the CNN superresolved solution I SR  RrHrW can be expressed as  I SR = f (cid:0)I LR; (cid:1) .|
|||Using sub-pixel convolution allows to process I LR directly in the LR space and then use nL1 = r2 output filters to obtain an HR output tensor with shape 1  r2  H  W that can be reordered  One of the most straightforward approaches for a CNN to process videos is to match the temporal depth of the input layer to the number of frames d0 = D0.|
||4 instances in total. (in cvpr2017)|
|973|Sun_Automatic_Concept_Discovery_ICCV_2015_paper|Image representation and classifier training: Similar to [15, 21], we extracted CNN activations as image-level features; such features have shown state-of-the-art performance in recent object recognition results [22, 13].|
|||We adapted the CNN implementation provided by Caffe [18], and used the 19-layer network architecture and parameters from Oxford [38].|
|||The classifiers were trained using the same Oxford CNN architecture used for feature extraction.|
|||To achieve this, we selected training images associated with the concepts from Flickr 8k dataset, and learned concept detectors using the same CNN feature extractors and classifier training strategies as our proposed pipeline.|
||4 instances in total. (in iccv2015)|
|974|Wang_Actionness_Estimation_Using_CVPR_2016_paper|Fully convolutional networks  The feature map processed in each convolutional layer of CNN can be seen as a three-dimensional volume of size h  w  c, where h and w are the height and width of the  map respectively, and c is the number of map channels (filters).|
|||The input of CNN is a raw image, with h  w pixels and c colors.|
|||The basic components in CNN contain convolutional operation, pooling operation, and activation function.|
|||Hence, this structure allows CNN to have the desired property of translation invariance.|
||4 instances in total. (in cvpr2016)|
|975|End-To-End Instance Segmentation With Recurrent Attention|h = CNN(I) denotes passing an image I through a CNN and returning the hidden activation h. I  = D-CNN(h) denotes passing an activation map h through a de-convolutional network (D-CNN) and returning an image I .|
|||The CNN in the box network outputs a H   W   L feature map ut (H  is the height; W  is the  6657  Figure 2: Left: Detailed network design.|
|||[44] formulate a dense CRF for instance segmentation; they apply a CNN on dense image patches to make local predictions,  and construct a dense CRF to produce globally consistent labellings.|
|||To process an entire image, they treat each element of a CNN feature map individually.|
||4 instances in total. (in cvpr2017)|
|976|Bulat_How_Far_Are_ICCV_2017_paper|The method of [35] uses a CNN cascade to regress the facial landmark locations.|
|||The approach of [50] fits a 3DMM in an iterative manner through a single CNN which is augmented by additional input channels (besides RGB) representing shape features at each iteration.|
|||For the needs of this work, we built a powerful CNN for 2D and 3D face alignment based on two components: (a) the state-of-the-art HourGlass (HG) network of [23], and (b) the hierarchical, parallel & multi-scale block recently proposed in [7].|
|||To this end, we propose a guided-by-2D landmarks CNN which converts 2D annotations to 3D and unifies all aforementioned datasets.|
||4 instances in total. (in iccv2017)|
|977|Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper|Boundary maps and superpixels were further improved by progress in CNN architectures and data augmentation methods, using U-Nets [29], FusionNets [30] or inception modules [24].|
|||Holisticallynested edge detection [42,43] couples the CNN loss at multiple resolutions using  Mutex Watershed  5  deep supervision and is successfully used as a basis for watershed segmentation of medical images in [44].|
|||The CNN can be either trained to predict boundary pixels [27,24] or undirected affinities [25,50] which express how likely it is for a pixel to belong to a different cell than its neighbors in the 6-neighborhood.|
|||Building on the work of [25], we train a CNN to predict short and long-range affinities and then use those directly as weights for the Mutex Watershed algorithm.|
||4 instances in total. (in eccv2018)|
|978|cvpr18-Learning to Sketch With Shortcut Cycle Consistency|In this work, sketches are stored as vector images and a RNN decoder is employed to generate sketches from a CNN encoder embedding, resulting in clean and sharp line strokes, which has shown better sketch generation performance compared to [10].|
|||(c) generative CNN encoder Ep.|
|||(d) conditional CNN decoder Dp.|
|||Sampling the Latent Space  With the help of the KL loss, we are able to exploit the embedding space from CNN encoder Ep by effectively sampling from the latent vector z.|
||4 instances in total. (in cvpr2018)|
|979|Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper|[60] presented a shallow CNN with six layers that takes an eye image as input and fuses this with the head pose in the last fully connected layer of the network.|
|||[28] introduced a CNN which estimates the gaze by combining the left eye, right eye and face images, with a face grid, providing  RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments  5  the network with information about the location and size of the head in the original image.|
|||A spatial weights CNN taking the full face image as input, i.e.|
|||Recently, Deng and Zhu [10] suggested a two-step training policy, where a head CNN and an eye CNN are trained separately and then jointly fine-tuned with a geometrically constrained gaze transform layer.|
||4 instances in total. (in eccv2018)|
|980|Huang_Cross-Domain_Image_Retrieval_ICCV_2015_paper|The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).|
|||By comparing the performance of CNN with SVM and SVR, we can find the effectiveness of SVR in the R-CNN framework.|
|||Furthermore, the detection performance is further improved by replacing the CNN with pre-trained NIN.|
|||To evaluate the impact of various detectors on retrieval, we compare the top-20 retrieval accuracy of DARN with  y c a r u c c A    l  a v e i r t e R k p o T     0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0    0  Online-offline Image Retrieval Accuracy     DSIFT+FV, C=64 (0.091)  DSIFT+FV, C=128 (0.105)  Pre-trained CNN (0.268)  Pre-trained NIN (0.274)  AN (0.390)  ARN (0.440)  DARN (0.539)  DARN with Conv5 (0.549)  DARN with Conv4-5 (0.570)  10  20  30  40  50  60  70  80  90  100  Top-k Retrieval Result  Figure 5.|
||4 instances in total. (in iccv2015)|
|981|Making the v in VQA Matter_ Elevating the Role of Image Understanding in Visual Question Answering|It uses a CNN embedding of the image, a Long-Short Term Memory (LSTM) embedding of the question, combines these two embeddings via a point-wise multiplication, followed by a multi-layer perceptron classifier to predict a probability distribution over 1000 most frequent answers in the training dataset.|
|||It should be noted that MCB uses image features from a more powerful CNN architecture ResNet [12] while the previous two models use image features from VGGNet [37].|
|||It is a 2-channel network that takes in an image CNN embedding as input in one branch, question LSTM embedding as input in another branch, and combines the two embeddings by a point-wise multiplication.|
|||Deeper LSTM and normalized CNN Visual Question Answering model.|
||4 instances in total. (in cvpr2017)|
|982|cvpr18-Crowd Counting via Adversarial Cross-Scale Consistency Pursuit|These methods cannot provide the distribution of crowd, and such low-level features are outperformed by features extracted from CNN [34] which have better and deeper representations.|
|||[30] trained a classic Alexnet style CNN model to predict crowd counts.|
|||Multi-column CNN is employed by [37, 3].|
|||Contextual information is fused with high-dimensional feature maps extracted from a multi-column CNN by a Fusion-CNN consisting of a set of convolutional and fractionally-strided layers.|
||4 instances in total. (in cvpr2018)|
|983|Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper|[4] proposed SRCNN by firstly introducing a three-layer CNN for image SR. Kim et al.|
|||Deep CNN for SR.|
|||It has been demonstrated that stacked residual blocks and LSC can be used to construct deep CNN in [23].|
|||Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image  restoration.|
||4 instances in total. (in eccv2018)|
|984|Weakly Supervised Actor-Action Segmentation via Robust Multi-Task Ranking|We extract CNN features from three time slices of a supervoxel, i.e.|
|||We zero out pixels outside the superpixel boundary and use the rectangle image patch surrounding the superpixel as input to a pretrained CNN to get fc vectors, similar to R-CNN [13].|
|||We extract CNN features (fc vectors) from three sampled time slices of an action tube.|
|||For supervoxel and action tube features, we use pretained GoogLeNet [50] to extract CNN deep features of the average pooling layer 1024-dimensional feature vector.|
||4 instances in total. (in cvpr2017)|
|985|Deep Multi-Scale Convolutional Neural Network for Dynamic Scene Deblurring|And, in [26], classification CNN is trained to estimate locally linear blur kernels.|
|||First, we propose a multi-scale CNN that directly restores latent images without assuming any restricted blur kernel model.|
|||[29] proposed an image deconvolution CNN to deblur a blurry image in a non-blind setting.|
|||And then smoothly varying blur kernel is obtained by optimizing an energy model that is composed of the CNN likelihoods and smoothness priors.|
||4 instances in total. (in cvpr2017)|
|986|Yamaguchi_Spatio-Temporal_Person_Retrieval_ICCV_2017_paper|As shown in Figure 4, we extract: (1) the CNN features extracted from the RGB images, (2) the CNN features extracted from the sequences of the optical flow maps, and (3) the 3D-CNN (or C3D) features [38] extracted from the sequences of RGB images from (a) the inside of tubes and (b) the whole frames for each.|
|||To extract the CNN features from the RGB images, we use the outputs from the fc7 layer in the VGG-16 layer net pretrained on ImageNet.|
|||To extract the CNN features from the optical flow images, we use the outputs from the fc7 layer in the VGG-16 layer net first pretrained on ImageNet and then re-trained on the first split of the UCF101 dataset [35].|
|||As clip features, we use the features obtained by concatenating the temporal means of the three types of features (i.e., the CNN features from RGB images, the CNN features from optical flow maps, and the C3D features), extracted from the whole frames.|
||4 instances in total. (in iccv2017)|
|987|cvpr18-Direction-Aware Spatial Context Features for Shadow Detection|This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels.|
|||[6] detected shadows using a patch-level CNN and a shadow prior map generated from hand-crafted features, while Nguyen et al.|
|||7455  Figure 2: The schematic illustration of the overall shadow detection network: (i) we extract features in different scales over the CNN layers from the input image; (ii) we embed a DSC module (see Figure 4) to generate direction-aware spatial context (DSC) features for each layer; (iii) we concatenate the DSC features with convolutional features at each layer and upsample the concatenated feature maps to the size of the input image; (iv) we combine the upsampled feature maps into the multi-level integrated features (MLIF), and predict a score map based on the features for each layer by a deep supervision mechanism [27]; and (v) lastly, we fuse the resulting score maps to produce the final shadow detection result.|
|||Taking a 2D feature map from a CNN as input, the spatial RNN model first uses a 11 convolution to perform an input-to-hidden data trans lation.|
||4 instances in total. (in cvpr2018)|
|988|Multi-Scale FCN With Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting in the Wild|The ability of CNN in extracting high level feature representations greatly improve the accuracy.|
|||Tian [31] adapted the idea of using CNN to generate proposals as originally presented in [8] for object detection.|
|||Here, the input is the cropped text block image obtained from text block FCN, which might contain several lines or words, and we propose two networks in a cascaded fashion to solve the problem: Text Line CNN (TL-CNN) and Instance-Aware CNN (IACNN).|
|||For IA-CNN, the shared CNN parts are also initialized with VGG-16 model.|
||4 instances in total. (in cvpr2017)|
|989|Rameswar_Panda_Contemplating_Visual_Emotions_ECCV_2018_paper|As an example, the authors in [63] learn a CNN model to recognize emotions in natural images and performs reasonably well on the Deep Emotion dataset [63].|
|||We call this Binary Cross-Dataset Generalization Test, as it asks the CNN model to predict the most trivial basic emotion category from an image.|
|||While it seems that one can directly train a CNN with such data, as in [32] for image classificaton, we found it is extremely hard to learn good features for our task, as emotions  Contemplating Visual Emotions  9  are intrinsically fine-grained, ambigious, and web data is more prone to label noise.|
|||Specifically, a CNN (pre-trained on ImageNet) is first finetuned with 2 basic emotions (positive/negative) at level-1 and then it serves to initialize a second one that discriminates six emotion categories at level-2 and the process is finally repeated for 25 fine-grained emotion categories at level-3.|
||4 instances in total. (in eccv2018)|
|990|DenseReg_ Fully Convolutional Dense Shape Regression In-The-Wild|In particular, instead of using a CNN as a black box regressor, we draw inspiration from the success of recent works on semantic part segmentation [43, 11], and landmark classification [5, 6].|
|||For any image that is brought into correspondence with the template domain, this induces a discrete labelling, which can be recovered by training a CNN for classification.|
|||K is the quantization step size (we consider  Given a common CNN trunk, we use two classification branches to predict qh, qv and two regression branches to predict rh, rv as convolution layers with kernel size 1  1.|
|||For the dense regression network, we adopt a ResNet101 [19] architecture with dilated convolutions (atrous) [10, 29], such that the stride of the CNN is 8.|
||4 instances in total. (in cvpr2017)|
|991|Accurate Single Stage Detector Using Recurrent Rolling Convolution|Among many variants of the CNN based approaches, they can be roughly divded into two streams.|
|||In [16], the authors used CNN instead  of selective search to perform region proposal.|
|||An inspiring work is [18] where the authors formalized the detection problem as a bounding box generation procedure and used Long Short-Term Memory (LSTM) [10] to learn this procedure over deep CNN features by using the Hungarian loss.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||4 instances in total. (in cvpr2017)|
|992|All You Need Is Beyond a Good Init_ Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks With Orthonormality and Modulation|Diagram of the plain CNN network architecture (left) and repetitive triple-layer module (right) in this paper.|
|||Actually, this structure is similar with the plain CNN designed by He et al.|
|||Combined with orthonormality, experiments show that a plain CNN shown in Fig.|
|||Obviously in CNN this property cannot be ensured because  3Another reason is that placing ReLU after BN guarantees approximately 50% activations to be nonzero, while the ratio may be unstable if putting it after convolution operation.|
||4 instances in total. (in cvpr2017)|
|993|cvpr18-Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection|Most  of  RGB-D  fusion  networks  [19,  25,  27]  combine  RGB  and  depth  modalities  by  only  fusing  their  deep  CNN  features  (i.e.,  late  fusion),  while  we  believe  that  the  cross-modal  complement  for  saliency  detection  exists  across  multiple  levels, which are not well-explored by previous works.|
|||3 (c)) is appended on the side  of each CNN level and cascaded from deeper to shallower  successively.|
|||[18]  combine  the  hand-designed  low-level  saliency  features from RGB and depth modalities as the joint input  and  train  a  CNN  from  scratch  to  generate  RGB-D  hyper-features.|
|||However, owing to the loss of information      in the feature-crafting process and the limited training data,  we argue that it may be hard to make full use of CNNs via  learning  a  CNN  from  scratch  to  fuse  the  handcrafted  RGB-D  features.|
||4 instances in total. (in cvpr2018)|
|994|Sergey_Prokudin_Deep_Directional_Statistics_ECCV_2018_paper|A number of probabilistic methods for head pose analysis exist in the literature [18,28,29], but none of them combine probabilistic framework with learnable hierarchical feature representations from deep CNN architectures.|
|||More recent versions make use of CNN models but still do not take a probabilistic approach [3,4].|
|||Another substitution of angular regression problem was proposed in a series of work [39,40,41], where CNN is trained to predict the 2D image locations of virtual 3D control points and the actual 3D pose is then computed by solving a perspective-n-point (PnP) problem that recovers rotations from 2D-3D correspondences.|
|||Massa, F., Marlet, R., Aubry, M.: Crafting a multi-task cnn for viewpoint estimation.|
||4 instances in total. (in eccv2018)|
|995|cvpr18-Neural Sign Language Translation|Given a sign video x, our CNN learns to extract non-linear frame level spatial representations as:  ft = SpatialEmbedding(xt)  (1)  where ft corresponds to the feature vector produced by propagating a video frame xt through our CNN.|
|||For the end-to-end experiments,  these errors are back propagated through the encoderdecoder network to the CNN and word embeddings, thus updating all of the network parameters.|
|||To achieve NMT from sign videos, we employed CNN based spatial embedding, various tokenization methods including state-of-the-art RNN-HMM hybrids [36] and attentionbased encoder-decoder networks, to jointly learn to align, recognize and translate sign videos to spoken text.|
|||Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled.|
||4 instances in total. (in cvpr2018)|
|996|Gao_Compact_Bilinear_Pooling_CVPR_2016_paper|While the distinction of the steps is less clear in a CNN than in a BoVW pipeline, one can view the first several convolutional layers as a feature extractor and the later fully connected layers as a pooling and encoding mechanism.|
|||This has been explored recently in methods combining the feature extraction architecture of the CNN paradigm, with the pooling & encoding steps from the BoVW paradigm [23, 8].|
|||Reducing the number of parameters in CNN is important for training large networks and for deployment (e.g.|
|||Local descriptors, xs are typically extracted using SIFT[24], HOG [9] or by a forward pass through a CNN [17].|
||4 instances in total. (in cvpr2016)|
|997|Novotny_Learning_3D_Object_ICCV_2017_paper|Thus, we propose to train a CNN vp that maps a single frame f i t to its absolute viewpoint gi t ) in the globally-aligned reference frame.|
|||We wish to learn this CNN from the viewpoints estimated by the algorithms of sec.|
|||(2) and (3) are used to constrain the training of a Siamese architecture, which, given two frames t and t, evaluates the CNN twice to obtain estimates ( Ri t ) = vp(f i t ).|
|||5220  While this CNN is only required to correctly predict relative viewpoint changes within each sequence, since the same CNN is used for all videos, the most plausible/regular solution for the network is to assign similar viewpoint predictions ( Ri t ) to images viewed from the same viewpoint, leading to a globally consistent alignment of the input sequences.|
||4 instances in total. (in iccv2017)|
|998|Wu_Recursive_Spatial_Transformer_ICCV_2017_paper|The pioneering DeepFace [17] adopts CNN with locally connected layers, and it uses a 3D alignment model to transform all detected faces to the frontal view before they are fed into the network.|
|||Inspired by spatial transformer, we design a novel Recursive Spatial Transformer (ReST) module for CNN which allows face alignment and recognition to be jointly opti Figure 2.|
|||Overview of the proposed ReST integrated in a CNN for alignment-free face recognition.|
|||In most CNN architectures, the first several convolution layers account for the major time complexity, and in our Fast ReST the first several convolution layers are shared between the ReST and DCNN, and also only few even no recursion (when k = K  1) need to re-compute these convolution feature maps.|
||4 instances in total. (in iccv2017)|
|999|Li_Recognizing_Car_Fluents_CVPR_2016_paper|3) ST-AOG-CNN is the instance of our ST-AOG based on the pyramid of CNN features, here, we use the max5 layer of the CNN architecture for comparison as [22].|
|||Surprisingly, CNN features perform worse than HOG features on the Car-Fluents dataset.|
|||In experiments, we find the extracted CNN feature on deep pyramid [22] is too coarse (the cell size is 16), even we resize original images by 2 times, it still miss many small parts.|
|||Based on recent study of using CNN for keypoints prediction [56], we believe a more effective method is required to integrate CNN feature with graphical models.|
||4 instances in total. (in cvpr2016)|
|1000|Kaufman_Temporal_Tessellation_A_ICCV_2017_paper|We also use CNN and LSTM models but in fundamentally different ways, as we later explain in Sec.|
|||Given a frame Ii we use an off the shelf CNN to encode its appearance.|
|||We found the VGG-19 CNN to be well suited for this purpose.|
|||In [38], a CNN followed by an LSTM was used to predict sounds for each video.|
||4 instances in total. (in iccv2017)|
|1001|Zhuang_Fast_Training_of_CVPR_2016_paper|We find that using deep CNN features in general improve  5959  the performance for these three hashing methods, compared with what was originally proposed.|
|||Again for fair comparison, for the deep CNN approach SFHC, we replace its network structure (convolutionpooling, fully-connected layers) with the VGG-16 model and end-to-end train the network based on the triplet hinge loss used in the original paper.|
|||Then CNN features are centered and normalized for evaluation.|
|||We then use the above pre-trained CNN model to initialize the deep CNN that models the hash functions of our proposed hashing method.|
||4 instances in total. (in cvpr2016)|
|1002|cvpr18-Rethinking Feature Distribution for Loss Functions in Image Classification|Recognition error rates (%) on MNIST test set using a 6-layer CNN with different loss functions.|
|||For CIFAR-100, we adopt the same CNN architecture used by the large-margin softmax loss [22], which follows the design philosophy of the VGG-net [27] consisting of 13 convolutional layers and 1 fully connected layer.|
|||Recognition error rates (%) on CIFAR-100 using a VGGlike 13 layer CNN with different loss functions.|
|||We use the CNN architecture as described in Sect.|
||4 instances in total. (in cvpr2018)|
|1003|Tan_Yu_Product_Quantization_Network_ECCV_2018_paper|However, SQ is based on the hand-crafted features or CNN features from the pretrained model, therefore it might not be optimal with respect to the a specific dataset.|
|||The overview of the proposed product quantization network, where CNN represents the convolutional neural network and SPQ represents the proposed soft product quantization layer.|
|||The asymmetric triplet loss takes as input a triplet consisting of the CNN feature of an anchor image (I), the SPQ feature of a positive sample (I+) and the SPQ feature of a negative sample (I  ).|
|||12 bits 24 bits 36 bits 48 bits Method 0.621 0.616 0.615 0.612 SH + CNN [23] ITQ + CNN [23] 0.719 0.739 0.747 0.756 LFH + CNN [23] 0.695 0.734 0.739 0.759 KSH + CNN [23] 0.768 0.786 0.790 0.799 SDH+ CNN [23] 0.780 0.804 0.815 0.824 FASTH+CNN [23] 0.779 0.807 0.816 0.825 0.611 0.618 0.625 0.608 CNNH [37] 0.674 0.697 0.713 0.715 NINH [22] DHN [46] 0.708 0.735 0.748 0.758 0.768 0.776 0.783 0.792 DQN [6] 0.752 0.790 0.794 0.812 DPSH [24] 0.773 0.808 0.812 0.824 DTSH [34] DSDH [23] 0.776 0.808 0.820 0.829 Ours  0.795 0.819 0.823 0.830  Table 4. mAP comparisons with existing state-of-the-art methods using AlexNet base model on the NUS-WIDE dataset.|
||4 instances in total. (in eccv2018)|
|1004|Kruse_Learning_to_Push_ICCV_2017_paper|1) with a common CNN architecture of six sequential convolutional layers with 33 kernels.|
|||4592  31.532.032.533.033.534.034.535.034.2434.3234.6431.7533.0834.0634.0934.32Wiener,ETonceWiener50,OurBACSF555,ETeachCSF555,ETonceCSF555,OurBAFDN5g,ETeachFDN5g,ETonceFDN5g,OurBAFigure 6: CNN outputs with associated weights (top) and model predictions (bottom) for first 5 stages of FDN10 G .|
|||4} for an we show the CNN outputs CNN example image (bottom far left, associated ground truth shown above;  = 1/1.52).|
|||Conclusion  We generalized efficient FFT-based deconvolution methods, specifically shrinkage fields, by introducing a CNN at each stage to provide more powerful regularization.|
||4 instances in total. (in iccv2017)|
|1005|cvpr18-Focus Manipulation Detection via Photometric Histogram Analysis|Here, we also show that our method is more accurate than both past forensic methods [11, 13, 20] and the modern vision baseline of CNN classification applied directly to the image pixels.|
|||1 quantifies this performance, and shows that a range of CNN models (AlexNet, CaffeNet, VGG16, VGG19) have classification accuracies in the range of 76-78%.|
|||1 show the classification accuracies of the same CNN models applied to these feature maps.|
|||3 show that modern CNN architectures dont perform any better than random when operating on image inputs, and that their accuracy is less than 67% when classifying the various feature maps.|
||4 instances in total. (in cvpr2018)|
|1006|David_Harwath_Jointly_Discovering_Visual_ECCV_2018_paper|We treat these final spectrograms as 1-channel images, and model them with the CNN displayed in Figure 3.|
|||Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input  7  4.3  Joining the Image and Audio Branches  Zhou et al [57] demonstrate that global average pooling applied to the conv5 layer of several popular CNN architectures not only provides good accuracy for image classification tasks, but also enables the recovery of spatial activation maps for a given target class at the conv5 layer, which can then be used for object localization.|
|||The text-based model we used is based on the architecture of the speech and image model, but replaces the speech audio branch with a CNN that operates on word sequences.|
|||Gu erin, J., Gibaru, O., Thiery, S., Nyiri, E.: CNN features are also great at unsupervised  classification.|
||4 instances in total. (in eccv2018)|
|1007|Multi-Object Tracking With Quadruplet Convolutional Neural Networks|They use gradient boosting to combine local features extracted by Siamese CNN and contextual features.|
|||The most distinctive part of our algorithm from existing multi-object tracking algorithms based on metric learning is that it learns metrics for both appearance and motion cues simultaneously in a single CNN framework using quadruplet relationships.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Learning In  by tracking: Siamese cnn for robust target association.|
||4 instances in total. (in cvpr2017)|
|1008|Yang_PanNet_A_Deep_ICCV_2017_paper|ResNet has been shown to improve CNN performance on image processing tasks, but has drawbacks in the pan-sharpening framework.|
|||We also compare with the state-of-the-art PNN [21], which uses a different deep CNN learning approach from ResNet.|
|||For SGD we set the weight decay to 107, momentum to  2Alternatively, one could learn the upsampling from a CNN [10].|
|||This is not possible with the PNN model, which inputs the raw image into the deep CNN and models all information.|
||4 instances in total. (in iccv2017)|
|1009|Guo_Lu_Deep_Kalman_Filtering_ECCV_2018_paper|[4] separately processed fragments using a deep CNN with shared weights which output comparable features.|
|||Indeed, the output of the CNN can be viewed as localized pattern activations.|
|||Both these classification tasks are performed by a deep CNN described later.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||4 instances in total. (in eccv2018)|
|1010|Dai_Towards_Diverse_and_ICCV_2017_paper|For example, [30] uses a CNN for deriving the visual features f (I), and an LSTM [8] net to express the sequential relations among words.|
|||In particular, we follow the setting in NeuralTalk22, adopting VGG16 [26] as the CNN architecture.|
|||), it embeds them into vectors f (I) and h(S) of the same dimension, respectively via a CNN and an LSTM net.|
|||Note that while the CNN and the LSTM net in E have the same structure as those in G, their parameters are not tied with each other.|
||4 instances in total. (in iccv2017)|
|1011|AdaScan_ Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos|[50] highlighted a drawback in the two-stream network that uses a standard image CNN instead of a specialized network for training videos.|
|||AdaScan is a deep CNN augmented with an specialized pooling module (referred to as Adaptive Pooling) that scans a video and dynamically pools the features of select frames to generate a final pooled vector for the video, adapted to the given task of action classification.|
|||(t+1)th frame given its CNN feature, (xt+1), and the pooled features till time t, (X, t).|
|||As the underlying operations for fimp() rely only on standard linear and non-linear operations, they are both fast to compute and can be incorporated easily inside a CNN network for end-to-end learning.|
||4 instances in total. (in cvpr2017)|
|1012|Zhao_Temporal_Action_Detection_ICCV_2017_paper|S-CNN [37] proposes a multi-stage CNN which boosts accuracy via a localization network.|
|||In this way, the heavy matrix multiplication can be done in the CNN for each video over all snippets, and for each proposal, we only have to pool over the network outputs.|
|||We use SGD to learn CNN parameters in our framework, with batch size 128 and momentum 0.9.|
|||On both versions of ActivityNet, the RGB and optical flow branches of the two-stream CNN are respectively trained for 9.5K and 20K iterations, with learning rates scaled down by 0.1 after every 4K and 8K iterations, respectively.|
||4 instances in total. (in iccv2017)|
|1013|Hou_VegFru_A_Domain-Specific_ICCV_2017_paper|Furthermore, the architecture of HybridNet is intuitively similar to the Bilinear CNN in [17], where the Bilinear Pooling is first proposed to aggregate the two-stream DCNN features for FGVC.|
|||The Bilinear CNN is eventually implemented by a single DCNN due to weight sharing, while HybridNet holds two DCNNs which do not share weights.|
|||Part-stacked cnn for  fine-grained visual categorization.|
|||Bilinear cnn models for fine-grained visual recognition.|
||4 instances in total. (in iccv2017)|
|1014|Fouhey_3D_Shape_Attributes_CVPR_2016_paper|To this end, we make a number of contributions: (i) we introduce and define a set of 3D Shape attributes, including planarity, symmetry and occupied space; (ii) we show that such properties can be successfully inferred from a single image using a Convolutional Neural Network (CNN); (iii) we introduce a 143K image dataset of sculptures with 2197 works over 242 artists for training and evaluating the CNN; (iv) we show that the 3D attributes trained on this dataset generalize to images of other (non-sculpture) object classes; and furthermore (v) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint.|
|||Outlier image removal: A 1-vs-rest SVM is trained for each work, using fc7 activations of a CNN [29] pretrained on ImageNet [39].|
|||Using the same CNN activation + SVM technique from the outlier removal stage, we re-sort the query results and add the top images after verification.|
|||Approach  We now describe the CNN architecture and loss functions that we use to learn the attribute predictors and shape embedding.|
||4 instances in total. (in cvpr2016)|
|1015|Lin_Unrolled_Memory_Inner-Products_ICCV_2017_paper|Related Work  High-Performance CNN Implementations.|
|||Many CNN implementations on GPUs inherit techniques directly from linear algebra libraries [15, 20], which tend to be highly-optimized routines that rely on non-disclosed knowledge about GPU designs.|
|||For example in DianNao [2], a (16  16)-by-(16) matrix-vector processor is used for both CNN and FC layers as in Figure 1(c); Lavin et al.|
|||c01 c11  Output  {z  (1)    }  An early CNN implementation technique involves converting convolutions into matrix multiplications (Figure 1(a)).|
||4 instances in total. (in iccv2017)|
|1016|Hasan_Learning_Temporal_Regularity_CVPR_2016_paper|recently proposed a supervised CNN to classify actions in videos [7, 39].|
|||trained a CNN to detect events in videos [40].|
|||learned a CNN to pool the trajectory information for recognizing actions [41].|
|||2, 4, 5  [32] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, CNN features off-the-shelf: an astounding baseline for recognition, in CVPR, 2014.|
||4 instances in total. (in cvpr2016)|
|1017|Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper|Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here.|
|||The benchmark, source code and the CNN model introduced in this paper is publicly available1.|
|||We have thus created a new, more realistic benchmark, and have also used it to evaluate a combined CNN approach to traffic sign detection and classification.|
|||We have used it to train our own CNN for this purpose.|
||4 instances in total. (in cvpr2016)|
|1018|Jayaraman_Slow_and_Steady_CVPR_2016_paper|In each case, by augmenting a small set of labeled exemplars with unlabeled video, the proposed method generalizes better than both a standard discriminative CNN as well as a CNN regularized with existing slow temporal coherence metrics [14, 30].|
|||In [30], a standard deep CNN architecture is augmented with a temporal coherence regularizer, then trained using video of objects on clean backgrounds rotating on a turntable.|
|||Recent work considers how the cameras ego-motion (e.g., as obtained from inertial sensors, GPS) can be exploited as supervision during CNN training [17, 2].|
|||3We choose CIFAR-100 for its compatibility with the 32  32 images  used throughout our results, which let us leverage standard CNN architectures known to work well with tiny images [1].|
||4 instances in total. (in cvpr2016)|
|1019|cvpr18-Multi-Scale Weighted Nuclear Norm Image Restoration|However, while direct end-to-end training of a CNN is particularly suitable for image denoising, it is not equally practical for all image restoration tasks.|
|||Indeed, as we demonstrate experimentally, our algorithm outperforms PPP with a CNN denoiser [49] as well as the regularizationby-denoising (RED) approach of [38] with the TNRD [11] denoiser.|
|||2): IRCNN [49] which uses a CNN denoiser, and RED [38] with TNRD [11] as the denoising engine.|
|||Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising.|
||4 instances in total. (in cvpr2018)|
|1020|Kejie_Li_Efficient_Dense_Point_ECCV_2018_paper|Given a single image at an arbitrary viewpoint, a CNN predicts multiple surfaces, each in a canonical location relative to the object.|
|||Given a single RGB image of an object from an arbitrary viewpoint, the CNN outputs a set of 2D pre-deformation depth maps and corresponding deformation fields at pre-defined canonical viewpoints.|
|||3 Method  Our goal is to learn a CNN that is able to reconstruct a dense 3D point cloud to represent 3D object shape from a single RGB image.|
|||Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||4 instances in total. (in eccv2018)|
|1021|cvpr18-The Best of Both Worlds  Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation|Modern CNN methods of motion analysis, on the other hand, excel at identifying well-known structures, but may not precisely characterize well-known geometric constraints.|
|||This has given rise to a range of methodssome that use mostly geometric techniques while largely ignoring appearance [16, 3, 48], and others that try to learn the entire pipeline using CNN architectures [43, 44, 18] attempting to learn both the image patterns and the flow patterns in CNNs.|
|||1508  In this paper, we combine careful motion modeling using classical ideas with a modern CNN for appearance modeling, yielding excellent results.|
|||To be able to model the motion of an object accurately we use a CNN to produce object proposal masks leveraging the semantics of high level image understanding.|
||4 instances in total. (in cvpr2018)|
|1022|Yunchao_Wei_TS2C_Tight_Box_ECCV_2018_paper|Two-step approaches first extract proposal representation leveraging handcrafted features or pre-trained CNN models and employ MIL to select the best object candidate for learning the object detector.|
|||[18] proposed to learn a context-aware CNN with contrast-based contextual modeling.|
|||Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic  segmentation-aware cnn model.|
|||Wei, Y., Xia, W., Lin, M., Huang, J., Ni, B., Dong, J., Zhao, Y., Yan, S.: Hcp: A flexible cnn framework for multi-label image classification.|
||4 instances in total. (in eccv2018)|
|1023|Building a Regular Decision Boundary With Deep Networks|We study specifically the  pointwise non linearity  in a CNN and its necessary conditions to reach good classification accuracy.|
|||Observe that:  ReLUK  k x =   ReLUK  1  ...  ReLUK 1  In this setting, one might interpret a CNN with depth N and width K as a CNN of depth N K and width K, since it is also a cascade of N K ReLUK 1 non-linearities and {, Wn}n linear operators.|
|||It means also that if k < K, by increasing the number of layers, a CNN using a ReLU nonlinearity can be rewritten with a ReLUk K non-linearity.|
|||They could permit to potentially improving the classification accuracy, by refining the boundary classification of a CNN in their neighborhood.|
||4 instances in total. (in cvpr2017)|
|1024|Shi_Learning_Long-Term_Dependencies_ICCV_2017_paper|The CNN representations are then fed into RNNs to learn temporal features.|
|||Unlike pure CNN models [33, 45], which are fully pre-trained on ImageNet, our network has new layers which can not be trained on image datasets.|
|||In order to conduct as many exploration experiments as we can, we use a relative small network, GoogLeNet [38], as our CNN implementation to test these parameter combinations.|
|||Compared to pure CNN models, our method adds new layers to the network, which can not be pre-trained on ImageNet.|
||4 instances in total. (in iccv2017)|
|1025|Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper|Several 3D CNN architectures have been proposed to capture temporal relations  1 https://github.com/mzolfaghari/ECO-efficient-video-understanding  2  M. Zolfaghari, K. Singh and T. Brox  between frames, but they are computationally expensive and, thus, can cover only comparatively small windows rather than the entire video.|
|||[6] employed a LSTM to integrate features from a CNN over time.|
|||Most approaches use a CNN pre-trained on image classification or action recognition to generate features [9,43,45].|
|||Xu, Z., Yang, Y., Hauptmann, A.G.: A discriminative CNN video representation for event detection.|
||4 instances in total. (in eccv2018)|
|1026|Murthy_Deep_Decision_Network_CVPR_2016_paper|In particular, CNN based models has been the top performers in computer vision related tasks till date but recent work on Recurrent Neural Network [RNN] [19] has shown to be effective as well.|
|||Many attempts have been made to come up with an effective CNN network architecture either by going deeper [24, 29] which were the top performers in the 2014 ImageNet challenge or by introducing new components:activation units like (a) rectified linear unit (ReLu) [12] helped in accelerating the learning and have a great influence on the performance of large models trained on large datasets, (b) Parametric rectified linear units (PReLu) [6] which replaces the parameter-free ReLU activation by a learned parametric activation unit to further improve the classification performance; regularizers like (a) dropout [27] randomly sets some activation units to zero in a given layer and provides the effect of model averaging, (b) dropconnect [31] instead of activation units the weights set to zero, (c) maxout [5] outputs the max of a set of inputs and this can be used as an alternative to dropout; normalization such as batch normalization [8] that normalizes the layer inputs providing an accelerated learning and improved perfor mance.|
|||A recent paper [7] attempted to build a hierarchical CNN but the main objective was to transfer knowledge from a large network to a small network to achieve scalability but without compromising on the performance.|
|||In comparison to the most recent work, Deep Neural Decision Forests (dNDF) [10], at a high level, both our proposed technique and their method deals with a realization of decision trees in the context of CNN, but there are some key differences: a) In dNDF, a decision tree is introduced after the fully connected layer as part of the CNN but in DDN, each node of a decision tree is a CNN.|
||4 instances in total. (in cvpr2016)|
|1027|Neural Scene De-Rendering|We also compare with two other frameworks: a traditional CNN with a fixed number of dimensions for the latent representation, and an end-to-end CNN+LSTM that aims to encode the image and then to sequentially explain objects from the encoding.|
|||Specifically,   CNN: Our CNN baseline assumes there are no more than X objects in an image, and objects are ordered by their category indices.|
|||From left to right: (a) input images, and results of (b) the CNN model, (c) the CNN+LSTM model, (d) our de-rendering framework with box proposals, (e) our framework with segment proposals, (f) same as (e) but trained with REINFORCE, and (g) our full model with analysis-by-synthesis refinement on top of (f).|
|||The CNN and CNN+LSTM baseline can capture some basic concepts (for example, there is a boy and a girl in the image), but can hardly go beyond those (Figure 8b and c).|
||4 instances in total. (in cvpr2017)|
|1028|cvpr18-Unsupervised Feature Learning via Non-Parametric Instance Discrimination|Exemplar CNN [5] appears similar to our work.|
|||Exemplar CNN is computationally demanding for large-scale datasets such as ImageNet.|
|||We use a backbone CNN to encode each image as a feature vector, which is projected to a 128-dimensional space and L2 normalized.|
|||The results of these methods are reported with AlexNet architecture [18] in their original papers, except for exemplar CNN [5], whose results are reported with ResNet-101 [3].|
||4 instances in total. (in cvpr2018)|
|1029|Kwak_Thin-Slicing_for_Pose_CVPR_2016_paper|Following the great success in image classification [32, 45, 48], CNN has recently become popular in pose estimation.|
|||The siamesetype CNN architecture was proposed to learn an embedding space that reflects a semantic distance between data [9], and has been applied to face verification [16] and visual representation learning [3, 20].|
|||(1)  Based on this, our triplet rank loss L is defined as a hinge loss with a safety margin:  L =  N  X  i=1  h||f (xi)  f (x+  i )||2  2  ||f (xi)  f (x  i )||2  2 + i+  ,  (2)  In our CNN architecture, all images need to be standarized into a fixed-size input image (i.e., 2242243) that captures a target human body.|
|||t-SNE visualization of CNN codes.|
||4 instances in total. (in cvpr2016)|
|1030|cvpr18-LEGO  Learning Edge With Geometry All at Once by Watching Videos|The learned CNN model shows significant improvement compared to other methods based on hand-crafted features [23, 32, 31].|
|||Motivated by traditional methods like SFM and DTAM, lots of CNN based methods are proposed to do single view geometry estimation with supervision from vieos, and yield impressive progress.|
|||Although these methods run effectively and could combine with CNN as a post processing component [3, 63, 53, 55], they are not very efficient in learning and inference when combined with CNN, due to the iterative loop.|
|||Geometryguided cnn for self-supervised video representation learning.|
||4 instances in total. (in cvpr2018)|
|1031|Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search|Given a query image and some attributes that need to modify, AMNet can manipulate the intermediate representation encoding the unwanted attributes and change them to the desired ones through following four novel components: (1) a dual-path CNN architecture for discriminative deep attribute representation learning; (2) a memory block with an internal memory and a neural controller for prototype attribute representation learning and hosting; (3) an attribute manipulation network to modify the representation of the query image with the prototype feature retrieved from the memory block; (4) a loss layer which jointly optimizes the attribute classification loss and a triplet ranking loss over triplet images for facilitating precise attribute manipulation and image retrieving.|
|||Attribute Representation Learning  To learn the discriminative attribute representation which is favorable for attribute-level manipulation, AMNet chooses the deep CNN architecture, e.g.|
|||A CNN (the same as the one chosen in the representation learner) with multiple fully-connected layers is trained to classify multiple attributes of the image.|
|||(1) The attribute-based retrieval model, which uses the same CNN chosen in AMNet, i.e.|
||4 instances in total. (in cvpr2017)|
|1032|Yiding_Liu_Affinity_Derivation_and_ECCV_2018_paper|For example, CNN based methods have surpassed humans in image classification [24].|
|||Sequential group network (SGN) [37] uses CNN to generate features and makes group decisions based on a series of networks.|
|||Any CNN for semantic segmentation would be feasible for our work.|
|||The first step utilizes CNN to obtain class information and pixel affinity of the input image, while the second step applies the graph merge algorithm on those results to generate the pixel-level masks for each instance.|
||4 instances in total. (in eccv2018)|
|1033|Yang_Unsupervised_Extraction_of_ICCV_2015_paper|Places CNN [4]  C3D [26]  f reeride parkour skating skiing  skydriving  surf ing  swimming Overall mAP  0.241 0.323 0.310 0.462 0.337 0.501 0.350 0.361  0.302 0.425 0.304 0.388 0.433 0.539 0.320 0.387  Table 2.|
|||Comparison of mean average precision (mAP) between 2D and 3D CNN features with a standard auto-encoder.|
|||The Places CNN features were temporally pooled by dividing each snippet into two uniform subsnippets and performing mean pooling on each, while the C3D features were simply mean pooled within each whole snippet.|
|||The performance benefits of employing the 3D spatial-temporal C3D features rather than the 2D single-frame features of Places CNN is shown in Table 2, where the results are obtained using standard auto-encoders trained on each type of feature without applying PCA.|
||4 instances in total. (in iccv2015)|
|1034|Yao_Boosting_Image_Captioning_ICCV_2017_paper|Following this philosophy, it is natural to employ a CNN instead of the encoder RNN for image captioning, which is regarded as an image encoder to produce image representations.|
|||Therefore, a valid question is how to incorporate high-level image attributes into CNN plus RNN image captioning architecture as complementary knowledge in addition to image representations.|
|||The main contribution of this work is the proposal of attribute augmented architectures by integrating the attributes into CNN plus RNN image captioning framework, which is a problem not yet fully understood in the literature.|
|||Boosting Image Captioning with Attributes  In this paper, we devise our CNN plus RNN architectures to generate descriptions for images under the umbrella of additionally incorporating the detected high-level attributes.|
||4 instances in total. (in iccv2017)|
|1035|cvpr18-Self-Supervised Multi-Level Face Model Learning for Monocular Reconstruction at Over 250 Hz|The feed forward CNN is jointly learned with a multi-level face model that goes beyond the low-dimensional subspace of current 3DMMs.|
|||To make end-to-end training on in-the-wild images feasible, we propose a hybrid convolutional auto-encoder that combines a CNN encoder with a differentiable expert-designed rendering layer and a self-supervision loss, both defined at multiple levels of details.|
|||[47] proposed a multi-purpose CNN for regressing semantic parameters (e.g., age, gender, pose) from face images.|
|||1, right) the feed forward network jointly with the corrective space based on a novel CNN architecture that does not rely on a densely annotated training corpus of ground truth geometry, skin reflectance and illumination.|
||4 instances in total. (in cvpr2018)|
|1036|Ikehata_CNN-PS_CNN-based_Photometric_ECCV_2018_paper|Our end-to-end learning-based algorithm builds upon the deep CNN trained on synthetic datasets, abandoning the modeling of complicated image formation process.|
|||To achieve this goal, we propose a CNN architecture for the calibrated photometric stereo problem which is invariant to both the number and order of input images.|
|||The missing data is generally considered problematic as CNN input and often interpolated [4].|
|||Within the CNN framework, two approaches are generally adopted to encode the rotation invariance.|
||4 instances in total. (in eccv2018)|
|1037|Xiaohan_Fei_Visual-Inertial_Object_Detection_ECCV_2018_paper|Right CNN as scoring mechanism.|
|||8  X. Fei and S. Soatto  Algorithm 1 Semantic Filter  t  q(k(i)  t  t = exp (cid:0)  CNN +   edge(cid:1).|
|||(best in color at 5) In each panel, top inset shows (left to right): edge map, Z-buffer, projection masks; bottom shows input RGB with predicted mean object boundary and CNN detection.|
|||(best in color at 5) Each column shows (top to bottom): One frame of the input video with CNN bounding box proposals with confidence > 0.8; Extracted edge map; Frame overlaid with predicted instance masks shaded according to Z-Buffer  darker indicates closer; Background reconstruction augmented with camera trajectory (orange dots) and semantic reconstruction from our visual-inertial-semantic SLAM; Ground truth dense reconstruction.|
||4 instances in total. (in eccv2018)|
|1038|cvpr18-Scale-Transferrable Object Detection|SSD [22] and MS-CNN [3] use feature maps from different layers within CNN to predict objects at different scales (See Figure 1(c)).|
|||The role of DenseNet is to integrate low-level and high-level features within a CNN to get more powerful features.|
|||It means that the CNN compresses the 33  33  3 image information into the 1  1  1664 feature map.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||4 instances in total. (in cvpr2018)|
|1039|Vo_Revisiting_IM2GPS_in_ICCV_2017_paper|Our proposed CNN architecture consists of the convolutional layers of the VGG-16 network [28] followed by a global max pooling layer.|
|||After training we use the CNN as a feature extractor and index a large dataset of reference image features.|
|||Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Particular object retrieval with integral max-pooling of cnn activations.|
||4 instances in total. (in iccv2017)|
|1040|ActionVLAD_ Learning Spatio-Temporal Aggregation for Action Classification|Most recent video representations for action recognition are primarily based on two different CNN architectures: (1) 3D spatio-temporal convolutions [49, 51] that potentially learn complicated spatio-temporal dependencies but have been so far hard to scale in terms of recognition performance; (2) Two-stream architectures [42] that decompose the video into motion and appearance streams, and train separate CNNs for each stream, fusing the outputs in the end.|
|||We use a standard CNN architecture (VGG-16) to extract features from sampled appearance and motion frames from the video.|
|||Our approach is general and may be applied to future video architectures as an ActionVLAD CNN layer, which may prove helpful for related tasks such as (spatio) temporal localization of human actions in long videos.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||4 instances in total. (in cvpr2017)|
|1041|Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper|To further improve the speed of SiamFC, [16] reduces the feature computation cost for easy frames, by using deep reinforcement learning to train policies for early stopping the feed-forward calculations of the CNN when the response confidence is high enough.|
|||SINT [29] also uses Siamese networks for visual tracking and has higher accuracy, but runs much slower than SiamFC (2 fps vs 86 fps) due to the use of deeper CNN (VGG16) for feature extraction, and optical flow for its candidate sampling strategy.|
|||The feature channels respond to target parts: images are reconstructed from conv5 of the CNN used in our tracker.|
|||4  Implementation Details  We adopt an Alex-like CNN as in SiamFC [4] for feature extraction, where the input image sizes of the object and search images are 127  127  3 and 255  255  3 respectively.|
||4 instances in total. (in eccv2018)|
|1042|Liuhao_Ge_Point-to-Point_Regression_PointNet_ECCV_2018_paper|Point-to-Point Regression PointNet for 3D Hand Pose Estimation  13     MSRA Dataset  100 %     D <   r o r r e    t s r o w h     t i  w   s e m a r f   f  o     n o  i t r o p o r P  90 %  80 %  70 %  60 %  50 %  40 %  30 %  20 %  10 %  0 %   0  NYU Dataset  Heatmap [5] (20.8mm) Feedback Loop [9] (16.2mm) DeepModel [19] (16.9mm) Crossing Nets [10] (15.5mm) LieX [49] (14.5mm) 3D CNN [8] (14.1mm) Hallucination Heat [16] REN [13] (13.4mm) DeepPrior++ [14] (12.3mm) PoseREN [15] (11.8mm) DenseReg [26] (10.2mm) Ours (9.1mm)     100 %     D <   r o r r e    t s r o w h     t i  w   s e m a r f   f  o     n o  i t r o p o r P  90 %  80 %  70 %  60 %  50 %  40 %  30 %  20 %  10 %  10  20  30 50 D: error threshold (mm)  40  60  70  80  0 %   0  10  20  ICVL Dataset  LRF [6] (12.6mm) RDF, Hierarchical [7] (9.9mm) DeepModel [19] (11.6mm) Crossing Nets [10] (10.2mm) REN [13] (7.6mm) DeepPrior++ [14] (8.1mm) PoseREN [15] (6.8mm) DenseReg [26] (7.3mm) Ours (6.3mm)     100 %     D <   r o r r e    t s r o w h     t i  w   s e m a r f   f  o     n o  i t r o p o r P  90 %  80 %  70 %  60 %  50 %  40 %  30 %  20 %  10 %  30  40  50  D: error threshold (mm)  60  70  80  0 %   0  10  20  RDF, Hierarchical [7] (15.2mm) Collaborative Filtering [48] Multiview CNNs [11] (13.1mm) LSN, Finger Jointly Reg.|
|||[47] LSN, Pose Classification [47] Crossing Nets [10] (12.2mm) 3D CNN [8] (9.6mm) DeepPrior++ [14] (9.5mm) PoseREN [15] (8.6mm) DenseReg [26] (7.2mm) Ours (7.7mm)  30  40  50  60  70  80  D: error threshold (mm)  Fig.|
|||25  20  )  m m  (    15  e c n a t s d  i  10       r o r r e n a e M  5  0     l  m a P     R b m u h T     T b m u h T  R   x e d n  I  T   x e d n  I  NYU Dataset  REN [13] (13.4mm) DeepPrior++ [14] (12.3mm) PoseREN [15] (11.8mm) DenseReg [26] (10.2mm) Ours (9.1mm)     18  16  14  )  m m  (    12  ICVL Dataset  RDF, Hierarchical [7] (9.9mm) REN [13] (7.6mm) PoseREN [15] (6.8mm) DenseReg [26] (7.3mm) Ours (6.3mm)     25  20  )  m m  (    MSRA Dataset     Multiview CNNs [11] (13.1mm) 3D CNN [8] (9.6mm) PoseREN [15] (8.6mm) DenseReg [26] (7.2mm) Ours (7.7mm)  e c n a t s d  i  10       r o r r e n a e M  8  6  4  2  0     n a e M  l  m a P     R b m u h T     T b m u h T  R   x e d n  I  T   x e d n  I     R g n R  i     T g n R  i  R e     l t t i  L  T e     l t t i  L  l     R e d d M  i  l     T e d d M  i  15  e c n a t s d  i  10       r o r r e n a e M  5  0     n a e M  t s i r  W     R b m u h T     T b m u h T  R   x e d n  I  T   x e d n  I  R e     l t t i  L  T e     l t t i  L     T g n R  i     R g n R  i  l     T e d d M  i  l     R e d d M  i  l     R e d d M  i  l     T e d d M  i     R g n R  i     T g n R  i  R e     l t t i  L  T e     l t t i  L  n a e M  Fig.|
|||Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3D hand pose estimation in single depth images: from single-view CNN to multi-view CNNs.|
||4 instances in total. (in eccv2018)|
|1043|Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper|The CNN is pre-trained on ImageNet [6] and finetuned on the 200 classes of the ImageNet Detection Challenge [45].|
|||The CNN parameters c contain approximately 60 million parameters.|
|||Object regions are embedded with a CNN (left).|
|||We list their performance with a CNN that is equivalent in power (AlexNet [28]) to the one used in this work, though similar to [54] they outperform our model with a more powerful CNN (VGGNet [47], GoogLeNet [51]).|
||4 instances in total. (in cvpr2015)|
|1044|Optical Flow in Mostly Rigid Scenes|(ii) [4] relies exclusively on the CNN to segment moving regions.|
|||Thus using the CNN to predict a semantic segmentation is not possible.|
|||Also, no ground truth semantic segmentation is provided, so training a CNN to recognize these categories is not possible.|
|||We modify the last layer of the CNN to predict 2 classes, rigid and independently moving, instead of the original 21.|
||4 instances in total. (in cvpr2017)|
|1045|cvpr18-Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network|[45] who used a CNN to update the parameters of a 3D morphable model.|
|||[11] built a CNN to regress a statistical shape model by computing a bijective mapping from a 3D template mesh to the 2D image.|
|||[43] trained a task-contrained CNN to a set of features trained from multiple tasks (TCDCN).|
|||Deep roots: Improving cnn efficiency with hierarchical filter groups.|
||4 instances in total. (in cvpr2018)|
|1046|Rupprecht_Learning_in_an_ICCV_2017_paper|First, it is general in the sense that it can easily retrofit any CNN architecture and loss function or even other learning methods, thus enabling multiple predictions for a wide variety of tasks.|
|||We also show the generic applicability of this method  in Section 4, where we use M with three different loss functions L and four different CNN architectures for f.|
|||Multiple Object Classification  Many previous approaches argue that single-label CNN models are not suitable for multi-label object recognition and propose multi-stage methods; we instead show that extending such a CNN architecture with the multiple hypothesis principle can achieve competitive performance for multiple labels, without the need for multi-stage pipelines.|
|||Additionally, a full CNN needs to be trained for every single output of the ensemble, whereas adding more hypotheses does not add much overhead in our approach.|
||4 instances in total. (in iccv2017)|
|1047|StyleNet_ Generating Attractive Visual Captions With Styles|For instance, [50] extracted global image features using hidden activations of a CNN and then fed them into a LSTM [52] which is trained to generate a sequence of words.|
|||The recipe for caption generation with the CNN and RNN models follows the encoder-decoder framework originally used in neural machine translation [42, 6, 1], where an encoder is used to map the sequence of words in the source language into a fixed-length vector, and a decoder, once initialized by that vector, is used to generate the words in the target language one by one.|
|||The commonly used strategies in literature [50, 52, 32] are to adopt a pretrained CNN model as an encoder to map an image to a fixed dimensional feature vector and then use a LSTM model as the decoder to generate captions based on the image vector.|
|||We use the 3D CNN (C3D) [45] pre-trained on the Sport 1M dataset [23] to construct video-clip features from both spatial and temporal dimensions.|
||4 instances in total. (in cvpr2017)|
|1048|Edgar_Margffoy-Tuay_Dynamic_Multimodal_Instance_ECCV_2018_paper|This work processes visual and natural language information through separate neural networks: a CNN extracts visual features from the image while an LSTM scans the query.|
|||Strided convolutions and pooling operations in the CNN downsample the feature maps to a low resolution output while producing large receptive fields for neurons in the final layers.|
|||This method reduces strides of convolutional layers and uses atrous convolution in the final layers of the CNN to compensate for the downsampling.|
|||Results show how this method performs poorly in comparison to our full approach, which confirms our hypothesis that naively using a CNN falls short for the task addressed in this paper.|
||4 instances in total. (in eccv2018)|
|1049|Bhagavatula_Faster_Than_Real-Time_ICCV_2017_paper|3DDFA [51] fits a dense 3D face model to the image via CNN and DDN [48] proposes a novel cascaded framework incorporating geometric constraints for localizing landmarks in faces and other non-rigid objects.|
|||The Multiple Scale Faster Region-based CNN approach [49] has shown good results and at a fast speed.|
|||We use the recent extension to this work, the Contextual Multi-Scale Region-based CNN (CMS-RCNN) approach [50] to perform the face detection in any experiment where face detection is needed.|
|||CMS-RCNN: contextual multi-scale region-based CNN for unconstrained face detection.|
||4 instances in total. (in iccv2017)|
|1050|Deep Reinforcement Learning-Based Image Captioning With Embedding Reward|An illustration of our policy network p that is comprised of a CNN and a RNN.|
|||Network architecture As shown in Figure 2 and 3, our policy network, value network both contain a CNN and a RNN.|
|||We adopt the same CNN and RNN architectures for them, but train them independently.|
|||We use VGG-16 [39] as our CNN architecture and LSTM [14] as our RNN architecture.|
||4 instances in total. (in cvpr2017)|
|1051|Liu_Learning_a_Recurrent_ICCV_2017_paper|[25] proposed multi-modal CNN for matching images and sentences.|
|||[15] aligned visual regions and sentences by integrating CNN and RNN.|
|||To efficiently extract dense region representations, CNN models are first recast to fully convolutional networks (FCNs) [24].|
|||Cross-modal retrieval with cnn visual features: A new baseline.|
||4 instances in total. (in iccv2017)|
|1052|Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper|More recent methods either generate proposals inside a CNN [46], or have implicit regions directly in the image or feature maps [32, 44].|
|||We define loss for a training sample bi with class label yi as,  L(bi, yi, ) = X  max(0, m  Sii + Sij)  (2)  jS,j6=i  where  refers to the parameters of the deep CNN and the projection matrix, and m is the margin.|
|||We use the (pre-trained) Inception-ResNet v2 model [51] as our base CNN for computing deep features.|
|||We pass these proposals through the base CNN and obtain a score for each test class as outlined in section 3.1.|
||4 instances in total. (in eccv2018)|
|1053|Johnson_Image_Retrieval_Using_2015_CVPR_paper| CNN [34]: L2 distance between the last layer features  GIST [48]: L2 distance between the GIST descriptors of the query image and each test image (see Sect.|
|||Rand SIFT GIST CNN SG-obj  SG SG [43]  [48]  [34]  (a)  (b)  (c)  Med r R@1 R@5 R@10 Med r R@1 R@5 R@10 Med IoU R@0.1 R@0.3 R@0.5  420 0  0.007 0.027  94 0  64 0  57  36  0.008 0.017 0.034 0.084 0.101 0.050 0.042 0.168 0.193 0.176      [24] 28  0.113 0.260 0.347  17  0.059 0.269 0.412 0.014 0.435 0.334 0.234  12  0.042 0.294 0.479 0.026 0.447 0.341 0.234  obj-attr obj-attr-rel 17.5 0.127 0.340 0.420  14 0.133 0.307 0.433 11 0.109 0.303 0.479 0.067 0.476 0.357 0.239  model can also be used to retrieve meaningful results given simpler, more human-interpretable scene graph queries.|
|||For the GIST and CNN baselines, we rank the test images based on the L2 distance between the descriptor for I1 and the descriptor for the test image.|
|||7 (a) plots the recall over k. Note that the GIST, SIFT, and CNN baselines are meaningless here, as they would always rank the query image highest.|
||4 instances in total. (in cvpr2015)|
|1054|cvpr18-Feature Selective Networks for Object Detection|formable Parts Models (DPMs) [10, 1, 8] dominated object detection for years before CNN sprang up.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
|||Gated bi-directional cnn for object detection.|
||4 instances in total. (in cvpr2018)|
|1055|Liu_Predicting_Eye_Fixations_2015_CVPR_paper|A brief review of CNN   A convolutional neural network (CNN) [28] is usually  composed  of  alternate  convolutional  and  max-pooling  layers  (denoted  as  C  layers  and  P  layers)  to  extract  hierarchical  features  to  represent  the  original  inputs,  subsequently with several fully connected layers (denoted  by FC layers) followed to do classification.|
|||l    to   lH ,  where   Considering a CNN with L layers, we denote the output  state  of  the  l-th  layer  as  ,  0H  to denote the input data.|
|||Saliency detection using Mr-CNN   Inspired  by  [30-32],  we  develop  a  CNN  architecture  with  multiple  resolutions  (or  scales)  to  simultaneously  learn early features, bottom-up saliency, top-down factors  and  their  integration  from  image  data  for  predicting  eye  fixations.|
|||The  CNN  routine  we  used  is  based  on  the  deepnet1 library.|
||4 instances in total. (in cvpr2015)|
|1056|Shetty_Speaking_the_Same_ICCV_2017_paper|Captioning models also benefit from augmenting the CNN features with explicit object detection features [36].|
|||Second is the global image CNN feature, xc, and is input to the LSTM at all time-steps through its own input matrix.|
|||Discriminator model  The discriminator network, D takes an image x, represented using CNN feature xc, and a set of captions Sp = {s1, .|
|||The CNN features are input to both the generator (at xp) and the discriminator.|
||4 instances in total. (in iccv2017)|
|1057|cvpr18-Learning Generative ConvNets via Multi-Grid Modeling and Sampling|Classification error of CNN classifier trained on the features of three grids learned from SVHN.|
|||Test error rate with # of labeled images  1,000 2,000 4,000  DGN [17]  Virtual adversarial [30]  Auxiliary deep generative model [29]  36.02 24.63 22.86    Supervised CNN with the same structure 39.04 22.26 15.24 19.73 15.86 12.71  multi-grid method + CNN classifier  6.3.|
|||Specifically, we build a two-layer classification CNN on top of the top layer feature maps of three grids.|
|||Learning FRAME models using CNN filters.|
||4 instances in total. (in cvpr2018)|
|1058|Hengcan_Shi_Key-Word-Aware_Network_for_ECCV_2018_paper|The proposed method can be implemented with any CNN and RNN.|
|||Since state-of-the-art methods [9, 18] often choose VGG16 [30] or Deeplab101 [2] as their CNN and use LSTM [8] as their RNN, to fairly compare our method with them, we also implement the proposed method with these CNN and RNN in our experiments.|
|||The dimensions of CNN and RNN features are both set to 1000 (i.e., Cf = Cq = 1000).|
|||We initialize the CNN from the weights pre-trained on ImageNet dataset [29], and initialize other parts from random weights.|
||4 instances in total. (in eccv2018)|
|1059|cvpr18-LSTM Pose Machines|The performance of  these methods has recently been surpassed by CNN based methods [4, 5, 23, 34, 35, 36].|
|||ConvNet2 is a multi-layer CNN network for extracting features while an additional ConvNet1 will be used in the first stage for initialization.|
|||Our method only needs to go through a single stage for every video frame thus performs significantly faster than the previous multi-stage CNN based methods.|
|||Conclusions  In this paper, we presented a novel recurrent CNN model with LSTM for video pose estimation.|
||4 instances in total. (in cvpr2018)|
|1060|Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper|In these works, the dynamic characteristics were explored in two ways: adding temporal information to CNN structures [1, 3, 27, 35] or developing a dynamic structure with LSTM [2,23].|
|||For adding temporal information, a four-layer CNN in [3] and a two-stream CNN in [1] were trained with both RGB frames and motion maps as the inputs.|
|||Similarly, in [35], the pair of consecutive frames concatenated with a static saliency map (generated by the static CNN) are fed into the dynamic CNN for video saliency prediction, allowing the CNN to generalize more temporal features.|
|||(2)  The inference module If is a CNN structure that consists of 4 convolutional layers and 2 deconvolutional layers with a stride of 2.|
||4 instances in total. (in eccv2018)|
|1061|Simultaneous Feature Aggregating and Hashing for Large-Scale Image Search|Experiments with CNN feature maps  Recently, in [42, 4, 3] the authors showed that the activations from the convolutional layers of a convolutional neural network (CNN) can be interpreted as local features describing image regions.|
|||Motivated by those works, in this section we perform the experiments in which activations of a convolutional layer from a pre-trained CNN are used as an alternative to SIFT features.|
|||To the best of our knowledge, this is the only work using end-to-end CNN for unsupervised hashing.|
|||Particular object retrieval In ICLR,  with integral max-pooling of CNN activations.|
||4 instances in total. (in cvpr2017)|
|1062|cvpr18-Detect-and-Track  Efficient Pose Estimation in Videos|To address this limitation, we propose a simple and effective approach which leverages the current state of the art method in pose prediction [17] and extends it by integrating temporal information from adjacent video frames by means of a novel 3D CNN architecture.|
|||The features are then fed into the 3D CNN head responsible for pose estimation.|
|||In terms of hand-crafted features, we specifically experiment with: 1) Visual similarity, defined as the cosine distance between CNN features extracted from the image patch represented by the detection; 2) Location similarity, defined as the box intersection over union (IoU) of the two detection boxes; and 3) Pose similarity, defined as the PCKh [53] distance between the poses in the two frames.|
|||One practical limitation with 3D CNN models is the GPU memory usage.|
||4 instances in total. (in cvpr2018)|
|1063|Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution|Experimental results demonstrate that our method is faster than several CNN based super-resolution models, e.g., SRCNN [7], SCN [33], VDSR [17], and DRCN [18].|
|||To facilitate training a deeper model with a fast  625  Table 1: Comparisons of CNN based SR algorithms: SRCNN [7], FSRCNN [8], SCN [33], ESPCN [28], VDSR [17], and the proposed LapSRN.|
|||[33] combine the domain knowledge of sparse coding with a deep CNN and train a cascade network (SCN) to upsample images to the desired scale factor progressively.|
|||The FSRCNN network [8] adopts a similar idea and uses a hourglass-shaped CNN with more layers but fewer parameters than that in ESPCN.|
||4 instances in total. (in cvpr2017)|
|1064|Connecting Look and Feel_ Associating the Visual and Tactile Properties of Physical Materials|Their other work [22] presented a CNN that learn visual representation self-supervised by features extracted from ambient sound.|
|||The input data from each modality goes through an independent CNN to form an embedding vector E, as a low-dimension representation of the fabrics.|
|||Ideally all the input data on the same fabric will make the same E through the networks, while two fabrics, when they are similar, will have a small distance D between the embedding vectors E, and  5583  two very different fabrics will have large D. We trained a joint CNN of the three modalities, and compared the performance of different architectures.|
|||Joint embeddings of shapes and images via cnn image purification.|
||4 instances in total. (in cvpr2017)|
|1065|Unsupervised Learning of Depth and Ego-Motion From Video|[14] propose to learn a single-view depth estimation CNN using projection errors to a calibrated stereo twin for supervision.|
|||Approach  Here we propose a framework for jointly training a single-view depth CNN and a camera pose estimation CNN from unlabeled video sequences.|
|||To improve the robustness of our learning pipeline to these factors, we additionally train a explainability prediction network (jointly and simultaneously with the depth and pose networks) that outputs a per-pixel soft mask Es for each target-source pair, indicating the  1In practice, the CNN estimates the Euler angles and the 3D translation  2For notation simplicity, we omit showing the necessary conversion to  vector, which are then converted to the transformation matrix.|
|||The last two rows demonstrate the potential downside of explainabilityweighted loss: the depth CNN has low confidence in predicting thin structures well, and tends to mask them as unexplainable.|
||4 instances in total. (in cvpr2017)|
|1066|Liangliang_Ren_Deep_Reinforcement_Learning_ECCV_2018_paper|The appearance models have evolved from intensity templates [19], color histograms [14], and sparse features [4], to the dominating deep features [47] extracted by CNN models.|
|||The shift process generally tends to be more efficient since less candidate regions are evaluated than in classification based methods  classification or detection-and-association problem [35] using CNN classifiers.|
|||1, the tracking result is estimated iteratively, instead of performing CNN classification on many candidate locations, thus leading to an efficient computation.|
|||The main contributions of our paper are on two-fold: 1) We propose an ActorCritic network to predict the object motion parameters and select actions on the tracking status, where the rewards for different actions are dedicatedly designed according to their impacts; 2) We formulate object tracking as an iterative shift problem, rather than CNN classification on possible bounding boxes, thus locates a target efficiently and precisely.|
||4 instances in total. (in eccv2018)|
|1067|Sequential Person Recognition in Photo Albums With a Recurrent Network|At each subsequent step, the input to the LSTM is a joint embedding of the CNN feature representation of the current person instance and its predicted label at the last step.|
|||On the test splits (test0 and test1), we extract CNN features from the f c7 layer of the fine-tuned networks for the two regions.|
|||Appearance-only: given the CNN network finetuned on a body region on the PIPA training set, we fine-tune the last fully-connected layer on the test instances on the either of the two test splits and evaluate on the other.|
|||For instance, higher accuracy on the day setting is reported in [12], in which the features are extracted from the face region using a CNN trained for the face recognition task.|
||4 instances in total. (in cvpr2017)|
|1068|Jin-Dong_Dong_DPP-Net_Device-aware_Progressive_ECCV_2018_paper|Classification accuracies of the generated bypass CNN models on a validation dataset are used as rewards for the controller.|
|||[5] use Monte Carlo Tree Search (MCTS) to search through the space of CNN architectures in a shallow-to-deep manner and randomly select which branch to expand at each node.|
|||Furthermore, for the completeness and comprehensive study, in the top half of Table.2, we include the results from the best models of previous NAS works [9, 11, 15], as well as the current state-of-the-art handcrafted mobile CNN models (bottom half) [18, 31].|
|||Moreover, DPP-NetPanacea outperforms NASNet (Mobile), a state-of-the-art mobile CNN designed by an architecture search method [9] in every metrics.|
||4 instances in total. (in eccv2018)|
|1069|cvpr18-Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions|The second uses upright CNN features densely extracted on a regular grid.|
|||DenseSfM skips feature detection and directly matches densely extracted CNN descriptors (which encode higher-level information compared to the gradient histograms used by RootSIFT).|
|||Dense CNN feature matching inside SfM improves pose estimation performance at high computational costs, but does not fully solve the problem.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
||4 instances in total. (in cvpr2018)|
|1070|Zhu_CoupleNet_Coupling_Global_ICCV_2017_paper|This is also the main reason that CNN completely surpassed the traditional methods in a short period time.|
|||Due to the RoI pooling operation, the global FCN describes the proposal as a whole with CNN features, which can be seen as a global structure description of the object. Therefore, it can easily deal with the objects with intact structure and finer scale.|
|||Since features extracted form different layers of CNN show various of scales, it is essential to normalize different features before coupling them together.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||4 instances in total. (in iccv2017)|
|1071|cvpr18-Dense 3D Regression for Hand Pose Estimation|For a given depth map, we use a 2D CNN to capture local surface patterns but also treat the depth map as a set of 3D points to arrive at a final pose estimate in 3D.|
|||More specifically, we use a CNN to estimate a dense vector field of offsets for each joint of hand.|
|||[12, 46, 25, 13, 5] are CNN based 3D holistic regression methods and outperforms other existing methods[11, 19].|
|||1, 2, 3, 5, view cnn to multi-view cnns.|
||4 instances in total. (in cvpr2018)|
|1072|cvpr18-Maximum Classifier Discrepancy for Unsupervised Domain Adaptation|Related Work  Training CNN for DA can be realized through various strategies.|
|||Matching distributions of the middle features in CNN is considered to be effective in realizing an accurate adaptation.|
|||This approach can train the CNN to simultaneously minimize both the divergence and category loss for the source domain.|
|||In this experiment, we employed the CNN architecture used in [7] and [3].|
||4 instances in total. (in cvpr2018)|
|1073|Gilles_Simon_A_Contrario_Horizon-First_ECCV_2018_paper|In [18], the offset probability density function (PDF) used for this sampling is a Gaussian model, fit from the CNN categorical probability distribution outputs.|
|||In [18],  is re-estimated each frame from the CNN output, while we take a constant, empirical value in our method.|
|||The predictive power of the CNN is interesting in bad images where analytical vision fails, assuming a large DS of similar examples is provided, along with a GT, to the learning process.|
|||By contrast, our method may provide accurate results in some images where the CNN fails due to insufficient representation in the learning DS.|
||4 instances in total. (in eccv2018)|
|1074|cvpr18-Referring Image Segmentation via Recurrent Refinement Networks|Given an input image and a natural language description as query, our model first uses LSTM and CNN to localize a rough image region of the target indicated by natural language descriptions.|
|||Given an input image I of size W  H, our model uses a CNN to extract a w  h  DI spatial feature map.|
|||The CNN to extract image feature maps is built upon DeepLab ResNet-101 [2] pre-trained on Pascal VOC [4], which reduces the stride of conv4 1 and conv5 1 to 1 and uses dilated convolution to compensate receptive fields.|
|||It is because the referring expressions on testA split only contain people, which is easier to segment since our CNN is pretrained on Pascal VOC [4].|
||4 instances in total. (in cvpr2018)|
|1075|Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network|One of the most popular CNN based work is the Fully Convolutional Network (FCN) [25].|
|||Furthermore, Adelaide [21] deeply incorporates CRF and CNN where hand-crafted potentials is replaced by convolutions and nonlinearities.|
|||Instead of  GCN, another trivial approach to form a large kernel is to use stack of small kernel convolutions(for example, stack of 3  3 kernels in Figure 4 D), , which is very common in modern CNN architectures such as VGG-net [31].|
|||7  Method FCN 8s [30] DPN [24] CRFasRNN [38] Scale invariant CNN + CRF [19] Dilation10 [37] DeepLabv2-CRF [7] Adelaide context [21] LRR-4x [12] Our approach  mean-IoU(%)  65.3 59.1 62.5 66.3 67.1 70.4 71.6 71.8 76.9  Table 10.|
||4 instances in total. (in cvpr2017)|
|1076|Ibrahim_A_Hierarchical_Deep_CVPR_2016_paper|Therefore, a deep CNN model is used to extract complex features for each person in addition to the temporal features captured by the first LSTM layer.|
|||At this moment, the concatenation of the CNN features and the LSTM layer represent temporal features for a person.|
|||In the first step, the person-level CNN and the first LSTM layer are trained in an end-to-end fashion using a set of training data consisting of person tracklets annotated with action labels.|
|||Similar to other approaches [9, 7, 38], we initialize our CNN model with the pre-trained AlexNet network and we fine-tune the whole network for the first LSTM layer.|
||4 instances in total. (in cvpr2016)|
|1077|cvpr18-Differential Attention for Visual Question Answering|Another interesting approach [18] used dynamic parameter prediction where weights of the CNN model for the image embedding are modified based on the question embedding using hashing.|
|||More formally, given an image xi we obtain an embedding gi using a CNN that we parameterize through a function G(xi, Wc) where Wc are the weights of the CNN.|
|||While comparing the network parameters, the comparable baseline we use is the basic model for VQA using LSTM and CNN [2].|
|||Deeper lstm and normalized cnn visual question answering model.|
||4 instances in total. (in cvpr2018)|
|1078|Huang_Learning_Policies_for_ICCV_2017_paper|Usually the deep CNN is fixed, and the DCF trackers trained on every convolutional layer are combined by a hierarchical ensemble method [27] or an adaptive Hedge algorithm [31].|
|||Another category of deep trackers [41, 30, 40] update a pre-trained CNN online to account for the target-specific appearance at test time.|
|||One common reason for the slow speed of the abovementioned deep trackers is they always conduct a complete feed-forward pass to the last CNN layer.|
|||[41, 30]) exploit the semantically robust features from the last CNN layer (fullyconnected).|
||4 instances in total. (in iccv2017)|
|1079|Hallucinating Very Low-Resolution Unaligned and Noisy Face Images by Transformative Discriminative Autoencoders|Comparison of our method with the CNN based face hallucination URDGN [25].|
|||The work in [28] employs a CNN to extract facial features and then generates high-frequency facial details based  3761  Figure 2.|
|||Comparison of our method with the CNN based face hallucination methods.|
|||[6] present a CNN based general purpose super-resolution method, also known as SRCNN.|
||4 instances in total. (in cvpr2017)|
|1080|Nam_Modelling_the_Scene_ICCV_2017_paper|In this paper, we show that color histogram based low-level features extracted using our deep network are more efficient for the given task compared to the highlevel CNN features extracted from above previous work.|
|||As we show in Section 5, applying conventional CNN based structures do not capture good features for the scene dependency in our task.|
|||Finally, only those selected patches are used for training the CNN weights as shown in Fig.|
|||One can expect that hierarchical CNN features are able to capture the local and the global scene context that are useful for the scene dependent imaging.|
||4 instances in total. (in iccv2017)|
|1081|Li_Factorized_Bilinear_Models_ICCV_2017_paper|In this paper, we propose the Factorized Bilinear (FB) model to enhance the capacity of CNN layers in a simple and effective way.|
|||In this way, all computational layers in CNN could have larger capacity with pairwise interactions.|
|||2(a), the bilinear pooling is applied after the last convolutional layer of a CNN (e.g.|
|||In the following experiments, we refer the CNN equipped with our FB model as Factorized Bilinear Network (FBN).|
||4 instances in total. (in iccv2017)|
|1082|Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes|Early approaches constrained their output to a bottom-up segmentation followed by a CNN based region classification [54].|
|||performs pixel-wise classification using CNN features originating from multiple scales, followed by aggregation of these noisy pixel predictions over superpixel regions [16].|
|||4152  Many approaches apply smoothing operations to the output of a CNN in order to obtain more consistent predictions.|
|||Since the success of the AlexNet architecture [31] in the ImageNet Large-Scale Visual Classification Challenge (ILSVRC) [47], the vision community has seen several milestones with respect to CNN architectures.|
||4 instances in total. (in cvpr2017)|
|1083|Pengfei_Zhang_Adding_Attentiveness_to_ECCV_2018_paper|For RGB-based action recognition, we design our system by applying an EleAtt-GRU network to the sequence of frame-level CNN features.|
|||For RGB-based action recognition, we take the CNN features extracted from existing, pre-trained models without finetuning on our datasets as the input to the RNN based recognition networks and evaluate the effectiveness of EleAttG on the NTU and the JHMDB datasets [18].|
|||EleAtt-X denotes the scheme with EleAttGs based on the RNN structure of X.  RNN structure  Scheme  Standard RNN  LSTM  GRU  Baseline(1-sRNN)  EleAtt-sRNN  Baseline(3-LSTM)  EleAtt-LSTM  Baseline(3-GRU)  EleAtt-GRU  CS  51.6 61.6  77.2 78.4  75.2 79.8  CV  57.6 67.2  83.0 85.0  81.5 87.1  strate this, we use CNN features extracted from RGB frames as the input of the RNNs for RGB based action recognition.|
|||Note that the performance is not optimized since we have not used the fine-tuned CNN model on this dataset for this task.|
||4 instances in total. (in eccv2018)|
|1084|Deep Feature Flow for Video Recognition|Modern CNN architectures [39, 41, 16] share a common structure.|
|||Following the modern CNN architectures [39, 41, 16] and applications [28, 4, 50, 13, 14, 12, 34, 8], without loss of generality, we decompose N into two consecutive subnetworks.|
|||To model such variations, we propose to also use a CNN to estimate the flow field and the scale field such  that all the components can be jointly trained end-to-end for the task.|
|||Flow Network We adopt the state-of-the-art CNN based FlowNet architecture (the Simple version) [9] as default.|
||4 instances in total. (in cvpr2017)|
|1085|Unambiguous Text Localization and Retrieval for Cluttered Scenes|In our proposed DTLN network, CNN is still employed to obtain deep convolutional representations of scene images, but we adopt Long Short Term Memory (LSTM) [21] based decoders to jointly model text instances and their context.|
|||For an input image, the DTLN model directly decodes the CNN features into a variable length set of text instance candidates.|
|||An LSTM-based decoder would smartly extract target scene text instances from these CNN encoded feature descriptors.|
|||VGG-16 net [20] trained on ImageNet dataset [25] is still used as the CNN architecture for CNNlocal and CNNglobal and we extract 1000-dimensional fc8 outputs as xbox and xcontext, and use the same LSTM implementation as in [26] and [29].|
||4 instances in total. (in cvpr2017)|
|1086|Superpixel-Based Tracking-By-Segmentation Using Markov Chains|AMCT+CNN integrates the feature descriptors from CNN as well as Lab color to learn SVR.|
|||We employ the CNN pre-trained for semantic segmentation [27], which is already used for online video segmentation in [35].|
|||Note that the accuracy of AMCT is improved by incorporating CNN features.|
|||AMCT+CNN tends to capture more precise target segmentation in high-jump, dunk, humming bird2 and motocross-jump sequences by incorporating high-level semantic representations produced by CNN [27].|
||4 instances in total. (in cvpr2017)|
|1087|Shangzhe_Wu_Deep_High_Dynamic_ECCV_2018_paper|The authors proposed a CNN that learns to merge LDR images aligned using optical flow into the final HDR image.|
|||rejection method [19], the flow-based method with CNN merger [14], and the single image HDR imaging [3].|
|||This can be due to the confusion caused by the background motion, which CNN is generally weak at dealing with.|
|||We conducted extensive quantitative and qualitative experiments to show that our non-flow-based CNN approach outperforms the state-of-the-arts, especially in the presence of large foreground motions.|
||4 instances in total. (in eccv2018)|
|1088|Xu_MSR-VTT_A_Large_CVPR_2016_paper|leverage CNN to learn the single frame representation as the input to the long-term recurrent convolutional networks to output sentences [7].|
|||Specifically, given an input video, 2-D CNN is utilized to  5291  video  a  softmax  group softmax  of  softmax  #end softmax  frames  2D CNN (AlexNet, GoogleNet, VGG)  ...  ...  LSTM  LSTM  LSTM  clip  Convolutional 3D  ...  ...  LSTM  pooling:  mean or  soft-attention  LSTM  LSTM  ... ... ... ...  LSTM  LSTM  #start  [1, 0, 0... 0]  a  [0, 1, 0, ..., 0]  group  [0, 0, 1, ..., 0]  dancing [0, 1, 0, ..., 0]  Figure 3.|
|||Moreover, optical flow or 3-D CNN is exploited to represent the motion information in the video.|
|||Then the video representation, which is comprised of 2-D CNN and/or 3-D CNN outputs, is feed into RNN.|
||4 instances in total. (in cvpr2016)|
|1089|Training Object Class Detectors With Click Supervision|After MIL converges (typically within 10 iterations), we perform two additional iterations where during the step (II) we deeply re-train the whole CNN network, instead of just an SVM on top of a fixed feature representation.|
|||Following [20, 11, 6, 7, 62, 75], we describe each object proposals with a 4096-dimensional feature vector using the Caffe implementation [25] of the AlexNet CNN [34].|
|||We pretrained the CNN on the ILSVRC [52] dataset using only image-level labels (no bounding box annotations).|
|||Unless stated otherwise, we use AlexNet [34] as the underlying CNN architecture for our method and for all compared methods.|
||4 instances in total. (in cvpr2017)|
|1090|Qiao_Less_Is_More_CVPR_2016_paper|Image features: To make fair a comparison, two types of image features, the low-level features in [19] and the fully connected layer activations from the imagenet-vggverydeep-19 [22] CNN are used in our experiments.|
|||3 different loss functions are used in [3] for their CNN structure: binary cross entropy (BCE), hinge loss (Hinge), and Euclidean distance (Euclidean).|
|||The lower part of Table 3 are methods with visual features extracted from a pretrained CNN and thus are more comparable to our method.|
|||Methods in the upper part of the table use low-level features and the remaining methods in the lower part use deep CNN features.|
||4 instances in total. (in cvpr2016)|
|1091|Simyung_Chang_Broadcasting_Convolutional_Network_ECCV_2018_paper|The module makes feature maps that represent the coordinate information and concatenates them with the original CNN feature maps.|
|||As mentioned in Section 3, RN learns the relations among objectified features of CNN through pairwise combinations, computations of which increase quadratically with the number of the objects while multiRN gains a comparable improvement on computation efficiency through manipulation of multiple relations induced by the BCN module.|
|||CNNh denotes the CNN that has 1 stride for fourth convolution layer instead of 2 stride to handle more objects  Model CNN+RN [29] CNN+RN CNN+RN CNNh+RN CNN+MLP CNN+CCE+MLP CNN+multiRN w/o BCN CNN+multiRN CNNh+multiRN  Relational Non-relational #Params Runtime1  94% 91.0% 89.9% 96.5% 74.2% 72.9% 88.7% 92.9% 96.7%  94% 99.6% 99.8% 99.9% 65.0% 64.5% 99.3% 99.9% 99.9%  19.5M 19.5M 365K 365K 239K 258K 224K 345K 345K   575.8ms 23.5ms 315.6ms  6.2ms 6.2ms 7.5ms 8.3ms 9.9ms  reported in [29] which consists of one trained for the Sort-of-CLEVR dataset and a shallower model for the CLEVR.|
|||To check the efficiency when the number of object inputs upsurges for the multiRN to manage, we reduce the stride of the last layer of the preceding CNN from 2 to 1, denoting as CNNh.|
||4 instances in total. (in eccv2018)|
|1092|Myunggi_Lee_Motion_Feature_Network_ECCV_2018_paper|Also, because MFNet is based on a 2D CNN architecture, it has fewer parameters compared to its 3D counterparts.|
|||small movement), while one pixel in the feature space at a higher hierarchy of a CNN can capture larger optical flow (i.e.|
|||Pre-3D CNN + Avg[8] MultiScale TRN[37] MultiScale TRN (10-crop)[37] MFNet-C50, K = 7 MFNet-S50, K = 7 MFNet-C50, K = 10 MFNet-S50, K = 10 MFNet-C101, K = 10   93.70% 95.31% 96.13% 96.31% 96.56% 96.50% 96.68%   99.59% 99.86% 99.65% 99.80% 99.82% 99.86% 99.84%  11.5% 33.01% 34.44% 37.31% 37.09% 40.30% 39.83% 43.92%  30.0% 61.27% 63.20% 67.23% 67.78% 70.93% 70.19% 73.12%  Table 4.|
|||Our models outperform Pre-3D CNN + Avg [8] and the MultiScale TRN [37].|
||4 instances in total. (in eccv2018)|
|1093|McCormac_SceneNet_RGB-D_Can_ICCV_2017_paper|compared pre-training a CNN (already with ImageNet initialisation) on lower quality OpenGL renderings against pre-training on high quality physically-based renderings, and found pre-training on high quality renderings outperformed on all three tasks.|
|||Our networks pre-trained on SceneNet RGB-D were both maintained at a constant learning rate as they were below 30 epochs the RGB CNN was pretrained for 15 epochs which took approximately 1 month on 4 Nvidia Titan X GPUs, and the RGB-D CNN was pretrained for 10 epochs, taking 3 weeks.|
|||This result suggests that a large-scale highquality synthetic RGB dataset with task-specific labels can be more useful for CNN pre-training than even a quite large (1M image) real-world generic dataset such as ImageNet, which lacks the fine-grained ground truth data (per-pixel labelling), or domain-specific content (indoor semantics).|
|||The results of our experiments, to the best of our knowledge, are the first to show an RGB-only CNN pre-trained from scratch on synthetic RGB images improve upon the performance of an identical network initialised with weights from a CNN trained on the large-scale real-world ImageNet dataset.|
||4 instances in total. (in iccv2017)|
|1094|Noh_Learning_Deconvolution_Network_ICCV_2015_paper|Recent semantic segmentation algorithms are often formulated to solve structured pixel-wise labeling problems based on CNN [1, 19].|
|||They convert an existing CNN architecture constructed for classification to a fully convolutional network (FCN).|
|||This approach is also employed to visualize activated features in a trained CNN [26] and update network architecture for performance enhancement.|
|||This visualization is useful for understanding the behavior of a trained CNN model.|
||4 instances in total. (in iccv2015)|
|1095|Jin_Video_Scene_Parsing_ICCV_2017_paper|More importantly, current CNN models are hungry for data and finely annotated video data for training are rather labor-intensive to collect and limited.|
|||Deep CNN models are prone to over-fitting when trained using a small training data set and thus generalize badly in real applications.|
|||Related Work  Recent image scene parsing progress is mostly stimulated by various new CNN architectures, including the fully convolutional architecture (FCN) with multi-scale or larger receptive fields [5, 22, 36, 49] and the combination of CNN with graphical models [3, 30, 48, 50, 31].|
|||Thus it is reasonable to expect further improvement on the performance by using more powerful front CNN architectures.|
||4 instances in total. (in iccv2017)|
|1096|POSEidon_ Face-From-Depth for Driver Pose Estimation|Depth input images are acquired by low cost sensors (black) and provided to a head localization CNN (blue) to suitably crop the images around the upper-body or head regions.|
|||The module based on a regressive CNN (Sect.|
|||The first one  i.e., the CNN directly working on depth data  plays the main role on the pose estimation, while the other two cooperate to reduce the estimation error.|
|||The baseline is a single CNN working on the source depth map.|
||4 instances in total. (in cvpr2017)|
|1097|Learned Contextual Feature Reweighting for Image Geo-Localization|(a) A contextual reweighting network takes convolutional features of a deep CNN as input to produce a spatial weighting mask (b) based on the learned contexts.|
|||In our work, we propose the incorporation of a context-adaptive feature preponderance into a CNN framework.|
|||NetVLAD: CNN architecture for weakly supervised place recognition.|
|||Particular object retrieval with integral max-pooling of cnn activations.|
||4 instances in total. (in cvpr2017)|
|1098|Gupta_Aligning_3D_Models_2015_CVPR_paper|We train this CNN on synthetic data using surface normal images instead of depth images as input.|
|||++]Estimate  Coarse PoseAlign to data3 layer CNN on normal images trained on synthetic dataSearch over scale, rotation,  translation and CAD model to  minimize re-projection errorRL  Pmax(3, 2)  D(0.5)  N  C(5, 128, 2)  RL  Pmax(3, 2)  N  C(3, (Npose + 1)Nclass, 1)  RL  Pave(14, 1).|
|||5.2.1 Object Detection and Instance Segmentation  We note that our instance segmentation system from [13] computed CNN features on bounding boxes, not free-form regions.|
|||We then used a CNN to estimate pose for each detected object.|
||4 instances in total. (in cvpr2015)|
|1099|Xihui_Liu_Show_Tell_and_ECCV_2018_paper|The captioning module, aiming at generating captions for given images, is composed of a CNN image encoder Ei(I) and an LSTM language decoder Dc(v).|
|||H. Li, J. Shao, D. Chen, X. Wang  We first encode images and captions into features in the same embedding space a CNN encoder Ei and a Gated Recurrent Unit (GRU) encoder Ec for captions,  v = Ei(I), c = Ec(C),  (3)  where I and C denote images and captions, and v and c denote visual features and caption features, respectively.|
|||When training the captioning module, the retrieval module and CNN image encoder are fixed.|
|||Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image captioning.|
||4 instances in total. (in eccv2018)|
|1100|Sunghun_Kang_Pivot_Correlational_Neural_ECCV_2018_paper|[19] trained a deep CNN on large video dataset while investigating the effectiveness of various temporal fusion.|
|||In the two stream networks [10,11,25], two separate CNN streamsone taking static image as input while the other taking optical floware considered, and intermediate features of the two streams leading up to the final prediction are fused either by the summation [10] or multiplicative operations [11].|
|||Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn architecture for weakly supervised place recognition.|
|||: Cnn architectures for largescale audio classification.|
||4 instances in total. (in eccv2018)|
|1101|Minhyeok_Heo_Monocular_Depth_Estimation_ECCV_2018_paper|The proposed CNN combines WSM upsampling blocks with a ResNet encoder.|
|||We highlight our main contributions as follows:   We propose a deep CNN with the novel WSM upsampling blocks for monoc ular depth estimation.|
|||[13] first applied a CNN to monocular depth estimation.|
|||[16] proposed a CNN for joint depth estimation and semantic segmentation, and refined a depth map using a two-layer CRF.|
||4 instances in total. (in eccv2018)|
|1102|cvpr18-Image Correction via Deep Reciprocating HDR Transformation|These methods tend to focus on image pixel balance via different approaches including histogram equalization [28], edge preserving filtering [11, 1], and CNN encoder-decoder [41].|
|||It contains two CNN networks.|
|||The first CNN network reconstructs the missing details in the HDR domain and the second CNN network transfers the details back to the LDR domain.|
|||1 and 2 are the CNN parameters.|
||4 instances in total. (in cvpr2018)|
|1103|cvpr18-The Unreasonable Effectiveness of Deep Features as a Perceptual Metric|Levels  Imgs/Patches  # Judgments  5 6 17 24  425    425  traditional traditional traditional traditional  continuous  5 4 5  trad + CNN continuous alg outputs trad.|
|||+ CNN continuous    .8k .8k 2.7k 3.0k  321.6k 53.8k 9.6k  25k 25k 250k 500k  349.8k 134.5k 28.8k  Judgment  Type  MOS MOS MOS MOS  2AFC 2AFC  Same/Not same  Table 1: Dataset comparison.|
|||Kim and Lee [25] use a CNN to predict visual similarity by training on low-level differences.|
|||Image quality assessment by comparing cnn features between images.|
||4 instances in total. (in cvpr2018)|
|1104|Zhu_Single_Image_Pop-Up_ICCV_2015_paper|We apply linear discriminant analysis due to efficiency in training and limited loss in detection accuracy [17, 12],  Method HOG-SVM CNN 0.53 mAP  0.41  Table 1: Comparison of CNN and HOG-SVM in part localization.|
|||Deep Part Classifiers HOG part detections serve as part proposals and are subsequently re-ranked by forwarding through a CNN and applying SVM on the extracted Pool5 layer features.|
|||Table 1 shows performance comparison of CNN and HOG-SVM on the 12 parts of PASCAL3D dataset car category.|
|||Estimating a single object instance in a PASCAL3D image (500x300 pixels) requires: 0.08 seconds building HOG pyramid; 1.41 seconds in filters convolution; 3.76 seconds in CNN classification and 1.52 seconds in ADMM.|
||4 instances in total. (in iccv2015)|
|1105|cvpr18-Towards Pose Invariant Face Recognition in the Wild|The DeepFace [30, 31] model uses a deep CNN coupled with 3D alignment.|
|||Different from conventional CNN based discriminators, we construct the second branch of the discriminator as the learner DL that dynamically predicts the suitable convolutional parameters of the first branch DM from a single sample.|
|||Discriminative Learning Sub(cid:173)Net  The DLN is a generic CNN for face recognition trained by our proposed enforced cross-entropy optimization strategy for learning discriminative yet generalizable facial representations.|
|||A light cnn for deep face representation with noisy labels.|
||4 instances in total. (in cvpr2018)|
|1106|Kanazawa_WarpNet_Weakly_Supervised_CVPR_2016_paper|In particular, it achieves matching accuracy over 13.6% higher than a baseline ILSVRC CNN [9].|
|||A CNN framework to predict dense optical flow on general scenes is proposed by [12], but in a supervised manner.|
|||Figure 7 shows a few qualitative matching results comparing the baseline CNN and WarpNet.|
|||We observe that VGG-M conv4 and DSP perform similarly, showing that while DSP obtains low recall at high precision, its overall match quality is similar to CNN features, an observation in line with [7].|
||4 instances in total. (in cvpr2016)|
|1107|Andrew_Gilbert_Volumetric_performance_capture_ECCV_2018_paper|Function F is learned using a CNN specifically a convolutional autoencoder consisting of successive three-dimensional (3D) alternate convolutional filtering operations and downor up-sampling with non linear activation layers.|
|||The iterative process fits vertices to the PVH output by the CNN using the marching cubes algorithm [27] with a dynamically chosen threshold, thus producing a highresolution triangle mesh, that is used as the geometric proxy for resampling of the scene appearance onto the texture.|
|||The sub-volume (patch) size i. e. receptive field of the autoencoder (VL and VH  Rnnn is varied across n = {16, 32, 64} the latter being a degenerate case where the entire volume is scaled and passed through the CNN in effect a global versus patch based filter of the volume.|
|||However, we are able to transfer our trained CNN models for 2 7 8 and 4 7 8 views on TotalCapture without any further training, to hallucinate volumes as if 8 cameras were used at acquisition.|
||4 instances in total. (in eccv2018)|
|1108|Yongyi_Lu_Attribute-Guided_Face_Generation_ECCV_2018_paper|For example, in single-image superresolution (SISR), a deep recursive CNN for SISR was proposed in [8].|
|||A deep CNN approach was proposed in [2] using bicubic interpolation.|
|||Preserving facial identity has also been explored in synthesizing the corresponding frontal face image from a single side-view face image [5], where the identity preserving loss was defined  4  Y. Lu, Y. W. Tai and C. K. Tang  based on the activations of the last two layers of the Light CNN [19].|
|||Wu, X., He, R., Sun, Z.: A lightened CNN for deep face representation.|
||4 instances in total. (in eccv2018)|
|1109|cvpr18-Who Let the Dogs Out  Modeling Dog Behavior From Visual Data|[39] also train a CNN to learn depth map and motion of the camera in two consecutive images.|
|||The encoder part of the model consists of a CNN and an LSTM.|
|||The CNN consists of two towers of ResNet-18 [10], one for each frame, whose weights are shared.|
|||The CNN baseline concatenates all the images into an input tensor for a ResNet18 model that classifies the future actions.|
||4 instances in total. (in cvpr2018)|
|1110|Gregoire_Payen_de_La_Garanderie_Eliminating_the_Dreaded_ECCV_2018_paper|Su and Grauman [56] introduce a Flat2Sphere technique to train a spherical CNN to imitate the results of an existing CNN facilitating large object detection at any angle.|
|||Therefore, the additional complexity of the spherical CNN introduced by [56] is not needed in the specific automotive context.|
|||1 for future comparison our code, models and evaluation data is publicly available at:  https://gdlg.github.io/panoramic  4  G. Payen de La Garanderie, A. Atapour Abarghouei, T. Breckon  Contemporary end-to-end CNN driven detection approaches are based on the R-CNN architecture introduced by Girshick [23].|
|||Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifier.|
||4 instances in total. (in eccv2018)|
|1111|cvpr18-Pix3D  Dataset and Methods for Single-Image 3D Shape Modeling|R@1 R@2 R@4 R@8 R@16 R@32  Azimuth  Elevation  3D-VAE-GAN [62] 0.02 0.03 0.07 0.12 0.21 0.42 0.51 0.57 0.64 0.71 MarrNet [60] Ours (w/ Pose) 0.42 0.48 0.55 0.63 0.70 0.53 0.62 0.71 0.78 0.85 Ours (w/o Pose)  0.34 0.78 0.76 0.90  # of views  4  8  12  24  4  6  12  Render for CNN 0.71 0.63 0.56 0.40 0.57 0.56 0.37 0.76 0.73 0.61 0.49 0.87 0.70 0.61 Ours  Table 4: Results for image-based shape retrieval, where R@K stands for Recall@K. Our model (without the pose estimation module) achieves the highest numbers.|
|||Our model outperforms Render for CNN [55] in both azimuth and elevation.|
|||We compare our method with Render for CNN [55].|
|||Table 5 suggests that our model outperforms Render for CNN in pose estimation.|
||4 instances in total. (in cvpr2018)|
|1112|Zhao_Towards_More_Accurate_ICCV_2017_paper|The  reason  for  selecting  FCN  instead  of  CNN  for  iris  feature extraction primarily lies in the previous analysis on  iris  patterns  in  Section  1.2,  i.e.,  the  most  discriminative  information of an iris probably comes from small and local  patterns.|
|||For  the  CNN  model,  we  have  chosen  the  popular  VGG-16  which  has  achieved superior performance in face recognition.|
|||Since  the  original  model  in  their  paper  is  not  publicly  available, we carefully implemented and trained the CNN  according to all the details in [28].|
|||Such results support our previous analysis that global and  high level features extracted by CNN may not be suitable  for  from  FCN+triplet  loss  strongly  suggests  that  it  is  necessary  to  account for bit-shifting and non-iris region masking when  learning spatially corresponding features through FCN.|
||4 instances in total. (in iccv2017)|
|1113|Shi_Weakly_Supervised_Object_ICCV_2017_paper|Due to the use of strong CNN features [29, 30], recent works on WSOL [9, 4, 13, 28, 27, 15] have shown remarkable progress.|
|||Every image in A or B is represented by a 4096-dimensional CNN feature vector covering the whole image, using the output of the fc7 layer of the AlexNet CNN architecture [30].|
|||Following [29, 8, 13, 14, 28], we describe the proposals by the output of the fc7 layer of the AlexNet CNN architecture [30].|
|||The CNN model is pre-trained for whole-image classification on ILSVRC [24], using the Caffe implementation [49].|
||4 instances in total. (in iccv2017)|
|1114|Kaipeng_Zhang_Super-Identity_Convolutional_Neural_ECCV_2018_paper|[37] proposed a bichannel CNN to hallucinate blurry facial images in the wild.|
|||CN NR is a Resnet-like [9] CNN and more details about the network structure are introduced in our supplementary material.|
|||It is trained by A-Softmax loss function [18] which encourages the CNN to learn discriminate identity features (i.e.|
|||5 Conclusion  In this paper, we present Super-Identity CNN (SICNN) to enhance the identity information during super resolving face images of size 1214 pixels with an 8 upscaling factor.|
||4 instances in total. (in eccv2018)|
|1115|cvpr18-People, Penguins and Petri Dishes  Adapting Object Counting Models to New Visual Domains and Object Types Without Forgetting|The core contributions of this paper can be summarised as follows:   An extendable CNN architecture for patch-based, multidomain object counting is developed for the first time;   A fully convolutional neural network is utilised to refine  a set of count estimates for an entire image;   A challenging, representative dataset for cell counting in a tissue culture/patient diagnosis setting is proposed;   Domain adaptation in object counting is shown to produce more efficient, higher accuracy counting models;   State-of-the-art counting accuracy is observed on leading benchmarks for several object types including the Shanghaitech [13] and Penguins [14] datasets.|
|||Vehicle counting has been tackled using a variety of techniques including SIFT-based regression [21], CNN regression [10], density heatmap generation [22] as well as an LSTM (Long Short-Term Memory) based approach from Zhang et al.|
|||[24] who trained a CNN model to learn a universal vision representation which can jointly perform non-related tasks from distinct visual domains by including domain-specific scaling and normalisation layers throughout the network.|
|||Our approach  The proposed object counting technique consists of a patch-based CNN regressor with significant inter-domain parameter sharing that can be quickly switched between a learned set of visual domains by interchanging a subset of domain specific parameters.|
||4 instances in total. (in cvpr2018)|
|1116|cvpr18-Structure Inference Net  Object Detection Using Scene-Level Context and Instance-Level Relationships|Modern CNN based object detection methods can be divided into two groups [25, 35]: (i) region proposals based methods (two-stage detectors) and (ii) proposal-free methods (one-stage detectors).|
|||R-CNN [16] extracts CNN features from the candidate regions and applies linear SVMs as the classifier.|
|||GBD-Net [43] proposes a novel gated bi-directional CNN to pass message between features of different support regions around objects.|
|||Gated bi-directional cnn for object detection.|
||4 instances in total. (in cvpr2018)|
|1117|cvpr18-Densely Connected Pyramid Dehazing Network|This was achieved by leveraging a linear transformation to embed both the transmission map and the atmospheric light into one variable and then learning a light-weight CNN to recover the clean image.|
|||One of the limitations of these methods is that they limit their capabilities by only considering the transmission map in their CNN frameworks.|
|||2) It is known that low-level features such as edges and contours can be captured in the shallow (first several) layers of a CNN structure [47].|
|||The feature loss is defined as  LE,f = X  k(V1(Gt(I)))c1,w1,h1  (V1(t))c1,w1,h1 k2  c1,w1,h1  + X  c2,w2,h2  k(V2(Gt(I)))c2,w2,h2  (V2(t))c2,w2,h2 k2,  (6)  where Vi represents a CNN structure and ci, wi, hi are the dimensions of the corresponding low-level feature in Vi.|
||4 instances in total. (in cvpr2018)|
|1118|cvpr18-Collaborative and Adversarial Network for Unsupervised Domain Adaptation|We add several domain classifiers on multiple CNN feature extraction blocks1, in which each domain classifier is connected to the hidden representations from one block and one loss function is defined based on the hidden presentation and the domain labels (e.g., source and target).|
|||Since deep learning methods have achieved excellent performance for many computer vision tasks including object recognition, a few deep transfer learning methods  1In this work, each block consists of several CNN layers.|
|||Each feature extraction block consist of a group of CNN layers.|
|||In particular, suppose in total m blocks are used, where each block consist of a group of CNN layers, we build a domain discriminator after each block, leading to m domain discriminators.|
||4 instances in total. (in cvpr2018)|
|1119|Lu_Decoder_Network_Over_ICCV_2017_paper|Io = arg min  Lcontent(f (Io), f (Ic))  Io  +Lstyle(f (Io), f (Is))  (1)  The content loss is usually defined as the distance between CNN features.|
|||The image style can be represented as the gram matrix [9] of CNN feature.|
|||Here the decoder network is denoted as g(Fo; ) and the network to extract CNN feature is denoted as f .|
|||[9] defines the content loss in a CNN layer and formulates the style loss in multi-layer.|
||4 instances in total. (in iccv2017)|
|1120|Oquab_Learning_and_Transferring_2014_CVPR_paper|In particular, we design a method that uses ImageNet-trained layers of CNN to compute efficient mid-level image representation for images in Pascal VOC.|
|||Transferring CNN weights  The CNN architecture of [24] contains more than 60 million parameters.|
|||The transfer method significantly improves over the NO PRETRAIN baseline where the CNN is trained solely on the action images from Pascal VOC, without pretraining on ImageNet.|
|||In order to better adapt the CNN to the subtleties of the action recognition task, and inspired by [6], our last results were obtained by training the target task CNN without freezing the FC6 weights.|
||4 instances in total. (in cvpr2014)|
|1121|Park_Force_From_Motion_CVPR_2016_paper|3835  2  2  2  Prior, p(g)  Likelihood, p({I }   |g)  F i=1  i  F Posterior, p(g|{I }   ) i=1  i  Prediction Ground truth  Error: 0.5 degree  Error: 1.1 degree  Error: 15.8 degree  (a) Gravity distribution  (b) CNN Visualization  Figure 2.|
|||The CNN correctly predicts gravity direction while the last image produces 15 degree error due to the tilted bicycler.|
|||Figure 2(b) illustrates the likelihood of the gravity directions learned by CNN as shown in the red heatmap and the ground truth gravity direction with dotted line.|
|||We use 29 Bike sequences ranging from 5 mins to 20 mins (about 1 million images8) to fine-tune the CNN pre-trained by [16] using Caffe [11].|
||4 instances in total. (in cvpr2016)|
|1122|Marie-Morgane_Paumard_Image_Reassembly_Combining_ECCV_2018_paper|[4] separately processed fragments using a deep CNN with shared weights which output comparable features.|
|||Indeed, the output of the CNN can be viewed as localized pattern activations.|
|||Both these classification tasks are performed by a deep CNN described later.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||4 instances in total. (in eccv2018)|
|1123|Singh_First_Person_Action_CVPR_2016_paper|We propose a new compact CNN architecture for recognizing the wearers actions, which takes as input egocentric cues and can be trained from limited training samples.|
|||The annotated datasets and the source code along with the pre-trained CNN models for the paper are available at the project page: http://cvit.iiit.ac.in/ projects/FirstPersonActions/.|
|||Implementation Details  We use Caffes CNN implementation [14] due to its speed In all experiments, we normalise data to and efficiency.|
|||Method  Features  Accuracy  Ego ConvNet (2D) Ego ConvNet (2D) Ego ConvNet (2D)  Ego ConvNet (3D) Ego ConvNet (3D) Ego ConvNet (3D)  H  H+C  H+C+M  H  H+C  H+C+M  Ego ConvNet (2D) + TDD Ego ConvNet (3D) + TDD  H+C+M+S+T H+C+M+S+T  Combined  H+C+M+S+T  51.76 54.13 57.61  50.82 53.15 55.79  65.29 66.96  68.50  Table 4: Effect of various CNN features on first person action recognition.|
||4 instances in total. (in cvpr2016)|
|1124|Chen_Semantic_Image_Segmentation_CVPR_2016_paper|Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering.|
|||A single unified CNN produces both coarse semantic segmentation scores and an edge map, which respectively serve as input multi-channel image and reference edge to a domain transform edge-preserving filter.|
|||Instead, we will learn the reference edge map from intermediate layer features of the same CNN that produces the semantic segmentation scores, as illustrated in Fig.|
|||Though DT is traditionally used for graphics applications [16], we use it to filter the raw CNN semantic segmentation scores to be better aligned with object boundaries, guided by the EdgeNet produced edge map.|
||4 instances in total. (in cvpr2016)|
|1125|Bertasius_DeepEdge_A_Multi-Scale_2015_CVPR_paper|learning and the use of the Nearest Neighbor algorithm within a CNN framework while DeepNet uses a traditional CNN architecture to predict contours.|
|||First, we define a novel multi-scale bifurcated CNN architecture that enables our network to achieve state-of-the-art contour detection results.|
|||Such an approach would reduce the number of CNN evaluations needed from 60K to 4 (one for each scale), which would allow our method to run in real time even on CPUs.|
|||In conclusion, our results suggest that pure CNN systems can be applied successfully to contour detection and possibly to many other low-level vision tasks.|
||4 instances in total. (in cvpr2015)|
|1126|cvpr18-Temporal Hallucinating for Action Recognition With Few Still Images|Here we choose the widely-used two-stream CNN architectures for comparison, i.e., Towards Good-Practice Net (TGPN) in [36] and Temporal Segment Net (TSN) in [37].|
|||This is mainly because that, TSN is a deeper two-stream CNN which generates more discriminative features than TGPN.|
|||We examine our HVM, according to action representations of different two-stream CNN architectures, i.e., Towards Good-Practice Net (TGPN) in [36] and Temporal Segment Net (TSN) in [37].|
|||This is mainly because that, TSN is a deeper two-stream CNN which generates more discriminative features than TGPN.|
||4 instances in total. (in cvpr2018)|
|1127|cvpr18-WILDTRACK  A Multi-Camera HD Dataset for Dense Unscripted Pedestrian Detection|The Deep Multi-camera Detection (DeepMCD) [12] method which integrates CNN features demonstrated stateof-the-art results and showed that the accuracy and the confidence of a CNN classifier increase as more views are used.|
|||The multiview CNN architecture is adapted so as to use the weights  from the base network and produces joint estimates by processing the multi-view streams in parallel.|
|||It introduces a Higher Order CRF, where unary potentials are produced by ROI pooling CNN [39].|
|||The recent work of [47] proposes a MCMT tracking framework that relies on a powerful CNN for detection purposes [39].|
||4 instances in total. (in cvpr2018)|
|1128|Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper|For example, the prevalent two-stream CNN model [17] augments a static CNN with a stream that processes optical flow.|
|||RNN [24, 5] and 3D CNN [7, 20], yet existing benchmarks have not produced strong evidence in favor of these models.|
|||This is because a CNN learned with cross-entropy loss is a maximum likelihood (ML) estimator  of ground-truth parameters .|
|||1,000 examples per class, that appear to suffice to control the variance of current CNN models.|
||4 instances in total. (in eccv2018)|
|1129|cvpr18-Net2Vec  Quantifying and Explaining How Concepts Are Encoded by Filters in Deep Neural Networks|In this paper, we conduct a thorough analysis to investigate how semantic concepts, such as objects and their parts, are encoded by CNN filters.|
|||Net2Vec  With our Net2Vec paradigm, we propose aligning concepts to filters in a CNN by (a) recording filter activations  8731  of a pre-trained network when probed by inputs from a reference, probe dataset and (b) learning how to weight the collected probe activations to perform various semantic tasks.|
|||Concept Segmentation  In this section, we show how learning to segment concepts can be used to induce concept embeddings using either all the filters available in a CNN layer or just a single filter.|
|||Conclusion  We present a paradigm for learning concept embeddings that are aligned to a CNN layers filter space.|
||4 instances in total. (in cvpr2018)|
|1130|cvpr18-Interpretable Video Captioning via Trajectory Structured Localization|Beside the hand-crafted visual features like Dense trajectories, researchers have started exploring CNN on video representation.|
|||image), it is natural to choose convolutional neural network, for temporal and sequential data, recurrent neural network (RNN) or its variants are very good options, while for spatialtemporal data, such as videos, a combination of CNN and RNN is a good alternative.|
|||As with the encoder, the architecture of the decoder can be CNN or RNN, depending on the type of y.|
|||Encoder: LSTM Encoding on Temporal Seg(cid:173)  ments  Deep convolutional neural networks have recently achieved many successes in visual recognition tasks and the pre-trained CNN models for object classification have been demonstrated very effective feature extractor for other vision tasks such as object detection and visual captioning.|
||4 instances in total. (in cvpr2018)|
|1131|Yin_Towards_Large-Pose_Face_ICCV_2017_paper| Use of a deep face recognition CNN to enforce that generated faces satisfy identity-preservation, besides realism and frontalization.|
|||Since the intent is for R to also be trainable with the rest of the framework, we use a CNN model based on CASIA-Net [37] for this regression task.|
|||Unconstrained face verification using deep CNN features.|
|||Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers.|
||4 instances in total. (in iccv2017)|
|1132|cvpr18-Multi-Shot Pedestrian Re-Identification via Sequential Decision Making|first extracted features with CNN from images and then use RNN and temporal pooling to aggregate those features.|
|||It is built with a CNN trained with three different loss functions.|
|||Image Level Feature Extraction  For single image feature extractor, a CNN is trained to embed an image into a latent space that preserves certain relationships of samples.|
|||Comparisons with State(cid:173)of(cid:173)the(cid:173)art Methods   Video fine-tune: Here we randomly sample 8 images from each sequence, averagely pool the features and use this sequence level feature to fine-tune the CNN as  Table 3 summarizes the CMC results of our model and other state-of-the-art multi-shot re-id methods.|
||4 instances in total. (in cvpr2018)|
|1133|cvpr18-Path Aggregation Network for Instance Segmentation|Similar to FPN, the improvement is independent of the CNN structures, such as those of [57, 32, 23].|
|||In comparison, the CNN trunk in FPN gives a long path (dashed red line in Figure 1) passing through 100+ layers from low layers to the topmost one.|
|||Object detection via a multiIn  region and semantic segmentation-aware CNN model.|
||3 instances in total. (in cvpr2018)|
|1134|Tsai_Video_Segmentation_via_CVPR_2016_paper|For the appearance term, we construct the color GMM in the first frame, and an online SVM model with CNN features [20] updated every frame.|
|||The weight 1 consists of col and cnn for the color and CNN features, respectively.|
|||For learning the online SVM model, we extract hierarchical CNN features [20] combining the first 5 convolutional layers from a pre-trained VGG net [28] into 1472 dimensional vectors.|
||3 instances in total. (in cvpr2016)|
|1135|Dense Captioning With Joint Inference and Visual Context|First row is the performance with CNN and RPN fixed, second row is the performance of corresponding models with end-to-end training.|
|||We begin fine-tuning the CNN layers after 200K iterations (3 epochs) and finish training after 600K iterations (9 epochs).|
|||Towards this end, we fix the weights of the CNN to those of VGG16 and use a hold-out region proposal network also trained on Visual Genome based on the fixed CNN weights.|
||3 instances in total. (in cvpr2017)|
|1136|cvpr18-Multi-View Harmonized Bilinear Network for 3D Object Recognition|Multi-view CNN (MVCNN) [29] max-pools the view-wise feature into a global feature as the representation of the 3D object.|
|||MVCNN [29] projects a 3D object into multiple views, extracts view-wise CNN features and max-pools them into a global representation of the 3D object.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||3 instances in total. (in cvpr2018)|
|1137|Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper|FR-IQA methods like [20, 29], NR-IQA algorithms like [15, 3], NSSbased approaches like [15, 29] and training-based methods like [28, 28], even the CNN architecture in [10] all depend on features based on pixel-level or patch-level local characteristic.|
|||Linear models can be applied to non-linear features to obtain promising results in many applications, such as CNN (the output layer), classification algorithms [5] and IQA [27].|
|||SROCC  LCC  0.882 0.892  BRISQUE CORNIA CNN SOM 0.923 0.899  0.892 0.880  0.920 0.903  Table 2.|
||3 instances in total. (in cvpr2015)|
|1138|Li_Conditional_Graphical_Lasso_CVPR_2016_paper|To meet these challenges, many image representation and feature learning schemes have been developed to gain variation-invariance, such as GIST [29], dense SIFT [4], VLAD [18], object bank [25], and deep CNN [22, 8].|
|||As for image features of the latter two datasets, two kinds of feature extractors are employed, i.e., the PHOW (a variant of dense SIFT descriptors extracted at multiple scales) features [4] and the deep CNN (convolutional neural network) features [22, 8].|
|||Label Graph Structure of CGL  To build up an intuition on structure learning of CGL, we employ PASCAL07 with CNN features to visualize the label correlations under different levels of sparsity regularization.|
||3 instances in total. (in cvpr2016)|
|1139|cvpr18-Fast and Accurate Single Image Super-Resolution via Information Distillation Network|propose a 20-layer CNN model known as VDSR [12], which adopts residual learning and adaptive gradient clipping to ease training difficulty.|
|||[12] propose a very deep CNN model with global residual architecture to achieve superior performance, which utilizes contextual information over large image regions.|
|||The authors also present a very deep end-to-end persistent memory network (MemNet) [23] for image restoration task, which tackles the long-term dependency problem in the previous CNN architectures.|
||3 instances in total. (in cvpr2018)|
|1140|cvpr18-Learning Visual Knowledge Memory Networks for Visual Question Answering|As the VQA task was proposed after deep learning approaches had already gained wide popularity, almost all current VQA solutions use CNN to model image input and recurrent neural network (RNN) to model the question [49].|
|||Numerous VQA methods [44, 35, 26, 46, 3] have incorporated spatial attention to learn specific CNN features according to the input question, rather than using holistic global features from the entire image.|
|||Similarly, spatial memory networks [44] also store CNN features from image grid regions/patches into memory, and selects certain parts of the information with an explicit attention mechanism for question answering.|
||3 instances in total. (in cvpr2018)|
|1141|cvpr18-Who's Better  Who's Best  Pairwise Deep Ranking for Skill Determination|Learning to Determine Skill  In this section we first give an overview of the skill determination problem and the Siamese two-stream CNN architecture we use to determine skill.|
|||1  2  3  4  g n  i l l  o R h g u o D  y r e g r u S  i  g n w a r D  g n i s U k c i t s p o h C  Best  sualize the top-down attention of the spatial CNN on example rankings for three datasets using [26] based on [35].|
|||Two stream action CNN analysis code.|
||3 instances in total. (in cvpr2018)|
|1142|Zeng_Huang_Deep_Volumetric_Video_ECCV_2018_paper| A novel multi-view 2D CNN network that maps multi-view images to a dense 3D probability field, which enables high-resolution reconstruction and robust motion capture from texture-less surfaces.|
|||We then collect the multi-scale CNN features learned at each projected location and aggregate them through a pooling layer to obtain the final global feature for the query point.|
|||Using only sparse-view RGB images as input, our novel multi-view CNN encodes the probability of a point lying on the surface of the capture subject, enabling subsequent high resolution surface reconstruction.|
||3 instances in total. (in eccv2018)|
|1143|Rui_Yu_Hard-Aware_Point-to-Set_Deep_ECCV_2018_paper|(1) that triplet loss aims to force the distance between an intra-class pair less than an inter-class pair by at least a margin m. While training a CNN with triplet loss, many of the possible triplets would easily satisfy the constraint  d(f a, f p) + m < d(f a, f n).|
|||In [18], a well-designed CNN can learn discriminative features by soft pixel attention and hard regional attention.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based CNN with improved triplet loss function.|
||3 instances in total. (in eccv2018)|
|1144|Wu_Anticipating_Daily_Intention_ICCV_2017_paper|It is well-known that ImageNet [5] pretrained CNN performs well on classifying a variety of objects.|
|||ResNet-based CNN [9].|
|||For fine-tuning the CNN model on our dataset, we set maximum iterations 20000, step-size 10000, momentum 0.9, every 10000 iteration weight decay 0.1, and learning rate 0.001.|
||3 instances in total. (in iccv2017)|
|1145|Zeming_Li_DetNet_Design_Backbone_ECCV_2018_paper|Recent CNN based object detectors, either one-stage methods like YOLO, SSD, and RetinaNet, or two-stage detectors like Faster R-CNN, R-FCN and FPN, are usually trying to directly finetune from ImageNet pre-trained models designed for the task of image classification.|
|||Recent CNN based object detectors can be categorized into one-stage detectors, like YOLO [29, 30], SSD [24], and RetinaNet [22], and two-stage detectors, e.g.|
|||With the rapid progress of deep convolutional neural networks, CNN based object detectors have yielded remarkable results and become a new trend in the detection literature.|
||3 instances in total. (in eccv2018)|
|1146|Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields|Our method takes the entire image as the input for a two-branch CNN to jointly predict confidence maps for body part detection, shown in (b), and part affinity fields for parts association, shown in (c).|
|||This property emerges because the CNN is trained with a large receptive field, and PAFs from non-adjacent tree nodes also influence the predicted PAF.|
|||However, the parsing time does not significantly influence the overall runtime because it is two orders of magnitude less than the CNN processing time, e.g., for 9 people, the parsing takes 0.58 ms while CNN takes 99.6 ms. Our method has achieved the speed of 8.8 fps for a video with 19 people.|
||3 instances in total. (in cvpr2017)|
|1147|Tekin_Learning_to_Fuse_ICCV_2017_paper|Approach  Our goal is to increase the robustness and accuracy of monocular 3D pose estimation by exploiting image cues to the full while also taking advantage of the fact that 2D joint locations can be reliably detected by modern CNN archi 3942  tectures.|
|||The second is a CNN trained to predict 3D pose from only the 2D confidence map (CM) stream.|
|||We have demonstrated that the resulting CNN pipeline significantly outperforms state-of-the-art methods on standard 3D human pose estimation benchmarks.|
||3 instances in total. (in iccv2017)|
|1148|cvpr18-Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning|Given the target domain of interest, we pre-train a CNN on the selected subset from the source domain based on the proposed domain similarity measure, and then fine-tune on the target domain.|
|||Recent FGVC methods typically incorporate useful fine-grained information into a CNN and train the network end-toend.|
|||Bilinear cnn models for fine-grained visual recognition.|
||3 instances in total. (in cvpr2018)|
|1149|cvpr18-SBNet  Sparse Blocks Network for Fast Inference|We verified the effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we report significant wall-clock speed-ups compared to dense convolution without noticeable loss of accuracy.|
|||1/(1-sparsity), and the released implementation of sub-manifold sparse CNN [9] (Sub-M).|
|||Deep roots: Improving cnn efficiency with hierarchical filter groups.|
||3 instances in total. (in cvpr2018)|
|1150|Multi-Task Correlation Particle Filter for Robust Object Tracking|Despite achieving the state-of-the-art performance, existing CNN based correlation filter trackers [25, 29] have several limitations.|
|||Since features from different layers can enhance and complement each other, existing CNN based correlation trackers (CF2 [25] and HDT [29]) perform well.|
|||Note that, a variety of features can be adopted, such as HOG, other layers of CNN features as in the HDT [29].|
||3 instances in total. (in cvpr2017)|
|1151|An_Contractive_Rectifier_Networks_ICCV_2015_paper|For MNIST dataset, our baseline CNN comprises of two alternating convolutional (filter size: 5  5, number of channels are 32 and 64 respectively) and max-pooling (size: 2  2, stride: 2) layers and one fully connected layer (number of neurons: 500) with a dropout rate of 0.5.|
|||For the other evaluated datasets, we trained our contractive rectifier network on top of the learned feature representations from a deep CNN architecture [34] which comprises 16 learnable weight layers (13 convolutional layers and 3 fully connected layers).|
|||The performance improvement of this work is due to the introduction of fractional max pooling to improve the quality of learnt CNN features, which can serve as a new baseline CNN architecture for the proposed CRN.|
||3 instances in total. (in iccv2015)|
|1152|Soft-Margin Mixture of Regressions|[35] adapted a CNN model for the sampling differences between training and testing data.|
|||Four observations can be made from Table 1: (1) For standard nonlinear regressions, divide-and-conquer outperforms universal non-linearity: see Hierarchical SVR (HSVR) [14]  LSVR [21] and KPLS[11]; (2) For CNNbased approach, divide-and-conquer high-layer objectives outperforms universal objectives: Ordinal objective (ORCNN [22]) Multi-scale objective (CPLF [33]  Universal nonlinear objective (DLA+KSVR [30]); (3) using divide-and-conquer techniques, standard nonlinear regression outperforms CNN without divide-and-conquer (HSVR  DLA+KSVR), and only after using the Ordinal objective (OR-CNN [22]) the CNN-based approach regained better results; (4) our SMMR approach produced the best result.|
|||Ordinal regression with multiple output cnn for age estimation.|
||3 instances in total. (in cvpr2017)|
|1153|Li_A_Two-Streamed_Network_ICCV_2017_paper|We propose a fastto-train two-streamed CNN that predicts depth and depth gradients, which are then fused together into an accurate and detailed depth map.|
|||As such, we think its better to represent a scene with both depth and depth gradients, and propose a fast-to-train two-streamed CNN to regress the depth and depth gradients (see Figure 2).|
|||Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
||3 instances in total. (in iccv2017)|
|1154|Heewon_Kim_Task-Aware_Image_Downscaling_ECCV_2018_paper|[32] showed that a different structural image prior is inherent in deep CNN architecture.|
|||[27]  4  H. Kim, M. Choi, B. Lim, and K. M. Lee  recently proposed a zero-shot SR (ZSSR) using deep learning, which trains an image-specific CNN with HR-LR pairs of patches extracted from the test image itself.|
|||Conversely, if  = , LSR is ignored, then and our framework becomes a downscaling CNN with ground truth downscaling method as bicubic downsampling.|
||3 instances in total. (in eccv2018)|
|1155|Zhao_Deeply-Learned_Part-Aligned_Representations_ICCV_2017_paper|The off-the-shelf CNN features, extracted from the model trained over ImageNet, without fine tuning, does not show the performance gain [33].|
|||Instead of only matching the images over the final representation, the matching map in the intermediate features is used to guide the feature extraction in the later layers through a gated CNN [43].|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in iccv2017)|
|1156|Xie_Genetic_CNN_ICCV_2017_paper|Hence, a CNN can be considered as a composite function, which is trained by back-propagating error signals defined by the difference between the supervision and prediction at the top layer.|
|||Designing powerful CNN structures is an intriguing problem.|
|||DisturbLabel: Regularizing CNN on the Loss Layer.|
||3 instances in total. (in iccv2017)|
|1157|Georgoulis_What_Is_Around_ICCV_2017_paper|Reflectance maps can be extracted from image collections [15], from a known class [34, 33], or using a CNN [35].|
|||Our CNN architecture (from left to right).|
|||This is an advantage of the proposed CNN architecture, as in the absence of information the fusion network learns how to perform sparse data interpolation for the missing pixels, as in [35].|
||3 instances in total. (in iccv2017)|
|1158|Zhou_Learning_Dense_Correspondence_CVPR_2016_paper|FlowNet [11] learns an optical flow CNN with a synthetic Flying Chairs dataset that generalizes well to existing benchmark datasets, yet still falls a bit short of state-of-the-art optical methods like DeepFlow [36] and EpicFlow [32].|
|||Learning dense correspondence  Given a set of training quartets {< s1, s2, r1, r2 >}, we  train the CNN to minimize the following objective: Lf low (cid:16)  Fs1,s2 , Fs1,r1 Fr1,r2 Fr2,s2(cid:17) ,  X  <s1,s2,r1,r2>  (1)  where  Fs1,s2 refers to the ground-truth flow between two synthetic views, Fs1,r1 , Fr1,r2 and Fr2,s2 are predictions made by the CNN along the transitive path.|
|||where  Ms1,s2 refers to the ground-truth matchability map between the two synthetic views, Ms1,r1 , Mr1,r2 and Mr2,s2 are CNN predictions along the transitive path, and Lmat denotes per-pixel cross-entropy loss.|
||3 instances in total. (in cvpr2016)|
|1159|Shuai_DAG-Recurrent_Neural_Networks_CVPR_2016_paper|[19] attachs the RGB raw data with the output of the Convolutional Neural Network (CNN) to produce the input for the same CNN in the next layer.|
|||In the patch-based CNN training, Farabet et al.|
|||Moreover, in comparison with state-of-the-art methods [3][14][26] [29], our CNN-65-DAG-RNN(8) outperforms theirs by a large margin (4.8% / 5.8%), demonstrating the profitability of adopting high-level features learned from CNN and context modeling with our DAG-RNNs.|
||3 instances in total. (in cvpr2016)|
|1160|Gordo_LEWIS_Latent_Embeddings_ICCV_2015_paper|Our CNN architecture replicates the one in Jaderberg et al.|
|||This is exactly the task for which our CNN is optimized.|
|||For this task we use a state-of-the-art dictionary CNN [11].|
||3 instances in total. (in iccv2015)|
|1161|Jialin_Wu_Dynamic_Sampling_Convolutional_ECCV_2018_paper|Meanwhile, properly enlarging receptive field is one of the most important concerns when designing CNN architectures.|
|||Specifically, we use two CNN sub-branches to generate the attention weights for samples and positions respectively.|
|||Given that directly generating the position-specific kernels W with shape same as conventional CNN will require the shape of the kernels to be (C Ck2, H, W ) as shown in Eq.|
||3 instances in total. (in eccv2018)|
|1162|Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper|Specifically, the objective is minimized when (i) the model orientation and acceleration is close to the IMU readings and (ii) the projected 3D joints of SMPL are close to 2D CNN detections [4] in the image.|
|||Black, B. Rosenhahn, G. Pons-Moll  4.2 Pose Candidate Assignment  Using the CNN method of Cao et al.|
|||Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation in the wild using improved cnn supervision.|
||3 instances in total. (in eccv2018)|
|1163|cvpr18-Dynamic Zoom-In Network for Fast Object Detection in Large Images|Applying state-of-the-art CNN detectors directly to those high resolution images requires a large amount of processing time.|
|||For example, one could partition an image into sub-images that satisfy memory constraints and apply the CNN to each sub-image.|
|||Improvement by CNN (Qnet*-CNN+Rnet vs. Qnet*FC+Rnet).|
||3 instances in total. (in cvpr2018)|
|1164|Shervin_Ardeshir_Integrating_Egocentric_Videos_ECCV_2018_paper|[22] uses a multi-channel CNN in a metric learning based approach.|
|||The matrix acquired by visual reasoning (in this case the supervised CNN based method) is shown in the middle (Rv).|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in eccv2018)|
|1165|cvpr18-Learning to Understand Image Blur|A standard way to deal with the challenge of variable scales is to re-scale the CNN for the same image and then aggregate the feature or score maps [14, 7], which significantly increases computation cost.|
|||Here we introduce the baselines: Baseline 1: Direct classification with CNN [34].|
|||The poor performance of Baseline 1: Direct CNN and Baseline 2: Bm implies the necessity to combine low-level blur  responses with high-level semantics for image blur categorization.|
||3 instances in total. (in cvpr2018)|
|1166|cvpr18-GroupCap  Group-Based Image Captioning With Structured Relevance and Diversity Constraints|The deep visual features of the given triplet images (the target (t), the positive (p), and the negative (n) images) are first extracted from a pre-trained CNN model, which are then sent to train a visual parsing tree (VP-Tree) model.|
|||In particular, we first employ a pre-trained CNN model to extract visual features from every given image.|
|||Given an image-caption pair, a deep visual feature G is first extracted from the last fullyconnected layer of a pretrained CNN [25].|
||3 instances in total. (in cvpr2018)|
|1167|Muhammed_Kocabas_MultiPoseNet_Fast_Multi-Person_ECCV_2018_paper|It is actually a ResNet [22] with two Feature Pyramid Networks (FPN) [34] (one for the keypoint subnet, the other for the person detection subnet) connected to it, FPN creates pyramidal feature maps with top-down connections from all levels of CNNs feature hierarchy to make use of inherent multi-scale representations of a CNN feature extractor.|
|||3) takes hierarchical CNN features (outputted by the corresponding FPN) and outputs keypoint and segmentation heatmaps.|
|||It takes hierarchical CNN features as input and outputs keypoint and segmentation heatmaps.|
||3 instances in total. (in eccv2018)|
|1168|cvpr18-Dynamic Feature Learning for Partial Face Recognition|With success of CNNs in face recognition, Some face networks such as VGGFace [26], Light CNN [39], FaceNet [32] and SphereFace [16] are proposed to further improve the performance of face recognition.|
|||For comparison, the Multi-Scale Region-based CNN (MR-CNN) [10], and two key-pointbased algorithms: MKDSRC-GTP [43] and I2C [11] are considered in this experiment.|
|||A lightened cnn for deep face representation.|
||3 instances in total. (in cvpr2018)|
|1169|Mao_Generation_and_Comprehension_CVPR_2016_paper|Model Architecture  Our baseline model is similar to other image captioning models that use a CNN to represent the image, followed by an LSTM to generate the text (see e.g., [40, 9, 53]).|
|||The main difference is that we augment the CNN representation of the whole image with a CNN representation of the region of interest, in addition to location information.|
|||In experiments, we only fine-tuned the weights for the last layer of the CNN and fixed all other layers.|
||3 instances in total. (in cvpr2016)|
|1170|Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper|We adopt the CNN model from [43].|
|||To do so, we train a CNN classifier that takes an input image and predicts which illumination condition the image was rendered with.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in eccv2018)|
|1171|Learning Detailed Face Reconstruction From a Single Image|The proposed architecture is composed of two main blocks, a network that recovers the coarse facial geometry (CoarseNet), followed by a CNN that refines the facial features of that geometry (FineNet).|
|||The usage of an end-to-end network here is exciting as it connects the problem of face reconstruction to the rapidly expanding applications solved by CNNs, potentially allowing us to further improve our results following new advances in CNN architectures.|
|||Recently, several notable pixelbased CNN architectures [12, 27, 14] were used for various fine grained tasks like semantic and instance segmentation [27, 14], optical flow [9], and human pose estimation [43].|
||3 instances in total. (in cvpr2017)|
|1172|Demirel_Attributes2Classname_A_Discriminative_ICCV_2017_paper|[22] propose to combine MLP and CNN networks handling text based information acquired from Wikipedia articles and visual information of images, respectively.|
|||[14] propose a similar model where a pre-trained CNN model is fine-tuned in an end-toend way to relate images with semantic class embeddings.|
|||[25] proposes to use convex combinations of fixed class name embeddings, weighted by class posterior probabilities given by a pre-trained CNN model, to map images to the class name embedding space.|
||3 instances in total. (in iccv2017)|
|1173|Vassileios_Balntas_RelocNet_Continous_Metric_ECCV_2018_paper|  Joint learning of feature descriptors and poses with a Siamese network As previously discussed, one of the main issues with the recent work on CNN relocalisers is the need to use the global world coordinate system as a training label.|
|||4.3 Pose Regression Experiments  In Table 2 we show the results of the proposed pose regression method, compared to several state-of-the-art CNN based methods for relocalisation.|
|||Crafting a Multi-task CNN for Viewpoint Estimation.|
||3 instances in total. (in eccv2018)|
|1174|Li_Learning_to_Disambiguate_ICCV_2017_paper|We gather images from the Visual Genome dataset [23] and select image pairs as those that possess the same category label and high CNN feature similarity.|
|||In the typical CNN-LSTM language generation framework, CNN features f are first extracted from an input image.|
|||Specif 3424  ically, we adopt Inception-ResNet [42] as the CNN part, followed by two stacked 512-d LSTMs.|
||3 instances in total. (in iccv2017)|
|1175|Lee_VPGNet_Vanishing_Point_ICCV_2017_paper|Recently, a few CNN based approaches have been developed to tackle the problem in an end-to-end fashion including learning-based algorithms.|
|||[21] uses both a CNN and a Recurrent Neural Network (RNN) to detect lane boundaries.|
|||Borji [4] has shown that a CNN can localize the VP.|
||3 instances in total. (in iccv2017)|
|1176|Gurari_Pull_the_Plug_CVPR_2016_paper|Given the recent rise of CNN features as standard baselines for learning, we also examine the value of a CNN baseline for making predictions.|
|||Comparison of our model with CPMC [10] and CNN features [25] for predicting the Jaccard score indicating the quality of a foreground segmentation.|
|||We observe that the off-the-shelf CNN feature yields negligible predictive power (Table 2, row 2).|
||3 instances in total. (in cvpr2016)|
|1177|Liu_Video_Frame_Synthesis_ICCV_2017_paper|We therefore use existing videos to train a CNN in an unsupervised fashion.|
|||A related unsupervised approach [19] uses a CNN to predict optical flow by synthesizing interpolated frames, and then inverting the CNN.|
|||To synthesize frames using voxel flow, DVF has to encode both appearance and motion information, which implicitly mimics a two-stream CNN [26].|
||3 instances in total. (in iccv2017)|
|1178|Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network|1.1.2 Design of convolutional neural networks  The state of the art for many computer vision problems is meanwhile set by specifically designed CNN architectures following the success of the work by Krizhevsky et al.|
|||[33] formulate a recursive CNN and present state-of-theart results.|
|||To achieve this, we train a generator network as a feed-forward CNN GG parametrized by G.|
||3 instances in total. (in cvpr2017)|
|1179|Chi_Li_A_Unified_Framework_ECCV_2018_paper|1: Illustration of different learning architectures for single-view object pose estimation: (a) each object is trained on an independent network; (b) each object is associated with one output branch of a common CNN root; and (c) our network with single output stream via class prior fusion.|
|||In summary, we make following contributions to scalable and accurate pose estima tion on multiple classes and multiple views:   We develop a multi-class CNN architecture for accurate pose estimation with three novel features: a) a single pose prediction branch which is coupled with a discriminative pose representation in SE(3) and is shared by multiple classes; b) a method to embed object class labels into the learning process by concatenating a tiled class map with convolutional layers; and c) deep supervision with an object mask which improves the generalization from synthetic data to real images.|
|||Massa, F., Marlet, R., Aubry, M.: Crafting a multi-task cnn for viewpoint estimation.|
||3 instances in total. (in eccv2018)|
|1180|cvpr18-Appearance-and-Relation Networks for Video Classification|proposed a new Two-Stream Inflated 3D CNNs based on 2D CNN inflation, which allows for pre-training with ImageNet models.|
|||Two stream CNN is a strong baseline for action recognition and its input has two modalities, i.e., RGB and Optical Flow.|
|||Finally, we compare with the recent state-of-theart methods, namely temporal segment network (TSN) [49] and Inflated 3D CNN (I3D) [2].|
||3 instances in total. (in cvpr2018)|
|1181|The Impact of Typicality for Informative Representative Selection|the dimension of CNN feature, Nf = 4096).|
|||In scene classification and object recognition, we consider the CNN features from image and region proposals [15] respectively.|
|||For CNN feature, Nf discussed in Sec.|
||3 instances in total. (in cvpr2017)|
|1182|Carl_Toft_Semantic_Match_Consistency_ECCV_2018_paper|Instead of explicitly representing a scene by a database of images, another approach is to implicitly model a scene by a CNN trained for pose regression [21, 22, 51] or place classification [52].|
|||Such methods implicitly represent the 3D scene structure via a random forest or CNN that predicts a 3D scene coordinate for a given image patch [33].|
|||Arandjelovi c, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: NetVLAD: CNN ar chitecture for weakly supervised place recognition.|
||3 instances in total. (in eccv2018)|
|1183|cvpr18-Single Image Dehazing via Conditional Generative Adversarial Network|Motivated by the success of the CNN in object detection, recognition and related tasks [8, 13, 34], CNN has been applied in image dehazing [3, 24].|
|||[24] and Cai et al [3] use CNN to estimate transmission maps, which overcome the limitations of [11] to some extent.|
|||[24] use a CNN to estimate the transmission map and then use the conventional method to recover clear images.|
||3 instances in total. (in cvpr2018)|
|1184|Tokmakov_Learning_Video_Object_ICCV_2017_paper|With these spatio-temporal CNN features in hand, we train the convolutional GRU component of the framework to learn a visual memory representation of object(s) in the scene.|
|||Furthermore, these two CNN architectures are primarily developed for images, and do not model temporal information in video.|
|||For the temporal stream we employ MPNet [43], a CNN pretrained for the motion segmentation task.|
||3 instances in total. (in iccv2017)|
|1185|Junwu_Weng_Deformable_Pose_Traversal_ECCV_2018_paper|Meanwhile, each convolution is independent, and CNN is suitable to model the spatial dependency of pixels in parallel rather than sequentially.|
|||Then a two-dimensional CNN is applied on the generated images to extract sequence-level CNN feature, by which the spatio-temporal dependency of joints is implicitly learned.|
|||Compared with the 2D CNN methods, we apply one-dimensional convolution on pose traversal data to extract frame-level pose feature, which is more flexible especially for tasks requiring frame-level feature.|
||3 instances in total. (in eccv2018)|
|1186|Detecting Oriented Text in Natural Images by Linking Segments|Our method is similar in spirit to a recent work [22], which detects text lines by finding and grouping a sequence of fine-scale text proposals through a CNN coupled with recurrent neural layers.|
|||Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness.|
|||Most state-of-the-art detection systems either classify some class-agnostic object proposals with CNN [5, 4, 19] or directly regress object bounding boxes from a set of preset boxes (e.g.|
||3 instances in total. (in cvpr2017)|
|1187|Perceptual Generative Adversarial Networks for Small Object Detection|Large and small objects exhibit different representations from high-level convolutional layers of a CNN detector.|
|||[29] proposed a deformation hidden layer for CNN to model mixture poses information, which can further benefit the pedestrian detection task.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||3 instances in total. (in cvpr2017)|
|1188|cvpr18-Relation Networks for Object Detection|It verifies the efficacy of modeling object relations in CNN based detection.|
|||The modern CNN based methods mostly have a simple regular network structure [25, 23].|
|||In principle, our approach is fundamentally different from and would complement most (if not all) CNN based object detection methods.|
||3 instances in total. (in cvpr2018)|
|1189|cvpr18-3D Object Detection With Latent Support Surfaces|Other work utilizes pretrained 2D detectors or region proposals as priors, and localizes 3D bounding boxes via a separate CNN [40, 8, 23].|
|||Comparing with other algorithms that use CNN features [40, 23] pretrained on external datasets, the performance of our algorithm is comparable even without the cascaded prediction step.|
|||Some previous work was specifically designed to solve this issue [8, 23] by using CNN features in RGB images, and we believe incorporating a similar approach in our cascaded prediction framework might also help resolve this failure case.|
||3 instances in total. (in cvpr2018)|
|1190|Babenko_Aggregating_Local_Deep_ICCV_2015_paper|Simultaneously, in [7] an even more performant desriptors were suggested based on extracting different fragments of the image, passing them through a CNN and then using VLAD-embedding [9] to aggregate the activations of a fully-connected layer.|
|||We extract deep convolutional features using the very deep CNN trained by Simonyan and Zisserman [24].|
|||Similar amount of improvement can be obtained by fine-tuning the original CNN on a specially collected dataset (in the same vein to [2]).|
||3 instances in total. (in iccv2015)|
|1191|Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper|Further work has focused on processing all regions with only single forward pass of the CNN [17, 13], and on eliminating explicit region proposal methods by directly predicting the bounding boxes either in the image coordinate system [46, 9], or in a fully convolutional [31] and hence position-invariant settings [40, 38, 37].|
|||We initialize the CNN with weights pretrained on ImageNet [39] and all other weights from a gaussian with standard deviation of 0.01.|
|||We begin fine-tuning the layers of the CNN after 1 epoch, and for efficiency we do not fine-tune the first four convolutional layers of the network.|
||3 instances in total. (in cvpr2016)|
|1192|Zagoruyko_Learning_to_Compare_2015_CVPR_paper|trained CNN features (these were l2-normalized to improve results).|
|||Here we also test the CNN network siam-SPP-l2, which is an SPP-based siamese architecture (note that siam-SPP is same as siam but with the addition of two SPP layers see also Fig.|
|||Conclusions  In this paper we showed how to learn directly from raw image pixels a general similarity function for patches, which is encoded in the form of a CNN model.|
||3 instances in total. (in cvpr2015)|
|1193|Shen_Learning_Deep_Neural_ICCV_2017_paper|[3] trained CNN with triplet samples and minimized feature distances between the same person and maximize the distances between different people.|
|||Each image is associated with three types of information, i.e., visual appearance, the timestamp, and the geo 1901  Siamese CNN for   Pairwise Similarity  Visual-spatio-temporal Proposal  LSTM  Candidate Spatial Paths  Path Proposal Generation  Classification  with Path Regularization  Figure 2: Illustration of the overall framework.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in iccv2017)|
|1194|cvpr18-Efficient Video Object Segmentation via Network Modulation|To model the temporal motion information, some works heavily rely on optical flow [34] [4], and use CNNs to learn mask refinement of an object from current frame to the next one [23], or combine the training of CNN with bilateral filtering between adjacent frames [18].|
|||[4] use a CNN to jointly estimate the optical flow and provide the learned motion representation to generate motion consistent segmentation across time.|
|||The visual modulator network is a CNN that takes the annotated visual object image as input and produces a vector of scale parameters for all modulation layers, while the spatial modulator network is a very efficient network that produces bias parameters based on the spatial prior input.|
||3 instances in total. (in cvpr2018)|
|1195|Missing Modalities Imputation via Cascaded Residual Autoencoder|Deep Residual Network Our proposed CRA is inspired by recent achievement in deep CNN for object recognition.|
|||Motivated by the CNN learning, we adopt the commonly used back-propagation scheme to minimize this loss, which relies on the recursively computed derivatives of L w.r.t.|
|||Features are extracted using a simple CNN (three conv and two fc layers).|
||3 instances in total. (in cvpr2017)|
|1196|Zhenyu_Zhang_Joint_Task-Recursive_Learning_ECCV_2018_paper|[5, 24] proposed a multi-stage CNN to resolve the monocular depth prediction.|
|||For examples, the  4  Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, J. Yang  literatures [21, 22] utilized CNN with hierarchical CRFs and multi-decoder to obtain depth estimation and semantic segmentation.|
|||Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: Cnn features off-the-shelf:  An astounding baseline for recognition.|
||3 instances in total. (in eccv2018)|
|1197|cvpr18-Parallel Attention  A Unified Framework for Visual Object Discovery Through Dialogs and Queries|Previous works [6, 43, 44] have primarily appled a CNN-RNN pipeline that uses a CNN to encode the image content and an LSTM to encode the expression.|
|||Object proposal features For each candidate object proposal Oi, the corresponding feature is composed of two parts, the first is the CNN feature ui extracted as described for the global visual features, the Conv5 3 feature from the VGG-16 for each object proposal.|
|||An empirical study of language cnn for image captioning.|
||3 instances in total. (in cvpr2018)|
|1198|Tung_Adversarial_Inverse_Graphics_ICCV_2017_paper|[11] contributed a deep learning library with many geometric operations including a differentiable camera projection layer, similar to those used in SfM networks [21, 33], 3D image interpreters [29], and depth from stereo CNN [9].|
|||Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
|||Monocap: Monocular human motion capture using a CNN coupled with a geometric prior.|
||3 instances in total. (in iccv2017)|
|1199|Weiwei_Shi_Transductive_Semi-Supervised_Deep_ECCV_2018_paper| is part of the CNN back 3.5 TSSDL Mean-Teacher (TSSDL-MT) Method  In our experiment, we also implemented a TSSDL variant that is the combination of TSSDL and the Mean Teacher methods (TSSDL-MT in short) [26].|
|||Shi, W., Gong, Y., Wang, J.: Improving cnn performance with min-max objective.|
|||Shi, W., Gong, Y., Tao, X., Wang, J., Zheng, N.:  Improving cnn performance accuracies with min-max objective.|
||3 instances in total. (in eccv2018)|
|1200|Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper|, xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such  4194  as GoogLeNet [18], VGG-16 [24], or C3D [27].|
|||The sequence of image features, X  = x L, is obtained by extracting the activation vector of a fully-connected layer of the CNN for each input image.1 The sequence of feature vectors is then fed to the LSTM encoder, and the hidden state of the LSTM is given by  2, .|
|||If we use the CNN features directly, then we assume ht = x t. The attention mechanism is realized by using attention weights to the hidden activation vectors throughout the in 4195  put sequence.|
||3 instances in total. (in iccv2017)|
|1201|Dong_Domain-Size_Pooling_in_2015_CVPR_paper|The fact that DSP-SIFT outperforms CNN in nearly all cases in Fischers dataset is surprising, considering that the neural network is trained by augmenting the dataset using similar types of transformations.|
|||In general, the performance of CNN descriptors is worse than DSP-SIFT but, interestingly, their mAPs do not change significantly if the network responses are computed on a resampled patch of size 69  69 to obtain lower dimensional descriptors.|
|||We also report mAP for CNN descriptors computed on 69  69 patches as in [15].|
||3 instances in total. (in cvpr2015)|
|1202|Yidan_Zhou_HBE_Hand_Branch_ECCV_2018_paper|[8, 17] applied a 3D CNN instead of 2D CNN to estimate per-voxel likelihood of 3D locations for each hand joint.|
|||They implemented an intricate 3D CNN in a voxel-to-voxel mapping manner for prediction.|
|||Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns.|
||3 instances in total. (in eccv2018)|
|1203|cvpr18-Finding Tiny Faces in the Wild With Generative Adversarial Network|As shown in Table 1 and Figure 2, we adopt a deep CNN architecture which has shown effectiveness for image super-resolution in [17].|
|||In contrast to their network, our generator network includes refinement sub-network which is also a deep CNN architecture.|
|||CMSRCNN: Contextual Multi-Scale Region-Based CNN for Unconstrained Face Detection, pages 5779.|
||3 instances in total. (in cvpr2018)|
|1204|Deep MANTA_ A Coarse-To-Fine Many-Task Network for Joint 2D and 3D Vehicle Analysis From Monocular Image|In these approaches, 3D object proposals are projected in 2D bounding boxes and given to a CNN based detector which jointly predicts the class of the object proposal and the object fine orientation (using angle regression).|
|||It is based on the Many-task CNN (Deep MANTA) which proposes accurate 2D vehicle bounding boxes using multiple refinement steps.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||3 instances in total. (in cvpr2017)|
|1205|FastMask_ Segment Multi-Scale Object Candidates in One Shot|We summarize our main contributions as follows:   First, we learn a novel weight-shared residual neck module to build a feature pyramid of CNN while preserving a well-calibrated feature semantics, for efficient multi-scale training and inference.|
|||More recently, DeepBox [17] trains a CNN to re-rank the proposals generated by EdgeBox, while MultiBox [7] generates the  proposals from convolutional feature maps directly.|
|||Both max pooling and average pooling are widely used components in modern CNN architectures on recognition and detection.|
||3 instances in total. (in cvpr2017)|
|1206|Harandi_Beyond_Gauss_Image-Set_ICCV_2015_paper|Therefore, we used the last layer of the CNN trained in [46] as frame descriptors.|
|||We then reduced the dimensionality of the CNN output to 400 using PCA.|
|||While this may be attributed in part to the CNN features, note that our approach still outperforms GDA and CDL based on the same features.|
||3 instances in total. (in iccv2015)|
|1207|Deeply Supervised Salient Object Detection With Short Connections|Compared with traditional methods that use hand-crafted features, CNN based methods have refreshed all the previous state-of-the-art records in nearly every sub-field of computer vision, including salient object detection.|
|||[29] proposed to use multi-scale features extracted from a deep CNN to derive a saliency map.|
|||Though significant progress have been achieved by these developments in the last two years, there is still a large room for improvement over the generic CNN models that do not explicitly deal with the scale-space problem.|
||3 instances in total. (in cvpr2017)|
|1208|cvpr18-Deep Semantic Face Deblurring|We exploit the semantic information of face within an end-to-end deep CNN for face image deblurring.|
|||We make the following contributions in this work:  We propose a deep multi-scale CNN that exploits global semantic priors and local structural constraints for face image deblurring.|
|||In contrast, we train an end-to-end deep CNN to bypass the blur kernel estimation step and do not use any reference images or manual annotations when deblurring an image.|
||3 instances in total. (in cvpr2018)|
|1209|cvpr18-Logo Synthesis and Manipulation With Clustered Generative Adversarial Networks|Clustered GAN Training  We propose a method for stabilizing GAN training and gaining additional control over the generator output by means of clustering (a) in the latent space of an autoencoder trained on the same data or (b) in the CNN feature space of a ResNet classifier trained on ImageNet.|
|||We performed clustering in the latent space of an Autoencoder or in the CNN features space of a ResNet classifier and conditioned DCGAN and improved WGAN utilizing either an Auxiliary Classifier or Layer Conditional model.|
|||(iii) We quantitatively validated our clustered GAN approaches on a CIFAR-10 and ImageNet, showcasing the benefits of meaningful synthetic labels obtained through clustering in the CNN feature space of a ResNet classifier.|
||3 instances in total. (in cvpr2018)|
|1210|cvpr18-W2F  A Weakly-Supervised to Fully-Supervised Framework for Object Detection|regression ability) of fully-supervised deep CNN models and the availability of large scale labeled datasets [30, 25], which include tight bounding box annotations.|
|||[2] present a two-stream CNN weakly supervised deep detection network (WSDDN), which selects the positive samples by multiplying the score of recognition and detection.|
|||In [18], relative improvement of output CNN scores are used instead of relying on the static absolute CNN score at training iterations.|
||3 instances in total. (in cvpr2018)|
|1211|FlowNet 2.0_ Evolution of Optical Flow Estimation With Deep Networks|Following FlowNet, several papers have studied optical flow estimation with CNNs: featuring a 3D CNN [30], unsupervised learning [1, 33], carefully designed rotationally invariant architectures [28], or a pyramidal approach based on the coarse-to-fine idea of variational methods [20].|
|||[29] formulate Deep Matching [31] as a CNN and optimize it end-to-end.|
|||In some cases, this refinement can be approximated by neural networks: Chen & Pock [9] formulate their reaction diffusion model as a CNN and apply it to image denoising, deblocking and superresolution.|
||3 instances in total. (in cvpr2017)|
|1212|Ying_Zhang_Deep_Cross-Modal_Projection_ECCV_2018_paper|The overall objective function is formulated as  L = Lcmpm + Lcmpc  (11)  At the test stage, given an image and text, we first extract the image feature x and text feature z with the visual CNN and Bi-LSTM network, respectively.|
|||Comparison of image-to-text (R@K(%)) and text-to-image (AP@K(%)) retrieval results on the CUB and Flowers dataset  Method  Image-to-Text Text-to-Image Image-to-Text Text-to-Image  CUB  Flowers  Bow [6]  Word2Vec [25] Word CNN [28]  Word CNN-RNN [28] GMM+HGLMM [14]  Triplet [15]  Latent Co-attention [15]  CMPM  CMPM+CMPC  R@1 44.1 38.6 51.0 56.8 36.5 52.5 61.5 62.1 64.3  AP@50  39.6 33.5 43.3 48.7 35.6 52.4 57.6 64.6 67.9  R@1 57.7 54.2 60.7 65.6 54.8 64.3 68.4 66.1 68.9  AP@50  57.3 52.1 56.3 59.6 52.8 64.9 70.1 67.7 69.7  5.1 Analysis of Cross-Modal Matching  Table 5 compares the proposed CMPM loss with the commonly used bi-directional ranking (Bi-rank) loss [39, 40, 21], the most similar N-pair loss [30], and Histogram Loss [34] with different batch size on the CUHK-PEDES dataset.|
|||We can also observe that the radius of image feature areas is smaller than text features, which indicates the scalar gap brought by different networks (i.e., the CNN network for image and Bi-LSTM for text).|
||3 instances in total. (in eccv2018)|
|1213|Hartmann_Learned_Multi-Patch_Similarity_ICCV_2017_paper|Conceptually, it is straight-forward to design a CNN for multi-view similarity.|
|||An extensive study of similarity measures based on different CNN architectures is presented in [26].|
|||We did not run the full experiment with LIFT, due to the excessive runtime of pixelwise CNN prediction without a fully convolutional architecture.|
||3 instances in total. (in iccv2017)|
|1214|He_Channel_Pruning_for_ICCV_2017_paper|Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction.|
|||Introduction  Recent CNN acceleration works fall into three categories: optimized implementation (e.g., FFT [47]), quantization (e.g., BinaryNet [8]), and structured simplification that convert a CNN into compact one [22].|
|||Specifically, given a trained CNN model, pruning each layer is achieved by minimizing reconstruction error on its output feature maps, as showned in Fig.|
||3 instances in total. (in iccv2017)|
|1215|Asynchronous Temporal Fields for Action Recognition|In this work, we focus on fully-connected random fields, that have been popular in image segmentation [18], where image filtering was used for efficient message passing, and later extended to use CNN potentials [39].|
|||The frame-intent potentials are predicted with a CNN from video frames (pixels and motion).|
|||These gradients are used to update the underlying CNN model.|
||3 instances in total. (in cvpr2017)|
|1216|Learning to Detect Salient Objects With Image-Level Supervision|Weakly Supervised Saliency Detection  A CNN for image-level tags prediction typically consists of a series of convolutional layers followed by several fully connected layers.|
|||The CNN takes image X as input and predicts a C-dimensional score vector y.|
|||Training the CNN involves minimizing some loss function L(l, y), which measures the accuracy of the predicted scores based on the ground truth label set.|
||3 instances in total. (in cvpr2017)|
|1217|Li_Situation_Recognition_With_ICCV_2017_paper|[9] designed a special hash function such that a CNN can be used on the original graphs.|
|||The first CNN (v(i)) is trained to predict verbs, and second CNN (n(i)) predicts the top K most frequent nouns (K = 2000 cover about 95% of nouns) in the dataset.|
|||Our CNN predicts the verb well.|
||3 instances in total. (in iccv2017)|
|1218|Tavakoli_Paying_Attention_to_ICCV_2017_paper|We boost the CNN features before feeding them to the language model with the saliency map of the image.|
|||The CNN feature maps and saliency maps are of size 7  7.|
|||We apply a 3  3 moving  2492  Deep CNN Feature Extraction  (VGG 16)  77512  Boost & Linearize  14608  LSTM Language Model  Caption  Saliency   Prediction  Figure 6.|
||3 instances in total. (in iccv2017)|
|1219|Removing Rain From Single Images via a Deep Detail Network|The deep CNN has not only achieved success on high-level vision tasks [12, 13] but has also been extended to problems such as image denoising [34, 36], super-resolution [7, 19], reducing artifacts of compression [8], image inpainting [26] and image dehazing [27].|
|||Intuitively, a goal may be to directly train a deep CNN architecture h(X) on multiple images to minimize the objective function  L = Pi kh(Xi)  Yik2 F ,  (1)  where F is the Frobenius norm.|
|||In Figure 3(c) we see that, even though this image is used as a training sample, the de-rained image output by a CNN has a severe color shift.|
||3 instances in total. (in cvpr2017)|
|1220|Perina_Summarization_and_Classification_ICCV_2017_paper|Particularly, one or multiple layers of the CNN activations may serve as highly non-linear feature extractors, and new classifiers for new data can be trained on these features [27].|
|||In this embedding space, similar images, even when the CNN may assign them to different classes, tend to map to nearby locations.|
|||In particular, it used two network architectures a CNN and a shallow network, that tries to manage domain shift between source and new target data in an online manner.|
||3 instances in total. (in iccv2017)|
|1221|Hadfield_Exploiting_High_Level_ICCV_2015_paper|For compact r i = CNN (Ir (xr  i ))  and t  j = CNN(cid:0)It(cid:0)xt j(cid:1)(cid:1) .|
|||The threshold  for CNN matching during triangulation was set to 0.5.|
|||In addition it may prove interesting to extend the CNN component to estimating larger parts of the representation, rather than only the triangulation cues.|
||3 instances in total. (in iccv2015)|
|1222|Shou_Temporal_Action_Localization_CVPR_2016_paper|Training procedure  The proposal network: We train a CNN network pro as the background segment filter.|
|||[17] used FV encoding of iDT with weighted saliency based pooling, and conducted late fusion with frame-level CNN features.|
|||[41] built a system on iDT with FV representation and frame-level CNN features, and performed postprocessing to refine the detection results.|
||3 instances in total. (in cvpr2016)|
|1223|Jin-Seok_Park_Double_JPEG_Detection_ECCV_2018_paper|4.1 Architecture  The proposed CNN takes histogram features and quantization tables as inputs.|
|||Histogram features: Since JPEG compression changes the statistical properties of each block rather than the semantic information of the entire image, DCT coefficient statistical characteristics were employed rather than the RGB image as CNN input [33].|
|||To overcome this limits, we have created a new dataset using JPEG quantization tables from actual forensic images and designed a novel deep CNN for double JPEG detection using statistical histogram features from each block with a vectorized quantization table.|
||3 instances in total. (in eccv2018)|
|1224|cvpr18-Deep Lesion Graphs in the Wild  Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-Scale Lesion Database|The CNN in each method was trained 5 times using different random seeds.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
|||Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning.|
||3 instances in total. (in cvpr2018)|
|1225|Park_Attributed_Grammars_for_ICCV_2015_paper|They used pre-trained HOG based Poselet approach for part detection and trained classifier with shallow CNN for each Poselet.|
|||[5, 24, 31, 35, 36] show significant improvement compared to previous methods by training keypoint specific part detectors based on the CNN framework for human body pose estimation.|
|||For CNN-based feature extraction, our model generalizes RCNN method[18] by applying it to part and its type with pre-trained ImageNet weights of CNN which is publicly available.|
||3 instances in total. (in iccv2015)|
|1226|Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Learning by tracking: Siamese cnn for robust target association.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||3 instances in total. (in iccv2017)|
|1227|Dapeng_Chen_Improving_Deep_Visual_ECCV_2018_paper|Recent methods mainly benefit from the advances of CNN architectures [26], which combine the above two aspects to produce robust and ID-discriminative image representation [1, 8, 28, 46, 50, 52].|
|||The baseline is just the visual CNN that produces the feature map (I), indicated by the red lines in Fig.|
|||With the proposed schemes, it can achieve the superior performance with the standard CNN architecture.|
||3 instances in total. (in eccv2018)|
|1228|Image-To-Image Translation With Conditional Adversarial Networks|In other words, we still have to tell the CNN what we wish it to minimize.|
|||If we take a naive approach, and ask the CNN to minimize Euclidean distance between predicted and ground truth pix 11125  els, it will tend to produce blurry results [40, 58].|
|||Coming up with loss functions that force the CNN to do what we really want  e.g., output sharp, realistic images  is an open problem and generally requires expert knowledge.|
||3 instances in total. (in cvpr2017)|
|1229|cvpr18-Single-Shot Refinement Neural Network for Object Detection|Object detection via a multiIn  region and semantic segmentation-aware CNN model.|
|||ME R-CNN: multi-expert  region-based CNN for object detection.|
|||Gated In ECCV, pages  bi-directional CNN for object detection.|
||3 instances in total. (in cvpr2018)|
|1230|Generalized Semantic Preserving Hashing for N-Label Cross-Modal Retrieval|We follow the same procedure as in [15] for extracting the CNN features.|
|||Unfortunately we could not replicate the results of CNN features for the Pascal dataset [5] as in [15] and so do not report those results here.|
|||The best results for the LabelMe dataset [18] are obtained when CNN [10] features was used.|
||3 instances in total. (in cvpr2017)|
|1231|Xuelin_Qian_Pose-Normalized_Image_Generation_ECCV_2018_paper|Cheng, D., Gong, Y., Zhou, S., JinjunWang, Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Zheng, Z., Zheng, L., , Yang, Y.: A discriminatively learned cnn embedding for  person re-identification.|
||3 instances in total. (in eccv2018)|
|1232|Bilen_Weakly_Supervised_Object_2015_CVPR_paper|We represent each selective search window region with a 4096 dimensional fc7 ReLU layer output of the CNN model that is provided by Donahue et al.|
|||Finally, the average training times for LSVM, SLSVM and our method are approximately 1, 1 and 2 hours respectively on a 16 core i-7 CPU, after the CNN features and the pairwise distances are pre-computed.|
|||Moreover, [30] relies on multiple appearance models and on a expensive super-vector encoding of the CNN features that significantly increases the dimensionality of the feature vector, whereas our method uses a single appearance model and does not bring any additional computational load during inference compared to the standard LSVM.|
||3 instances in total. (in cvpr2015)|
|1233|cvpr18-Generative Adversarial Learning Towards Fast Weakly Supervised Detection|[8] proposed a two-stream end-to-end CNN architecture.|
|||[28] further proposed to add a contrast-based contextual stream to form a three-stream CNN architecture.|
|||[27] also learned a multi-label classification at the first step, followed by online supportive sample harvesting (augmented with a relative CNN score improvement metric) to detect object proposals to learn Fast RCNN [17].|
||3 instances in total. (in cvpr2018)|
|1234|cvpr18-Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering|Typically, attention models operate on CNN features corresponding to a uniform grid of equally-sized image regions (left).|
|||In each case, the baseline (ResNet), uses a ResNet [13] CNN pretrained on ImageNet [34] to encode each image in place of the bottom-up attention mechanism.|
|||Deeper lstm and normalized cnn visual question answering model.|
||3 instances in total. (in cvpr2018)|
|1235|Collaborative Deep Reinforcement Learning for Joint Object Search|Acceleration were made by sharing computation and pooling over the feature maps from the CNN layers [7, 10].|
|||proposed AttentionNet where at each current window, a CNN was trained to predict quantized weak directions for the next step to simulate a gradual attention shift.|
|||o is a feature vector of the observed region (plus some extra margin for context) extracted from a CNN layer, and h is a fixed-size vector of the action history.|
||3 instances in total. (in cvpr2017)|
|1236|cvpr18-A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation|4, compared to deep CNN model, our HGSM is a shallow model with much fewer parameters (the deep c-BiGAN only provides intermediate eye shape), thus it cannot perform as well as CNN on within-dataset experiments.|
|||The reason is that CNN tends to over-fit on the dataset with large amount of parameters.|
|||However, CNN cannot generalize well on cross-dataset experiments, while HGSM encodes universal eye model knowledge and can generalize better than CNN for cross-dataset evaluation.|
||3 instances in total. (in cvpr2018)|
|1237|Lin_Learning_Compact_Binary_CVPR_2016_paper|Their success is attributed to training a deep CNN to learn rich midlevel image representations on millions of images.|
|||[21] take deep CNN to learn a set of hash functions, but they require pair-wised similarity labels or triplets training data.|
|||SSDH [49] constructs hash functions as a latent layer in the deep CNN and achieves state-of-the-art image retrieval performance, but their method belongs to supervised learning.|
||3 instances in total. (in cvpr2016)|
|1238|cvpr18-Domain Adaptive Faster R-CNN for Object Detection in the Wild|The recent trend of using synthetic data for training deep CNN models presents a similar challenge due to the visual mismatch with reality.|
|||Ganin and Lempitsky [15] implemented a gradient reverse layer (GRL), and integrated it into a CNN for image classification in the unsupervised domain adaptation scenario.|
|||Object detection via a multiIn  region and semantic segmentation-aware CNN model.|
||3 instances in total. (in cvpr2018)|
|1239|Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper|However, supervised training CNN for such tasks often involves in constructing large-scale, diverse datasets with dense pixelwise ground truth labels.|
|||We compare our flow network with conventional variational algorithms, supervised CNN methods, and several unsupervised CNN models on the KITTI flow 2012 and 2015 datasets.|
|||Garg, R., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation:  Geometry to the rescue.|
||3 instances in total. (in eccv2018)|
|1240|Fabio_Tosi_Beyond_local_reasoning_ECCV_2018_paper|This strategy was extended in [6] by feeding to the CNN also the input reference image with promising results deploying, however, a larger amount of training samples.|
|||The very first attempt to use deep learning in stereo matching was proposed in the seminal work of Zbontar and LeCun [27] aimed at inferring matching cost through a CNN by processing images patches.|
|||To this aim we propose to combine with a CNN cues inferred with two complementary strategies, based on two very different receptive  14  F. Tosi, M. Poggi, A. Benincasa, S. Mattoccia  Fig.|
||3 instances in total. (in eccv2018)|
|1241|Zuxuan_Wu_DCAN_Dual_Channel-wise_ECCV_2018_paper|Exploring statistics in each channel of CNN feature maps, our framework performs channel-wise feature alignment, which preserves spatial structures and semantic information, in both an image generator and a segmentation network.|
|||Instance normalization is motivated by the fact that mean and standard deviation in each channel of CNN feature maps contain the style information of an image, and hence they are used to translate feature maps of a source image into a normalized version based on a reference image for each channel.|
|||3.1 Channel-wise Feature Alignment  The mean and standard deviation of each channel in CNN feature maps have been shown to capture the style information of an image [16,15,17], and hence channel-wise alignment of feature maps is adopted for fast style transfer with a simple instance normalization step.|
||3 instances in total. (in eccv2018)|
|1242|Yumin_Suh_Part-Aligned_Bilinear_Representations_ECCV_2018_paper|Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel  parts-based cnn with improved triplet loss function.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recogni tion.|
|||Zheng, Z., Zheng, L., Yang, Y.: A discriminatively learned cnn embedding for person re identification.|
||3 instances in total. (in eccv2018)|
|1243|Funk_Beyond_Planar_Symmetry_ICCV_2017_paper|Our contributions are:  to build the first deep, dense, and multiple symmetry detector that mimics sophisticated human symmetry perception beyond planar symmetries;   to convert sparse symmetry labels into dense heatmaps  to facilitate CNN training using human labels;   to systematically and extensively validate and compare the predictive power of the trained CNN against existing algorithms on both mathematically well-defined and human perceived symmetries.|
|||[59] use a deep CNN to learn symmetry at multiple scales and fuse the final output together.|
|||Dense CNN Regression  Fully Convolutional Networks [45], with CNN regressing to 2D ground-truth, have been utilized for semantic segmentation [6, 45] and pose detection [4, 53, 74].|
||3 instances in total. (in iccv2017)|
|1244|Wang_Supervised_Quantization_for_CVPR_2016_paper|The reason might be the powerful discrimination ability of the original CNN features.|
|||To achieve a comprehensive analysis, we provide the Euclidean baseline (see Figure 3) that simply computes the distances between the query and the database vectors using the original CNN features and returns the top R retrieved items.|
|||This shows that our approach is able to learn better quantizer through the supervision though it is known that the CNN features are already good.|
||3 instances in total. (in cvpr2016)|
|1245|Simo-Serra_Discriminative_Learning_of_ICCV_2015_paper|For this we use a Siamese network architecture [2] that employs two CNNs with identical parameters to compare pairs of patches; treating the CNN outputs as patch descriptors, we minimize a loss that enforces the L2 norm of their difference to be small for corresponding patches and large otherwise.|
|||One key distinction between [10, 33] and our work is that we aim at using the CNN outputs of our Siamese networks as direct counterparts to traditional descriptorsnamely, unlike [10, 33, 34] there is no non-linear metric network following the Siamese network application, but rather we simply use the L2 distance to compare patches.|
|||CNN(cid:173)based Descriptors  When designing the structure of the CNN we are limited by the size of the input data: in our case 6464 patches, from the MVS dataset [3], while we extract descriptors of the same size as SIFT [16], i.e.|
||3 instances in total. (in iccv2015)|
|1246|Hsiao_Learning_the_Latent_ICCV_2017_paper|four  baseline:  compare with a state-of-the-art  Baselines We (i) StyleNet [33]: feature for clothing that fine-tunes a CNN using 123 metadata labels (e.g., redsweater) on images from the Fashion 144K dataset [32], (ii) vanilla ResNet-50 [13]: the last layer of a state-of-the-art CNN pretrained for ImageNet, (iii) Attr-ResNet: ResNet50 fine-tuned to classify the same 148 attributes3 used by our method with the same training data, and (iv) Attributes: indicator vectors using the same attributes as our model.|
|||Our model discovers the human-perceived styles better than the CNN and attributes clusters (see Supp).|
|||In particular, we collect 350 triplets labeled by 5 human annotators and learn a ranking function [17] on top of the attribute and CNN descriptors that respects human-given judgments.|
||3 instances in total. (in iccv2017)|
|1247|Luo_Efficient_Deep_Learning_CVPR_2016_paper|In the context of stereo estimation, [29] utilize CNN to compute the matching cost between two image patches.|
|||In similar spirit to [29], [28] investigated different CNN based architectures for comparing image patches.|
|||Unary CA SGM[30] Post[30] Slanted[27] Ours(9) Ours(19) Ours(29) Ours(37) MC-CNN-acrt[29] MC-CNN-fast[29]  X  X  X  X  X  X  X  X  X  X  X  X  15.25  11.43  5.18  4.41  4.25  8.95  8.00  4.74  4.23  4.20  7.23  6.60  4.62  4.31  4.14  7.13  6.58  4.73  4.38  4.19  12.45  14.96  7.78  3.48  3.10  3.11   5.05  4.74  4.79  X  X  X  Table 5: Comparison of smoothing methods using different CNN output.|
||3 instances in total. (in cvpr2016)|
|1248|Recasens_Following_Gaze_in_ICCV_2017_paper|We used a 6layer CNN to generate the spatial map from the input image.|
|||We set the origin of the cone at the head of the person ue and let a CNN generate v 2 R3, the direction of the cone and  2 R, its aperture.|
|||We define T1, a 5-layer CNN following the structure defined in [17].|
||3 instances in total. (in iccv2017)|
|1249|DESIRE_ Distant Future Prediction in Dynamic Scenes With Interacting Agents|During the process, we combine 1) past motion history through the embedding vector HX, 2) semantic scene context through a CNN with parameters , and 3) interaction among multiple agents by using interaction features (Sec.|
|||i  We achieve the goal by having an RNN that takes follow ing input xt at each time step:  xt = h(vi,t), p(yi,t; (I)), r(yi,t; yj\i,t, h Yj\i  )i  (4)  i  where vi,t is a velocity of Y (k) at t,  is a f c layer with a ReLU activation that maps the velocity to a high dimensional representation space, p(yi,t; (I)) is a pooling operation that pools the CNN feature (I) at the location yi,t, r(yi,t; yj\i,t, h Yj\i ) is the interaction feature computed by a fusion layer that spatially aggregates other agents hidden vectors, similar to SocialPooling (SP) layer [3].|
|||Combined with CNN features, the SCF module provides the RNN decoder with both static and dynamic scene information.|
||3 instances in total. (in cvpr2017)|
|1250|Xiao_Discovering_the_Spatial_ICCV_2015_paper|For ranker learning, we use both the LLC encoding of dense-SIFT [40] stacked with a two-layer Spatial Pyramid (SP) grid [24], and pool-5 activation features from the ImageNet pre-trained CNN (Alexnet architecture) implemented using Caffe [19, 14].2 We set  = 0.05, Ninit = 5, Niter = 80, Kinit = 20, Kpert = 20, Kens = 60, xy = 0.6, and  = {1/4, 1}.|
|||For Global [30], we use the authors code with the same features as our approach (dense-SIFT+LLC+SP and pool-5 CNN features).|
|||Notably, even with the weaker dense-SIFT features, our method outperforms Global [30] that uses the more powerful CNN features for all attributes except Masculinelooking, which may be better described with a global feature.|
||3 instances in total. (in iccv2015)|
|1251|Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper|A recent wave of work using CNN for patch-based edge prediction [10, 34, 2, 19] contains an alternative common thread that focuses on three aspects: automatic feature learning (property 1), multi-scale response fusion (property 2), and possible engagement of different levels of visual perception (property 3).|
|||In addition to the speed gain over patch-based CNN edge detection methods, the performance gain is largely due to three aspects: (1) FCN-like image-to-image training allows us to simultaneously train on a significantly larger amount of samples (see Table 4); (2) deep supervision in our model guides the learning of more transparent features (see Table 2); (3) interpolating the side outputs in the end-to-end learning encourages coherent contributions from each layer (see Table 3).|
|||Model parameters In contrast to fine-tuning CNN to perform image classification or semantic segmentation, adapting CNN to perform low-level edge detection requires special care.|
||3 instances in total. (in iccv2015)|
|1252|cvpr18-IVQA  Inverse Visual Question Answering|The image encoder is a CNN that generates a feature representation of the image.|
|||With the described CNN image encoder and LSTM based answer encoder as input, a LSTM question decoder can be used to generate questions conditioned on both the images and answer.|
|||The similarity of the images is measured using the image CNN feature.|
||3 instances in total. (in cvpr2018)|
|1253|Bach_Analyzing_Classifiers_Fisher_CVPR_2016_paper|Figures 7 and 8 present the results quantitatively and as exemplary heatmaps for the BVLC Reference [10], VGG CNN S [4], which has slightly lower error rate than the former, and GoogleNet [27].|
|||From left to right: Input, heatmaps for BVLC CaffeNet, VGG CNN S and GoogleNet.|
|||DNN context scores for ImageNet 2012 for the BVLC CaffeNet (bvlc, left three bars), VGG CNN S (vgg, middle three) and GoogleNet (ggm, right three).|
||3 instances in total. (in cvpr2016)|
|1254|Chuanxia_Zheng_T2Net_Synthetic-to-Realistic_Translation_ECCV_2018_paper|In [4] a twoscale CNN architecture was proposed to learn the depth map from raw pixel values.|
|||This was followed by several CNN-based methods, which included combining deep CNN with continuous CRFs for estimating depth values [23], simultaneously predicting semantic labels and depth maps [37], and treating the depth estimation as a classification task [1].|
|||Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||3 instances in total. (in eccv2018)|
|1255|Attentional Push_ A Deep Convolutional Network for Augmenting Image Salience With Shared Attention Modeling in Social Scenes|For training, we use transfer learning to initialize and train the Attentional Push CNN by minimizing the classification error of following the actors gaze location on a 2-D grid using a large-scale gaze-following dataset.|
|||The Attentional Push CNN is then fine-tuned along with the augmented saliency CNN to minimize the Euclidean distance between the augmented saliency and ground truth fixations using an eye-tracking dataset, annotated with the head and the gaze location of the scene actors.|
|||We explain the structure and the training scheme for the Attentional Push CNN and the augmented saliency CNN in Section 3 and 4, respectively.|
||3 instances in total. (in cvpr2017)|
|1256|Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper|Previous CNN frameworks only address particular tasks of super-resolution [7, 8, 13], noise/artifact removal [6], image filtering [18], etc.|
|||Challenge in Our Blind Reversion To show the limitation of existing CNN in our task, we apply state-of-theart VDSR [14], FSRCNN [8] and PSPNet [8], to directly regress the data after edit.|
|||(a) is the image edited by Photoshop Express; (b) represents a CNN network for image regression; (c) is the output of the network; (d) is the ground truth image; (e) is the close-up patches cropped from the edited image, output and ground truth image.|
||3 instances in total. (in iccv2017)|
|1257|Yu_Compressive_Quantization_for_ICCV_2017_paper|For each object proposal, we further extract its feature by max-pooling the last convolutional layer of VGG-16 CNN model [26] pre-trained on Imagenet dataset.|
|||Efficient diffusion on region manifolds: Recovering small objects with compact cnn representation.|
|||Particular object retrieval with integral max-pooling of cnn activations.|
||3 instances in total. (in iccv2017)|
|1258|Galliani_Just_Look_at_CVPR_2016_paper|Given an input image (top left) and an incomplete normal map from multi-view stereo (top right), we reconstruct the missing normals by CNN regression on the image (bottom left).|
|||The corresponding mean and median errors are 13 and 9, respectively, for the MVS points; and 12, respectively 6 for the CNN prediction.|
|||Instead, we view surface normal estimation as a discriminative regression problem and train a CNN to predict normal vectors from raw image patches.|
||3 instances in total. (in cvpr2016)|
|1259|cvpr18-Deep Ordinal Regression Network for Monocular Depth Estimation|In this paper, we have developed an deep ordinal regression network (DORN) for monocular depth estimation MDE from a single image, consisting of a clean CNN architecture and some effective strategies for network optimization.|
|||Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
|||Ordinal In  regression with multiple output cnn for age estimation.|
||3 instances in total. (in cvpr2018)|
|1260|Khosla_Understanding_and_Predicting_ICCV_2015_paper|Analysis of the responses of the high-level CNN layers shows which objects and regions are positively, and negatively, correlated with memorability, allowing us to create memorability maps for each image and provide a concrete method to perform image memorability manipulation.|
|||MemNet: CNN for Memorability  Given the recent success of convolutional neural networks (CNN) in various visual recognition tasks [10, 21, 29, 35, 32, 37], we use them here for memorability prediction.|
|||Visualizing the CNN features after fine-tuning, arranged in the order of their correlation to memorability from highest (top) to lowest (bottom).|
||3 instances in total. (in iccv2015)|
|1261|Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper|Although the approaches substantially improve the performance over the  prior arts, training CNN requires a large number of finequality segmentation annotations, which are difficult to collect due to extensive labeling cost.|
|||Related Work  Recent success in CNN has brought significant progress on semantic segmentation in the past few years [4, 11, 10, 21, 25].|
|||Training We initialize the encoder by fine-tuning the pretrained CNN from ImageNet [6] to perform multi-class classification on the combined datasets of PASCAL VOC and MS-COCO.|
||3 instances in total. (in cvpr2016)|
|1262|Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper|The CNN based self-training methods share the same goal of adversarial training based global and class-wise feature alignment methods [9, 17], but it try to solve domain adaption by a simpler and more elegant way.|
|||We propose a typical CNN based self-training (ST) framework for domain adaptation in semantic segmentation of which workflow is shown in figure refflow, taking adapting from GTA5  Cityscapes as an example.|
|||yt,n = (cid:2)y(1)  n=1  X t,n, ..., y(C)  c=1  t=1  y  k > 0  t,n log(pn(c|w, It)) + k|yt,n|1i y(c)  t,n (cid:3)  {{e(i)|e(i)  RC}  0},  t, n  (4)  Since yt,n is required to be either a discrete one-hot vector or a zero vector, the pseudo-label configuration can be optimized via the following solver:  y(c) t,n =     1, if c = arg max  pn(c|w, It),  c  pn(c|w, It) > exp(k)  0, otherwise  (5)  Unlike traditional self-training adaptation with handcrafted features that learn a domain-invariant classifier, CNN based self-training can learn not only domaininvariant classifier but also domain-invariant features.|
||3 instances in total. (in eccv2018)|
|1263|The VQA-Machine_ Learning How to Use Existing Vision Algorithms to Answer New Questions|One of the advantages of this approach is its ability to exploit a pre-trained CNN model.|
|||Dynamic Memory Networks (DMN) [36] retrieves the facts required to answer the question, where the facts are simply CNN features calculated over small image patches.|
|||The whole system is implemented on the Torch7 [6] and trained end-to-end but with fixed CNN features.|
||3 instances in total. (in cvpr2017)|
|1264|Large Margin Object Tracking With Circulant Feature Maps|[21] exploit features extracted from a pretrained deep CNN and learn adaptive CFs on several CNN layers to improve track[28] present a ing accuracy and robustness.|
|||sequential training method for CNN that is regarded as an ensemble with each channel of the output feature map as an individual base learner.|
|||As shown in Table 2, DeepLMCF shows the best tracking accuracy and robustness in all OPE, TRE and SRE evaluation metrics benefited by the hierarchical CNN features and LMCF performs second while with the fastest speed.|
||3 instances in total. (in cvpr2017)|
|1265|cvpr18-SeedNet  Automatic Seed Generation With Deep Reinforcement Learning for Robust Interactive Segmentation|By fine-tuning FCN block, the CNN structure can be used efficiently for interactive segmentation problems.|
|||[15] improved segmentation performance by creating global and local branches based on CNN architecture.|
|||As other segmentation algorithms can be applied in this way, better results can be expected using CNN based algorithms, such as iFCN [34].|
||3 instances in total. (in cvpr2018)|
|1266|Tulyakov_Weakly_Supervised_Learning_ICCV_2017_paper|Our work is also close to [15], where unsupervised learning is used to train regression CNN for predicting depth from a single image.|
|||Fortunately, in this work we use deep metric that works very well with random CNN weights, as shown in  4.7, thus we were able to use contrastive methods right from the start.|
|||Parameter Number of CNN layers Number of features per layer Receptive field Activation function Equivalent patch size Similarity metric  KITTI12,15 MB 4 64 3x3x64 ReLU 9x9 Cosine  5 64 3x3x64 ReLU 11x11 Cosine  Table 1.|
||3 instances in total. (in iccv2017)|
|1267|Dynamic Facial Analysis_ From Bayesian Filtering to Recurrent Neural Network|As a generic and learning-based approach for time series prediction, RNN avoids tracker-engineering for tasks performed on videos, much like CNN avoids feature-engineering for tasks performed on images.|
|||In contrast, the proposed RNN-based approach is a generic, end-to-end approach that (1) directly learns optimal feature extraction via CNN and tracking via RNN from training data, and (2) can be easily applied to different sub-tasks of facial analysis from videos.|
|||Instead, RNN can be easily concatenated with a CNN that performs frame-wise feature extraction, and forms an end-to-end network for joint estimation and tracking.|
||3 instances in total. (in cvpr2017)|
|1268|cvpr18-Lightweight Probabilistic Deep Networks|The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem.|
|||[10] introduced FlowNet, a CNN architecture that learns to predict optical flow from data directly.|
|||The CNN is discriminatively trained on the synthetic FlyingChairs dataset using the endpoint error (EPE) as loss.|
||3 instances in total. (in cvpr2018)|
|1269|Matsukawa_Hierarchical_Gaussian_Descriptor_CVPR_2016_paper|Convolutional Neural Network (CNN) is one of the stateof-the art recognition algorithms that leverage hierarchal structure [16] and CNN has been recently adopted for person re-identification [1, 21].|
|||This is because the CNN requires a large number of labeled train 1364  1 2  G  ...  ... ...  Region  y  M  G R  G  y  R ...  Pixel  features G  y  +  Sym  +1d  1  2  ...  G  +  Sym  +1m  R  (a) Dense patch extraction  (b) Patch  Gaussian  patch Gaussian  (c) Flatten   (d) Region  (e) Flatten   (f) Feature  Gaussian  region Gaussian  vector  Figure 3.|
|||Although several descriptors are proposed by focusing on CNN like hierarchy [8, 39], they require learning processes for feature extraction in each hierarchy.|
||3 instances in total. (in cvpr2016)|
|1270|Disentangled Representation Learning GAN for Pose-Invariant Face Recognition|[40] use a multi-task CNN to rotate a face with any pose and illumination to a target pose, and the L2 loss-based reconstruction of the input is the second task.|
|||Different from the discriminator in conventional GAN, our D is a multi-task CNN consisting of two parts: D = [Dd, Dp].|
|||Unconstrained face verification using deep CNN features.|
||3 instances in total. (in cvpr2017)|
|1271|Link the Head to the _Beak__ Zero Shot Learning From Noisy Text Description at Part Precision|Images are encoded by a part-based CNN that detect bird parts and learn part-specific representation.|
|||The VPDE-net is fed with bird images, detects the bird parts, and learns CNN feature representation for every part.|
|||Visual Parts CNN Detector/Encoder (VPDE)  Detecting semantic parts facilitates modeling a representation that can be related to unstructured text terms at the part-level.|
||3 instances in total. (in cvpr2017)|
|1272|cvpr18-Neural Baby Talk|The image is first resized to 576576 and we random crop 512  512 as the input to the CNN network.|
|||, vN } and CNN features from the last convolution layer at K grids V = {v1, .|
|||Note that we do not finetune the CNN network during training.|
||3 instances in total. (in cvpr2018)|
|1273|cvpr18-CBMV  A Coalesced Bidirectional Matching Volume for Disparity Estimation|For example, the Zbontar and LeCun [50, 51] uses a MC-CNN method of Siamese CNN to estimate the matching volume, that contains the matching likelihood/cost for each allowable disparity of every pixel, and conventional steps to optimize the volume and extract the final disparity map.|
|||Poggi and Mattoccia [31] pose confidence estimation as a regression problem and solve it using a CNN trained on small patches of disparity maps based on the observation that patterns in the disparity map can indicate whether a certain disparity assignment is correct.|
|||The same authors [32] improved a number of previous methods by training a CNN to refine confidence maps.|
||3 instances in total. (in cvpr2018)|
|1274|Shuhan_Chen_Reverse_Attention_for_ECCV_2018_paper|Due to the repeated stride and pooling operations in CNN architectures,  2  Shuhan Chen et al.|
|||Then, some simple yet effective structures are constructed to combine the complementary cues of shallow and deep CNN features, which capture low-level spatial details and high-level semantic information respectively, such as skip connections [12], short connections [11], dense connections [17], adaptive aggregation [13].|
|||Li, G., Yu, Y.: Visual saliency detection based on multiscale deep cnn features.|
||3 instances in total. (in eccv2018)|
|1275|Bambach_Lending_A_Hand_ICCV_2015_paper|For object detection, the now-standard approach is to divide an image into candidate windows, rescale each window to a fixed size, fine-tune a CNN for window classification [14,34], and then perform non-maximum suppression to combine the output of the region-level classifier into object detection results.|
|||Window Classification using CNNs  Given our accurate, efficient window proposal technique, we can now use a standard CNN classification framework to classify each proposal (after resizing to the fixed-sized in 1952  put of the CNN).|
|||To address this question, we fine-tuned another CNN to classify whole frames as one of our four different activities.|
||3 instances in total. (in iccv2015)|
|1276|cvpr18-Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification|3.1.1 Attribute-Identity Transferable Joint Learning  Identity and Attribute Branches For building an efficient yet strong deep re-id model, we choose the lightweight Mo 2277  AttributesLabelEncoderDecoderIdentity Inferred Attribute (IIA) SpaceCNNCNN(cid:3031)(cid:3047)(cid:3047)Identity BranchIdentity Label(cid:3045)(cid:3032)(cid:3030)Attribute Branch(cid:3047)(cid:3047),(cid:3031)(cid:3031)Identity Discriminative Space(cid:3031)Attribute and Identity Discriminative Space(cid:3047)(cid:3047)+(cid:3031)(cid:3047)(cid:3047)Sigmoid Cross Entropy Loss(cid:3031)SoftmaxCross Entropy Lossabc(cid:3047)(cid:3047),Sigmoid Cross Entropy Loss(cid:3047)(cid:3047)+(cid:3031)(cid:3045)(cid:3032)(cid:3030)Mean Square Error(cid:3047)(cid:3045)(cid:3046)(cid:3033)(cid:3032)r(cid:3047)(cid:3045)(cid:3046)(cid:3033)(cid:3032)r(cid:3047)(cid:3047)(cid:3031)bileNet as the CNN architecture1 for both identity and attribute branches.|
|||Comparisons to Alternative Fusion Methods  We compare the TJ-AIDL with two multi-source fusion methods: (a) Independent Supervision: Independently train a deep CNN model for either attribute or identity label in the source domain and use the concatenated feature vectors of the two models for re-id matching in the target domain.|
|||This is achieved by introducing an Identity Inferred Attribute space for interactive attribute and identity discriminative learning in a two-branches CNN architecture.|
||3 instances in total. (in cvpr2018)|
|1277|Qinghao_Hu_Training_Binary_Weight_ECCV_2018_paper|3.1 Preliminary  Given an L-layer pre-trained CNN model, let W  RT S be the full-precision weights of lth layer.|
|||(18)  end  end  end  end Fine-tune the binarized CNN model return {Ul} l=1 and {Dl}  l=1 , {Vl}  L  L  L  l=1;  3.4 Fine-tuning  After direct semi-binary decomposition or minimizing the featuremaps quantization loss, we get the U, V and D for each layer.|
|||The microarchitecture design is based on [29], which is a state-of-the-art CNN accelerator.|
||3 instances in total. (in eccv2018)|
|1278|Tao_Detail-Revealing_Deep_Video_ICCV_2017_paper|We accordingly propose a sub-pixel motion compensation (SPMC) layer in a CNN framework.|
|||The final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal image details.|
|||Detail Fusion Besides motion compensation, proper image detail fusion from multiple frames is the key to the success of video SR. We propose a new CNN framework that incorporates the SPMC layer, and effectively fuses image information from aligned frames.|
||3 instances in total. (in iccv2017)|
|1279|Yu_Sketch_Me_That_CVPR_2016_paper|However, such a network with three branches naively requires a prohibitive O(N 3) annotations given that CNN models already require a large number of data instances N .|
|||A CNN model, Sketch-a-Net was developed specifically for sketch recognition in [32], and achieves state-of-the-art recognition performance to date on TU-Berlin [7].|
|||Data Augmentation  It is increasingly clear that CNN performance ceiling in practice is imposed by limits on available data, with additional data improving performance [33].|
||3 instances in total. (in cvpr2016)|
|1280|Tian_Pedestrian_Detection_Aided_2015_CVPR_paper|(2) These multiple tasks from multiple sources are trained using a single task-assistant CNN (TA-CNN), which is carefully designed to bridge the gaps between different datasets.|
|||For example, ConvNet [28] employed convolutional sparse coding to unsupervised pre-train CNN for pedestrian detection.|
|||The JointDeep model [22] designed a deformation hidden layer for CNN to model mixture poses information.|
||3 instances in total. (in cvpr2015)|
|1281|Li_Dual-Glance_Model_for_ICCV_2017_paper|We concatenate its output with the CNN features for p1, p2 and p to form a single feature vector, which is subsequently passed through another two fc layers to produce first glance score, s1.|
|||We then process I with a CNN to generate a convolutional feature map conv(I).|
|||Union-CNN: Following the predicate prediction model in [25], a single CNN model is used to classify the union region of the individual pair of interest.|
||3 instances in total. (in iccv2017)|
|1282|cvpr18-Learning Rich Features for Image Manipulation Detection|[27] use an SRM filter kernel as initialization for a CNN to boost the detection accuracy.|
|||[5] add a low pass filter layer before a CNN to detect median filtering tampering techniques.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||3 instances in total. (in cvpr2018)|
|1283|Zolfaghari_Chained_Multi-Stream_Networks_ICCV_2017_paper|The spatio-temporal CNN can capture the temporal dynamics of pose.|
|||[2] firstly used a 3D CNN to learn spatio-temporal features from video and in the next step they employed an  LSTM to classify video sequences.|
|||[36] proposed a two-stream CNN to capture the complementary information from appearance and motion, each modality in an independent stream.|
||3 instances in total. (in iccv2017)|
|1284|Zhiwen_Fan_A_Segmentation-aware_Deep_ECCV_2018_paper|In [4], a deep cascaded CNN (DC-CNN) is proposed to cascade several basic blocks to learn the mapping with each block containing the nonlinear convolution layers and a nonadjustable data fidelity layer.|
|||2.3 Multilayer Feature Aggregation  The works [23] on visualization of deep CNN has revealed the feature maps at different layers describe the image in different scales and views.|
|||We compare the proposed SADFN5 with other state-of-the-art CS-MRI models including transform learning MRI (TLMRI) [12], patch-based nonlocal operator (PANO) [10], fast composite splitting algorithm (FCSA) [8] , graph-based redundant wavelet tranform (GBRWT) [11], and the deep models such as vanilla CNN [15], U-Net [17] the Pre-RecNet5 (which is also the state-ot-the-art DC-CNN with 5 blocks [4]).|
||3 instances in total. (in eccv2018)|
|1285|Meng_From_Keyframes_to_CVPR_2016_paper|In light of the good performance reported using features from the top layers of deep convolutional neural networks (CNNs) for many computer vision tasks such as object detection [14], image retrieval [2] and object instance search [35], we choose to represent each object proposal by the CNN feature after dimension reduction.|
|||The CNN features are then reduced to 256-dimension by PCA and whitening [16].|
|||Note that LDA-WCP is based on quantized SIFT features, while the others except for Objectness are based on CNN features.|
||3 instances in total. (in cvpr2016)|
|1286|Gan_Learning_Attributes_Equals_CVPR_2016_paper|For the first three image datasets, we use the Convolutional Neural Network (CNN) implementation provided by Caffe [36], particularly with the 19-layer network architecture and parameters from Oxford [65], to extract 4,096-dimensional CNN feature representations from images (i.e., the activations of the first fullyconnected layer fc6).|
|||For the UCF101 dataset, we use the 3D CNN (C3D) [74] pre-trained on the Sport 1M dataset [39] to construct video-clip features from both spatial and temporal dimensions.|
|||(%, in accuracy)  the same CNN features (for AWA, CUB, and aPascalaYahoo) and C3D features (for UCF101) we extracted for the baselines and our approach.|
||3 instances in total. (in cvpr2016)|
|1287|cvpr18-DVQA  Understanding Data Visualizations via Question Answering|All of the models that process images use the ImageNet pre-trained ResNet-152 [10] CNN with 448  448 images resulting in a 14142048 feature tensor, unless otherwise noted.|
|||It concatenates the LSTM and CNN embeddings, and then feeds them to an MLP with one 1024-unit hidden layer and a softmax output layer.|
|||An image patch is extracted from this region, which is resized to 128  128, and then a small 3-layer CNN is applied to it.|
||3 instances in total. (in cvpr2018)|
|1288|Graph-Structured Representations for Visual Question Answering|This contrasts with the typical approach of representing the image with CNN activations (which are sensitive to individual object locations but less so to relative position) and the processing words of the question serially with an RNN (despite the fact that grammatical structure is very non-linear).|
|||This contrasts with typical CNN and RNN models which are limited to spatial feature maps and sequences of words respectively.|
|||Most VQA systems are trained end-to-end from questions and images to answers, with the exception of the visual feature extractor, which is typically a CNN pretrained for image classification.|
||3 instances in total. (in cvpr2017)|
|1289|Galoogahi_Learning_Background-Aware_Correlation_ICCV_2017_paper|The excellent performance of deep convolutional neural networks (CNNs) on several challenging vision tasks [19, 31, 24, 14] has encouraged more recent works to either exploit CNN deep features within CF framework [26, 9, 11] or design deep architectures [2, 28, 34, 5, 32, 33] for robust visual tracking.|
|||Compared to hand-crafted features such as HOG, learning CF trackers using CNN features significantly improves their robustness against geometric and photometric variations [11].|
|||However, extracting CNN features from each frame and training/updating CF trackers over high dimensional deep features is computationally expensive.|
||3 instances in total. (in iccv2017)|
|1290|Chunze_Lin_Graininess-Aware_Deep_Feature_ECCV_2018_paper|While many CNN based methods have been proposed [18, 11, 48, 4, 26], there are still some shortcomings for methods in this category.|
|||[4] extended Faster R-CNN [35] by replacing the downstream classifier with an independent deep CNN and added a segmentation loss to implicitly supervise the detection, which made the features be more semantically meaningful.|
|||Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||3 instances in total. (in eccv2018)|
|1291|cvpr18-Deep Cross-Media Knowledge Transfer|For text pathway, we first embed each word into a vector via Word2Vec model [24], and then generate the 300-d input feature vector of each text with Word CNN [16].|
|||As for text, we use the same 300-d Word CNN text features for all the methods, which is the same with our DCKT.|
|||Cross-modal retrieval with CNN visual features: A new baseline.|
||3 instances in total. (in cvpr2018)|
|1292|Hsieh_Drone-Based_Object_Counting_ICCV_2017_paper|Method  YOLO [21]  Faster R-CNN [22]  *YOLO  *Faster R-CNN  *Faster R-CNN (RPN-small) One-Look Regression [18]  Our Car Counting CNN Model  400  Np MAE 156.72 156.76  400   400 400   156.00 111.40 39.88 21.88 22.76  RMSE  200.54 200.59  200.42 149.35 47.67 36.73 34.46  We compare three methods on the PUCPR+ dataset, where the maximum number of cars is 311 in a single scene.|
|||The experiment results show that by incorporating the spatially regularized information, our Car Counting CNN model boosts the performance of counting.|
|||Method  YOLO [21]  Faster R-CNN [22]  *YOLO  *Faster R-CNN  *Faster R-CNN (RPN-small) One-Look Regression [18]  Our Car Counting CNN Model  200  Np MAE 102.89 103.48  200   200 200   48.89 47.45 24.32 59.46 23.80  RMSE  110.02 110.64  57.55 57.39 37.62 66.84 36.79  6.|
||3 instances in total. (in iccv2017)|
|1293|cvpr18-An Unsupervised Learning Model for Deformable Medical Image Registration|The novelty of this work is that:   we present a learning-based solution requiring no supervised information such as ground truth correspondences or anatomical landmarks during training,   we propose a CNN function with parameters shared across a population, enabling registration to be achieved through a function evaluation, and   our method enables parameter optimization for a variety of cost functions, which can be adapted to various tasks.|
|||Both propose a neural network consisting of a CNN and spatial transformation function [23] that warps images to one another.|
|||VoxelMorph CNN Architecture  The parametrization of g is based on a convolutional neural network architecture similar to UNet [22, 36].|
||3 instances in total. (in cvpr2018)|
|1294|cvpr18-Deep Spatio-Temporal Random Fields for Efficient Video Segmentation|The CRF and CNN architecture is jointly trained end-toend, while CRF inference is exact and particularly efficient.|
|||Finally, the state-of-the-art on this task [14] improves over PSPnet[40] by warping the feature maps of a static segmentation CNN to emulate a video segmentation network.|
|||3 the Aln terms are image-dependent and delivered by a fully-convolutional embedding branch that feeds from the same CNN backbone architecture, and is denoted by Av in Fig.|
||3 instances in total. (in cvpr2018)|
|1295|cvpr18-SemStyle  Learning to Generate Stylised Image Captions Using Unaligned Text|The lower left of Figure 2 describes the term generator, which takes an image as input, extracts features using a CNN (Convolutional Neural Network) and then generates an ordered term sequence summarising the image semantics.|
|||The image feature is extracted from the second last layer of the Inception-v3 [48] CNN pre-trained on ImageNet [43].|
|||Image embeddings come from the second last layer of the Inception-v3 CNN [48] and are 2048 dimensional.|
||3 instances in total. (in cvpr2018)|
|1296|Kim_SAN_Learning_Relationship_ECCV_2018_paper|We study the effect of the scale difference in CNN by using a channel activation matrix that represents the relationship between scale change and channel activation, then design a structure for SAN and a unique learning method based on channel routing mechanism that considers the relationship between channels without the spatial information.|
|||We discuss the effect of the scale difference in CNN and present the proposed SAN and the training mechanism for SAN in Section 3.|
|||In this work, we discuss the feature difference in CNN caused by the scale variance and make  4  Y. Kim et al.|
||3 instances in total. (in eccv2018)|
|1297|Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper|*EV[S][p]) end  end shape=face["shape"][:]; tex=face["texture"][:]; camera = Uniform(-1,1,1,2); light = Uniform(-1,1,1,2)  # Approximate Renderer rendered_img= MeshRenderer(shape,tex,light,camera)  # Representation Layer ren_ftrs = getFeatures("CNN_Conv6", rendered_img)  # Comparator #Using Pixel as Summary Statistics observe(MvNormal(0,0.01), rendered_img-obs_img) #Using CNN last conv layer as Summary Statistics observe(MvNormal(0,10), ren_ftrs-obs_cnn)  end  global obs_img = imread("test.png") global obs_cnn = getFeatures("CNN_Conv6", img) #Load args from file TR = trace(PROGRAM,args=[MU,PC,EV,VERTEX_ORDER]) # Data-Driven Learning learn_datadriven_proposals(TR,100000,"CNN_Conv6") load_proposals(TR) # Inference infer(TR,CB,20,["DATA-DRIVEN"]) infer(TR,CB,200,["ELLIPTICAL"])  Figure 2: Picture code illustration for 3D face analysis: Modules from Figure 1a,b are highlighted in bold.|
|||The observe directive constrains the program execution based on both the pixel data and CNN features.|
|||The summary statistic function dd used were the top convolutional-layer features from the pretrained ImageNet CNN model[20].|
||3 instances in total. (in cvpr2015)|
|1298|cvpr18-Arbitrary Style Transfer With Deep Feature Reshuffle|The recent work [14] showed that the representations of image content and style were separable by variant CNN convolutional layers.|
|||Inspired by the success of CNN in style transfer, we also use neural representation for image decoupling, and better matching.|
|||[14] pioneer the neural texture synthesis and style transfer by successfully applying CNN (pre-trained VGG networks [36]) to this problem.|
||3 instances in total. (in cvpr2018)|
|1299|Pishchulin_DeepCut_Joint_Subset_CVPR_2016_paper|Further, two CNN variants are proposed to generate representative sets of body part candidates.|
|||Further finetuning to LSP leads to remarkable 82.8% PCK: CNN learns LSP-specific image representations.|
|||The latter is interesting, as [7] use a stronger spatial model that predicts the pairwise conditioned on the CNN features, whereas DeepCuts use geometric-only pairwise connectivity.|
||3 instances in total. (in cvpr2016)|
|1300|Filippos_Kokkinos_Deep_Image_Demosaicking_ECCV_2018_paper|Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
|||Lefkimmiatis, S.: Universal denoising networks: A novel cnn architecture for image denoising.|
|||Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image  restoration.|
||3 instances in total. (in eccv2018)|
|1301|Predicting Ground-Level Scene Layout From Aerial Imagery|Main Contributions: The main contributions of this work are: (1) a novel convolutional neural network (CNN) architecture that relates the appearance of an aerial image to the semantic layout of a ground image of the same location, (2) demonstrating the value of our training strategy for pretraining a CNN to understand aerial imagery, (3) extensions of the proposed technique to the tasks of ground image localization, orientation estimation, and synthesis, and (4) an extensive evaluation of each of these techniques on large, real-wold datasets.|
|||They observe that the visual appearance of different views is highly correlated and propose a CNN architecture for estimating appearance flows, a representation of which pixels in the input image can be used for reconstruction.|
|||Mnih and Hinton propose a CNN for detecting roads in aerial imagery [22] using GIS data as ground truth.|
||3 instances in total. (in cvpr2017)|
|1302|Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper|Each block increases mesh resolution and estimates vertex locations, which are then used to extract perceptual image features from the 2D CNN for the next block.|
|||The image feature network is a 2D CNN that extract perceptual feature from the input image, which is leveraged by the mesh deformation network to progressively deform an ellipsoid mesh into the desired 3D model.|
|||: Syncspeccnn: Synchronized spectral CNN for 3d shape  segmentation.|
||3 instances in total. (in eccv2018)|
|1303|Liang_Semantic_Object_Parsing_CVPR_2016_paper|One key bottleneck to increase the network capability is the longchain problem in deep CNN structures, that is, information from previous computations rapidly attenuates as it progresses through the chain.|
|||LG-LSTM integrates several novel local-global LSTM layers into the CNN architecture for semantic object parsing.|
|||Implementation Details: In our experiments, five LGLSTM layers are appended to the convolutional layers right before the prediction layer of the basic CNN architecture.|
||3 instances in total. (in cvpr2016)|
|1304|Real-Time Neural Style Transfer for Videos|Introduction  Recently, great progress has been achieved by applying deep convolutional neural networks (CNNs) to image transformation tasks, where a feed-forward CNN receives an input image, possibly equipped with some auxiliary information, and transforms it into a desired output image.|
|||[12] proposed to train a feed-forward CNN using a similar perceptual loss defined on the VGG-16 network [23] to replace the time-consuming optimization process, which enables real-time style transfer for images.|
|||One is the feed-forward CNN based method proposed by Johnson et al.|
||3 instances in total. (in cvpr2017)|
|1305|cvpr18-Human Semantic Parsing for Person Re-Identification|Methodology  In this work, unless specified otherwise, we use Inception-V3 [37] as the CNN backbone for both human semantic parsing and person re-identification models.|
|||In SPReID, we pool the output activations of the CNN backbone multiple times, each time using one of the five probability maps.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in cvpr2018)|
|1306|cvpr18-Zero-Shot Sketch-Image Hashing|Type  Method  SBIR  Zero-Shot  Softmax Baseline Siamese CNN [49] SaN [70] GN Triplet [52] 3D Shape [62] DSH (64 bits) [38] CMT [56] DeViSE [19] SSE [75] JLSE [76] SAE [31] ZSH (64 bits) [67]  Proposed ZSIH (64 bits)  mAP @all  0.099 0.143 0.104 0.211 0.062 0.164 0.084 0.071 0.108 0.126 0.210 0.165 0.254  Sketchy (Extended)  Precision  Feature  @100  Dimension  0.176 0.183 0.129 0.310 0.070 0.227 0.096 0.078 0.154 0.178 0.302 0.217 0.340  4096  64 512 1024  64  64 (binary)  300 300 100 100 300  64 (binary) 64 (binary)  Retrieval Time (s) 3.9  101 5.2  103 4.4  102 8.9  102 5.6  103 6.3  105 3.1  102 3.2  102 1.1  102 1.1  102 3.1  102 6.3  105 6.5  105  mAP @all  0.083 0.122 0.096 0.189 0.057 0.122 0.065 0.067 0.096 0.107 0.161 0.139 0.220  TU-Berlin (Extended)  Precision  Feature  @100  Dimension  0.139 0.153 0.112 0.241 0.063 0.198 0.082 0.075 0.133 0.165 0.210 0.174 0.291  4096  64 512 1024  64  64 (binary)  300 300 220 220 300  64 (binary) 64 (binary)  Retrieval Time (s) 4.7  101 6.3  103 5.1  102 1.4  101 7.0  103 7.5  105 3.7  102 3.7  102 1.3  102 1.3  102 3.7  102 7.5  105 7.9  105  Table 3.|
|||Siamese CNN [49], SaN [70], GN Triplet [52], 3D Shape [62] and DSH [38] are involved as SBIR baselines.|
|||To some extent, the SBIR baselines based on positive-negative samples, e.g., Siamese CNN [49] and GN Triplet [52], have the ability to generalize the learned representations to unseen classes.|
||3 instances in total. (in cvpr2018)|
|1307|Xiang_Li_Adversarial_Open-World_Person_ECCV_2018_paper|[38] came up with domain guided drop out model for training CNN with multiple domains so as to improve the feature learning procedure.|
|||But their work lacks the ability to distinguish very similar identities, and with some deep CNN models coming up, features from multiple camera views can be well expressed by joint camera learning.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identiication by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in eccv2018)|
|1308|Kaicheng_Yu_Statistically-motivated_Second-order_Pooling_ECCV_2018_paper|Note that, while we introduce our SMSO pooling strategy within a CNN formalism, it applies to any method relying on second-order representations.|
|||If the elements xi  Rc of a data matrix X  Rnc follow a zero mean multivariate Gaussian distribution xi  Nc(0, ), then the covariance matrix Y of X is said to follow a Wishart distribution, denoted by (2) Note that, in the bilinear CNN [36], the mean is typically not subtracted from the data.|
|||Lin, T., RoyChowdhury, A., Maji, S.: Bilinear Cnn Models for Fine-Grained Visual  Recognition.|
||3 instances in total. (in eccv2018)|
|1309|cvpr18-Manifold Learning in Quotient Spaces|Autoencoers  have also been used, for instance in [12], where additionally to a 3D autoencoder a CNN is learned to predict the latent vector from an image.|
|||[31] learns a CNN which predicts an arrangement of cuboids which fits the input voxelic model.|
|||Usual pooling units in CNNs allow local translation invariance, but deeper work has been done since to achieve better invariance proper[16] proposes to learn a CNN locally invariant to ties.|
||3 instances in total. (in cvpr2018)|
|1310|cvpr18-Look, Imagine and Match  Improving Textual-Visual Cross-Modal Retrieval With Generative Models|[4] proposed a cross-modal feature embedding framework that use CNN and Skip-Gram [20] to extract cross-modal feature representations, and then associated them with a structured objective in which the distance between the matched image-caption pair is smaller than that between the mismatched pair.|
|||All the modules are randomly initialized before training except for the CNN encoder and decoder.|
|||An empirical study of  language cnn for image captioning.|
||3 instances in total. (in cvpr2018)|
|1311|Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper|In recent years, with the advent of deep convolutional neural network (CNN), a new generation of more effective object detection methods based on CNN significantly improve the state-of-the-art performances, which can be roughly divided into two categories, i.e., the one-stage approach and the two-stage approach.|
|||[44] present an unsupervised method using the convolutional sparse coding to pre-train CNN for pedestrian detection.|
|||Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers.|
||3 instances in total. (in eccv2018)|
|1312|cvpr18-Mining on Manifolds  Metric Learning Without Labels|Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations.|
|||Particular object retrieval with integral max-pooling of cnn activations.|
||3 instances in total. (in cvpr2018)|
|1313|Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper|Finally, false positives (non-text candidates) are identified and eliminated with CNN classifiers [16, 15, 37, 11].|
|||[10], we also adopt CNN classifiers for false positive removal.|
|||Different from [11, 10], which only used CNN classifier for patch or character level discrimination, we train two classifiers that work at character level and text region level, respectively.|
||3 instances in total. (in cvpr2015)|
|1314|Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper|Since it was not clear whether this task could be solved with a standard CNN architecture, we additionally developed an architecture with a correlation layer that explicitly provides matching capabilities.|
|||Zbontar and LeCun [36] train a CNN with a Siamese architecture to predict similarity of image patches.|
|||The simplest solution is to apply a conventional CNN in a sliding window fashion, hence computing a single prediction (e.g.|
||3 instances in total. (in iccv2015)|
|1315|Sun_DL-SFA_Deeply-Learned_Slow_2014_CVPR_paper|[12] proposes to use a modified CNN for action recognition, operating on spatio-temporal outer boundaries volume.|
|||[11] propose a 3D CNN for action recognition with combination of multiple hand-wired features as input.|
|||Algorithm  Hessian[32] + ESURF [16]  pLSA [26]  Dense + HOF[16]  Cuboid [5] + HOG3D [13]  GRBM [29] 3D CNN [11]  HMAX[10]  Hierarchical ISA with dense sampling[17]  Harris3D [24] + HOG/HOF [16] (from [31])  Harris3D [24] + HOF [16] (from [31])  Hierarchical ISA with dense sampling [17]  One Layer SFA  Our Method (DL-SFA)  Table 3.|
||3 instances in total. (in cvpr2014)|
|1316|S3Pool_ Pooling With Stochastic Spatial Sampling|Related Work  The idea of spatial feature pooling dates back to the seminal work by Hubel and Wiesel [11] about complex cells in the mammalian visual cortex and the early CNN architectures developed by Yann Lecun et al.|
|||In modern CNN architectures, spatial pooling plays a fundamental role in achieving invariance (to some extent) to image transformations, and produces more compact representations for efficient processing in subsequent layers.|
|||During testing time, a straightforward but inefficient approach is to take the average classification outputs from many instances of CNN with S3Pool, which can otherwise act as a finite sample estimate of the expectation of S3Pool downsampling.|
||3 instances in total. (in cvpr2017)|
|1317|Angela_Dai_3DMV_Joint_3D-Multi-View_ECCV_2018_paper|One approach towards analyzing these reconstructions is to leverage a CNN with 3D convolutions, which has been used for shape classification [43, 30], and recently also for predicting dense semantic 3D voxel maps [36, 5, 8].|
|||The spatial extent of a 3D CNN can also be increased with dilated convolutions [44], which have been used to predict missing voxels and infer semantic labels [36], or by using a fully-convolutional networks, in order to decouple the dimensions of training and test time [8].|
|||The multi-view CNN approach by Su et al.|
||3 instances in total. (in eccv2018)|
|1318|cvpr18-ScanComplete  Large-Scale Scene Completion and Semantic Segmentation for 3D Scans|To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size.|
|||One can also increase the spatial extent of a 3D CNN with dilated convolutions [42].|
|||Data Generation  To train our ScanComplete CNN architecture, we prepare training pairs of partial TSDF scans and their complete TDF counterparts.|
||3 instances in total. (in cvpr2018)|
|1319|Pyramid Scene Parsing Network|[42] that the empirical receptive field of CNN is much smaller than the theoretical one especially on high-level layers.|
|||This global prior is designed to remove the fixed-size constraint of CNN for image classification.|
|||Given an input image (a), we first use CNN to get the feature map of the last convolutional layer (b), then a pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatenation layers to form the final feature representation, which carries both local and global context information in (c).|
||3 instances in total. (in cvpr2017)|
|1320|Sun_Lattice_Long_Short-Term_ICCV_2017_paper|[9] study a number of ways of fusing CNN towers both spatially and temporally in order to take advantage of this spatio-temporal information from the appearance and optical flow networks.|
|||However, a CNN based method cannot accurately model the dynamics by simply averaging the scores across the time domain, even if the appearance features already achieve remarkable performance on other computer vision tasks.|
|||Implementation Details  We choose part of VGG16 [27], which consists of 13 convolutional layers, as our CNN feature extractor for the RGB and optical flow images.|
||3 instances in total. (in iccv2017)|
|1321|Lin_Cascaded_Feature_Network_ICCV_2017_paper|Given the color image, we use CNN to compute the convolutional feature map.|
|||[3] propose to learn CNN using the combination of RGB and depth image pairs such that the convolutional feature maintains depth information.|
|||Fusenet: Incorporating depth into semantic segmentation via fusionbased cnn architecture.|
||3 instances in total. (in iccv2017)|
|1322|Adria_Recasens_Learning_to_Zoom_ECCV_2018_paper|Their proposal involves the replacement of any standard convolutional layer in a CNN with a deformable layer which learns to estimate offsets to the standard kernel sampling locations, conditioned on the input.|
|||First, while their method  Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks  5  samples from the same low-resolution input as the original CNN architecture, our saliency sampler is designed to sample from any available resolution, allowing it to take advantage of higher resolution data when available.|
|||This layer can be easily added in a standard CNN and preserve differentiability needed for training by backpropagation.|
||3 instances in total. (in eccv2018)|
|1323|Fu_Beyond_Tree_Structure_ICCV_2015_paper|Some incorporates CNN part detectors and graphical models with either piecewise training [7] or joint training [45].|
|||This is due to the huge number of parameters to be learned in the CNN model.|
|||In the later future, we will try to combine stronger feature representation such as CNN feature to boost the performance of our model further.|
||3 instances in total. (in iccv2015)|
|1324|Xiaoqing_Yin_FishEyeRecNet_A_Multi-Context_ECCV_2018_paper|Such high-level semantic supervision is, however, missing in the CNN used for extracting low-level features.|
|||The work most related to our method is [18], where CNN was employed for radial lens distortion correction.|
|||[19] proposed a deep CNN solution for image denoising by integrating the modules of image denoising and high-level tasks like segmentation into a unified framework.|
||3 instances in total. (in eccv2018)|
|1325|Borji_iLab-20M_A_Large-Scale_CVPR_2016_paper|We study: 1) invariance and selectivity of different CNN layers, 2) knowledge transfer from one object category to another, 3) systematic or random sampling of images to build a train set, 4) domain adaptation from synthetic to natural scenes, and 5) order of knowledge delivery to CNNs.|
|||We observe that fine tuning the Alexnet on iLab20M boosts the performance on iLab-20M to 100% while hindering the accuracy over ImageNet as CNN features are now tailored (and are hence selective) to our images.|
|||Bilinear cnn models for fine-grained visual recognition.|
||3 instances in total. (in cvpr2016)|
|1326|Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper|Their network structure, designed for object recognition, is the same as the existing CNN architectures.|
|||We present an expanded CNN structure of our model for illustration purposes in Figure 3(c).|
|||If parameters are not allowed to be shared and CNN chains vary their depths, the number of free parameters grows fast (quadratically).|
||3 instances in total. (in cvpr2016)|
|1327|Li_Adversarial_Examples_Detection_ICCV_2017_paper|Other commonly used layers in a CNN include max-pooling layers, or other normalization layers [13] such as batch normalization layers [10].|
|||5: For each image I, project its CNN filter output of layer m ZmI using PCA: zmI = W(ZmI  e1), and normalize them by dividing the standard deviation s on each respective dimension.|
|||3  3) average filter on the adversarial image before using the CNN to classify it.|
||3 instances in total. (in iccv2017)|
|1328|Jiuxiang_Gu_Unpaired_Image_Captioning_ECCV_2018_paper|They encode the image with a CNN and use a Long ShortTerm Memory (LSTM) network as the decoder, and the decoder is trained to maximize the log-likelihood estimation of the target captions.|
|||This might be due to different implementation details, e.g., AIC-I2T utilizes Inception-v3 for the image CNN while we use ResNet-101.|
|||Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image  captioning.|
||3 instances in total. (in eccv2018)|
|1329|Learning Object Interactions and Descriptions for Semantic Image Segmentation|To address data limitation in image segmentation, this work proposes to jointly train CNN from two sources of  Beautiful girl with long blonde hair wearing hat happy time playing with lamb on sheep farm, holding a small little lamb in her arms outside in the morning sunshine.|
|||With VOC12 and IDW, a novel CNN structure namely  15859  IDW-CNN and its training algorithm are carefully devised, where knowledge of these two datasets can be transferred from each other.|
|||[16] transformed fully-connected layers of CNN into fully convolutional layers (FCN), making accurate per-pixel classification possible by the contemporary CNN architectures that were pre-trained on ImageNet [27].|
||3 instances in total. (in cvpr2017)|
|1330|Zhu_Modeling_Deformable_Gradient_2015_CVPR_paper|In this paper, we choose CNN super-resolution [6] result for the X0 initialization.|
|||Convolutional Neural Network CNN [6] and Deep Network Cascade DNC [4]).|
|||For each group, from left to right, from top to bottom: DPSR [30], DNC [4], CNN [6], and our method.|
||3 instances in total. (in cvpr2015)|
|1331|Dual Attention Networks for Multimodal Reasoning and Matching|[37] predicts the answer from a concatenation of CNN image features and bag-of-word question features.|
|||[23] impose a dynamic parameter layer on a CNN which is learned by the question, while Andreas et al.|
|||[19] construct a CNN to combine an image and sentence fragments into a joint representation, from which the matching  300  score is directly inferred.|
||3 instances in total. (in cvpr2017)|
|1332|Nan_Yang_Deep_Virtual_Stereo_ECCV_2018_paper|[6,5] propose a two scale CNN architecture which directly predicts the depth map from a single image.|
|||CNN-SLAM [39] extends LSD-SLAM [9] by predicting depth with a CNN and refining the depth maps using Bayesian filtering [9,7].|
|||Garg, R., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||3 instances in total. (in eccv2018)|
|1333|Gatys_Image_Style_Transfer_CVPR_2016_paper|We can visualise the information at different processing stages in the CNN by reconstructing the input image from only knowing the networks responses in a particular layer.|
|||On top of the original CNN activations we use a feature space that captures the texture information of an input image.|
|||We reconstruct the style of the input image from a style representation built on different subsets of CNN layers ( conv1 1 (a), conv1 1 and conv2 1 (b), conv1 1, conv2 1 and conv3 1 (c), conv1 1, conv2 1, conv3 1 and conv4 1 (d), conv1 1, conv2 1, conv3 1, conv4 1 and conv5 1 (e).|
||3 instances in total. (in cvpr2016)|
|1334|Zhu_Monocular_Free-Head_3D_ICCV_2017_paper|In this work, we propose to separately model head pose and eyeball movement with two CNN networks, and then connect them with a gaze transform layer.|
|||For joint modeling, we follow [51] to model gaze as a linear function of the head pose label and eye patch features, using our tiny-AlexNet as the CNN structure; for separate modeling, we remove the linear function, and train a eyeball movement model with the same network structure as in joint modeling, then predict gaze vector by combining head pose and eyeball movement prediction following Eq.|
|||Training strategy  Given the network structure shown in Figure 4, we can directly use gaze label and apply gaze loss Lg to guide the learning process of the two CNN networks.|
||3 instances in total. (in iccv2017)|
|1335|Themos_Stafylakis_Zero-shot_keyword_search_ECCV_2018_paper|In [9], CNN features are combined with Gated Recurrent Units (GRUs) in an end-to-end visual ASR architecture, capable of performing sentence-level visual ASR on a relatively easy dataset (GRID [10]).|
|||In [28], the authors introduce a KWS system based on sequence training, composed of a CNN for acoustic modeling and an aggregation stage, which aggregates the frame-level scores into a sequence-level score for words.|
|||It has been verified that CNN features encoding spatiotemporal information in their first layers yield much better performance in lipreading, even when combined with deep LSTMs or GRUs in the backend [34, 9, 13].|
||3 instances in total. (in eccv2018)|
|1336|Li_Performance_Guaranteed_Network_ICCV_2017_paper|XNOR uses H to approximate the input tensor X: X  H and they solve the following optimization problem:  , B,  , H  = argmin ,B,,H  (cid:9)X (cid:10) W  H (cid:10) B(cid:9)2  (4)  Algorithm 1 Training an L-layers CNN with binary weights: Input: A minibatch of inputs and targets (I, Y ), cost function C(Y, Y ), current weight W t and current learning rate t Output: Updated weight W t+1 and updated learning rate t+1  1: Binarizing weight filters: 2: for l = 1 to L do 3:  for k = 1 to cout do Alk = 1 lk(cid:9)lk Blk = sign(W t lk)  n (cid:9)W t (cid:5)Wlk = AlkBlk  4:  5:  6: 7: Y =BinaryForward(I, B, A) 8: 9: W t+1 =UpdateParameters(W t, C  (cid:2)W 10: t+1 =UpdateLearningrate(t, t)  =BinaryBackward( C  Y  , (cid:5)W)  C  (cid:2)W  , t)  As showed in [21], an approximate solution to this problem is:      H  =  B =  (cid:9)X(cid:9)l1 sign(H)  (cid:9)W (cid:9)l1 sign(W )  (5)  1 n 1 n  We can use an algorithm similar to Algorithm 1 to train XNOR.|
|||The structure of our CNN is:  (32)C5  S  M P 3  N  (32)C5  S  M P 3 N  (64)C5  S  AP 3  10F C  SOF T M AX  (22) Where C5 is a 5  5 convolution layer, S is a sigmoid activation layer, MP3 is a max-pooling layer with kernel size 3 and stride 2, AP3 is a average-pooling layer with kernel size 3 and stride 2, N is a LRN layers, FC is a fully connected layer and SOFTMAX is a softmax loss layer.|
|||Since our CNN structure is not as complex as ConvNet [4] (ConvNet has six convolutional layers and two fully connected layers and each layer has more perceptions), our baseline (without using any binary approximation) accuracy is not as high as theirs.|
||3 instances in total. (in iccv2017)|
|1337|Zhou_Adaptive_Feeding_Achieving_ICCV_2017_paper|In our case, we make use of CompACT-Deep which incorporates CNN features into the cascading detectors.|
|||Object detection via a multiregion and semantic segmentation-aware CNN model.|
|||Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers.|
||3 instances in total. (in iccv2017)|
|1338|Geng_Learning_Image_and_ICCV_2015_paper|However, the parameters of CNN is shared by all the pairs.|
|||Next, as shown in Algorithm 3, we solve for X and CNN with fixed T .|
|||d) Weighted Matrix Factorization (WMF) [30]: It decomposes the userimage matrix into latent user and image features by the weighted matrix factorization [13] and uses CNN to regress images to the image vectors.|
||3 instances in total. (in iccv2015)|
|1339|Deep Joint Rain Detection and Removal From a Single Image|Then, we consider rain accumulation and heavy rain situations, where we generalize our CNN to a recurrent model.|
|||We compare the four versions of our approaches, JORDER(one version of our methods that has only one convolution path in each recurrence without using dilated convolutions), JORDER (Section 4), JORDER-R (Section 5.1), JORDER-R-DEVEIL (Section 5.2) with five image decomposition (ID) [23], state-of-the-art methods: CNN-based rain drop removal (CNN) [13], discriminative sparse coding (DSC) [28], layer priors (LP) [26] and a common CNN baseline for image processing  SRCNN [22], trained for deraining.|
|||Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising.|
||3 instances in total. (in cvpr2017)|
|1340|Sohn_Unsupervised_Domain_Adaptation_ICCV_2017_paper|Face Recognition Engine  Our face recognition engine is also on a deep CNN trained on CASIA-webface dataset [40].|
|||When frame-level features are aggregated by discriminator-guided fusion, the three-way model F improves its performance to 95.38%, which is highly competitive to the performance of previous state-of-the-art face recognition engines such as FaceNet [26] (95.12%), CenterFace [36] (94.9%), or CNN with different feature aggregation methods [39] (e.g., 95.20% with average pooling) as shown in Table 3.|
|||Unconstrained  face verification using deep cnn features.|
||3 instances in total. (in iccv2017)|
|1341|Qi_Hierarchically_Gated_Deep_CVPR_2016_paper|The CNN models have been generalized to scene labeling.|
|||We compare with both CNN and LSTM paradigms of state-of-the-art methods.|
|||The results show that the proposed HGDN model achieves very competitive accuracy on both datasets, outperforming both CNN and LSTM paradigms when no extra training data are involved in pretraining the model.|
||3 instances in total. (in cvpr2016)|
|1342|cvpr18-Single View Stereo Matching|[4] propose the first CNN framework that predicts the depth in a coarse-tofine manner.|
|||[1] provide a novel insight by incorporating pair-wise depth relation into CNN training.|
|||Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||3 instances in total. (in cvpr2018)|
|1343|Li_Image2song_Song_Retrieval_ICCV_2017_paper|These works [17, 18, 25] commonly utilized deep CNN and RNN to encode the image and corresponding sentences into a common embedding space, respectively.|
|||However, different from most of the previous works that just make use of the CNN features in the top K regions [15, 47], we use the whole prediction results to  Fine-tuned model  Parameter   transferring  CNN  CNN  tree dress flower stock smiling  red  .|
|||We adopt the powerful VGG net [33] as the CNN architecture.|
||3 instances in total. (in iccv2017)|
|1344|Multiple Instance Detection Network With Online Instance Classifier Refinement|Meanwhile, recent efforts tend to combine MIL and CNN by either using CNN as an off-the-shelf feature extractor [3, 7, 28, 30, 31] or training an end-to-end MIL network [4, 16].|
|||Recently, some researchers combined CNN and MIL to train an end-to-end network for WSOD [4, 16, 24].|
|||[24] trained a CNN network using the maxpooing MIL strategy to localize objects.|
||3 instances in total. (in cvpr2017)|
|1345|cvpr18-LAMV  Learning to Align and Match Videos With Kernelized Temporal Layers|RMAC is a pooling layer that extracts bounding boxes from an arbitrary activation map in a CNN stack, and pools them into a fixed-size vector.|
|||The CNN can be fine-tuned [12], but we found that a pre-trained CNN works just as well in a context where the type of images to match is not known in advance.|
|||Particular object retrieval with integral max-pooling of CNN activations.|
||3 instances in total. (in cvpr2018)|
|1346|Conigliaro_The_S-Hock_Dataset_2015_CVPR_paper|The CNN is composed by 5 layers: an input layer followed by 2 sets of convolution-pooling layers (see Fig.|
|||[33], WArCo [43], CNN and SAE but in neural networks approaches the computation workload is much smaller.|
|||The ice rink information increases the accuracy by approximately 2% on both CNN and SAE frameworks.|
||3 instances in total. (in cvpr2015)|
|1347|Deep Hashing Network for Unsupervised Domain Adaptation|We implement the neural network as a deep CNN which consists of 5 convolution layers conv1 conv5 and 3 fully connected layers fc6 fc8 followed by a loss layer.|
|||The feature representations transition from generic to task-specific as one goes up the layers of a deep CNN [49].|
|||Network Architecture: Owing to the paucity of images in a domain adaptation setting, we circumvent the need to train a deep CNN with millions of images by adapting the pre-trained VGG-F [8] network to the DAH.|
||3 instances in total. (in cvpr2017)|
|1348|Curtis_Wigington_Start_Follow_Read_ECCV_2018_paper|3a) that is fed to a CNN to regress (xi+1, yi+1, i+1) (Fig.|
|||A CNN regresses a transform change (d) used to compute the next transformation (e).|
|||The architecture of the LF is a 7-layer CNN with 3x3 kernels and 64, 128, 256, 256, 512, and 512 feature maps on the 6 convolution layers.|
||3 instances in total. (in eccv2018)|
|1349|cvpr18-Revisiting Salient Object Detection  Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects|Some CNN based methods exploit superpixel and object region proposals to achieve accurate salient object detection [9, 19, 17, 22, 39, 18].|
|||Such methods follow a multi-branch architecture where a CNN is used to extract semantic information across different levels of abstraction to generate an initial saliency prediction.|
|||[26] integrate local and global features through a CNN that is structured as a multi-resolution grid.|
||3 instances in total. (in cvpr2018)|
|1350|Chanho_Kim_Multi-object_Tracking_with_ECCV_2018_paper|For tracking multiple people, a recent state-of-the-art batch approach [38] relies upon person re-identification techniques which leverage a deep CNN network that can recognize a person that has left the scene and re-entered.|
|||(a) The Bilinear LSTM network with a multiplicative relationship between the memory and the input CNN features.|
|||Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||3 instances in total. (in eccv2018)|
|1351|cvpr18-Detect Globally, Refine Locally  A Novel Approach to Saliency Detection|Particularly, a recurrent module is employed to progressively refine the inner structure of the CNN over multiple time steps.|
|||utilize multi-scale features extracted from a deep CNN via exploiting contextual information.|
|||A hierarchical recurrent CNN is adopted to progressively recover image details of saliency maps through integrating local context information.|
||3 instances in total. (in cvpr2018)|
|1352|Mottaghi_A_Coarse-to-Fine_Model_2015_CVPR_paper|We refer to them as local, because typically CNN units respond on portions of the objects and implicitly act as a part detector.|
|||We use the CNN implementation of [10], but use only five convolutional layers to compute the features.|
|||Note that the 1-layer hierarchy is already better than the current state-of-the-art (compare its results to DPM-VOC+VP [22] in Table 1, which is the state-of-theart in viewpoint estimation) partially because of the powerful CNN features.|
||3 instances in total. (in cvpr2015)|
|1353|Yang_Shi_Question_Type_Guided_ECCV_2018_paper|The authors in [27] show that post-processing CNN with region-specific image features [3] such as Faster R-CNN [22] can lead to an improvement of VQA performance.|
|||The authors in [27] show that post-processing CNN with region-specific image features [3] can lead to an improvement of VQA performance.|
|||In [4], authors propose a baseline that combines LSTM embedding of the question and CNN embedding of the image via a point-wise multiplication followed by a multi-layer perceptron classifier.|
||3 instances in total. (in eccv2018)|
|1354|Li_Iterative_Instance_Segmentation_CVPR_2016_paper|In the CNN era, Hariharan et al.|
|||[35] show that inference in CRFs can be viewed as recurrent neural nets and trained together with a CNN to label pixels, resulting in large gains.|
|||Segmentation System  For our segmentation system, we use a CNN that takes a 224  224 patch as input and outputs a 50  50 heatmap prediction.|
||3 instances in total. (in cvpr2016)|
|1355|Yangyu_Chen_Less_is_More_ECCV_2018_paper|For example, it takes millions of floating point calculation to extract a frame-level visual feature for a moderate-sized CNN model.|
|||If the policy decides to pick the current frame, the frame feature will be extracted by a pretrained CNN and embedded into a lower dimension, then passed to the encoder unit, and the template will be updated:  g  gt.|
|||If certain frame is picked, the pretrained CNN will be used to extract visual features of this frame.|
||3 instances in total. (in eccv2018)|
|1356|cvpr18-Fully Convolutional Adaptation Networks for Semantic Segmentation|To achieve this target, the techniques such as multiple instance learning [20], EM algorithm [18] and constrained CNN [19] are exploited in the literature.|
|||Specifically, a pre-trained CNN is u 6812  Fully Convolutional NetworksAANAANSource-domainImageTarget-domainImageSemanticClassificationSoftmaxLossSemantic SegmentationJoint LearningSemantic Feature(Source domain)Semantic Feature(Target domain)Domain DiscriminatorASPP1x1ConvSource DomainTarget Domain...Representation Adaptation Network (RAN)SemanticClassificationTest Phaseone image to that of one domain (  Gl t of the target domain) by averaging Gl over all the images in target domain.|
|||Suppose every convolutional layer l in the CNN has Nl response maps, where Nl is the number of channels, and the size of each response map is Hl  Wl, where Hl and Wl denotes the height and width of the map, respectively.|
||3 instances in total. (in cvpr2018)|
|1357|Synthesizing Normalized Faces From Facial Identity Features|Compared to a regular CNN with the same decoder capacity, our method reproduces finer details.|
|||CNN w/o Data Aug. FC w/ Data Aug. CNN w/ Data Aug.  w/ FaceNet loss  w/o FaceNet loss  Input  FN L2 error : 0.42  FN L2 error: 0.8  Figure 9.|
|||Output from various configurations of our system: CNN texture decoder trained with only 1K raw images, fully-connected decoder and CNN trained on 1M images using the data augmentation technique of Sec.|
||3 instances in total. (in cvpr2017)|
|1358|Kruthiventi_Saliency_Unified_A_CVPR_2016_paper|Proposed (CNN) refers to the results from the CNN alone, whereas the Proposed (CNN+CRF) refers to the final binary segmentation results obtained after refining the maps obtained from CNN using CRF.|
|||4 illustrate that our method outperforms the winner of LSUN Saliency Challenge 2015, JuntingNet [49, 50], a CNN based model, by a significant margin.|
|||Proposed (CNN) refers to the results using the CNN alone, whereas the Proposed (CNN+CRF) refers to the final results obtained after refining the CNNs output using CRF.|
||3 instances in total. (in cvpr2016)|
|1359|What Can Help Pedestrian Detection_|In [33], given region proposals generated by a Region Proposal Network (RPN), CNN features extracted by an RoI pooling layer [13] are fed into a boosted forest; while in Cai et al.|
|||We conjecture the reason is that in deep convolutional networks, CNN features are more discriminative than hand-crafted features like HOG.|
|||Training Details  Loss Function During the training phase, besides the raw image and groundtruth bounding boxes for standard Faster R-CNN framework, the HyperLearner also takes a channel feature map as its supervisor, which is typically generated by another CNN (e.g., semantic segmentation and edge).|
||3 instances in total. (in cvpr2017)|
|1360|cvpr18-Fooling Vision and Language Models Despite Localization and Attention Mechanism|relying on a CNN for image encoding and a RNN for question encoding [17, 49, 62].|
|||For each region, the model uses a CNN to compute the embedding and then uses an RNN to generate a sequence of tokens from the embedding to form the caption.|
|||That is, to answer a question, a model first computes an attention map, which is a weight distribution over local features extracted from a CNN based on the image and the question.|
||3 instances in total. (in cvpr2018)|
|1361|Xiankai_Lu_Deep_Regression_Tracking_ECCV_2018_paper|The FCNT makes the first effort to learn regression networks over two CNN layers.|
|||Ensemble learning is exploited in the STCT to select CNN feature channels.|
|||3.3 Convolutional Layer Connection  It has been known that CNN models consist of multiple convolutional layers emphasizing different levels of semantic abstraction.|
||3 instances in total. (in eccv2018)|
|1362|Yan-Pei_Cao_Learning_to_Reconstruct_ECCV_2018_paper|[26] extends 3D-EPN by introducing a local 3D CNN to perform patch-level surface refinement.|
|||Therefore, the computational and memory cost are significantly reduced, while the OctNet itself, as a processing module, can be plugged into most existing 3D CNN architectures transparently.|
|||Without using octree-based data structures, 3D-EPN employs a hybrid approach, which first completes the input model at a low resolution (323) via a 3D CNN and then uses voxels from similar high-resolution models in the database to produce output distance volumes at 1283 voxel resolution.|
||3 instances in total. (in eccv2018)|
|1363|cvpr18-Deep Learning Under Privileged Information Using Heteroscedastic Dropout|The loss value of a CNN can be viewed as analogous to the slack variables.|
|||Image Classification We show the CNN architecture we used in our experiments, along with the re-parameterization trick and heteroscedastic dropout connections.|
|||Where we report No-x, we describe the results of a classical CNN learning method.|
||3 instances in total. (in cvpr2018)|
|1364|cvpr18-Semantic Video Segmentation by Gated Recurrent Flow Propagation|This is fed as the hidden state to a gated recurrent unit (GRU) where the other input is the estimate xt computed by a single frame CNN for semantic segmentation.|
|||Due to GPU memory constraints, the per-frame semantic segmentation CNN computations had to be performed one frame at a time with only the final output saved in memory.|
|||2) where the parameters of the STGRU gf and the parameters of the static segmentation CNN s are refined, while the parameters of the FlowNet f are frozen.|
||3 instances in total. (in cvpr2018)|
|1365|Lele_Chen_Lip_Movements_Generation_ECCV_2018_paper|Compared with time-delayed RNN proposed in [29], CNN can learn delay from the dataset rather than set it as a hyper-parameter.|
|||For video stream, we use four 3D CNN layers to extract video features.|
|||We fine-tune the FlowNet [11], which is pre-trained on FlyingChairs dataset, to extract optical flows, then apply four 3D CNN layers to extract features.|
||3 instances in total. (in eccv2018)|
|1366|Durand_MANTRA_Minimum_Maximum_ICCV_2015_paper|Internal CNN representations trained on large scale datasets [17] currently provide state-of-the-art features for various tasks, e.g., image classification or object detection [24, 10].|
|||Despite their excellent performances, current CNN architectures only carry limited invariance properties.|
|||Features Each image region is described using deep features computed with Caffe CNN library [14].|
||3 instances in total. (in iccv2015)|
|1367|Ji_SurfaceNet_An_End-To-End_ICCV_2017_paper|For instance, [32] uses a CNN instead of hand-crafted features for finding correspondences among image pairs and [10] predicts normals for depth maps using a CNN, which improves the depth map fusion.|
|||In [10], a CNN is trained to predict  the normals of a given depth map based on image appearance.|
|||Using the 2D representation, standard 2D CNN architectures can be applied.|
||3 instances in total. (in iccv2017)|
|1368|Zhang_Zero-Shot_Learning_via_CVPR_2016_paper|Except for [2] where AlexNet [18] is utilized for extracting CNN features, for all the other methods we use vgg-verydeep-19 [33] CNN features.|
|||In order to show  2Our code and CNN features can be downloaded at https://  zimingzhang.wordpress.com/.|
|||This is probably because the global patterns such as texture in the images are similar, leading to highly similar yet discriminative CNN features.|
||3 instances in total. (in cvpr2016)|
|1369|Qian_Multi-Scale_Deep_Learning_ICCV_2017_paper|Recently, inspired by the success of convolutional neural networks (CNN) in many computer vision problems, deep CNN architectures [1, 48, 27, 51, 44, 50, 4] have been widely used for person re-id.|
|||However, recent efforts [9, 39] on visualizing what each layer of a CNN actually learns reveal that higher-layers of the network capture more abstract semantic concepts at global scales with less spatial information.|
|||Multi-scale triplet cnn for person re-identification.|
||3 instances in total. (in iccv2017)|
|1370|Tao_Attributes_and_Categories_2015_CVPR_paper|The CNN is trained using ImageNet categories.|
|||the second fully connected layer of a CNN [22] as an additional feature, as it has been shown the activations of the top layers of a CNN capture high-level category-related information [47].|
|||The CNN is trained using ImageNet categories.|
||3 instances in total. (in cvpr2015)|
|1371|Wu_Deep_Facial_Action_ICCV_2017_paper|To address the overfitting problem of CNN due to limited training data, Han et al.|
|||[14] proposed an incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN through introducing an incremental boosting layer and a new loss function.|
|||Other than using CNN to capture spatial representation only, Jaiswal and Valstar [9] combined convolutional neural networks and bi-directional long short-term memory neural networks (CNN-BLSTM) to jointly learn shape, appearance and dynamics in a deep learning manner for AU recognition.|
||3 instances in total. (in iccv2017)|
|1372|Hang_Zhao_The_Sound_of_ECCV_2018_paper|We set the learning rate of the audio analysis network and the audio synthesizer both as 0.001, and the learning rate of the video analysis network as 0.0001 since we adopt a pre-trained CNN model on ImageNet.|
|||: Geometry-guided CNN for self supervised video representation learning (2018)  14.|
|||: Cnn architectures for largescale audio classification.|
||3 instances in total. (in eccv2018)|
|1373|Niu_Hierarchical_Multimodal_LSTM_ICCV_2017_paper|Particularly, an encoder-decoder framework [17] [3] is often adopted by those methods, where a CNN is used to represent an image, and an RNN is used to generate descriptions conditioned on the image representation.|
|||The CNN part of our model comes from Karpathy et al.|
|||In particular, we first extract image features by using the CNN and retrieve the nearest sentence vector hd,0  {hd,0}d=D d=1 in the embedding space, which is regarded as the caption for the image.|
||3 instances in total. (in iccv2017)|
|1374|Multiple People Tracking by Lifted Multicut and Person Re-Identification|In this section, we investigate several CNN architectures for re-identification for the multi-person tracking task.|
|||Our basic CNN architecture is VGG-16 Net [26].|
|||Learning by tracking: Siamese CNN for robust target association.|
||3 instances in total. (in cvpr2017)|
|1375|cvpr18-Embodied Question Answering|Our agent takes egocentric 224224 RGB images from the House3D renderer as input, which we process with a CNN consisting of 4 t55 Conv, BatchNorm, ReLU, 22 MaxPoolu blocks, producing a fixed-size representation.|
|||As such, we pretrain the CNN under a multi-task pixel-to-pixel prediction framework.|
|||Treating the above CNN as a shared base encoder network, we train multiple decoder heads for 1) RGB reconstruction, 2) semantic segmentation, and 3) depth estimation (annotations for which are available from the House3D renderer).|
||3 instances in total. (in cvpr2018)|
|1376|cvpr18-Learning Monocular 3D Human Pose Estimation From Multi-View Images|8438  Formally, let f denote the mapping, with parameters , encoded by a CNN taking a monocular image I  Rwh3 as input and producing a 3D human pose p = f(I)  R3NJ as output, where NJ is the number of human joints in our model and the kth column of p denotes the position of joint k relative to the pelvis.|
|||Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue.|
|||Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision.|
||3 instances in total. (in cvpr2018)|
|1377|Xuan_Chen_Focus_Segment_and_ECCV_2018_paper|In addition, the deep convolution features extracted by each CNN could not be fully utilized, thus reducing computational efficiency.|
|||A multi-scale 3D CNN named DeepMedic proposed by [14] has two convolutional pathways, in order to better utilize multiscale features for prediction.|
|||Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon, D.K., Rueckert, D., Glocker, B.: Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation.|
||3 instances in total. (in eccv2018)|
|1378|Kobayashi_Structured_Feature_Similarity_CVPR_2016_paper|Then, the method is also applied to CNN features [5] on scene classification tasks using Scene-15 [12] and SUN397 [30] datasets.|
|||52.10.3 54.30.3  Ours  periment we employ the very deep CNN model [23] trained on ImageNet dataset and extract the features as in [5] by using pool5 layer to produce 51288 TENSOR feature from an image of 256256 pixels in scene-15 and 5121212 feature from an image of 384384 pixels in SUN-3975.|
|||As shown in Table 4, the proposed method favorably improves the performance of the original CNN feature and is competitive with the state-ofthe-art method [32] which employs CNN features trained on a large scene dataset.|
||3 instances in total. (in cvpr2016)|
|1379|cvpr18-Deep Cost-Sensitive and Order-Preserving Feature Learning for Cross-Population Age Estimation|Since CNN can capture useful lowlevel features independent of the training data [39, 31], the main purpose of this stage is to extract useful and transferable low-level aging features from the large sized source population and then transfer them to the target population.|
|||Inspired by the great success of CNN on learning hierarchical feature representations, our proposed Deep Cross-Population (DCP) age estimation model deals with this problem by using a new two-stage learning framework, which first learns transferable low-level aging features in a  400  novel cost-sensitive feature learning stage, and then learns to align high-level aging features across two populations in a novel order-preserving feature alignment stage.|
|||Ordinal regression with multiple output CNN for age estimation.|
||3 instances in total. (in cvpr2018)|
|1380|Dong_Class_Rectification_Hard_ICCV_2017_paper|For multi-class classification CNN model training (CNN model details in Network Architecture, Sec.|
|||We adopted the five layers CNN network architecture of  DeepID2 [42] as the basis for training all six imbalanced data learning methods including both our CRL models (C&I), the same for LMLE as reported in [21].|
|||We also tested the training time cost of LMLE independently on an identical hardware setup as for CRL: LMLE took 388 hours to train whilst CRL  4We trained an independent LMLE CNN model for each attribute label.|
||3 instances in total. (in iccv2017)|
|1381|Huang_A_Coarse-Fine_Network_ICCV_2017_paper|spired by object proposals in object detection, part-based R-CNN [49] extracts CNN features from bottom-up proposals and learns whole-object and part detectors with geometric constraints.|
|||[40] jointly trained a CNN and a graphical model, incorporating long-range spatial relations to remove outliers on the regressed confidence maps.|
|||Part-stacked cnn for  fine-grained visual categorization.|
||3 instances in total. (in iccv2017)|
|1382|Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper|We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize convolutional networks with arbitrary linear constraints on the structured output space of pixel labels.|
|||Our Constrained CNN is guided by weak annotations and trained end-to-end.|
|||Learning The CNN architecture used in our experiments is derived from VGG 16-layer network [29].|
||3 instances in total. (in iccv2015)|
|1383|cvpr18-Residual Dense Network for Image Super-Resolution|However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance.|
|||They neglect to adequately utilize information from each Conv layer and only adopt CNN features from the last Conv layer in LR space for upscaling.|
|||When compared with persistent CNN models ( SRDenseNet [31] and MemNet [26]), our RDN performs the best on all datasets with all scaling factors.|
||3 instances in total. (in cvpr2018)|
|1384|Chang_Liu_Linear_Span_Network_ECCV_2018_paper|The CNN based approaches achieve huge performance gain compared with the conventional approaches.|
|||8, it is illustrated that the best conventional approach is SE with F-measure (ODS) of 0.739 and all the CNN based approaches achieve much better detection performance.|
|||We explore the Linear Span Units (LSUs) to learn a CNN based mask reconstruction model.|
||3 instances in total. (in eccv2018)|
|1385|cvpr18-Egocentric Activity Recognition on a Budget|[4] have used Long Short-Term Memory (LSTM) networks over features obtained from a CNN applied to single frames of videos.|
|||The CNN acts as a feature extractor while the LSTM captures the temporal structure of the data.|
|||Then, we unfreeze the 3 last blocks of the CNN and train again for 20 more epochs.|
||3 instances in total. (in cvpr2018)|
|1386|Sumer_Self-Supervised_Learning_of_ICCV_2017_paper|Toshev and Szegedy [31] estimated joint locations directly regressing in a CNN architecture.|
|||Instead of simply regressing joint locations, Chen and Yuille [10] learned pairwise part relations combining CNN with graphical models.|
|||The two auxiliary tasks are trained in a Siamese CNN architecture and the learned features are eventually used as pose embeddings in order to retrieve similar postures and estimate pose.|
||3 instances in total. (in iccv2017)|
|1387|Learning From Simulated and Unsupervised Images Through Adversarial Training|Figure 7 and Table 2 compare the performance of a gaze estimation CNN trained on synthetic data to that of another CNN trained on refined synthetic data, the output of SimGAN.|
|||Training the CNN on the refined images outperforms the state-of-the-art on the MPIIGaze dataset, with a relative improvement of 21%.|
|||Quantitative Results: We train a fully convolutional hand pose estimator CNN similar to Stacked Hourglass Net [25] on real, synthetic and refined synthetic images of the NYU hand pose training set, and evaluate each model on all real images in the NYU hand pose test set.|
||3 instances in total. (in cvpr2017)|
|1388|Yang_Shen_Egocentric_Activity_Prediction_ECCV_2018_paper|[22] proposed a two-stream network using CNN to analyze appearance and motion information separately.|
|||[8] using the observed gaze; (b) Two-stream CNN results with object-cnn, SVMfusion and joint training [22]; (c) 2D and 3D Ego ConvNet results (H: Hand mask, C: Camera/Head motion, M: Saliency map) [28].|
|||Poleg, Y., Ephrat, A., Peleg, S., Arora, C.: Compact CNN for indexing egocentric  videos.|
||3 instances in total. (in eccv2018)|
|1389|Yiran_Zhong_Open-World_Stereo_Video_ECCV_2018_paper|[18] argued that the depth CNN predictor can be learned without a pose CNN predictor.|
|||It consists of a CNN Feature-Net, a Match-Net and two convolutional-LSTM recurrent blocks to learn temporal dynamics in the scene.|
|||Garg, R., Kumar, B.V., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||3 instances in total. (in eccv2018)|
|1390|Ge_Robust_3D_Hand_CVPR_2016_paper|Robust 3D Hand Pose Estimation in Single Depth Images:  from Single-View CNN to Multi-View CNNs  Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann  Institute for Media Innovation  Nanyang Technological University, Singapore  {ge0001ao, hliang1}@e.ntu.edu.sg, {jsyuan, danielthalmann}@ntu.edu.sg  Abstract  Articulated hand pose estimation plays an important role in human-computer interaction.|
|||nal planes, and each projected image is then fed into a separate CNN to generate a set of heat-maps for hand joints following similar pipeline in [29].|
|||Compared to the method of single view CNN in [29], our proposed method of multi-view CNNs has the following advantages:   In the single view CNN, the depth of a hand joint is taken as the corresponding depth value at the estimated 2D position, which may result in large depth estimation errors even if the estimated 2D position is only slightly deviated from the true joint position, as shown in Fig.|
||3 instances in total. (in cvpr2016)|
|1391|Veit_Learning_Visual_Clothing_ICCV_2015_paper|Dataset  Training the Siamese CNN to learn the function f requires positive and negative examples of clothing pairs.|
|||Then, we show how to train a Siamese CNN to learn a feature transformation from the image space into the latent style space.|
|||Training a Siamese CNN requires positive (similar style) as well as negative (dissimilar style) training examples.|
||3 instances in total. (in iccv2015)|
|1392|cvpr18-Progressive Attention Guided Recurrent Network for Salient Object Detection|Moreover, in most of the state-of-the-art CNN based methods, saliency values are estimated by dealing with multi-scale side-output convolutional features.|
|||[2] propose a SCA-CNN network that incorporates spatial and channelwise attention in CNN for image captioning.|
|||Consider a CNN which is composed of L convolutional blocks.|
||3 instances in total. (in cvpr2018)|
|1393|Instance-Aware Image and Sentence Matching With Selective Multimodal LSTM|F is the number of feature maps in the last convolutional layer of CNN while G is twice the dimension of hidden states in the BLSTM.|
|||We regard the output vector of the last fully-connected layer in the CNN as the global context m  RD for the image, and the hidden state at the last timestep in a sentence-based LSTM as the global context n  RE for the sentence.|
|||For efficient optimization, we fix the weights of CNN and use pretrained weights as stated in Section 4.2.|
||3 instances in total. (in cvpr2017)|
|1394|Youngjae_Yu_A_Joint_Sequence_ECCV_2018_paper|We use bidirectional LSTM networks (BLSTM) encoder [40,41] for word sequence and CNN encoder for video frames.|
|||For visual domain, we use 1-d CNN encoder representation for vt, hcnn  R2,048 instead, xv,t = [hcnn  w,t, hb  v,t , vt].|
|||: CNN Architectures for Large-Scale Audio Classification.|
||3 instances in total. (in eccv2018)|
|1395|Xin_Li_Contour_Knowledge_Transfer_ECCV_2018_paper|However, training deep CNN models requires a large amount of pixellevel annotations, which have to be created manually in a time-consuming and expensive way.|
|||This enables the deep CNN network to learn detailed object shape information and improve the overall performance.|
|||Feeding more training samples to the deep CNN models can lead to better performance.|
||3 instances in total. (in eccv2018)|
|1396|Zimmermann_Learning_for_Active_ICCV_2017_paper|Previous work  High performance of image-based models is demonstrated in [14], where a CNN pooling results from multiple rendered views outperforms commonly used 3D shape descriptors in object recognition task.|
|||Their CNN policy successively proposes views to capture with RGB camera to minimize categorization error.|
|||The proposed 3D-reconstruction CNN outperforms a state-ofthe-art approach by 20% in recall, and it is shown that when learning is coupled with planning, recall increases by additional 8% on the same false positive rate.|
||3 instances in total. (in iccv2017)|
|1397|Shuangjun_Liu_Inner_Space_Preserving_ECCV_2018_paper|The key components of the ISP-GPM are: (1) a CNN interface converter to make the LDPD compatible with the first convolutional layer of the ISP-GPM interface, and (2) a generative pose machine to generate reposed figures using the regression structure of hourglass networks when stacked in a cGAN framework in order to force the pose descriptor into the regenerated images.|
|||4.1 CNN Interface Converter We employed an LDPD in the 2D image domain, which in the majority of the human pose dataset such as Max Planck institute informatics (MPII) [3] and Leeds sports pose (LSP) [29] is defined as the vector of 2D joint position  Inner Space Preserving Generative Pose Machine  7  coordinates.|
|||To make this descriptor compatible with the convolutional layer interface of ISP-GPM, we need a CNN interface converter.|
||3 instances in total. (in eccv2018)|
|1398|cvpr18-FOTS  Fast Oriented Text Spotting With a Unified Network|[14] propose deep recurrent models to encode the max-out CNN features and adopt CTC to decode the encoded sequence.|
|||[31] use an attention-based sequenceto-sequence structure to automatically focus on certain extracted CNN features and implicitly learn a character level language model embodied in RNN.|
|||R2cnn: Rotational region cnn for orientation robust scene text detection.|
||3 instances in total. (in cvpr2018)|
|1399|Qingqiu_Huang_Person_Search_in_ECCV_2018_paper|The IDE descriptor is a CNN feature of  12  Q. Huang, W. Liu, D. Lin  the whole person instance, extracted by a Resnet-50 [12], which is pre-trained on ImageNet [31] and finetuned on the training set of CSM.|
|||From the results in Table 4, we can see that: (1) Even with a very powerful CNN trained on a large-scale dataset, matching portrait and candidates by visual cues cannot solve the person search problem well due to the big gap of visual appearances between the portraits and the candidates.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||3 instances in total. (in eccv2018)|
|1400|Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper|Tracking by discriminative appearance modeling: One simple yet effective manner of using deep networks for visual tracking is to directly apply correlation filters on the multi-dimensional feature maps of deep Convolutional Neural Networks (CNNs), where the pre-trained CNN model is fixed.|
|||Another category of deep trackers [34, 35, 24] update a pre-trained CNN online to account for the target-specific appearance at test time.|
|||[4] uses a CRF to refine segmentation results obtained from a CNN and Zheng et al.|
||3 instances in total. (in eccv2018)|
|1401|Learning From Synthetic Humans|In Section 4 we describe our CNN architecture for human body part segmentation and depth estimation.|
|||[8] learn a CNN for optical flow estimation using synthetically generated images of rendered 3D moving chairs.|
|||Learning camera viewpoint using cnn to improve 3D body pose estimation.|
||3 instances in total. (in cvpr2017)|
|1402|Huang_Wavelet-SRNet_A_Wavelet-Based_ICCV_2017_paper|Wavelet-SRNet: A Wavelet-based CNN for Multi-scale Face Super Resolution  Huaibo Huang1 ,2 ,3, Ran He1 ,2 ,3, Zhenan Sun1 ,2 ,3 and Tieniu Tan1 ,2 ,3  1School of Engineering Science, University of Chinese Academy of Sciences  2Center for Research on Intelligent Perception and Computing, CASIA  3National Laboratory of Pattern Recognition, CASIA  huaibo.huang@cripac.ia.ac.cn, {rhe, znsun, tnt}@nlpr.ia.ac.cn  Abstract  Most modern face super-resolution methods resort to convolutional neural networks (CNN) to infer highresolution (HR) face images.|
|||To address these challenges, this paper presents a wavelet-based CNN approach that can ultra-resolve a very low resolution face image of 16  16 or smaller pixelsize to its larger version of multiple scaling factors (2, 4, 8 and even 16) in a unified framework.|
|||Different from conventional CNN methods directly inferring HR images, our approach firstly learns to predict the LRs corresponding series of HRs wavelet coefficients before reconstructing HR images from them.|
||3 instances in total. (in iccv2017)|
|1403|Tanmay_Gupta_Imagine_This_Scripts_ECCV_2018_paper|The location and scale predictors have an identical feature computation backbone comprising of a CNN and a bidirectional LSTM.|
|||The CNN encodes Vi1 (8 sub-sampled frames concatenated along the channel dimension) as a set of convolutional feature maps which capture appearance and positions of previous entities in the scene.|
|||Similar to the layout composers feature computation backbone, Q consists of a CNN to independently encode every frame of Vi1 and an LSTM to encode (T, ei) which are concatenated together along with a 2-D coordinate grid to get per-frame feature maps.|
||3 instances in total. (in eccv2018)|
|1404|Oberweger_Training_a_Feedback_ICCV_2015_paper|We use a first CNN 1 to predict an initial estimate of the 3D pose given an input depth image of the hand.|
|||Pose space  We use a CNN to implement the synthesizer, and we train it using the set T of annotated training pairs.|
|||For [21], we use their best CNN that incorporates a 30D pose embedding.|
||3 instances in total. (in iccv2015)|
|1405|Xi_Zhang_Attention-aware_Deep_Adversarial_ECCV_2018_paper|It consists of three major components: (1) a feature learning module that uses CNN or MLP to extract high level semantic representations for the multi-modal data; (2) an attention module that generates the adaptive attention masks and divides the feature representations into the attended and unattended feature representations; and (3) a hashing module that focuses on learning the binary codes for the multi-modal data.|
|||Pairwise relationship guided deep hashing (PRDH) [36] also adopts deep CNN models to learn feature representations and hash codes simultaneously.|
|||Since the experimental settings of PRDH in [37] are different from those of the proposed method, we carefully implement PRDH using the same CNN network and the same settings for a fair comparison.|
||3 instances in total. (in eccv2018)|
|1406|Al-Halah_Fashion_Forward_Forecasting_ICCV_2017_paper|Our experiments analyze the tradeoffs of various forecasting models and representations, the latter of which reveals the advantage of unsupervised style discovery based on visual semantic attributes compared to off-the-shelf CNN representations, including those fine-tuned for garment classification.|
|||In particular, we train a CNN with an AlexNet-like architecture on the DeepFashion dataset to perform clothing classification (see Supp.|
|||Since fashion elements can be local properties (e.g., v-neck) or global (e.g., a-line), we use the CNN to extract two representations at different abstraction levels: 1) FC7: features extracted from the last hidden layer; 2) M3: features extracted from the third max pooling layer after the last convolutional layer.|
||3 instances in total. (in iccv2017)|
|1407|Interspecies Knowledge Transfer for Facial Keypoint Detection|Unfortunately, training a CNN from scratch typically requires large amounts of labeled data, which can be timeconsuming and expensive to collect.|
|||While there are large datasets with human facial keypoint annotations (e.g., AFLW has 26000 images [23]), there are, unfortunately, no large datasets of animal facial keypoints that could be used to train a CNN from scratch (e.g., the sheep dataset from [51] has only 600 images).|
|||For this, we train a CNN that takes as input an animal image and warps it via a thin plate spline (TPS) [4] transformation.|
||3 instances in total. (in cvpr2017)|
|1408|cvpr18-Nonlinear 3D Face Morphable Model|Further, we learn the fitting algorithm to our nonlinear 3DMM, which is formulated as a CNN encoder.|
|||Since 3D vertices are not defined on a 2D grid, this representation will be parameterized as a vector, which not only loses the spatial relation of vertices, but also prevents it from leveraging the convenience of deploying CNN on 2D imagery.|
|||Also, the texture decoder DT is a CNN constructed by fractionally-strided convolution layers.|
||3 instances in total. (in cvpr2018)|
|1409|Bo_Dai_Rethinking_the_Form_ECCV_2018_paper|Particularly, the encoder-decoder paradigm [1], which uses a CNN [19] to encode visual features and then uses an LSTM net [6] to decode them into a caption, was shown to outperform classical techniques and has been widely adopted.|
|||Given an image I, a CNN first turns it into a multi-channel feature map V that preserves high-level spatial structures.|
|||During training, we first fix the CNN encoder and optimize the decoder with learning rate 0.0004 in the first 20 epochs, and then jointly optimize both the encoder and decoder, until the performance on the validation set saturates.|
||3 instances in total. (in eccv2018)|
|1410|cvpr18-Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network|Visual saliency detection based on multiscale IEEE Transactions on Image Processing,  deep CNN features.|
|||HCP: A flexible CNN framework for multi-label image classification.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||3 instances in total. (in cvpr2018)|
|1411|cvpr18-MoNet  Deep Motion Exploitation for Video Object Segmentation|Superior to employing fully-supervised CNN-based models to learn motion patterns [29], the DT layer is free of ground truth optical flow to learn a CNN model, and much simpler yet provides comparable performance (see results in Tab.|
|||[11] proposed a two-stream CNN to extract features from input frames and optical flow to jointly segment the object.|
|||For example, [3] proposed to independently process each frame using CNN without any temporal information.|
||3 instances in total. (in cvpr2018)|
|1412|cvpr18-FSRNet  End-to-End Learning Face Super-Resolution With Facial Priors|It is a consensus that end-to-end training is desirable for CNN [16], which has been validated in many areas, e.g., speech recognition [8] and image recognition [20].|
|||The authors also proposed a deep end-to-end persistent memory network to address the long-term dependency problem in CNN for image restoration [33].|
|||Wavelet-srnet: A wavelet-based CNN for multi-scale face super resolution.|
||3 instances in total. (in cvpr2018)|
|1413|3D Human Pose Estimation From a Single Image via Distance Matrix Regression|In particular, we first estimate 2D joints using a recent CNN detector [51].|
|||[33] addressed this limitation by augmenting the training data for a CNN with automatically synthesized images made of realistic textures.|
|||In contrast to recent CNN based methods for 3D human pose estimation [23, 46] we do not need to explicitly modify our networks to model the underlying joint dependencies.|
||3 instances in total. (in cvpr2017)|
|1414|Po-Yu_Huang_Efficient_Uncertainty_Estimation_ECCV_2018_paper|[24], popularize CNN architectures for dense predictions without any fully connected layers.|
|||DeepLab [5] replaced fully connected CRF(conditional random field) to the last layer of CNN for improving the performance.|
||2 instances in total. (in eccv2018)|
|1415|Yi_Wei_Quantization_Mimic_Towards_ECCV_2018_paper|Keywords: Model acceleration, model compression, quantization, mimic, object detection  1  Introduction  In recent years, CNN achieved great success on various computer vision tasks.|
|||3.2 Mimic  In popular CNN detectors, the feature map from feature extractors (e.g.|
||2 instances in total. (in eccv2018)|
|1416|Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals|Visual features Motivated by the recent success of recurrent convolutional neural networks (CNN) for video analysis [15], we visually represent each video frame using frame-level CNN features xv t = CNNc (vt).|
||| C3D [62]: a spatial-temporal CNN baseline, which is combined with a linear SVM.|
||2 instances in total. (in cvpr2017)|
|1417|Saito_Temporal_Generative_Adversarial_ICCV_2017_paper|Natural image generation  Supervised learning with Convolutional Neural Networks (CNNs) has recently shown outstanding performance in many tasks such as image classification [8, 9, 11] and action recognition [14, 16, 33, 43], whereas unsupervised learning with CNN has received relatively less attention.|
|||In supervised learning of videos, while a common approach is to use dense trajectories [45, 30, 29], recent methods have employed CNN and achieved state-of-the-art results [14, 16, 33, 43, 24, 46, 47].|
||2 instances in total. (in iccv2017)|
|1418|Huang_Centered_Weight_Normalization_ICCV_2017_paper|CNN architectures on CIFAR dataset  In this part, we highlight that the proposed centered weight normalization method also works well on several popular state-of-the-art CNN architectures3, including VGG [31], GoogLeNet [34], and residual network [14, 15].|
|||We  3The details of the used CNN architectures are shown in the supple mentary materials.|
||2 instances in total. (in iccv2017)|
|1419|Kuang-Jui_Hsu_Unsupervised_CNN-based_co-saliency_ECCV_2018_paper|In this work, the extractor f can be a pre-trained CNN model for image classification, e.g., AlexNet [43] or VGG19 [44], with the softmax function and the last fully connected layer removed.|
|||Our approach excels them by performing these steps simultaneously and adopting CNN models.|
||2 instances in total. (in eccv2018)|
|1420|cvpr18-Anticipating Traffic Accidents With Adaptive Loss and Large-Scale Incident DB|[22] train CNN to extract feature for action anticipation in self-supervised manner.|
|||Recognition of transitional action for short-term action prediction using discriminative temporal CNN feature.|
||2 instances in total. (in cvpr2018)|
|1421|Santiago_Cadena_Diverse_feature_visualizations_ECCV_2018_paper|We showcase our visualization approach on a CNN trained to predict re sponses to natural images in primary visual cortex of the primate brain.|
|||Complex (left) and simple (right) cells  tational diferences in the canonical directions between two architectures that would not have been observed with conventional activity maximization  6 Phase invariance in Primary visual cortex (V1)  As a inal practical use case, we applied our method to a three-layer CNN that has been trained to predict neural responses in V1 when monkeys are shown natural images (data from [4]; see also their Fig.|
||2 instances in total. (in eccv2018)|
|1422|cvpr18-Partially Shared Multi-Task Convolutional Neural Network With Local Constraint for Face Attribute Learning|Motivated by the success of convolutional neural network (CNN) [16, 35, 31, 6, 24, 10, 11], the deep CNN representations have been widely employed for face attribute exploit a face learning.|
|||In particular, Multi-task deep CNN (MCNN) is introduced by sharing the lower layers of network for all the attributes and sharing the higher layers for closely related attributes through a split structure.|
||2 instances in total. (in cvpr2018)|
|1423|Su_3D-Assisted_Feature_Synthesis_ICCV_2015_paper|As experiments (Sec 5.3) demonstrate, our feature augmentation scheme can further [31] learns to boost the performance of CNN features.|
|||In a different direction, by running a CNN classifier backwards, [1] is able to synthesize views of novel objects by using a manually specified input vector encoding the object and view, or to interpolate between multiple views of a given 3D model.|
||2 instances in total. (in iccv2015)|
|1424|cvpr18-Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation|VESPCN estimates the optical flow between input LR frames with a learned CNN to warp frames by a spatial transformer [13], and finally produces an HR frame through another deep network.|
|||Previous end-to-end CNN based VSR methods focus on explicit motion estimation and compensation to better reconstruct HR frames.|
||2 instances in total. (in cvpr2018)|
|1425|Litany_Deep_Functional_Maps_ICCV_2017_paper|[41] employed a classical (extrinsic) CNN architecture trained on huge training sets for  learning invariance to pose changes and clothing.|
|||[29] with the introduction of the geodesic CNN model, a deep learning architecture where the classical convolution operation is replaced by an intrinsic (albeit, non-shift invariant) counterpart.|
||2 instances in total. (in iccv2017)|
|1426|Balntas_Pose_Guided_RGBD_ICCV_2017_paper|Intuitively, our method aims to train a CNN in such a way that the distance in the resulting D  dimensional embedding space is analogous to the pose differences.|
|||In turn, in [6] a CNN was used to learn projections of 3D control points for acute object tracking, while in [16] a CNN is utilized in a probabilistic framework to perform analysis-by-synthesis as a final refinement step for the object pose estimation.|
||2 instances in total. (in iccv2017)|
|1427|Ionescu_Matrix_Backpropagation_for_ICCV_2015_paper|Turaga et al [39] learn a model end-to-end based on a CNN by optimizing a standard segmentation criterion.|
|||Our architecture is also complementary to structured output formulations such as MRFs[5, 33, 8] which have been  demonstrated to provide useful smoothing on top of highperforming CNN pixel classifier predictions [30].|
||2 instances in total. (in iccv2015)|
|1428|Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper|We took pre-trained face-recognition CNN network, namely FaceNet  12  B. Gecer, B. Bhattarai, J. Kittler, and T.K.|
|||We generated a synthetic dataset of face images closer to a photorealistic domain and combined it with a real face image dataset to train a face recognition CNN and improved the performance in recognition and verification tasks.|
||2 instances in total. (in eccv2018)|
|1429|Matthew_Trumble_Deep_Autoencoder_for_ECCV_2018_paper|Descriptors learned via CNN have been used in 2D pose estimation from low-resolution 2D images [27] and real-time multi-subject 2D pose estimates were demonstrated by cao [28].|
|||Trumble [31] used a spherical histogram and later voxel input to regress a pose estimate using a CNN [32].|
||2 instances in total. (in eccv2018)|
|1430|Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper|For clarity, we denote backbone as the CNN architecture for image feature extraction and head as the modules applied to individual RoIs.|
|||Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic segmentation-aware cnn model.|
||2 instances in total. (in eccv2018)|
|1431|cvpr18-Convolutional Neural Networks With Alternately Updated Clique|A notable trend of those CNN architectures is that the layers are going deeper, from AlexNet [23] with 5 convolutional layers, the VGG network and GoogleLeNet with 19 and 22 layers, respectively [32, 36], to recent ResNets [13] whose deepest model has more than one thousand layers.|
|||2417  Params CIFAR-10 CIFAR-100 1.86M 1.7M  8.69 11.66 9.22 7.33 7.00 5.77 5.83 5.93 5.12 5.10 5.06  31.75 37.8 33.78 28.2 27.55 23.79 23.42 27.32 23.98 23.32 23.14  Model Recurrent CNN [26] Stochastic Depth ResNet [18] dasNet [35] FractalNet [25] DenseNet (k = 12, T = 36) [17] DenseNet (k = 12, T = 96) [17] DenseNet (k = 24, T = 96) [17] CliqueNet (k = 36, T = 12) CliqueNet (k = 64, T = 15) CliqueNet (k = 80, T = 15) CliqueNet (k = 80, T = 18) DenseNet (k = 12, T = 96) [17] DenseNet (k = 24, T = 246) [17] CliqueNet (k = 36, T = 12) CliqueNet (k = 36, T = 12) CliqueNet (k = 36, T = 12) CliqueNet (k = 80, T = 15) CliqueNet (k = 150, T = 30)  A B  C  FLOPs   0.53G 3.54G 13.78G 0.91G 4.21G 6.45G 9.45G   X X 0.58G X X 10.84G X 0.91G X 0.98G X X 0.98G X X 6.88G X X X 8.49G   38.6M 1.0M 7.0M 27.2M 0.94M 4.49M 6.94M 10.14M  0.8M 15.3M 0.98M 1.04M 1.08M  8M  10.02M  SVHN  1.80 1.75   1.87 1.79 1.67 1.59 1.77 1.62 1.56 1.51  1.76 1.74   1.69 1.53 1.64  5.92 5.19 5.8 5.69 5.61 5.17 5.06  24.15 19.64 26.41 26.45 25.55 22.78 21.83  Table 4.|
||2 instances in total. (in cvpr2018)|
|1432|Episodic CAMN_ Contextual Attention-Based Memory Networks With Iterative Feedback for Scene Labeling|Namely, the output of the CNN in the previous iteration is fed back to the input of the same CNN in next iteration.|
|||This recurrent CNN has been successfully applied to scene labeling.|
||2 instances in total. (in cvpr2017)|
|1433|Wood_Rendering_of_Eyes_ICCV_2015_paper|Trained on our SynthesEyes dataset the CNN achieves a statistically  3762  MPII GazeUT Gaze502502550gaze pitch (degrees)SynthesEyes502502550502502550502502550502502550head pose pitch (degrees)yaw (degrees)UTSynth.Synth.+UTUT subsetSynth.targetedSynth.targeted+UT subset04812162024Error on MPIIGaze (degrees)13.9113.5511.129.688.947.90Lower bound =6.33 (within-MPIIGaze trained model)(a)  (b)  Figure 10: Example fits of our SynthesEyes eye-CLNF on in-the-wild images (a) and webcam images (b).|
|||We first trained the same CNN model on the SynthesEyes dataset and fine-tuned the model using the UT dataset.|
||2 instances in total. (in iccv2015)|
|1434|Shih_Where_to_Look_CVPR_2016_paper|Visual features for each region are encoded using the top two layers (including the output layer) of a CNN trained on ImageNet [18].|
|||Word+Whole image: We concatenate CNN features computed over the entire image with the language features and score them using a three-layer neural network, essentially replacing the region-selection layer with features computed over the whole image.|
||2 instances in total. (in cvpr2016)|
|1435|Liu_Robust_Video_Super-Resolution_ICCV_2017_paper|[30, 22], demonstrating the benefit of domain expertise from sparse coding in the task of image SR. A very deep CNN with residual architecture is proposed by Kim et al.|
|||[17] first generate an ensemble of SR draft via motion compensation under different parameter settings, and then use a CNN to reconstruct the HR frame from all drafts.|
||2 instances in total. (in iccv2017)|
|1436|Fan_Complex_Event_Detection_ICCV_2017_paper|Afterwards, the Long Short-Term Memory (LSTM) [7] network takes CNN features as inputs and generates an output at each step.|
|||Exploiting image-trained CNN architectures for unconstrained video classification.|
||2 instances in total. (in iccv2017)|
|1437|cvpr18-Learning Face Age Progression  A Pyramid Architecture of GANs|A CNN based generator G learns the age transformation.|
|||In our case, the CNN based generator takes young faces as inputs, and learns a mapping to a domain corresponding to elderly faces.|
||2 instances in total. (in cvpr2018)|
|1438|Jiyang_Gao_CTAP_Complementary_Temporal_ECCV_2018_paper|In each unit, the central frame is sampled to calculate the appearance CNN feature, it is the output of Flatten 673 layer in ResNet [29].|
|||Flow-16 only uses denseflow CNN features, and the unit size is set to 16, which is the same as [2](nu = 16), Twostream-6 use two-stream features and unit size is 6 (nu = 6).|
||2 instances in total. (in eccv2018)|
|1439|Xiaopeng_Zhang_ML-LocNet_Improving_Object_ECCV_2018_paper|[30] incorporated a soft proposal module into a CNN to guide the network to focus on the discriminative visual evidence.|
|||Li, Y., Liu, L., Shen, C., Van Den Hengel, A.: Mining mid-level visual patterns  with deep cnn activations.|
||2 instances in total. (in eccv2018)|
|1440|Mingfei_Gao_C-WSL_Count-guided_Weakly_ECCV_2018_paper|2 Related Works  MIL-based CNN Methods.|
|||Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||2 instances in total. (in eccv2018)|
|1441|Das_Learning_Cooperative_Visual_ICCV_2017_paper|Given some feature extractor (i.e., a pretrained CNN model, say VGG-16), no human annotation is required to produce the target description ygt (simply forward-prop the image through the CNN).|
|||The VGG-16 CNN in A-BOT is pretrained on ImageNet.|
||2 instances in total. (in iccv2017)|
|1442|Johnson_Love_Thy_Neighbors_ICCV_2015_paper|To make predictions for an image, we sample several of its nearest neighbors to form a neighborhood and we use a CNN to extract visual features.|
|||We use a CNN [31, 27]  to extract d-dimensional features from the images x and zi.|
||2 instances in total. (in iccv2015)|
|1443|Zhang_Efficient_and_Accurate_2015_CVPR_paper|Actually, in the recent work [3, 10] the approximations are applied on a single layer of large CNN models, such as those trained on ImageNet [2, 16].|
|||We sample the responses from a CNN model (with 7 convolutional layers, detailed in Sec.|
||2 instances in total. (in cvpr2015)|
|1444|cvpr18-Alive Caricature From 2D to 3D|[17] developed a sketch system using a CNN to model 3D caricatures from simple sketches.|
|||Large pose 3D face reconstruction from a single image via  direct volumetric CNN regression.|
||2 instances in total. (in cvpr2018)|
|1445|cvpr18-Edit Probability for Scene Text Recognition|[31] proposed the end-to-end neural networks that first capture visual feature representation by using CNN or RNN, then the CTC [12] loss was combined with the neural network outputs for calculating the conditional probability between the predicted and the target sequences.|
|||These methods first combined CNN and RNN for encoding text images into feature representations, then employed a frame-wise loss to optimize the model.|
||2 instances in total. (in cvpr2018)|
|1446|Alayrac_Joint_Discovery_of_ICCV_2017_paper|For each detected object, represented by a set of bounding boxes over the course of the tracklet, we compute a CNN feature from each (extended) bounding box that we then average over the length of the tracklet to get the final representation.|
|||The CNN feature is extracted with a ROI pooling [36] of ResNet50 [20].|
||2 instances in total. (in iccv2017)|
|1447|cvpr18-Don't Just Assume; Look and Answer  Overcoming Priors for Visual Question Answering|For non yes/no questions, the GVQA components that get activated are  1) Visual Concept Classifier (VCC) which takes as input the image features extracted from CNN and Qmain given by the question Extractor, 2) Answer Cluster Predictor (ACP) whose input is the entire question.|
|||Deeper lstm and normalized cnn visual question answering model.|
||2 instances in total. (in cvpr2018)|
|1448|Zhaoyi_Yan_Shift-Net_Image_Inpainting_ECCV_2018_paper|MNPS [41] adopts a multi-stage scheme to combine CNN and examplar-based inpainting, and generally works better than Content-Aware Fill [11] and context encoder [28].|
|||In comparison to the competing methods, our Shift-Net combines CNN and examplar-based inpainting in an end-to-end manner, and generally is able to generate visual-pleasing results.|
||2 instances in total. (in eccv2018)|
|1449|Shi_Robust_Scene_Text_CVPR_2016_paper|In [16], a CNN with a structured output layer is constructed for unconstrained text recognition.|
|||Similar to the conventional structures [33, 21], the CNN contains convolutional layers, pooling layers and fully-connected layers.|
||2 instances in total. (in cvpr2016)|
|1450|Lee_Automatic_Content-Aware_Color_CVPR_2016_paper|Semantic clustering  Inspired by recent breakthroughs in the use of CNN [16], we represent the semantic information of an image using a CNN feature, trained on the ImageNet dataset [8].|
|||We perform k-means clustering on the CNN feature vectors for each image in the large photo collection to obtain semantic content clusters.|
||2 instances in total. (in cvpr2016)|
|1451|Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper|[24] first trained a CNN for making a coarse global  prediction based on the entire image, then another CNN was  used  to  refine  this  prediction  locally.|
|||Li and Yu [34] predicted the  saliency score for each superpixel by using multiscale CNN  features.|
||2 instances in total. (in cvpr2016)|
|1452|Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper|Some [5, 36, 15] follow a CNNRNN framework: first high-level features are extracted from a CNN trained on the image classification task, and then a recurrent model learns to predict subsequent words of a caption conditioned on image features and previously predicted words.|
|||Deep Lexical Classifier  The lexical classifier (Fig 2, left) is a CNN which maps images to semantic concepts.|
||2 instances in total. (in cvpr2016)|
|1453|cvpr18-Efficient Interactive Annotation of Segmentation Datasets With Polygon-RNN++|We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate highresolution objects in images.|
|||We use a CNN+RNN architecture as in [4], with a CNN serving as an image feature extractor, and the RNN decoding one polygon vertex at a time.|
||2 instances in total. (in cvpr2018)|
|1454|Agustsson_Anchored_Regression_Networks_ICCV_2017_paper|VGG-16 is a CNN introduced by Simonyan and Zisserman [35] for the task of image classification on the ImageNet challenge [33].|
|||Ordinal regression with multiple output cnn for age estimation.|
||2 instances in total. (in iccv2017)|
|1455|3D Human Pose Estimation = 2D Pose Estimation + Matching|Given this conditional Independence, one can write:  p(X, x, I) = p(X|x) (cid:2) (cid:3)(cid:4) (cid:5)  NN   p(x|I) (cid:2) (cid:3)(cid:4) (cid:5)  CNN  p(I)  (2)  We tackle the second term with a image-based CNN that predicts 2D keypoint heatmaps.|
|||We model the conditional of 2D pose given an image as  P (x|I) = CN N (I)  (3)  where we assume CNN is a nonlinear function that returns N 2D heatmaps (or marginal distributions over the location of individual joints).|
||2 instances in total. (in cvpr2017)|
|1456|Novotny_Cascaded_Sparse_Spatial_ICCV_2015_paper|CNN-SPP feature  The max-pooled activations of the rectified CNN filters coming from the last convolutional layer of the ZF-5 CNN network [39] are another utilized proposal descriptor.|
|||Recently DeepMultiBox [11] attained 29.0 mAP on VOC07-TEST with their proposals and using a classspecific detector with CNN architecture from [22] while considering just 10 candidate regions per image.|
||2 instances in total. (in iccv2015)|
|1457|cvpr18-Correlation Tracking via Joint Discrimination and Reliability Learning|Since most of CNN models are pre-trained with respect to the task  489  of object classification or detection, they tend to retain the features useful for distinguishing different categories of objects, and lose much information for instance level classification.|
|||Deeply investing the representation property of different convolution layers in the CNN model, Ma et al.|
||2 instances in total. (in cvpr2018)|
|1458|Liao_Mutual_Enhancement_for_ICCV_2017_paper|Then feature extraction is performed using a CNN for each proposed region.|
|||Logo recognition using cnn features.|
||2 instances in total. (in iccv2017)|
|1459|Puscas_Unsupervised_Tube_Extraction_ICCV_2015_paper|In our experiments we used CNN features: (r) is the 4096-dimensional feature vector extracted from the last fully-connected layer (F C7) of the ImageNet trained net described in [15].|
|||Acknowledgements  We want to thank NVIDIA for their donation of a K40 GPU which was used to extract the CNN features.|
||2 instances in total. (in iccv2015)|
|1460|Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper|(a) The CNN implementation of MuLA for one stage.|
|||() and EJ  (t)  (t)  e  e  8  X. Nie, J. Feng and S. Yan  Mutual Adaptation Module This module includes two adapters A  () and  (t)  a  a    (t)  (t)  and (t)  S and R(t)   which are used to tailor  () to predict adaptive parameters (t)  () () with the same small CNN for predicting convolution kernels of  A preliminary representations R(t) and A counterpart models, as shown in Fig.|
||2 instances in total. (in eccv2018)|
|1461|Siyang_Li_Unsupervised_Video_Object_ECCV_2018_paper|A dual branch CNN is used in [5], [16], [32].|
|||CTN [19] uses a CNN to identify confident foreground and background regions based on  4  S. Li et al.|
||2 instances in total. (in eccv2018)|
|1462|Riegler_Conditioned_Regression_Models_ICCV_2015_paper|The special structure of a CNN architecture is not suited (i) to directly concatenate low-resolution patches x L with a blur kernel k(i).|
|||For training the CNN we follow the procedure outlined in [6].|
||2 instances in total. (in iccv2015)|
|1463|Tsogkas_AMAT_Medial_Axis_ICCV_2017_paper|adapt a CNN with side outputs [53] for object skeleton extraction [38].|
|||We also tried to retrain the CNN used in [38], but the outputs we obtained  2713  Figure 4: From left to right: Input image, AMAT axes (unused points in black), medial point groups (color-coded), groundtruth skeletons.|
||2 instances in total. (in iccv2017)|
|1464|Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper|[8] apply shallow networks to the task of SISR by training a CNN via backpropagration to learn a mapping from the bicubic interpolation of the LR input to a high-resolution image.|
|||This circumvents the nuisance of having to feed a large image with added redundancies into the CNN and allows most computation to be done in the LR image space, resulting in a smaller network and larger receptive fields of the filters relative to the output image.|
||2 instances in total. (in iccv2017)|
|1465|Chen_Show_Adapt_and_ICCV_2017_paper|A sentence y is first encoded by CNN [18] with highway connection [19] into a sentence representation.|
|||The former is a CNN model trained to predict semantic attributes and the latter is an LSTM model trained on unpaired text.|
||2 instances in total. (in iccv2017)|
|1466|Yiru_Zhao_A_Principled_Approach_ECCV_2018_paper|Given a training tuple < a, p, n, l >, where the anchor image a is labeled as  class l, the no-bias softmax loss is defined as  LF,cls =  log  eWlF (a) k=1 eWkF (a)  PK  (10)  where F () denotes the output feature of a CNN model.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in eccv2018)|
|1467|Lee_Semantic_Line_Detection_ICCV_2017_paper| We propose the semantic line detector, by employing a CNN with multi-task, multi-scale learning.|
|||One approach is to crop local regions from an image and feed them into a CNN [18, 29].|
||2 instances in total. (in iccv2017)|
|1468|Gao_TALL_Temporal_Activity_ICCV_2017_paper|It utilizes a CNN model to extract visual features of the clips and a Long Shortterm Memory (LSTM) network to extract sentence embeddings.|
|||R-CNN [7] consists of selective search, CNN feature extraction, SVM classification and bounding box regression.|
||2 instances in total. (in iccv2017)|
|1469|Nagpal_Face_Sketch_Matching_ICCV_2017_paper|Results and Observations  Effectiveness of DeepTransformer is evaluated with multiple input features, namely Dense Scale Invariant Feature Transform (DSIFT) [7], Dictionary Learning (DL) [22], Class Sparsity based Supervised Encoder (L-CSSE) [24], Light CNN [46], and VGG-Face [33].|
|||A lightened CNN for deep face  representation.|
||2 instances in total. (in iccv2017)|
|1470|Human Shape From Silhouettes Using Generative HKS Descriptors and Cross-Modal Neural Networks|We first train a CNN to find a richer body shape representation space from pose invariant 3D human shape descriptors.|
|||In contrast to these methods, we first learn an embedding space from 3D shape descriptors, that are invariant to isometric deformations, by training a CNN to regress directly to 3D body shape vertices.|
||2 instances in total. (in cvpr2017)|
|1471|cvpr18-Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks|The reason is that the feature space from CNN is already a semantic space [64], which is meant to have semantic loss since its construction.|
|||In particular, we followed the new split provided by [62] as the ILSVRC [50] 1K ImageNet classes, widely used as a pre-training source for CNN features, have already included the test classes in the conventional split of the benchmarks, hence violating the fundamental assumption of ZSL that the classes at test should be strictly unseen at training.|
||2 instances in total. (in cvpr2018)|
|1472|Andrew_Owens_Audio-Visual_Scene_Analysis_ECCV_2018_paper|This allows them to address the problem with a 2D CNN that takes 5 channel-wise concatenated frames cropped around a mouth as input (they also propose using their image features for self-supervision; while promising, these results are very preliminary).|
|||We provide the results in Table 1, and compare our model to other unsupervised learning and 3D CNN methods.|
||2 instances in total. (in eccv2018)|
|1473|Asymmetric Feature Maps With Application to Sketch Based Retrieval|The descriptor has recently shown competitive results with CNN based approaches [1].|
|||Ap plying the trigonometric polynomial scoring for ranking all  4After evaluation we discovered that the dataset contains a small amount of training ImageNet images, which can potentially affect with QE by CNN descriptors.|
||2 instances in total. (in cvpr2017)|
|1474|An Empirical Evaluation of Visual Question Answering for Novel Objects|4395  I: CNN   A: stop     Q: What  does  the   sign say  LSTM        FC S: The peacock is blue in colorMultiplyLayerSkip connectionLSTMdecoderS: The peacock is blue in colorCNNLSTMI:Input Image(zero image)CNN He    plays ... gameLSTM encoder START         He     plays ... game   END......LSTM decoderVQA dataset  #images #ques #ans per ques #ques Types #words per ans  204,721 614,163  10  more than 20 one or more  BookCorpus  #books #sentences #unique words avg #words / sent.|
|||Deeper lstm and normalized cnn visual question answering model.|
||2 instances in total. (in cvpr2017)|
|1475|Medhini_Gulganjalli_Narasimhan_Straight_to_the_ECCV_2018_paper|Given an image and a question about the image, we obtain an Image + Question Embedding through the use of a CNN on the image, an LSTM on the question, and a Multi Layer Perceptron (MLP) for combining the two modalities.|
|||Pictorially, we illustrate the construction of an image-question embedding gNN(x, Q), via LSTM and CNN net representations that are combined via an MLP.|
||2 instances in total. (in eccv2018)|
|1476|Jain_Active_Image_Segmentation_CVPR_2016_paper|For region appearance Rc ij, we extract a CNN feature for the regions tight bounding box.|
|||It is important to note that all methods are using identical CNN features and the same propagation algorithm, hence our gains exactly show the impact of making wiser annotation choices.|
||2 instances in total. (in cvpr2016)|
|1477|cvpr18-Attention-Aware Compositional Network for Person Re-Identification|Inspired by the multi-stage CNN [6] for human pose estimation, we utilize a two-stage network to learn part attentions.|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in cvpr2018)|
|1478|Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images Using Weakly-Supervised Joint Convolutional Sparse Coding|Recently, an efficient CNN based approach was proposed in [5], which directly learned an end-to-end mapping between LR and HR images to perform complex nonlinear regression tasks.|
|||[10] introduced a CNN algorithm of artistic style, that new images can be generated by performing a pre-image search in high-level image content to match generic feature representations of example images.|
||2 instances in total. (in cvpr2017)|
|1479|cvpr18-Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors|DeepParts [21] model constructs a set of data-driven part prototypes, trains a CNN classifier to detect each of them, and finally explores their ensemble to improve the detection of occluded objects.|
|||First, our model can be plugged into any singlestage CNN architecture, whereas DeepParts is a stand-alone pedestrian detector.|
||2 instances in total. (in cvpr2018)|
|1480|cvpr18-Improvements to Context Based Self-Supervised Learning|For  instance, the self-supervised network was trained on a transfer task (fine-tuned) to classify objects in the PASCAL VOC dataset [13], and compared with a CNN trained on a supervised task, such as learning to classify the ImageNet dataset [7].|
|||For instance, [12] trained a CNN to recognize which transformation had been performed on an image.|
||2 instances in total. (in cvpr2018)|
|1481|cvpr18-Analyzing Filters Toward Efficient ConvNet|Netvlad: Cnn architecture for weakly supervised place recognition.|
|||Part-stacked cnn for In CVPR, pages 1173  fine-grained visual categorization.|
||2 instances in total. (in cvpr2018)|
|1482|cvpr18-NeuralNetwork-Viterbi  A Framework for Weakly Supervised Video Learning|Purely CNN based approaches such as structured segment networks [37] or temporal convolutional networks [18] have recently shown convincing results on several action segmentation benchmarks.|
|||Deep hand: How to train a CNN on 1 million hand images when your data is continuous and weakly labelled.|
||2 instances in total. (in cvpr2018)|
|1483|cvpr18-Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser|The parameters of the CNN are shared and fixed.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||2 instances in total. (in cvpr2018)|
|1484|Quan_Object_Co-Segmentation_via_CVPR_2016_paper|Firstly, we feed each image into the pre-trained CNN and  extract the responses from the last convolutional layer as the  higher-level  image  representations,  which  consists  of  512  feature maps with size of 1717.|
|||24-D  CNN  feature  vector  is  denoted  by   4c .|
||2 instances in total. (in cvpr2016)|
|1485|Wang_Deep_Metric_Learning_ICCV_2017_paper|3.1. Review of triplet loss  Suppose that we are given a set of training images {(x, y),    } of K classes, where x 2 RD denotes the feature embedding of each sample extracted by CNN and y 2 {1,    , K} its label.|
|||Joint embeddings of shapes and images via CNN image purification.|
||2 instances in total. (in iccv2017)|
|1486|cvpr18-SINT++  Robust Visual Tracking via Adversarial Positive Instance Generation|Other CNN based methods either combine several CNN models (e.g.|
|||the DeepTrack model [23]) or establish an end-to-end CNN model, such as the GOTURN tracker [13] with no online training but offline learn a generic relationship between object motion and appearance from large number of videos.|
||2 instances in total. (in cvpr2018)|
|1487|A-Fast-RCNN_ Hard Positive Generation via Adversary for Object Detection|Object detection via a multiIn  region and semantic segmentation-aware cnn model.|
|||Gated  bi-directional cnn for object detection.|
||2 instances in total. (in cvpr2017)|
|1488|Hengshuang_Zhao_ICNet_for_Real-Time_ECCV_2018_paper|Recently, CNN based methods largely improve the performance.|
|||While in our cascade structure, only the lowest-resolution input is fed into the heavy CNN with much reduced computation to get the coarse semantic prediction.|
||2 instances in total. (in eccv2018)|
|1489|Li_A_Multi-Level_Contextual_CVPR_2016_paper|We fine-tune a CNN pre-trained for image classification with the body regions to extract body features using the soft-max classification objective over identities.|
|||Face Region  We implemented a deep CNN based face recognition system following Sun et al.|
||2 instances in total. (in cvpr2016)|
|1490|Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper|The visual predictions from an ImageNet-trained CNN are used as weak labels to train the network with unlabeled video.|
|||For each example, we show sample video frames, ImageNet CNN visual object predictions, as well as the corresponding audio basis-object relation map predicted by our MIML network.|
||2 instances in total. (in eccv2018)|
|1491|cvpr18-Lean Multiclass Crowdsourcing|We extract PreLogit features (xi) from an Inception-v3 [30] CNN for each image i, and use these features (fixed for all iterations) to train the weights  of a linear SVM (using a one-vs-rest strategy), followed by probability calibration using Platt scaling [25].|
|||Fine-tuning a CNN on each iteration would lead to better performance [1, 24, 38], but is out of scope.|
||2 instances in total. (in cvpr2018)|
|1492|cvpr18-SplineCNN  Fast Geometric Deep Learning With Continuous B-Spline Kernels|As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights.|
|||SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation.|
||2 instances in total. (in cvpr2018)|
|1493|Mottaghi_Newtonian_Scene_Understanding_CVPR_2016_paper|The first row resembles the standard CNN architecture for image classification introduced by [21].|
|||10  10  age row (refer to Figure 4) by a publicly available 3 pretrained CNN model.|
||2 instances in total. (in cvpr2016)|
|1494|Deep Sketch Hashing_ Fast Free-Hand Sketch-Based Image Retrieval|Methods  Dimension  HOG [8]  GF-HOG [18] SHELO [49]  LKS [50]  Siamese CNN [46]  SaN [67]  GN Triplet [52] 3D shape [61] Siamese-AlexNet Triplet-AlexNet  DSH  (Proposed)  1296 3500 1296 1350  64 512 1024  64  4096 4096  32 (bits) 64 (bits) 128 (bits)  MAP  0.091 0.119 0.123 0.157 0.322 0.154 0.187 0.054 0.367 0.448  0.358 0.521 0.570  Precision  @200 0.120 0.148 0.155 0.204 0.447 0.225 0.301 0.072 0.476 0.552  0.486 0.655 0.694  TU-Berlin Extension Retrieval time per query (s)  Memory load(MB)  (204,489 gallery images)  1.43 4.13 1.44 1.51  7.70102  0.53 1.02  7.53102  5.35 5.35  5.57104 7.03104 1.05103  2.02  103 5.46  103 2.02  103 2.11  103  99.8  7.98  102 1.60  103 99.8 MB 6.39  103 6.39  103  0.78 1.56 3.12  MAP  0.115 0.157 0.161 0.190 0.481 0.208 0.529 0.084 0.518 0.573  0.653 0.711 0.783  Precision  @200 0.159 0.177 0.182 0.230 0.612 0.292 0.716 0.079 0.690 0.761  0.797 0.858 0.866  Sketchy Retrieval time per query (s)  Memory load(MB)  (73,002 gallery images)  0.53 1.41 0.50 0.56  2.76102  0.21 0.41  2.64 102  1.68 1.68 s  2.55104 2.82104 3.53104  7.22  102 1.95  103 7.22  102 7.52  102  35.4  2.85  102 5.70  102  35.6  2.28  103 2.28  103  0.28 0.56 1.11  * denotes we directly use the public models provided by the original papers without any fine-tuning on TU-Berlin Extension or Sketchy datasets.|
|||We fine-tune Siamese CNN and SaN on TU-Berlin and Sketchy datasets, while the public models of GN Triplet and 3D shape are only allowed for direct feature extraction without any retraining.|
||2 instances in total. (in cvpr2017)|
|1495|Song_CREST_Convolutional_Residual_ICCV_2017_paper|Existing CNN trackers mainly explore the pre-trained object recognition networks and build upon discriminative or regression models.|
|||On the other hand, regression based methods typically regress CNN features into soft labels (e.g., a two dimensional Gaussian distribution).|
||2 instances in total. (in iccv2017)|
|1496|Person Search With Natural Language Description|[28] learned feature embedding for each word in a sentence, and connected it with the image CNN features by a multi-modal layer to generate image captions.|
|||[37] extracted high-level image features from CNN and fed it into LSTM for estimating the output sequence.|
||2 instances in total. (in cvpr2017)|
|1497|Active Convolution_ Learning the Shape of Convolution for Image Classification|Pooling is another basic operation in a CNN to reduce the resolution and enable translation invariance.|
|||The main contribution of this paper is this convolution unit to provide greater flexibility and representation power to a CNN for a meaningful improvement in the image classification benchmark.|
||2 instances in total. (in cvpr2017)|
|1498|Lior_Talker_Efficient_Sliding_Window_ECCV_2018_paper|While deep CNN based patch matching algorithms [17,18] might be used for template matching, their goal is to match similar patches (as in stereo matching); hence, they are trained on simple, small changes in patch appearance.|
|||Finally, deep CNN based methods for visual object tracking [1,4] do match a template, however, usually for specific object classes known a priori.|
||2 instances in total. (in eccv2018)|
|1499|cvpr18-Efficient Large-Scale Approximate Nearest Neighbor Search on OpenCL FPGA|We evaluate our method using three common benchmarks,YFCC100M, BigANN [12] and Deep1B[1].YFCC100M consists of 100M of feature vectors from CNN model.|
|||SIFT1M dataset contains 1 million 128-dimensional SIFT vectors and 10K query vectors; BIGANN (SIFT1B) contains 1 billion SIFT vectors and 10K queries; YFCC100M dataset contains 95 million CNN descriptors; DEEP1B contains 1 billion 100-dimension CNN representations for images and 10K query vectors.|
||2 instances in total. (in cvpr2018)|
|1500|cvpr18-Facelet-Bank for Fast Portrait Manipulation| We propose a set-to-set CNN framework for face manipulation.|
|||Note that CNN has local receptive fields, which force the system to capture the relation between certain visual patterns (the mouth in this case) and the corresponding manipulation operations (add beard in this case).|
||2 instances in total. (in cvpr2018)|
|1501|Minghao_Guo_Dual-Agent_Deep_Reinforcement_ECCV_2018_paper|The feature extracted by the pre-trained CNN is trained with ImageNet [37], which helps the parameters of the Q-Network to converge faster.|
|||Kumar, A., Chellappa, R.: Disentangling 3d pose in a dendritic cnn for uncon strained 2d face alignment.|
||2 instances in total. (in eccv2018)|
|1502|cvpr18-Zigzag Learning for Weakly Supervised Object Detection|However, these methods either detach the feature extraction and model training into separate steps [9], [10], or simply utilize the high representation ability of CNN without considering model overfitting [11], which results in limited performance.|
|||We choose two CNN models to evaluate our approach: 1) CaffeNet [23], which we refer to as model S (meaning small), and 2) VGG-VD [24] (the 16-layer model is used), which we call model L (meaning large).|
||2 instances in total. (in cvpr2018)|
|1503|Mir_Rayat_Imtiaz_Hossain_Exploiting_temporal_information_ECCV_2018_paper|[43] used a CNN to first align bounding boxes of successive frames so that the person in the image is always at the center of the box and then extracted 3D HOG features densely over the spatio-temporal volume from which they regress the 3D pose of the central frame.|
|||Mehta, D., Rhodin, H., Casas, D., Sotnychenko, O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation using transfer learning and improved cnn supervision.|
||2 instances in total. (in eccv2018)|
|1504|Isola_Discovering_States_and_2015_CVPR_paper|Images are visualized using t-SNE [30] in CNN feature space.|
|||Discovering relevant transformations  To implement g() (Equation 2), we used logistic regressions trained on CNN features [8] (Caffe Reference ImageNet Model2, layer fc7 features).|
||2 instances in total. (in cvpr2015)|
|1505|Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper|In our primary network, which unless otherwise specified is a ResNet-101 architecture (deep CNN module in Figure 2), given an image x, we obtain three-dimensional feature representations:  k1(x) = 1(x), k1(x)  RH1W1F1 ,  k2(x) = 2(k1(x)), k2(x)  RH2W2F2 .|
|||International Conference on Computer  trained holistic CNN model.|
||2 instances in total. (in eccv2018)|
|1506|InstanceCut_ From Edges to Instances With MultiCut|As an example, [16, 53] address these challenges with a complex multi-loss cascade CNN architectures, which are, however, difficult to train.|
|||These are exactly the ingredients of our approach: (i) a standard CNN that outputs an instanceagnostic semantic segmentation, and (ii) a new CNN that outputs all boundaries of instances.|
||2 instances in total. (in cvpr2017)|
|1507|Xiaoyang_Guo_Learning_Monocular_Depth_ECCV_2018_paper|For deep CNN based supervised methods [6, 5], neural networks are directly trained with ground-truth depths, where   Corresponding author  2  X. Guo, H. Li, S. Yi, J. Ren and X. Wang  conditional random fields (CRF) are optionally used to refine the final results.|
|||Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||2 instances in total. (in eccv2018)|
|1508|SEUNG_HYUN_LEE_Self-supervised_Knowledge_Distillation_ECCV_2018_paper|Although VGG is somewhat poorer than the state-of-the-art CNN models in terms of ratio of accuracy and parameter size, it is widely used because of its simple structure and ease of implementation.|
|||MobileNet is a CNN with small parameter size and computational cost designed for use in mobile or embedded environments.|
||2 instances in total. (in eccv2018)|
|1509|Attend in Groups_ A Weakly-Supervised Deep Learning Framework for Learning From Web Data|propose to pretrain CNN on simple examples and adapt it to harder images by leveraging the structure of data and categories in a two-step manner.|
|||To guide the models focus on the objects specified by the question or caption, attention models are designed to pay attention to local CNN features in the input image [1,24,46,47,50].|
||2 instances in total. (in cvpr2017)|
|1510|cvpr18-CartoonGAN  Generative Adversarial Networks for Photo Cartoonization|Li and Wand [20] obtained style transfer by local matching of CNN feature maps and using a Markov Random Field for fusion (CNNMRF).|
|||[3] proposed a method to improve comic style  9466  transfer by training a dedicated CNN to classify comic/noncomic images.|
||2 instances in total. (in cvpr2018)|
|1511|cvpr18-Video Rain Streak Removal by Multiscale Convolutional Sparse Coding|[11] firstly developed a deep CNN (called DerainNet) model to extract discriminative features of rains in high frequency layer of an image.|
|||To improve the invariance of CNN activations, Gong et al.|
||2 instances in total. (in cvpr2018)|
|1512|Marc_Oliu_Folded_Recurrent_Neural_ECCV_2018_paper|[12] use a plain multi-scale CNN in an adversarial setting and propose the Gradient Difference Loss to sharpen the predictions.|
|||[24] use an AE architecture with a two-stream encoder: for motion, a CNN + LSTM encodes difference images; for appearance, a CNN encodes the last input frame.|
||2 instances in total. (in eccv2018)|
|1513|cvpr18-One-Shot Action Localization by Learning Sequence Matching Network|[38, 49, 28] use LSTM in addition to CNN to better model the temporal dynamics each proposal.|
|||3D CNN and temporal convolution have proven to be effective in capturing spatial temporal features, and is reported to achieve state-of-the-art results for both action recognition and action localization tasks.|
||2 instances in total. (in cvpr2018)|
|1514|cvpr18-Matryoshka Networks  Predicting 3D Geometry via Nested Shape Layers|[26] trained a CNN to generate RGBD images from arbitrary views of an object.|
|||[24] projected object surfaces to geometry images in order to build on image-based CNN architectures.|
||2 instances in total. (in cvpr2018)|
|1515|cvpr18-Single-Shot Object Detection With Enriched Semantics|Gidaris and Komodakis [6] used semantic segmentationaware CNN features to augment detection features by concatenation at the highest level, but our work differs in a way that we put the segmentation information at the lowest detection feature map, and we use activation instead of concatenation to combine object detection features and segmentation features.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||2 instances in total. (in cvpr2018)|
|1516|Hongyu_Xu_Deep_Regionlets_for_ECCV_2018_paper|By exploiting multiple layers within a deep CNN network directly, the single-stage detectors achieved high speed but their accuracy is typically low compared to two-stage detectors.|
|||Therefore, both the selected regions and regionlet learning should be able to be trained by CNN networks.|
||2 instances in total. (in eccv2018)|
|1517|cvpr18-Deeply Learned Filter Response Functions for Hyperspectral Reconstruction|Interestingly, our hardware implementation of optimized filter responses has parallels with ASP vision [10], which uses custom CMOS diffractive image sensors to directly compute a fixed first layer of the CNN to save energy, data bandwidth, and CNN FLOPS.|
|||Our aim is to leverage the CNN and deep learning framework to optimize camera filter design.|
||2 instances in total. (in cvpr2018)|
|1518|Self-Critical Sequence Training for Image Captioning|This feature is derived as in [6] as follows: given CNN features at N locations {I1, .|
|||2) Spatial CNN features for Attention models: (Att2in) We encode each image using the residual convolutional neural network Resnet-101 [22].|
||2 instances in total. (in cvpr2017)|
|1519|Scene Parsing Through ADE20K Dataset|With the success of convolutional neural networks (CNN) for image classification [16], there is growing interest for semantic image parsing using CNNs with dense output, such as the multiscale CNN [11], recurrent CNN [25], fully CNN [19], deconvolutional neural networks [24], encoder-decoder SegNet [1], multi-task network cascades [9], and DilatedNet [4, 34].|
|||SegNet has encoder and decoder architecture for image segmentation; FCN upsamples the activations of multiple layers in the CNN for pixelwise segmentation; DilatedNet drops pool4 and pool5 from fully convolutional VGG-16 network, and replaces the following convolutions with dilated convolutions (or atrous convolutions), a bilinear upsampling layer is added at the end.|
||2 instances in total. (in cvpr2017)|
|1520|Yonetani_Recognizing_Micro-Actions_and_CVPR_2016_paper|A CNN trained on a relevant dataset (e.g., action recognition datasets such as UCF101 [33] and HMDB51 [14])  can also be used as a feature descriptor.|
|||[40], we concatenated the outputs of the conv4 layer of spatial CNN and the conv3 layer of temporal CNN as second-person features.|
||2 instances in total. (in cvpr2016)|
|1521|cvpr18-Learning to Promote Saliency Detectors|The CNN embeddings can be trained using a gradientbased optimization algorithm through maximizing the log likelihood with respect to  and  on the training set:  L = X  m,n  tmn log p(Cm1|xmn)+(1tmn) log p(Cm2|xmn),  (4) tmn = 1 when where tmn is the label of pixel xmn.|
|||Output: CNN embedding (; ) and (; )  1 for training iterations do  2  3  4  5  6  Sample a pair of training image and ground truth map (Xm, tm) from the training set.|
||2 instances in total. (in cvpr2018)|
|1522|Buckler_Reconfiguring_the_Imaging_ICCV_2017_paper|RedEye [30] computes initial convolutions for a CNN using a custom sensor ADC, and Chen et al.|
|||[13] studies the impact of sensor noise and blurring on CNN accuracy and develops strategies to tolerate it.|
||2 instances in total. (in iccv2017)|
|1523|Armand_Zampieri_Multimodal_image_alignment_ECCV_2018_paper|3  Introducing neural networks  3.1 Learning iterative processes  As neural networks have proved useful to replace hand-designed features for various tasks in the literature recently, and convolutional ones (CNN) in particular in computer vision, one could think, for mono-modal image alignment, of training a CNN in the Siamese network setup [3,6], in order to learn a relevant distance between image patches.|
|||The multi-modal version of this would consist in training two CNN (one per modality) with same output size, in computing the Euclidean norm of the difference of their outputs as a dissimilarity measure, and in using that quantity within a standard non-rigid alignment algorithm, such as a gradient descent over (1).|
||2 instances in total. (in eccv2018)|
|1524|Wei_Region_Ranking_SVM_CVPR_2016_paper|For example, CNN features [4, 18, 31] can only be computed on square regions of a certain size.|
|||Also, computing CNN features is time consuming so the total number of regions is usually limited to ten [18] or at most several hundred [31, 35].|
||2 instances in total. (in cvpr2016)|
|1525|Prabhu_Attribute-Graph_A_Graph_ICCV_2015_paper|A high-capacity CNN is then utilised to obtain 4096 dimensional features for the obtained region proposals.|
|||Experimental setup  We perform object detection and classification, using Regions with CNN (RCNN), the algorithm of Girshick et al.|
||2 instances in total. (in iccv2015)|
|1526|Temporal Action Localization by Structured Maximal Sums|Our method, however, has the advantage of modeling the temporal evolution of actions, and utilizes powerful CNN features, which we train end-to-end.|
|||Since both the localization and classification losses are sub-differentiable, the parameters of the two CNN streams can be learned by backpropagation.|
||2 instances in total. (in cvpr2017)|
|1527|Xia_Learning_Discriminative_Reconstructions_ICCV_2015_paper|Images are represented by 2048-dimensional deep learning features extracted from a pre-trained CNN model.2 The concept car wheel, which contains 1,981 positive images and equal number of outliers, is used as an example set here, as well as the rest illustrative experiments in this paper.|
|||In most of our experiments, images are represented by features extracted from a seven-layer CNN [12], which is pre-trained on ImageNet.|
||2 instances in total. (in iccv2015)|
|1528|Learning to Align Semantic Segmentation and 2.5D Maps for Geolocalization|We then iteratively apply this CNN until converging to a good pose.|
|||Very recently, [13] uses a CNN to predict a 6 DoF camera pose directly from an image, where the idea of transfer learning  from large scale data classification to the task of re-localization  is adopted.|
||2 instances in total. (in cvpr2017)|
|1529|Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper|Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson, S.: Cnn features off-the shelf: an astounding baseline for recognition.|
|||Tolias, G., Sicre, R., J egou, H.: Particular object retrieval with integral max pooling of cnn activations.|
||2 instances in total. (in eccv2018)|
|1530|Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper|Owing to the design of CNN structures, the receptive field of it is limited to local regions [47,27].|
|||Recently, CNN based methods [26,4,5,42,45,6] have achieved remarkable success in scene parsing and semantic segmentation tasks.|
||2 instances in total. (in eccv2018)|
|1531|Yim_Rotating_Your_Face_2015_CVPR_paper|Many previous works have efficiently used CNN to train the DNN model from images [16, 9].|
|||However, CNN shares filters over all images, when it is inappropriate to apply filters, which share weights, to the Remote Code attached image.|
||2 instances in total. (in cvpr2015)|
|1532|Minho_Shim_Teaching_Machines_to_ECCV_2018_paper|In action recognition task, combinations of CNN and RNN [9] are widely used.|
|||We used 5 CNN layers to extract spatial features for each frame, and those features are fed into a RNN layer to make temporal features.|
||2 instances in total. (in eccv2018)|
|1533|Chandra_Dense_and_Low-Rank_ICCV_2017_paper|Deeplab [5, 7] successfully exploited the Dense-CRF [20] framework, allowing a CNN trained for semantic segmentation to refine object boundaries while compensating for the effects of spatial downsampling within the network.|
|||Ubernet: A universal cnn for the joint treatment In POCV  of low-, mid-, and highlevel vision problems.|
||2 instances in total. (in iccv2017)|
|1534|Simo-Serra_Fashion_Style_in_CVPR_2016_paper|The classification loss lC serves to learn useful feature maps while the ranking loss lR on the triplet of Feature CNN encourages them to learn a discriminative feature representation.|
|||We compare against strong CNN baselines and all variations of our model.|
||2 instances in total. (in cvpr2016)|
|1535|Zhang_A_Self-Paced_Multiple-Instance_ICCV_2015_paper|Specifically,  for  each  image,  we  first  extract  the  features  via  a  CNN  pre-trained on the ImageNet with the same architecture as  the CNN S model proposed in [34].|
|||In  our  experiments,  the  CNN  was  implemented  via  the  MatConvNet toolbox [42].|
||2 instances in total. (in iccv2015)|
|1536|Shmelkov_Incremental_Learning_of_ICCV_2017_paper|The first CNN (top) is trained on three classes, including person, and localizes the rider in the image.|
|||The second CNN (bottom) is an incrementally trained version of the first one for the category horse.|
||2 instances in total. (in iccv2017)|
|1537|Unsupervised Monocular Depth Estimation With Left-Right Consistency|With na ve sampling the CNN produces a disparity map aligned with the target instead of the input.|
|||Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
||2 instances in total. (in cvpr2017)|
|1538|cvpr18-Deflecting Adversarial Attacks With Pixel Deflection|These perturbations can cause a CNN to misclassify an image into a different class (e.g.|
|||by taking the gradient of a class probability with respect to an input pixel through both the CNN and the transformation.|
||2 instances in total. (in cvpr2018)|
|1539|cvpr18-CRRN  Multi-Scale Guided Concurrent Reflection Removal Network|As shown in Figure 1, the structure of GiN is a mirror-link framework with the encoder-decoder CNN architecture.|
|||On the other hand, training a deep learning network directly on the images may suffer from gradient vanishing problem and the CNN may also introduce the color shift to the estimated image [8].|
||2 instances in total. (in cvpr2018)|
|1540|Zhang_SketchNet_Sketch_Classification_CVPR_2016_paper|The ImageNet ILSVRC-2012 dataset [27] is utilized to pre-train the CNN model by optimizing multinomial logistic regression objective function in the image classification task.|
|||We use the pre-trained parameters of convolutional layers and fully-connected layers to initialize the CNN part of SketchNet in our method.|
||2 instances in total. (in cvpr2016)|
|1541|Gul_Varol_BodyNet_Volumetric_Inference_ECCV_2018_paper|[45] present one of the first approaches in this direction and train a CNN to estimate the 3D shape parameters from silhouettes, but assume a frontal input view.|
|||Jackson, A.S., Bulat, A., Argyriou, V., Tzimiropoulos, G.: Large pose 3D face reconstruction from a single image via direct volumetric CNN regression.|
||2 instances in total. (in eccv2018)|
|1542|Seong-Jin_Park_SRFeat_Single_Image_ECCV_2018_paper|To further improve the accuracy and also speed and memory efficiency, a number of CNN models have been proposed since then [24, 31, 47, 41, 6, 44].|
||| H   To recover I h from I l, we design a new deep CNN (DCNN)-based generator utilizing multiple long-range skip connections.|
||2 instances in total. (in eccv2018)|
|1543|Zhongzheng_Ren_Learning_to_Anonymize_ECCV_2018_paper|In the past several years, CNN models have obtained particularly successful results.|
|||Existing CNN based classifiers are easily fooled [54,5,12,28,30] even when the input images are perturbed in an unnoticeable way to human eyes.|
||2 instances in total. (in eccv2018)|
|1544|cvpr18-Disentangling Structure and Aesthetics for Style-Aware Image Completion|Structure and composition are typically preserved poorly within NST; an issue addressed by Li and Wand [17] who replaced the conventional Grammian approach to NST with a combination of a feed-forward CNN and a generative MRF (CNNMRF).|
|||Liao [18] and Upchurch [26] combined the concept of image analogy and deep features generated from a CNN to achieve  1849  3.1.1 Style Embedding  Our coarse-to-fine in-painting process selects and stylizes patches at multiple resolutions (sec.|
||2 instances in total. (in cvpr2018)|
|1545|Vincent_Leroy_Shape_Reconstruction_Using_ECCV_2018_paper|A trained CNN is used to recognize the photoconsistent configurations given pairs of color samples within the 3D patch.|
|||In the following, we first provide details about the 3D sampling regions before describing the CNN architecture used for the classification and its training.|
||2 instances in total. (in eccv2018)|
|1546|Ngoc-Trung_Tran_Generative_Adversarial_Autoencoder_ECCV_2018_paper|Lastly, we compare our model with recent SN-GAN [23] on CIFAR-10 and STL-10 datasets with standard CNN architecture.|
|||First two rows (CelebA, CIFAR-10) follow the experimental setup of [20], and the remaining rows follow the experimental setup of [23] using standard CNN architectures.|
||2 instances in total. (in eccv2018)|
|1547|Chen_Coherent_Online_Video_ICCV_2017_paper|It generates more impressive results compared to traditional methods because CNN provides more semantic representations of styles.|
|||FlowNet [17] is the first deep CNN designed to directly estimate optical flow and achieve good results.|
||2 instances in total. (in iccv2017)|
|1548|Binge Watching_ Scaling Affordance Learning From Sitcoms|Finally, we have also trained a CNN classifier for empty scenes.|
|||This hard negative mining procedure turns out to be very effective and improve the generalization power of the CNN across all TV series.|
||2 instances in total. (in cvpr2017)|
|1549|Improved Texture Networks_ Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis|(x, x0) that compares the responses of deep CNN filters extracted from the generated image x and a content image x0.|
|||We argue that learning to discard contrast information by using standard CNN building block is unnecessarily difficult, and is best done by adding a suitable layer to the architecture.|
||2 instances in total. (in cvpr2017)|
|1550|cvpr18-3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children With Autism|We feed the CNN with the 3d skeleton features obtained with Kinect, as well as DMHS-SMPL-T, which was the best performing RGB model in the Moving Pose framework.|
|||Results for our CNN model, jointly trained to regress both valence and arousal, are shown in table 5.|
||2 instances in total. (in cvpr2018)|
|1551|Yawei_Luo_Macro-Micro_Adversarial_Network_ECCV_2018_paper|Based on CNN architecture, the pixel-wise classification loss is usually used [19,34,10] which punishes the classification error for each pixel.|
|||In summary, the utilization of context in CNN remains an open problem.|
||2 instances in total. (in eccv2018)|
|1552|Brook_Roberts_A_Dataset_for_ECCV_2018_paper|Typical state-of-the-art CNN models need large amounts of labelled data to detect lane instances reliably (e.g.|
|||More recent methods use CNN based segmentation [2,4], and RNNs [11] for detecting lane boundaries.|
||2 instances in total. (in eccv2018)|
|1553|Zhenyu_Wu_Towards_Privacy-Preserving_Visual_ECCV_2018_paper|Particular attentions should be paid towards the budget cost (second term) defined in (2), which we refer as the  Challenge: if we use fb with some pre-defined CNN architecture, how could we be sure that it is the best possible privacy prediction model?|
|||In comparison, our proposed solution is motivated by at least three folds: i) for single utility task (which is not just limited to action recognition), running fd on device is much more compact and efficient than full fT For example, our fT model (11-layer C3D net) has over 70 million parameters, while fd is a much more compact 3-layer CNN with 1.3 million parameters.|
||2 instances in total. (in eccv2018)|
|1554|Namhyuk_Ahn_Fast_Accurate_and_ECCV_2018_paper|These models decrease the number of model parameters effectively when compared to the standard CNN and show good performance.|
|||However, there are two downsides to these models: 1) They first upsample the input image before feeding it to the CNN model, and 2) they increase the depth or the width of the network to compensate for the loss due to using a recursive network.|
||2 instances in total. (in eccv2018)|
|1555|Iscen_Efficient_Large-Scale_Similarity_CVPR_2016_paper|We evaluate our approach for global descriptors obtained from both SIFT and CNN features.|
|||Particular object retrieval  with integral max-pooling of cnn activations.|
||2 instances in total. (in cvpr2016)|
|1556|Moltisanti_Trespassing_the_Boundaries_ICCV_2017_paper|[20] assessed how frame-level classifications using multi-region two-stream CNN are pooled to achieve video-level recognition results.|
|||Approaches that improve robustness using data augmentation could be attempted, however a broader look at how the methods could be inherently more robust is needed, particularly for CNN architectures.|
||2 instances in total. (in iccv2017)|
|1557|Ji_Zhu_Online_Multi-Object_Tracking_ECCV_2018_paper|[27] adopt a Siamese CNN to learn local features from both RGB images and optical flow maps.|
|||Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||2 instances in total. (in eccv2018)|
|1558|Matthias_Muller_TrackingNet_A_Large-Scale_ECCV_2018_paper|Such trackers are usually trained offline on a large-scale dataset using either deep regression [15] or a CNN matching function [2,45,13].|
|||We include CFNet [45] and SiameseFC [2] to represent CNN matching trackers and MEEM [51] and DLSSVM [39] for structured SVM-based trackers.|
||2 instances in total. (in eccv2018)|
|1559|cvpr18-Pose Transferrable Person Re-Identification|Multi-scale triplet CNN for person re-identification.|
|||A discriminatively learned CNN embedding for person re-identification.|
||2 instances in total. (in cvpr2018)|
|1560|A Unified Approach of Multi-Scale Deep and Hand-Crafted Features for Defocus Estimation|They use a CNN to estimate pre-defined discretized motion blur kernels.|
|||The deep feature is extracted from a CNN which directly processes color image patches in the RGB space for feature extraction.|
||2 instances in total. (in cvpr2017)|
|1561|cvpr18-Action Sets  Weakly Supervised Action Segmentation Without Ordering Constraints|Related Work  Strong feature extractors developed in classical action recognition such as Fisher vectors of improved dense trajectories [35] or a variety of sophisticated CNN methods [30, 8, 11, 4] have also pushed the advances in untrimmed action segmentation.|
|||Although CNN features are successful in some action detection methods [39, 29], they usually require to be retrained using full supervision.|
||2 instances in total. (in cvpr2018)|
|1562|Wei_Should_We_Encode_ICCV_2017_paper|[10, 11], which developed a deep CNN (called DerainNet) model to extract discriminative features of rains in high frequency layer of an image.|
|||Similarly, this method also needs to collect a set of labeled images (with/without rain streaks) to train the CNN parameters.|
||2 instances in total. (in iccv2017)|
|1563|Action-Decision Networks for Visual Tracking With Deep Reinforcement Learning|However, due to the gap between classification and tracking problem, the pre-trained CNN is not sufficient to solve the difficult tracking issues.|
|||Small CNN models such as VGG-M [4] are more effective in the visual tracking problem than deep models [24].|
||2 instances in total. (in cvpr2017)|
|1564|cvpr18-Photographic Text-to-Image Synthesis With a Hierarchically-Nested Adversarial Network|[2] explore the usage of the perceptional loss [15] with a CNN pretrained on ImageNet and Dash et al.|
|||The proposed G is a CNN (defined in Section 3.4), which  produces multiple side outputs:  X1, ..., Xs = G(t, z),  (2)  where t  pdata denotes a sentence embedding (generated by a pre-trained char-RNN text encoder [33]).|
||2 instances in total. (in cvpr2018)|
|1565|Learning Motion Patterns in Videos|Related CNN architectures.|
|||Our CNN model predicts labels for every pixel, similar to CNNs for other tasks, such as semantic segmentation [13, 22, 31], optical flow [8] and disparity/depth [23] estimation.|
||2 instances in total. (in cvpr2017)|
|1566|Multi-Attention Network for One Shot Learning|The image is fed into a CNN to extract the local visual features and the class tag is represented by its distributed semantic embedding e.g., word2vector [14] or GloVe [16].|
|||In this work, we extract the convolutional feature maps of a CNN and view them as an array of local features.|
||2 instances in total. (in cvpr2017)|
|1567|Seonwook_Park_Deep_Pictorial_Gaze_ECCV_2018_paper|Several CNN architectures have been proposed for person-independent gaze estimation in unconstrained settings, mostly differing in terms of possible input data modalities.|
|||attained by simple adoption of newer CNN architectures ranging from LeNet5 [25, 43], AlexNet [14, 44], to VGG-16 [45], the current state-of-the-art CNN architecture for appearance-based gaze estimation.|
||2 instances in total. (in eccv2018)|
|1568|3D Shape Segmentation With Projective Convolutional Networks|This method employs graph cuts with a CNN on per-face geometric descriptors (also used in ShapeBoost), plus geometric cues for the pairwise term.|
|||Fusenet: Incorporating depth into semantic segmentation via fusion-based CNN architecture.|
||2 instances in total. (in cvpr2017)|
|1569|cvpr18-EPINET  A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth From Light Field Images|Heber and Pock [12] proposed a combination of a CNN and a variational optimization.|
|||They trained a CNN to predict EPI line orientations, and formulated a global optimization with a higher-order regularization to refine the network predictions.|
||2 instances in total. (in cvpr2018)|
|1570|Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper|We use similar CNN architectures for inverting both feature representations.|
|||Then we apply a CNN to F, as described above.|
||2 instances in total. (in cvpr2016)|
|1571|Kalogeiton_Action_Tubelet_Detector_ICCV_2017_paper|Related work  Almost all recent works [23, 27, 30, 36] for action localization build on CNN object detectors [19, 25].|
|||In the following, we review recent CNN object detectors and then state-of-the-art action localization approaches.|
||2 instances in total. (in iccv2017)|
|1572|Carlucci_AutoDIAL_Automatic_DomaIn_ICCV_2017_paper|We find that our unsupervised domain adaptation approach  outperforms state-of-the-art methods and can be applied to different CNN architectures, consistently improving their performance in domain adaptation problems.|
|||Our DA-layers are endowed with a set of alignment parameters, also learned by the network, which allow the CNN not only to align the source and target feature representations but also to automatically decide at each layer the required degree of adaptation.|
||2 instances in total. (in iccv2017)|
|1573|Multi-Way Multi-Level Kernel Modeling for Neuroimaging Classification|Method  SVM/SVM+PCA  K3rd [25]  sKL [36]  FK [28]  DuSK [13]  STTK [23]  3D CNN [10]  MMK  Data Post-processing Correlation Exploited Kernel Explored Parameters  Vectors One-way  Vectors One-way  Matrices One-way  Matrices One-way  Single-level  Single-level  Single-level  Single-level  C,   C,   C,   C,   3D Tensor Multi-way Single-level  C, , R  4D Tensor Multi-way Single-level C, , R, ERt  3D Tensor Multi-way Multi-level  Many*  3D Tensor Multi-way Multi-level  C, , R   SVM+PCA: We also implemented a vector-based subspace learning algorithm, which first uses principal component analysis (PCA) to reduce the input dimension and then feeds into SVM model.|
|||In particular, as can be seen from the results, 3D CNN achieves a relatively lower accuracy on ADNI dataset, which is because the deep learning method needs a large number of training data to train the deep neural network.|
||2 instances in total. (in cvpr2017)|
|1574|Auston_Sterling_ISNN_-_Impact_ECCV_2018_paper|Hershey, S., Chaudhuri, S., Ellis, D.P.W., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., Slaney, M., Weiss, R.J., Wilson,  ISNN: Impact Sound Neural Network  17  K.: Cnn architectures for large-scale audio classification.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||2 instances in total. (in eccv2018)|
|1575|Dai_Temporal_Context_Network_ICCV_2017_paper|Temporal Context Network applies a two stream CNN on a video for obtaining an intermediate feature representation.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||2 instances in total. (in iccv2017)|
|1576|Gilad_Divon_Viewpoint_Estimation_-_ECCV_2018_paper|Based on these insights, a network was designed such that: (i) The architecture jointly solves detection, classification, and pose estimation, using the most advanced CNN for performing the two former tasks.|
|||Massa, F., Marlet, R., Aubry, M.: Crafting a multi-task CNN for viewpoint esti mation.|
||2 instances in total. (in eccv2018)|
|1577|Krafka_Eye_Tracking_for_CVPR_2016_paper|4.1, we describe how we design an end-to-end CNN for robust eye tracking.|
|||We believe the large-scale data allows the CNN to effectively identify the fine-grained differences across peoples faces (their eyes) and hence make accurate predictions.|
||2 instances in total. (in cvpr2016)|
|1578|Redmon_You_Only_Look_CVPR_2016_paper|784  VOC 2012 test MR CNN MORE DATA [11] HyperNet VGG HyperNet SP Fast R-CNN + YOLO MR CNN S CNN [11] Faster R-CNN [27] DEEP ENS COCO NoC [28] Fast R-CNN [14] UMICH FGS STRUCT NUS NIN C2000 [7] BabyLearning [7] NUS NIN R-CNN VGG BB [13] R-CNN VGG [13] YOLO Feature Edit [32] R-CNN BB [13] SDS [16] R-CNN [13]  mAP aero 73.9 85.5 84.2 71.4 84.1 71.3 83.4 70.7 85.0 70.7 84.9 70.4 70.1 84.0 82.8 68.8 82.3 68.4 82.9 66.4 80.2 63.8 78.0 63.2 77.9 62.4 62.4 79.6 76.8 59.2 77.0 57.9 74.6 56.3 71.8 53.3 69.7 50.7 49.6 68.1  bike 82.9 78.5 78.3 78.5 79.6 79.8 79.4 79.0 78.4 76.1 73.8 74.2 73.1 72.7 70.9 67.2 69.1 65.8 58.4 63.8  bird 76.6 73.6 73.3 73.5 71.5 74.3 71.6 71.6 70.8 64.1 61.9 61.3 62.6 61.9 56.6 57.7 54.4 52.0 48.5 46.1  boat bottle bus 57.8 79.4 78.7 55.6 78.6 55.5 79.1 55.8 76.0 55.3 77.5 53.9 51.9 74.1 74.1 52.3 77.8 52.3 70.3 44.6 70.3 43.7 68.2 45.7 69.1 39.5 41.2 65.9 62.9 37.5 68.3 38.3 65.2 39.1 59.6 34.1 61.3 28.3 29.4 56.6  62.7 53.7 53.6 43.4 57.7 49.8 51.1 53.7 38.7 49.4 43.0 42.7 43.3 41.9 36.9 22.7 33.1 32.6 28.8 27.9  car 77.2 79.8 79.6 73.1 73.9 75.9 72.1 69.0 71.6 71.2 67.6 66.8 66.4 66.4 63.6 55.9 62.7 60.0 57.5 57.0  cat 86.6 87.7 87.5 89.4 84.6 88.5 88.6 84.9 89.3 84.6 80.7 80.2 78.9 84.6 81.1 81.4 69.7 69.8 70.8 65.9  chair 55.0 49.6 49.5 49.4 50.5 45.6 48.3 46.9 44.2 42.7 41.9 40.6 39.1 38.5 35.7 36.2 30.8 27.6 24.1 26.5  cow table 79.1 62.2 52.1 74.9 52.1 74.9 57.0 75.5 61.7 74.3 55.3 77.1 73.4 57.8 53.1 74.3 55.0 73.0 55.8 68.6 51.7 69.7 49.8 70.0 50.0 68.1 67.2 46.7 43.9 64.3 48.5 60.8 44.6 56.0 41.7 52.0 35.9 50.7 48.7 39.5  dog 87.0 86.0 85.6 87.5 85.5 86.9 86.1 85.0 87.5 82.7 78.2 79.0 77.2 82.0 80.4 77.2 70.0 69.6 64.9 66.2  horse mbike person plant sheep sofa 83.4 65.8 59.4 81.7 59.3 81.6 68.5 80.9 61.2 79.9 60.9 81.7 68.8 80.0 59.5 81.3 65.7 80.5 60.0 77.1 58.0 75.2 55.7 74.5 56.2 71.3 74.8 54.2 52.0 71.6 54.8 72.3 46.4 64.4 40.9 61.3 38.6 59.1 57.3 38.1  84.7 83.3 83.2 81.0 81.7 80.9 80.7 79.5 80.8 79.9 76.9 77.9 76.1 76.0 74.0 71.3 71.1 68.3 65.8 65.4  73.4 73.5 73.2 71.5 69.0 72.6 69.6 72.4 68.3 69.0 68.3 67.9 66.9 65.4 63.4 52.2 61.3 57.8 58.8 54.5  78.9 81.8 81.6 74.7 76.4 79.6 70.4 72.2 72.0 68.7 65.1 64.0 64.7 65.2 60.0 63.5 60.2 57.8 57.1 53.2  45.3 48.6 48.4 41.8 41.0 40.1 46.6 38.9 35.1 41.4 38.6 35.3 38.4 35.6 30.8 28.9 33.3 29.6 26.0 26.2  train 80.3 79.9 79.7 82.1 77.7 81.2 75.9 76.7 80.4 72.0 68.7 68.7 66.9 67.4 63.5 73.9 61.7 59.3 58.9 50.6  tv  74.0 65.7 65.6 67.2 72.1 61.5 71.4 68.1 64.2 66.2 63.3 62.6 62.7 60.3 58.7 50.8 57.8 54.1 50.7 51.6  Table 3: PASCAL VOC 2012 Leaderboard.|
|||Object detection via a multiregion & semantic segmentation-aware CNN model.|
||2 instances in total. (in cvpr2016)|
|1579|Object Region Mining With Adversarial Erasing_ A Simple Classification to Semantic Segmentation Approach|[17] introduced a constrained CNN model to address this problem.|
|||Hcp: A flexible cnn framework for multi-label image classification.|
||2 instances in total. (in cvpr2017)|
|1580|Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper|Mehta, D., Rhodin, H., Casas, D., Sotnychenko, O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation in the wild using improved cnn supervision.|
|||Zhou, X., Zhu, M., Pavlakos, G., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Monocap: Monocular human motion capture using a cnn coupled with a geometric prior.|
||2 instances in total. (in eccv2018)|
|1581|Thewlis_Unsupervised_Learning_of_ICCV_2017_paper|Quantitative results  In this section we evaluate the performance of our unsupervised landmarks quantitatively by testing how well they  Method  TCDCN [74]  Cascaded CNN [56]  CFAN [70]  Our Method (50 points)  Mean Error  7.95 9.73 15.84 6.67  Table 2.|
|||Method RCPR [8]  Cascaded CNN [56]  CFAN [70] TCDCN [74]  RAR [64]  Our Method (51 points)  Mean Error  11.6 8.97 10.94 7.65 7.23 10.53  Table 3.|
||2 instances in total. (in iccv2017)|
|1582|Keren_Ye_ADVISE_Symbolism_and_ECCV_2018_paper|We extract the images Inception-v4 CNN feature (1536D) using [58], then use a fully-connected layer with parameter w  R2001536 to project it to the 200-D joint embedding space:  v = w  CN N (x)  (3)  Text embedding.|
|||More specifically, we use the Inception-v4 model [58] to extract the 1536-D CNN features for all symbol proposals.|
||2 instances in total. (in eccv2018)|
|1583|Supancic_Tracking_as_Online_ICCV_2017_paper|Interactive Training and Evaluation  In this section, we describe our procedure for interactively learning CNN parameters w (that encode tracker action policies) from streaming video datasets.|
|||This ROI is resized to a canonical image size (e.g., 224  224) and processed with a CNN (VGG16) to produce a convolutional feature map.|
||2 instances in total. (in iccv2017)|
|1584|Aliakbarian_Encouraging_LSTMs_to_ICCV_2017_paper|Our CNN model for feature extraction is based on the VGG-16 structure with some modifications.|
|||The CNN that extracts these features learns to focus on the discriminative parts of the images, thus discarding the irrelevant information.|
||2 instances in total. (in iccv2017)|
|1585|Qiao_ScaleNet_Guiding_Object_ICCV_2017_paper|MultiBox [10] and SSD [29] compute object regions by bounding box regression based on CNN feature maps directly.|
|||More recently, DeepMask [33] assesses objectness and predicts object masks in a sliding window fashion based on CNN features, which achieved the state-of-the-art performance on the PASCAL VOC [11] and the MS COCO [26] datasets.|
||2 instances in total. (in iccv2017)|
|1586|cvpr18-LIME  Live Intrinsic Material Estimation|Structuring the CNN architecture in this way gives the opportunity for intermediate supervision of the physical quantities and leads to higher-quality results than other competing approaches, as shown in Section 6.|
|||The main reason is that the corresponding image-to-image translation task is easier, in the sense that the CNN has only to learn a per-pixel color function, instead of a color transform in combination with a spatial reordering of the pixel, as is the case for reflectance and environment maps.|
||2 instances in total. (in cvpr2018)|
|1587|Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper|Convergence We suggest a way to speed-up the training: residual-learning CNN and extremely high learning rates.|
|||We now study the effect of this modification to a standard CNN structure in detail.|
||2 instances in total. (in cvpr2016)|
|1588|cvpr18-Squeeze-and-Excitation Networks|The development of new CNN architectures is a challenging engineering task, typically involving the selection of many new hyperparameters and layer configurations.|
|||Deep roots: Improving CNN efficiency with hierarchical filter groups.|
||2 instances in total. (in cvpr2018)|
|1589|Dahl_Pixel_Recursive_Super_ICCV_2017_paper|[7] employed a three layer CNN with MSE loss.|
|||The conditioning network is a CNN that receives a low resolution image as input and outputs logits predicting the conditional log-probability of each high resolution (HR) image pixel.|
||2 instances in total. (in iccv2017)|
|1590|Luo_Deep_Dual_Learning_ICCV_2017_paper|I  LT  I  TL  I  LT	  I  T  (a) multitask + missing  (b) multitask + pseudo  (c) dual image segmentation  labels  : CNN segmentation  labels : CNN reconstruction  : forward propagation  Figure 1: Comparisons of recent semi-supervised learning settings.|
|||In other words, when a weakly labeled image I w is presented, the CNN produces a labelmap Lw, as its parameters are learned to do so from the fully annotated images.|
||2 instances in total. (in iccv2017)|
|1591|Learning Random-Walk Label Propagation for Weakly-Supervised Semantic Segmentation|Z+ path stopping time Pa|b 2  distribution of a given b i 2 {0, 1}|i| \ |i| Kronecker delta vector Shannon entropy of p H(p) cross-entropy of p, q H(p, q) KL(p k q) KL-divergence of p, q  training a CNN to predict the labels thus inferred.|
|||Method  Given densely labeled training images,  typical approaches for deep-learning-based semantic segmentation minimize the following cross-entropy loss [12, 3]:  min  2 Xx2X  H(y(x) , Q,I (x)),  (1)  where y(x) 2 |L| is the indicator vector of the ground truth label y(x) and Q,I (x) is a label distribution predicted by a CNN with parameters  evaluated on image I at location x.|
||2 instances in total. (in cvpr2017)|
|1592|Seung-Wook_Kim_Parallel_Feature_Pyramid_ECCV_2018_paper|However, the different abstraction levels of CNN feature layers often limit the detection performance, especially on small objects.|
|||The early CNN-based object detectors utilize a deep CNN (DCNN) model as part of an object detection system.|
||2 instances in total. (in eccv2018)|
|1593|Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper|Therefore, a set of preliminary experiments was conducted using a dummy SubUNet (One layer of BLSTM with 100 units in each direction) with all well known CNN architectures, to check the practical limitations of memory use on  2https://github.com/BVLC/caffe/pull/4681/ 3https://github.com/neccam/SubUNets  a single Titan X GPU.|
|||Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled.|
||2 instances in total. (in iccv2017)|
|1594|Jia_Guiding_the_Long-Short_ICCV_2015_paper|For most evaluated methods, they use CNN with deeper network architecture such as OxfordNet [30] and GoogLeNet [33].|
|||Methods which do not use a deeper CNN include LRCN-CaffeNet [7] and m-RNNAlexNet [26].|
||2 instances in total. (in iccv2015)|
|1595|Jayaraman_Learning_Image_Representations_ICCV_2015_paper|The work of [16] quantifies the invariance/equivariance of various standard representations, including CNN features, in terms of their responses to specified in-plane 2D image transformations (affine warps, flips of the image).|
|||We use grayscale camera 0 frames (see [7]), downsampled to 3232 pixels, so that we can adopt CNN architecture choices known to be effective for tiny images [1].|
||2 instances in total. (in iccv2015)|
|1596|cvpr18-Unsupervised Learning of Monocular Depth Estimation and Visual Odometry With Deep Feature Reconstruction|The known camera motion between stereo cameras TLR constrains the Depth CNN and Odometry CNN to predict depth and relative camera pose with actual scale.|
|||An extensive study of CNN architectures more suitable for odometry estimation and a possible way of integrating the map information over time are challenging but very fruitful future directions.|
||2 instances in total. (in cvpr2018)|
|1597|cvpr18-View Extrapolation of Human Body From a Single Image|In our opinion, the reason why CNN doesnt work is that the projection involves complicated per-pixel matrix calculation and homogeneous coordinates normalization, which are difficult for CNN to fit.|
|||Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||2 instances in total. (in cvpr2018)|
|1598|Guo_Learning_Dynamic_Siamese_ICCV_2017_paper|f l() represents a CNN to extract the deep feature at lth layer.|
|||2 with black dashed lines and formulate it as  Sl t = corr(f l(O1), f l(Zt)),  (1)  21764  where Sl t is a response map denoting the similarity between O1 and candidate patches in Zt; f l() represents the lth layer deep feature of some properly trained CNN model, e.g.|
||2 instances in total. (in iccv2017)|
|1599|cvpr18-Independently Recurrent Neural Network (IndRNN)  Building a Longer and Deeper RNN|On the other hand, comparing with the deep CNN architectures which could be over 100 layers such as the residual CNN [12] and the pseudo-3D residual CNN (P3D) [37], most of the existing RNN architectures only consist of several layers (2 or 3 for example [23, 39, 26]).|
|||Also the layers used to process the input can be of the residual structures in the same way as in CNN [12].|
||2 instances in total. (in cvpr2018)|
|1600|Hongmei_Song_Pseudo_Pyramid_Deeper_ECCV_2018_paper|3.1 Spatial Saliency Learning via PDC Module  A typical CNN model is comprised of a stack of convolution layers, interleaved with non-linear downsampling operation (e.g., max pooling) and point-wise nonlinearity (e.g., ReLU ).|
|||The recently proposed dilated convolution [50] provides a good alternative that efficiently computes dense CNN features at any receptive field sizes without loss of resolution.|
||2 instances in total. (in eccv2018)|
|1601|cvpr18-Fine-Grained Video Captioning for Sports Narrative|Unfortunately, most of the previous works [38, 42, 18] only extracts very coarse CNN features for video representation.|
|||Later works [38, 9] use CNN features to represent the whole content of the [31] detect people in movies to refer to them in video.|
||2 instances in total. (in cvpr2018)|
|1602|Meinhardt_Learning_Proximal_Operators_ICCV_2017_paper|As expected the deep CNN from [15] which was specifically trained on demosaicking outperforms our approach.|
|||Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||2 instances in total. (in iccv2017)|
|1603|cvpr18-Learning to Adapt Structured Output Space for Semantic Segmentation|[24], one can transform a classification CNN (e.g., AlexNet [19], VGG [33], or ResNet [11]) to a fully-convolutional network (FCN) for semantic segmentation.|
|||Numerous methods [7, 8, 25, 26, 34, 36, 37] are developed based on CNN classifiers due to performance gain.|
||2 instances in total. (in cvpr2018)|
|1604|Deep Watershed Transform for Instance Segmentation|In a similar spirit, [12] selects proposals using CNN features and non-maximum suppression.|
|||CNN: [15] leverages only a CNN trained to provide multiple outputs to simultaneously predict instance numbers, bounding box coordinates, and category confidence scores for each pixel.|
||2 instances in total. (in cvpr2017)|
|1605|Ye_Yuan_3D_Ego-Pose_Estimation_ECCV_2018_paper|Visual motion features from the optical flow are extracted by a CNN and passed to a bidirectional LSTM before going into a multilayer perceptron (MLP) which makes the state predictions.|
|||All three networks employ the same architecture for processing the optical flow: a CNN with three convolutional layers of kernel size 4 and stride 4 is used and the size of its hidden channels are (32, 32, 8), and a bidirectional LSTM is used to distill temporal information from the CNN features.|
||2 instances in total. (in eccv2018)|
|1606|Royer_Classifier_Adaptation_at_2015_CVPR_paper|To avoid a bias due to this choice, we report results for two different base classifiers: CNN is a convolutional network classifier based on the CCV library.5 We downloaded pre-trained models that achieve close to state-of-theart performance for ILSVRC2010 and ILSVRC2012 from the libCCV website.|
|||on ILSVRC2012, the CNN error rate is reduced from 16.1% to 5.2%, and the SVM error rate from 52.8% to 21.0%).|
||2 instances in total. (in cvpr2015)|
|1607|Detecting Visual Relationships With Deep Relational Networks|To utilize this information, we extract an appearance feature for each candidate pair of objects, by applying a CNN [49, 50] to an enclosing box, i.e.|
|||(2) Joint-CNN [48] also works poorly, as its hard for the CNN to learn a common feature representation for both relationship predicates and objects.|
||2 instances in total. (in cvpr2017)|
|1608|Quanlong_Zheng_Task-driven_Webpage_Saliency_ECCV_2018_paper|Recent works have made significant performance improvements, due to the strong representation power of CNN features.|
|||Separate CNNs: We train 5 separate CNNs for each of the 5 tasks, and select the corresponding CNN for a given task, to predict the saliency.|
||2 instances in total. (in eccv2018)|
|1609|You_Image_Captioning_With_CVPR_2016_paper|Visual features of CNN responses v and attribute detections {Ai} are injected into RNN (dashed arrows) and get fused together through a feedback loop (blue arrows).|
|||The CNN image feature v is only used in the initial input node x0, which is expected to give RNN a quick overview of the image content.|
||2 instances in total. (in cvpr2016)|
|1610|cvpr18-Cascade R-CNN  Delving Into High Quality Object Detection|To reduce redundant CNN computations in the R-CNN for speeds-up, the SPP-Net [17] and Fast R-CNN [13] introduced the idea of region-wise feature extraction.|
|||Object detection via a multiIn  region and semantic segmentation-aware CNN model.|
||2 instances in total. (in cvpr2018)|
|1611|Fanyi_Xiao_Object_Detection_with_ECCV_2018_paper|The STMMs design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection.|
|||However, [51,50] use optical flow, which needs to be computed either externally e.g., using [5], or in-network through another large CNN e.g., FlowNet [13].|
||2 instances in total. (in eccv2018)|
|1612|Brazil_Illuminating_Pedestrians_via_ICCV_2017_paper|Object detection via a multiregion and semantic segmentation-aware cnn model.|
|||Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||2 instances in total. (in iccv2017)|
|1613|Cakir_MIHash_Online_Hashing_ICCV_2017_paper|For feature representation, we use CNN features extracted from the f c7 layer of a VGG-16 network [21] pre-trained on ImageNet.|
|||We extract CNN features from the f c7 layer of an AlexNet [8] pre-trained on ImageNet, and reduce the dimensionality to 128 using PCA.|
||2 instances in total. (in iccv2017)|
|1614|Anurag_Ranjan_Generating_3D_Faces_ECCV_2018_paper|[48] also present a spectral CNN for labeling nodes but does not involve any mesh dimensionality reduction.|
|||Jackson, A.S., Bulat, A., Argyriou, V., Tzimiropoulos, G.: Large pose 3D face reconstruction from a single image via direct volumetric CNN regression.|
||2 instances in total. (in eccv2018)|
|1615|Yikang_LI_Factorizable_Net_An_ECCV_2018_paper|(1) RPN is used for object region proposals, which shares the base CNN with other parts.|
|||Dataset  Model  PhrDet  SGGen  Rec@50 Rec@100 Rec@50 Rec@100  Speed  VRD [37]  VG-MSDN [28, 35]  VG-DR-Net [6, 28]  LP [37] 16.17 ViP-CNN [34] 22.78 DR-Net [6] 19.93 ILC [42] 16.89 Ours Full:1-SMP 25.90 Ours Full:2-SMP 26.03 ISGG [58] 15.87 MSDN [35] 19.95 Ours-Full: 2-SMP 22.84 DR-Net [6] 23.95 Ours-Full: 2-SMP 26.91  1.18 0.78 2.83 2.70 0.45 0.55 1.64 3.56 0.55 2.83 0.55  Only consider the post-processing time given the CNN features and object detection results.|
||2 instances in total. (in eccv2018)|
|1616|ScanNet_ Richly-Annotated 3D Reconstructions of Indoor Scenes|For the semantic voxel labeling task, we introduce a new volumetric CNN architecture.|
|||As a baseline evaluation, we run the 3D CNN approach of Qi et al.|
||2 instances in total. (in cvpr2017)|
|1617|Fang_RMPE_Regional_Multi-Person_ICCV_2017_paper|Representative works include DeepPose (Toshev et al) [34], DNN based models [24, 11] and various CNN based models [19, 33, 23, 4, 38].|
|||In our work, we use a CNN based SPPE method to estimate poses, while Pishchulin et al.|
||2 instances in total. (in iccv2017)|
|1618|Heber_Neural_EPI-Volume_Networks_ICCV_2017_paper|Modern CNN architectures basically alternate between convolutions and Rectified Linear Units (ReLUs) [24].|
|||They also used a 4D anisotropic diffusion tensor to guide the regularization and a confidence measure to gauge the reliability of the CNN prediction.|
||2 instances in total. (in iccv2017)|
|1619|Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper|DeepTrack [17] learns two-layer CNN classifiers from binary samples and does not require a pre-training procedure.|
|||Fan and Ling [6] combines a DSST tracker [4] with a CNN detector [31] that verifies and potentially corrects proposals of the short-term tracker.|
||2 instances in total. (in eccv2018)|
|1620|cvpr18-AON  Towards Arbitrarily-Oriented Text Recognition|[13] and [31] proposed the end-to-end neural networks that combines CNN and RNN for visual feature representation, then the CTC [10] Loss was combined with the RNN outputs for calculating the conditional probability between the predicted and the target sequences.|
|||R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection.|
||2 instances in total. (in cvpr2018)|
|1621|cvpr18-Fast and Accurate Online Video Object Segmentation via Tracking Parts|The CF2 method [30] learns correlation filters adaptively based on CNN features, thereby enhancing the ability to handle challenging factors such as deformation and occlusion.|
|||Similar to semantic segmentation, our objective is to minimize the weighted cross-entropy loss for a binary (foreground/background) task:  L(P ) = (1  w) X  log E(yij = 1; )  i,jf g  w X  i,jbg  log E(yij = 0; ),  (2)  where  denotes CNN parameters, yij denotes the network prediction for the input part P at pixel (i, j) and w is the foreground-background pixel-number ratio used to balance the weights [45].|
||2 instances in total. (in cvpr2018)|
|1622|Non-Uniform Subset Selection for Active Learning in Structured Data|In order to represent the scene nodes, we extract CNN features ( R40961) from fc-7 layer of VGG-net [52] pre-trained on the places-205 dataset.|
|||We use the pipeline of R-CNN [17] to detect the objects and then extract CNN features from fc-7 layer of Alex-net [26], pre-trained on ImageNet [12].|
||2 instances in total. (in cvpr2017)|
|1623|cvpr18-Improving Object Localization With Fitness NMS and Bounded IoU Loss|Modern object detection methods most often utilize a CNN [11] based bounding box regression and classification stage followed by a Non-Max Suppression method to identify unique object instances.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||2 instances in total. (in cvpr2018)|
|1624|cvpr18-Beyond Trade-Off  Accelerate FCN-Based Face Detector With Higher Accuracy|Note that CNN can better explore the scale and spatial information in the high-level representation and this also proves that the network can learn both of the scale and spatial information of the face at the same time.|
|||Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection.|
||2 instances in total. (in cvpr2018)|
|1625|Hierarchical Boundary-Aware Neural Encoder for Video Captioning|[44] used CNN features extracted from single frames, mean pooled them to represent the entire video, and then fed the resulting vector to a LSTM layer [12] for the generation of the caption.|
|||LSTM-YT used a mean pool strategy on frame-level CNN features to encode the input video, while the caption was generated by a LSTM layer.|
||2 instances in total. (in cvpr2017)|
|1626|Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations|Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
|||Learning camera viewpoint using CNN to improve 3D body pose estimation.|
||2 instances in total. (in cvpr2017)|
|1627|cvpr18-Dynamic Video Segmentation Network|Unfortunately, contemporary state-of Equal contribution  Figure 1: Comparison of frames at timestamps t and t + 10 in two video sequences  the-art CNN models usually employ deep network architectures to extract high-level features from raw data [14, 15, 16, 17, 18], leading to exceptionally long inference time.|
|||DN is a lightweight CNN consists of only a single convolutional layer and three fullyconnected layers.|
||2 instances in total. (in cvpr2018)|
|1628|cvpr18-Seeing Small Faces From Robust Anchor's Perspective|Its clear that our detector consistently achieves best performance on all face cases against recent published face detection methods: SFD [48], SSH [26], ScaleFace [44], HR [11], CMS-RCNN [50], Multitask Cascade CNN [47], LDCF+ [27] and Faceness [42].|
|||Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection.|
||2 instances in total. (in cvpr2018)|
|1629|Kar_Amodal_Completion_and_ICCV_2015_paper|We impose an L2 penalty on the targets and regress from the extracted CNN image features to the targets.|
|||We initialize our model using the AlexNet [19] CNN pretrained for Imagenet [5] classification and then finetune the model specific to our task using backpropagation.|
||2 instances in total. (in iccv2015)|
|1630|HOPE_ Hierarchical Object Prototype Encoding for Efficient Object Instance Search in Videos|In this work, we exploit CNN features to represent object proposals.|
|||Particular object retrieval with integral max-pooling of cnn activations.|
||2 instances in total. (in cvpr2017)|
|1631|Zhou_Towards_3D_Human_ICCV_2017_paper|Monocular 3d human pose estimation using transfer learning and improved cnn supervision.|
|||Monocap: Monocular human motion capture using a cnn coupled with a geometric prior.|
||2 instances in total. (in iccv2017)|
|1632|HPatches_ A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors|Finally, we evaluate several recent deep descriptors including the siamese variants of DeepCompare [41] (DCS, DC-S2S) with one and two stream CNN architectures for one or two patch crops, DeepDesc [30] (DDESC), which exploits hard-negative mining, and the TFeat margin* (TFM) and ratio* (TF-R) of the TFeat descriptor [4], based on shallow convolutional networks, triplet learning constraints and fast hard negative mining.|
|||In terms of speed, the binary descriptors BRIEF and ORB are 4 times faster than the most efficient CNN based features i.e.TF-.|
||2 instances in total. (in cvpr2017)|
|1633|cvpr18-OLÉ  Orthogonal Low-Rank Embedding - A Plug and Play Geometric Loss for Deep Learning|We illustrate this on different datasets and using four of the most popular CNN architectures: VGG [32], ResNets [11], PreResNets [12] and DenseNets [14].|
|||Person re-identification by multichannel parts-based CNN with improved triplet loss function.|
||2 instances in total. (in cvpr2018)|
|1634|Yabin_Zhang_Fine-Grained_Visual_Categorization_ECCV_2018_paper|Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition.|
|||Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: Cnn features off-the-shelf: an astounding baseline for recognition.|
||2 instances in total. (in eccv2018)|
|1635|Hung_Scene_Parsing_With_ICCV_2017_paper|Recently, CNN based methods such as the fully convolutional neural network (FCN) [20] have achieved the stateof-the-art results in semantic segmentation.|
|||The global context network is designed based on a CNN classification network (e.g., AlexNet [14] or VGG [23]) without the last fully-connected and softmax layers.|
||2 instances in total. (in iccv2017)|
|1636|Fully-Adaptive Feature Sharing in Multi-Task Networks With Applications in Person Attribute Classification|Ubernet: Training a universal cnn for low-, mid, and highlevel vision using diverse datasets and limited memory.|
|||Person attribute recogIn ICCV  nition with a jointly-trained holistic cnn model.|
||2 instances in total. (in cvpr2017)|
|1637|Najibi_SSH_Single_Stage_ICCV_2017_paper|WIDER Dataset Result  We compare SSH with HR [7], CMS-RCNN [38], Multitask Cascade CNN [37], LDCF [20], Faceness [34], and Multiscale Cascade CNN [35].|
|||Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection.|
||2 instances in total. (in iccv2017)|
|1638|cvpr18-Learning Dual Convolutional Neural Networks for Low-Level Vision|[38] develop a CNN model to approximate a number of filters.|
|||While the residual learning (i.e., VDSR) approach performs better than the SRCNN, the generated images with the plain CNN model [5] contain blurry boundaries or rainy streaks (Figure 8(d)).|
||2 instances in total. (in cvpr2018)|
|1639|Tz-Ying_Wu_Liquid_Pouring_Monitoring_ECCV_2018_paper|Some methods further demonstrate that liquid amount can be estimated by combining semantic segmentation CNN and LSTM [34,7].|
|||Ch eron, G., Laptev, I., Schmid, C.: P-cnn: Pose-based cnn features for action  recognition.|
||2 instances in total. (in eccv2018)|
|1640|Chen_No_More_Discrimination_ICCV_2017_paper|[18] utilize CNN for performing pixel-level classification, which is able to produce pixel-wise outputs of arbitrary sizes.|
|||In order to achieve high resolution prediction, [22, 2] further adapt deconvolution layers into CNN with  1https://maps.googleblog.com/2014/04/go-back-in-time-with-street view.html  promising performances.|
||2 instances in total. (in iccv2017)|
|1641|Finding Tiny Faces|We then feed the scaled input into a CNN to predict template responses (for both detection and regression) at every resolution.|
|||Cms-rcnn: Contextual multi-scale region-based cnn for unconstrained face detection.|
||2 instances in total. (in cvpr2017)|
|1642|Saha_AMTnet_Action-Micro-Tube_Regression_ICCV_2017_paper|Subsequently, CNN features are ex 1A ROI is a rectangular bounding box parameterized as 4 coordinates  in a 2D plane [x1 y1 x2 y2].|
|||The mini-batch is firstly used to compute the actionness classification and 3D proposal regression losses (g) ( 4.1), and secondly, to pool CNN features (for each 3D proposal) using a bilinear interpolation layer (h) ( 3.4).|
||2 instances in total. (in iccv2017)|
|1643|Li_FoveaNet_Perspective-Aware_Urban_ICCV_2017_paper|However, directly applying the generic CNN based image parsing models usually leads to unsatisfactory results on urban scene images for self-driving cars, since they ignore the important perspective geometry of scene images.|
|||Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||2 instances in total. (in iccv2017)|
|1644|Feichtenhofer_Detect_to_Track_ICCV_2017_paper|The detector scores across the video are re-scored by a 1D CNN model.|
|||Object detection via a multiregion and semantic segmentation-aware cnn model.|
||2 instances in total. (in iccv2017)|
|1645|Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation|(KB)  Gated CNN ECCV2016 [43] ECCV2016 [42] SSDL SCSP CVPR2016 [3] NSL CVPR2016 [53] ICCV2015 [61] eSDC CVPR 2013 [58] KISSME (LOMO) CVPR 2012 [16] SDALF CVPR2010 [10]  BoW+KISSME  CSBT  Ours  65.9 39.4 51.9 55.4 39.6 33.5 40.5 20.5  42.9  39.6 19.6   17.7 13.5 19.0 8.2  20.3   2.08e+00 7.47e+02   4.32e+04   1.21e+01 4.13e+06 1.54e+04 1.45e+06   2.53e+02  9.31e+04  4.7e-04  1.52e+03  Deep Learning DeepReID [19], Improved Deep [1], DCSL [55], SIR+CIR [45], EDM [40], SSDL [42] and Gated CNN [43]; 4) Semantic Attribute based Matching Semantic [41]; 5) SDALF [10].|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in cvpr2017)|
|1646|Dosovitskiy_Learning_to_Generate_2015_CVPR_paper|Conceptually the generative network, which we formally refer to as g(c, v, ), looks like a usual CNN turned upside down.|
|||5.2.1 Correspondences  The ability of the generative CNN to interpolate between different chairs allows us to find dense correspondences between different object instances, even if their appearance is very dissimilar.|
||2 instances in total. (in cvpr2015)|
|1647|Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper|It is straightforward to incorporate CNN into our framework, e.g., directly replacing HOG (used below) with CNN feature maps.|
|||In practice, some descriptors pooled on spatial grids such as HOG [17] or high-level features from CNN [38] can be utilized.|
||2 instances in total. (in cvpr2016)|
|1648|cvpr18-Learning to Evaluate Image Captioning|The model comprises three major components: a CNN to compute image representations, an RNN with LSTM cells to encode the caption, and a binary classifier as the critique.|
|||We use a state-ofthe-art CNN architecture to capture high-level image representations, and a RNN with LSTM cells to encode captions.|
||2 instances in total. (in cvpr2018)|
|1649|Improving Pairwise Ranking for Multi-Label Image Classification|We define g() as an MLP on top of f (x), which is the second to the last layer of the CNN (e.g., fc7 layer) as input to the MLP and adding two hidden layers on top, where each hidden layer is followed by a ReLU nonlinearlity [19].|
|||(22) are  For the label prediction model, we use VGG16 [26] pretrained on the ImageNet ILSVRC challenge dataset [23] as our CNN model, replace the softmax loss from the original model with our LSEP loss, and finetune it for 10 epochs.|
||2 instances in total. (in cvpr2017)|
|1650|cvpr18-CarFusion  Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles|For each hypothesis, The structured points are 14 car keypoints, obtained by training the Stacked hourglass CNN architecture [32] the KITTI dataset [24, 18].|
|||This could be due to the limitation of the CNN architecture where the training image is down sampled substantially.|
||2 instances in total. (in cvpr2018)|
|1651|cvpr18-Beyond the Pixel-Wise Loss for Topology-Aware Delineation|A fully-connected network was replaced by a CNN in [15] for  In [14] a differentiable Intersection-overroad detection.|
|||In the task of edge detection, nested, multiscale CNN features were utilized by Holistically-Nested Edge Detector [34] to directly produce an edge map of entire image.|
||2 instances in total. (in cvpr2018)|
|1652|RAFAEL_FELIX_Multi-modal_Cycle-consistent_Generalized_ECCV_2018_paper|The visual features, represented by x, are extracted from a state-of-art CNN model, and the semantic features, represented by a, are available from the training set.|
|||For the synthetic visual representations, we generate 2048-dim CNN features using one of the feature generation models, presented in Sec.|
||2 instances in total. (in eccv2018)|
|1653|Rothe_Some_Like_It_CVPR_2016_paper|Then we extract CNN features.|
|||We augment each movie with the poster image from IMDB and extract the same deep CNN features as for the Hot-or-Not dataset.|
||2 instances in total. (in cvpr2016)|
|1654|Jingyi_Zhang_Generative_Domain-Migration_Hashing_ECCV_2018_paper|We compare GDH with 8 existing category-level SBIR methods, including 4 hand-crafted methods: LSK [38], SEHLO [37], GF-HOG [13] and HOG [4]; and 4 deep learning based methods: 3D shape [50], Sketch-a-Net (SaN) [56], GN Triplet [40] and Siamese CNN [35].|
|||Methods  Dimension  MAP  Retrieval time per query (s)  Memory cost (MB)  (204,489 images)  MAP  Retrieval time per query (s)  Memory cost (MB)  (73,002 images)  TU-Berlin Extension  Sketchy  HOG [4]  GF-HOG [13] SHELO [37]  LKS [38]  Siamese CNN [35]  SaN [56]  GN Triplet [40] 3D shape [50] Siamese-AlexNet Triplet-AlexNet  1296 3500 1296 1350  64 512 1024  64  4096 4096  GDH  (Proposed)  32 (bits) 64 (bits) 128 (bits)  0.091 0.119 0.123 0.157 0.322 0.154 0.187 0.072 0.367 0.448 0.563 0.690 0.659  1.43 4.13 1.44 0.204  7.70102  0.53 1.02  7.53102  5.35 5.35  5.57104 7.03104 1.05103  2.02  103 5.46  103 2.02  103 2.11  103  99.8  7.98  102 1.60  103  99.8  6.39  103 6.39  103  0.78 1.56 3.12  0.115 0.157 0.182 0.190 0.481 0.208 0.529 0.084 0.518 0.573 0.724 0.810 0.784  0.53 1.41 0.50 0.56  2.76102  0.21 0.41  2.64 102  1.68 1.68 s  2.55104 2.82104 3.53104  7.22  102 1.95  103 7.22  102 7.52  102  35.4  2.85  102 5.70  102  35.6  2.28  103 2.28  103  0.28 0.56 1.11   denotes that we directly use the public models provided by the original papers without any  fine-tuning on the TU-Berlin Extension and Sketchy datasets.|
||2 instances in total. (in eccv2018)|
|1655|Liang_A_Unified_Multiplicative_ICCV_2015_paper|From our perspective, the feature we used is highly sparse because the rectified linear units of CNN discard the negative part of the input.|
|||In addition, the features extracted from a higher CNN layer represent abstract visual patterns instead of low-level patterns, such as color or shape.|
||2 instances in total. (in iccv2015)|
|1656|Wang_Whats_Wrong_With_CVPR_2016_paper|Experimental Settings  In this paper, we use the pre-trained CNN model [15] as feature extractors for object detector learning.|
|||Feeding an image into the CNN model, the activations of a convolutional layer are n  m  d (e.g., 14  14  512 for the last convolutional layer) with n, m corresponding to different spatial locations and d the number of feature maps.|
||2 instances in total. (in cvpr2016)|
|1657|cvpr18-Style Aggregated Network for Facial Landmark Detection|Many of them leverage deep CNN to learn facial features and regressors in an end-to-end fashion [51, 31, 73] with a cascade architecture to progressively update the landmark estimation [73, 51, 10].|
|||[66] propose a deep deformation network to incorporates geometric constraints within the CNN framework.|
||2 instances in total. (in cvpr2018)|
|1658|Fabian_Manhardt_Deep_Model-Based_6D_ECCV_2018_paper|We teach the CNN to align contours between synthetic object renderings and scene images under changing illumination and occlusions and show that our approach can deal with a variety of shapes and textures.|
|||3 Methodology  In this section we explain our approach to train a CNN to regress a 6D pose refinement from RGB information alone.|
||2 instances in total. (in eccv2018)|
|1659|cvpr18-Detail-Preserving Pooling in Deep Networks|This is the worst case overhead for the majority of existing CNN architectures.|
|||Experiments  We verify the effectiveness of our approach with a series of experiments on different datasets and CNN architectures.|
||2 instances in total. (in cvpr2018)|
|1660|Video2Shop_ Exact Matching Clothes in Videos to Online Shopping Images|For these baselines, average pooling and max pooling are directly used on the CNN features of clothing trajectories.|
|||Fisher vector and VLAD are used to encode the CNN features of shopping images and clothing trajectories, respectively.|
||2 instances in total. (in cvpr2017)|
|1661|Zhang_Truncating_Wide_Networks_ICCV_2017_paper|As shown in [11, 29], the parameter size of a trained CNN can be reduced by constructing a new CNN with less redundant weights.|
|||The CNN architecture employed for classification using the Cifar-10 and Cifar-100.|
||2 instances in total. (in iccv2017)|
|1662|Mostafa_Ibrahim_Hierarchical_Relational_Networks_ECCV_2018_paper|Formally, given a video frame, the ith person representation P l  i in the lth relational  layer is computed as follows:  P 0  i = CN N (Ii) i = X P l jE l  i  F l(P l1  i  P l1  j  ; l)  (1)  (2)  6  Mostafa S. Ibrahim and Greg Mori  i is the initial ith person representation derived from a CNN on cropped image i is the set of relationship edges from the ith person in the graph Gl used for the i  RNl where Nl is the output size  where P 0 Ii, E l lth layer, and  is the concatenation operator.|
|||That is, we again stack multiple relational layers of increasing size that decode a compressed feature vector to its original CNN representation.|
||2 instances in total. (in eccv2018)|
|1663|Dong_Yang_Proximal_Dehaze-Net_A_ECCV_2018_paper|The compared methods include dark channel prior (DCP) [11], fast visibility restoration (FVR) [30], boundary constraint and contextual regularization (BCCR) [18], gradient residual minimization (GRM) [5], color attenuation prior (CAP) [38], non-local dehazing (NLD) [2], multi-scale CNN (MSCNN) [20], DehazeNet [4] and AOD-Net [14].|
|||Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image restora tion.|
||2 instances in total. (in eccv2018)|
|1664|cvpr18-Hierarchical Novelty Detection for Visual Object Recognition|Hence, it is desirable to extend the CNN architectures for detecting the novelty of an object (i.e., deciding if the object does not match any previously trained object classes).|
|||We take ResNet-101 [12] as a visual feature extractor (i.e., the penultimate layer of the CNN before the classification layer) for all compared methods.|
||2 instances in total. (in cvpr2018)|
|1665|Li_Deep_Contrast_Learning_CVPR_2016_paper|As a result, these methods usually have to run a CNN at least thousands of times (once for every patch) to obtain a complete saliency map.|
|||The hole algorithm, which is also called `a trous algorithm, was originally developed for efficient computation of the undecimated wavelet transform [35], and has recently been implemented in Caffe [6, 27] to efficiently compute dense CNN feature maps at any target subsampling rate without introducing any approximation.|
||2 instances in total. (in cvpr2016)|
|1666|He_Deep_Direct_Regression_ICCV_2017_paper|For most CNN based detection methods like Fast-RCNN [3], Faster-RCNN, SSD, Multi-Box [2], the regression task is trained to regress the offset values from a proposal to the corresponding ground truth (See Fig.1.a).|
|||Features for small texts could fade a lot after the first down-sampling operations, and large texts would lose much context information causing  the CNN could only see some simple strokes of the large texts.|
||2 instances in total. (in iccv2017)|
|1667|Bendale_Towards_Open_World_2015_CVPR_paper|Recent studies on ImageNet dataset using SVMs or CNN require days to train their system [30, 17], e.g.|
|||5-6 CPU/GPU days in case of CNN for 1000 category image  1  Figure 1: In open world recognition, the system must be able to recognize objects and associate them with known classes while also being able to label classes as unknown.|
||2 instances in total. (in cvpr2015)|
|1668|Yang_Feng_Video_Re-localization_via_ECCV_2018_paper|Motivated by bilinear CNN [18], we propose a bilinear matching method to further exploit the interactions between  hq i , which can be written as:  i and  hr  tij =  hq  i W b  j   hr i + bb j,  (4)  where tij is the j-th dimension of the bilinear matching result, given by ti = [ti1, ti2, .|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||2 instances in total. (in eccv2018)|
|1669|Zero-Shot Classification With Discriminative Semantic Representation Learning|Zero(cid:173)Shot Classification  We compared the proposed approach with a baseline IAP [16] with CNN features and a few state-of-the-art ZSL methods recently developed in the literature and reported the results in Table 2.|
|||We compared to their results produced in the same experimental setting as other methods, i.e., using CNN features and label attributes.|
||2 instances in total. (in cvpr2017)|
|1670|AMC_ Attention guided Multi-modal Correlation Learning for Image Search|Equipped with a more complex RNN / CNN model to process caption modality, AMC models will expect further boost in performance.|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||2 instances in total. (in cvpr2017)|
|1671|Li_Learning_From_Noisy_ICCV_2017_paper|However, our method has a big advantage over the Baseline Ensemble in that we only need one forward pass on our CNN model while Baseline Ensemble need to run two CNN models, which is twice  1916  T  mAP(%)  0.5 41.5  1  5  41.6  39.8  10 35.3  Table 5: mAP performance using different T on the Species-I dataset  time consuming as well as doubles storage compared to ours.|
|||Since we are training a high capacity CNN model, we can make a reasonable assumption that the bias term l( s, y) is close to zero.|
||2 instances in total. (in iccv2017)|
|1672|cvpr18-Learning Intrinsic Image Decomposition From Watching the World| indicates WHDR is evaluated based on CNN classifer outputs for pairs of pixels rather than full decompositions.|
|||Additionally, like other CNN approaches [28, 34], the direct predictions from our network may not strictly satisfy I = R  S since the  9045  MSE  LMSE  DSSIM  Method  Training set GT  refl.|
||2 instances in total. (in cvpr2018)|
|1673|cvpr18-Vision-and-Language Navigation  Interpreting Visually-Grounded Navigation Instructions in Real Environments|Image and action embedding For each image observation ot, we use a ResNet-152 [22] CNN pretrained on ImageNet [46] to extract a mean-pooled feature vector.|
|||As we have discretized the agents heading and elevation changes in 30 degree increments, for fast training we extract and pre-cache all CNN feature vectors.|
||2 instances in total. (in cvpr2018)|
|1674|Loss Max-Pooling for Semantic Image Segmentation|All our reported numbers and plots are obtained from finetuning the MS-COCO [32] pre-trained CNN of [9], which is available for download4.|
|||We only report results obtained from a single CNN as opposed to using an ensemble of CNNs, trained using the stochastic gradient descent (SGD) solver with polynomial decay of the learning rate (poly as described in [9]) setting both, decay rate and momentum to 0.9.|
||2 instances in total. (in cvpr2017)|
|1675|Minxian_Li_Unsupervised_Person_Re-identification_ECCV_2018_paper|(3) Our TAUDL is simpler to train with a simple end-to-end model learning, as compared to the alternated deep CNN training and clustering required by PUL and a two-stage model training of TJ-AIDL.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in eccv2018)|
|1676|Francisco_M._Castro_End-to-End_Incremental_Learning_ECCV_2018_paper|In all the experiments below, iCaRL refers to the final method  End-to-End Incremental Learning  9  in [23], and hybrid1 refers to their variant, which uses a CNN classifier instead of NMC.|
|||For 50 classes, although we achieve the same score as Hybrid1 (the variant of iCaRL using CNN classifier), we are 1% lower than iCaRL.|
||2 instances in total. (in eccv2018)|
|1677|cvpr18-Revisiting Deep Intrinsic Image Decompositions|A second more recent method from [18] trains an encoder-decoder CNN to learn albedo, shading and specular images with millions of object-level synthetic intrinsic images via rendering ShapeNet [5] 1; however, this approach does not apply to scene-level images as we consider herein.|
|||Note that [1] uses a number of specialized priors appropriate for this simplified object-level data, while end-to-end CNN approaches like ours and [19] have less advantage here due to limited training data (110 images).|
||2 instances in total. (in cvpr2018)|
|1678|Matteo_Fabbri_Learning_to_Detect_ECCV_2018_paper|The computed feature maps are subsequently processed by a three-branch multi-stage CNN where each branch focuses on a different aspect of body pose estimation: the first branch predicts the heatmaps of the visible parts, the second branch predicts the heatmaps of the occluded parts and the third branch predicts the part affinity fields (PAFs), which are vector fields used to link parts together.|
|||Architecture of the three-branch multi-stage CNN with corresponding kernel size (k) and number of feature maps (n) indicated for each convolutional layer  and the presence of a joint is indeed strongly influenced by the persons silhouette (e.g.|
||2 instances in total. (in eccv2018)|
|1679|Akata_Evaluation_of_Output_2015_CVPR_paper|Overall, CNN outperforms FV, while GOOG gives the best performing results; therefore in the following, we comment only on our results obtained using GOOG.|
|||This was also observed for CNN features learned with a large number of unlabeled surrogate classes [10].|
||2 instances in total. (in cvpr2015)|
|1680|Lean Crowdsourcing_ Combining Humans and Machines in an Online System|Binary computer vision model: We use a simple computer vision model based on training a linear SVM on features from a general purpose pre-trained CNN feature extractor (our implementation uses VGG), followed by probability calibration using Platt scaling [25] with the validation splits described in Sec.|
|||This results in probability estimates p(yi|xi, ) = ( (xi)) for each image i, where (xi) is a CNN feature vector,  is a learned SVM weight vector,  is probability calibration scalar from Platt scaling, and () is the sigmoid function.|
||2 instances in total. (in cvpr2017)|
|1681|cvpr18-Deep Unsupervised Saliency Detection  A Multiple Noisy Labeling Perspective|Li and Yu [19] used learned features from an existing CNN model to replace the handcrafted features.|
|||Deep Noise Model based Saliency Detector  Network Architecture We build our latent saliency prediction module upon the DeepLab network [4], where a deep CNN (ResNet-101 [10] in particular) originally designed for image classification is re-purposed by 1) transforming all fully connected layers to convolutional layers and 2) increasing feature resolution through dilated convolution [4].|
||2 instances in total. (in cvpr2018)|
|1682|Zhang_Learning_High_Dynamic_ICCV_2017_paper|4, we first describe our CNN architecture, loss function, and training parameters.|
|||The proposed CNN architecture.|
||2 instances in total. (in iccv2017)|
|1683|cvpr18-Learning Deep Sketch Abstraction|In particular, we move away from the conventional CNN modeling of sketches [47, 46] where sketches are essentially treated the same as static photos, and employ a RNN-based classifier that fully encodes stroke-level ordering information.|
|||State-of-the-art FG-SBIR models [45, 35] adopt a multi-branch CNN to learn a joint embedding where photo and sketch domains can be compared.|
||2 instances in total. (in cvpr2018)|
|1684|Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper|Other works converted the input triangular meshes or point clouds to regular voxel grids [2429] for CNN to process.|
|||: Dynamic graph CNN for learning on point clouds.|
||2 instances in total. (in eccv2018)|
|1685|Liu_SGN_Sequential_Grouping_ICCV_2017_paper|Recently, Bai and Urtasun [3] utilized a CNN to learn an energy of the watershed transform.|
|||Network Structure We exploit a CNN to perform this pixel-wise labeling task.|
||2 instances in total. (in iccv2017)|
|1686|Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper|Otherwise, the image is represented as the average of 14  14 vectors at the output of the CNN (section 4.1).|
|||Bilinear cnn mod els for fine-grained visual recognition.|
||2 instances in total. (in iccv2017)|
|1687|cvpr18-Deep Parametric Continuous Convolutional Neural Networks|Following standard CNN architectures, we can add batch normalization, non-linearities and residual connections between lay 2592  ers.|
|||Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation.|
||2 instances in total. (in cvpr2018)|
|1688|cvpr18-A High-Quality Denoising Dataset for Smartphone Cameras|Application to CNN Training  To further investigate the usefulness of our high-quality ground truth images, we use them to train the DnCNN denoising model [33] and compare the results with the same model trained on post-processed low-ISO images [25] as another type of ground truth.|
|||Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising.|
||2 instances in total. (in cvpr2018)|
|1689|cvpr18-Learning Transferable Architectures for Scalable Image Recognition|We collected the following set of operations based on their prevalence in the CNN literature:   identity  1x7 then 7x1 convolution  3x3 average pooling  5x5 max pooling  1x1 convolution  3x3 depthwise-separable conv  5x5 depthwise-seperable conv  7x7 depthwise-separable conv   1x3 then 3x1 convolution  3x3 dilated convolution  3x3 max pooling  7x7 max pooling  3x3 convolution  In step 5 the controller RNN selects a method to combine the two hidden states, either (1) element-wise addition between two hidden states or (2) concatenation between two hidden states along the filter dimension.|
|||Accuracy versus computational demand (left) and number of parameters (right) across top performing published CNN architectures on ImageNet 2012 ILSVRC challenge prediction task.|
||2 instances in total. (in cvpr2018)|
|1690|CLEVR_ A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning|CNN+LSTM+SA: Again, the question and image are encoded using a CNN and LSTM, respectively.|
|||CNN+LSTM(+MCB) performs on par with LSTM, suggesting that CNN features contain little information relevant to counting.|
||2 instances in total. (in cvpr2017)|
|1691|cvpr18-Learning Globally Optimized Object Detector via Policy Gradient|The method, called R-CNN, employs CNN for localization by performing a region-based recognition operation, which decomposes images with multiple objects into several preFast R-CNN [5] extracted regions of interest (RoIs).|
|||and Faster R-CNN [22] further develop and accelerate the method by sharing CNN feature and combining CNN-based region proposal networks, respectively.|
||2 instances in total. (in cvpr2018)|
|1692|cvpr18-Ordinal Depth Supervision for 3D Human Pose Estimation|Monocular 3D human pose estimation in the wild using improved CNN supervision.|
|||MonoCap: Monocular human motion capture using a CNN coupled with a geometric prior.|
||2 instances in total. (in cvpr2018)|
|1693|cvpr18-Two Can Play This Game  Visual Dialog With Discriminative Question Generation and Answering|[41] jointly trains a CNN with a language RNN to generate sentences, [42] extends [41] with additional attention parameters and learns to identify salient objects for caption generation.|
|||Image Representation: To obtain an image representation we make use of pretrained CNN features to represent images.|
||2 instances in total. (in cvpr2018)|
|1694|Tushar_Nagarajan_Attributes_as_Operators_ECCV_2018_paper|For all methods, we use the same ResNet-18 image features used in our method; this ensures any performance differences can be attributed to the model rather than the CNN architecture.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel  parts-based cnn with improved triplet loss function.|
||2 instances in total. (in eccv2018)|
|1695|Trigeorgis_Mnemonic_Descent_Method_CVPR_2016_paper|Figure 5: Top: Comparison between hand-crafted SIFT features vs. end-to-end CNN features tailored for face alignment.|
|||5 we study the effect of learning features using the CNN in comparison to the SIFT [32] features which are commonly used in many of the cascaded regression algorithms for face alignment [56, 50, 64].|
||2 instances in total. (in cvpr2016)|
|1696|Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper|We observer that training a CNN from scratch using both clean and noisy data is better than just using the clean one.|
|||Unlike previous approaches which were developed to handle noisy labels in small-scale or moderate-scale datasets, we design a new learning curriculum that allows our training strategy with a standard CNN to work practically well on the large-scale dataset, e.g., the WebVision database which contains over 2,400,000 web images with massive noisy labels.|
||2 instances in total. (in eccv2018)|
|1697|Detecting Masked Faces in the Wild With LLE-CNNs|5), including: 1) the Proposal module extracts face proposals and describe them with noisy descriptors, 2) the KNN module refines such descriptor with respect to its nearest neighbors retrieved from a large pool of synthesized faces and non-faces, and 3) the Verification module jointly performs classification and regression tasks with a unified CNN to identify candidate facial regions and adjust their accurate positions.|
|||CMS-RCNN: contextual multi-scale region-based CNN for unconstrained face detection.|
||2 instances in total. (in cvpr2017)|
|1698|Zhixin_Shu_Deforming_Autoencoders_Unsupervised_ECCV_2018_paper|These works have shown that one can improve the accuracy of both classification and localization tasks by injecting deformations and alignment within traditional CNN architectures.|
|||To prevent this problem, instead of the shape decoder CNN directly predicting the local warping field W (p) = (Wx(x, y), Wy(x, y)), we consider a differential decoder that generates the spatial gradient of the warping field: xWx and yWy, where c denotes the c  th component of the spatial gradient vector.|
||2 instances in total. (in eccv2018)|
|1699|Martin_Sundermeyer_Implicit_3D_Orientation_ECCV_2018_paper|The strategies reach from ignoring one axis of rotation [40,9] over adapting the discretization according to the object [17] to the training of an extra CNN to predict symmetries [28].|
|||5: Autoencoder CNN architecture with occluded test input  1) Render clean, synthetic object views at equidistant viewpoints from a full  view-sphere (based on a refined icosahedron [8])  2) Rotate each view in-plane at fixed intervals to cover the whole SO(3) 3) Create a codebook by generating latent codes z  R128 for all resulting  images and assigning their corresponding rotation Rcam2obj  R3x3  At test time, the considered object(s) are first detected in an RGB scene.|
||2 instances in total. (in eccv2018)|
|1700|Chao_Wang_Discriminative_Region_Proposal_ECCV_2018_paper|1) to find and extract the discriminative region for producing masked fake sample, while the reviser adopts CNN to distinguish the real from the masked fake to provide constructive revisions for generator.|
|||For this purpose, we mask the corresponding real sample using the fake discriminative region to make masked fake sample, and then design a reviser using CNN to distinguish real from masked fake to optimize the generator for synthesizing high-quality images.|
||2 instances in total. (in eccv2018)|
|1701|Chang_They_Are_Not_CVPR_2016_paper|Figure 4: Performance comparison of CNN on MEDTest 2014 dataset, wSML+, and the hybrid of CNN and wSML+.|
|||A discriminative CNN video representation for event detection.|
||2 instances in total. (in cvpr2016)|
|1702|Sun_Compositional_Human_Pose_ICCV_2017_paper|Monocular 3d human pose estimation in the wild using improved cnn supervision.|
|||Monocap: Monocular human motion capture using a cnn coupled with a geometric prior.|
||2 instances in total. (in iccv2017)|
|1703|Yu-Ting_Chen_Leveraging_Motion_Priors_ECCV_2018_paper|We propose a two-stream CNN to firstly encode image appearance I and motion prior m(I) separately.|
|||The policy model  (consist of policy CNN and memory network) takes both the image I and the motion prior m(I) as inputs and predicts an action, selecting m(I) as a good prior or not.|
||2 instances in total. (in eccv2018)|
|1704|Adversarial Discriminative Domain Adaptation|We first pre-train a source encoder CNN using labeled source image examples.|
|||Next, we perform adversarial adaptation by learning a target encoder CNN such that a discriminator that sees encoded source and target examples cannot reliably predict their domain label.|
||2 instances in total. (in cvpr2017)|
|1705|cvpr18-Accurate and Diverse Sampling of Sequences Based on a “Best of Many” Sample Objective|We use a CNN to extract a summary of a visual observation of a scene.|
|||We use a 6 layer CNN to extract visual features (see supplementary material).|
||2 instances in total. (in cvpr2018)|
|1706|Cross-View Image Matching for Geo-Localization in Urban Environments|[22] explored several CNN architectures with a new distance based logistic loss for matching ground-level query images to overhead satellite images.|
|||We use the CNN trained on ImageNet [9] as pre-trained model and fine-tune it on our dataset.|
||2 instances in total. (in cvpr2017)|
|1707|Lee_RoomNet_End-To-End_Room_ICCV_2017_paper|In other words, the raw CNN predictions need to be post-processed by an expensive hypotheses testing stage to produce the final layout.|
|||Architecture of RoomNet  We design a CNN to delineate room layout structure using 2D keypoints.|
||2 instances in total. (in iccv2017)|
|1708|cvpr18-Cascaded Pyramid Network for Multi-Person Pose Estimation|firstly introduce CNN to solve pose estimation problem in the work of DeepPose [38], which proposes a cascade of CNN pose regressors to deal with pose estimation.|
|||[37] attempt to solve the problem by predicting heatmaps of keypoints using CNN and graphical models.|
||2 instances in total. (in cvpr2018)|
|1709|Zheng_Query-Adaptive_Late_Fusion_2015_CVPR_paper|We generate an l2-normalized, 4096-dim CNN descriptor for an input image.|
|||When using kNN, the averaged reference is  Holidays  Holidays+1M  Ukbench  Feature Combinations  Graph Global Ours 80.88 76.39 BoW + GIST 80.91 76.57 BoW + RAND 70.59 81.47 BoW + GIST + RAND 84.47 81.58 BoW + HS 86.27 83.36 BoW + CNN 87.95 BoW + HS + CNN 83.75 87.98 BoW + GIST + RAND + HS + CNN 81.04  81.54 81.18 81.65 84.18 86.60 87.23 87.34  Ours 67.65 67.92 68.33 72.83 73.70 74.96 75.03  Co-IDX* Global* Ours* Ours 3.590 3.596 3.590 3.755 3.802 3.840 3.841  3.205 3.254 3.308 3.572 3.611 3.677 3.690  2.766 2.701 2.829 3.504 3.562 3.661 3.608  3.177 3.210 3.263 3.541 3.624 3.750 3.752  Table 2.|
||2 instances in total. (in cvpr2015)|
|1710|cvpr18-VITON  An Image-Based Virtual Try-On Network|Recently, Chen and Kolton [6] trained a CNN using a regression loss as an alternative to GANs for this task without adversarial training.|
|||Without adversarial training, CRN regresses to a target image using a CNN network.|
||2 instances in total. (in cvpr2018)|
|1711|cvpr18-Efficient Optimization for Rank-Based Loss Functions|Specifically, we pass the image as input to the CNN and use the activation vector of the penultimate layer of the CNN as the feature vector.|
|||[9], we use the CNN that is trained on the ImageNet data set [7], by rescaling each candidate window to a fixed size of 224  224.|
||2 instances in total. (in cvpr2018)|
|1712|cvpr18-Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria|Comparison with the CNN and Doctors  The effectiveness of CNNs has been proven in many tasks of computer vision due to their powerful feature representation, especially for classification tasks.|
|||There are the classification performance of the pre-trained CNN models and those fine-tuned (ft) on SD-198 dataset with the SVM classifier.|
||2 instances in total. (in cvpr2018)|
|1713|cvpr18-Hallucinated-IQA  No-Reference Image Quality Assessment via Adversarial Learning|This approach is refined to a multi-task CNN [18], where the network learns both distortion type and quality score simultaneously.|
|||CORNIA [47] CNN [17]  SOM [51] Ours Ours+Oracle  SROCC  LCC  0.892 0.880  0.920 0.903  0.923 0.899  0.934 0.917  0.939 0.920  Table 4: Cross-dataset evaluation (SROCC).The models are trained on the LIVE database and tested on the subset of TID2008.|
||2 instances in total. (in cvpr2018)|
|1714|Simon_Hecker_Learning_to_Drive_ECCV_2018_paper|The driving model consists of CNN networks for feature encoding, LSTM networks to integrate the outputs of the CNNs over time; and fully-connected networks (FN) to integrate information from multiple sensors to predict the driving maneuvers.|
|||For the CNN feature encoder, we take ResNet34 [77] model pre-trained on the ImageNet [78] dataset.|
||2 instances in total. (in eccv2018)|
|1715|Chang_Transformed_Low-Rank_Model_ICCV_2017_paper|Due to the space limitation, more results including the comparison with CNN [13] are included in the supplementary material.|
|||To remedy this, incorporating the additional spectral of temporal information to the decomposition-based framework or learning the rain streak specific based CNN may facilitate to advance this issue.|
||2 instances in total. (in iccv2017)|
|1716|Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper|This work also proposes an improved Siamese network (SiamImp) as baseline, by reducing the total stride and the number of final CNN output channels in SiamFC.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in eccv2018)|
|1717|cvpr18-Blazingly Fast Video Object Segmentation With Pixel-Wise Metric Learning|Other recent techniques obtain segmentation and flow simultaneously [8, 38], train a trident network to improve upon the errors of optical flow propagation [18], or use a CNN in the bilateral space [17].|
|||Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in cvpr2018)|
|1718|cvpr18-An End-to-End TextSpotter With Explicit Alignment and Attention|Thirdly, both approaches, together with a new RNN branch for word recognition, are integrated elegantly into a CNN detection framework, resulting in a single model that can be trained in an end-to-end manner.|
|||As shown in Figure 2, we introduce a new recurrent branch for word recognition, which is integrated into our CNN model in parallel with the existing detection branch for text bounding box regression.|
||2 instances in total. (in cvpr2018)|
|1719|Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs|[27] introduces a heuristic for linearizing selected graph neighborhoods so that a conventional 1D CNN can be used.|
|||This model exactly corresponds to a regular CNN with three convolutions with filters of size 55, 33, and 33 interlaced with max-poolings of size 22, finished with two fully connected layers.|
||2 instances in total. (in cvpr2017)|
|1720|cvpr18-SPLATNet  Sparse Lattice Networks for Point Cloud Processing| SPLATNet allows an easy specification of filter neigh borhood as in standard CNN architectures.|
|||SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation.|
||2 instances in total. (in cvpr2018)|
|1721|Liu_Local_Regularity-driven_City-scale_2014_CVPR_paper|We specify a cost matrix CNN , where the diagonal entry cii is the regularity score of the ith facade, and the nondiagonal entry cij penalizes the overlapping area between facades i and j:  cij = r  (cid:88)  (x,y)fifj  s(x, y),  (8)  where r adjusts the tolerance of overlapping (r =  means strictly no overlapping is allowed).|
|||This also guarantees CNN to be positive-definite as long as there exists no image region being covered by more than two facades simultaneously.|
||2 instances in total. (in cvpr2014)|
|1722|Ruoxi_Deng_Learning_to_Predict_ECCV_2018_paper|DeepEdge [10] extracts multiple patches surrounding an edge candidate point (extracted by the Canny detector) and feeds these patches into a multi-scale CNN to decide if it is an edge pixel.|
|||This simple solution helps CNN manage to better train the network.|
||2 instances in total. (in eccv2018)|
|1723|Horn_Building_a_Bird_2015_CVPR_paper|Effect of computer vision algorithm: Figure 8b uses computer vision algorithms based on raw image-level CNN-fc6 features (obtaining an accuracy of 35% on 555 categories) while Figure 8c uses a more sophisticated method [5] based on pose normalization and features from multiple CNN layers (obtaining an accuracy of 74% on 555 categories).|
|||Despite this, we found that learning algorithms based on CNN features and part localization were surprisingly robust to mislabeled training examples as long as the error rate is not too high, and we would like to emphasize that ImageNet and CUB-200-2011 are still very useful and relevant datasets for research in computer vision.|
||2 instances in total. (in cvpr2015)|
|1724|cvpr18-DeepVoting  A Robust and Explainable Deep Network for Semantic Part Detection Under Partial Occlusion|These mid-level cues are called visual concepts [26], i.e., a set of intermediate CNN states which are closely related to semantic parts.|
|||Part-stacked cnn for fine-grained visual categorization.|
||2 instances in total. (in cvpr2018)|
|1725|cvpr18-Unifying Identification and Context Learning for Person Recognition|photo with a CNN pretrained on Places [29].|
|||Particularly, it takes about 30 minutes to perform inference over the whole test set of PIPA, with one single 2.2 GHz CPU, while the feature extractors take over 40 hours to detect regions and compute CNN features for all test photos, with a Titan X GPU.|
||2 instances in total. (in cvpr2018)|
|1726|Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper|We see that similar to PointNet, Spider CNN picks up representative critical points.|
|||Yi, L., Su, H., Guo, X., Guibas, L.: Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation.|
||2 instances in total. (in eccv2018)|
|1727|Kontschieder_Deep_Neural_Decision_ICCV_2015_paper|2 provides a schematic illustration of this idea, showing how  1470  Deep CNN with parameters   FC  f4  f2  f5  f1  f6  f3  f7  f11  f9  f12  f8  f13  f10  f14  d1  d8  d2  d3  d9  d10  d4  d5  d6  d7  d11  d12  d13  d14  1 2  3 4  5 6  7 8  9 10  11 12  13 14  15 16  Figure 2.|
|||Top: Deep CNN with variable number of layers, subsumed via parameters . FC block: Fully Connected layer used to provide functions fn(; ) (here: inner products), described in Equ.|
||2 instances in total. (in iccv2015)|
|1728|cvpr18-Crowd Counting With Deep Negative Correlation Learning|Hydra CNN [15] was proposed to estimate object densities in different crowded scenarios in a scale-aware manner.|
|||[4] D-ConvNet-v1  85.2 49.1  142.3 99.2  221.4 140.4  357.8 226.1  397.7 364  624.1 545.8  and performs better than the shallower CNN models in [5] in both cases.|
||2 instances in total. (in cvpr2018)|
|1729|Coskun_Long_Short-Term_Memory_ICCV_2017_paper|In order to get initial 3D human pose estimations on the RGB videos, we refine a Inception-v4 CNN model that was pre-trained on ImageNet [23].|
|||CNN  CNN + Kalman  CNN + LSTM CNN + LSTM-KF  (Ours)  Ground Truth  Estimated  Figure 6.|
||2 instances in total. (in iccv2017)|
|1730|Worrall_Interpretable_Transformations_With_ICCV_2017_paper|Generative: [11] generate views of 3D chairs by regressing appearance with a CNN from an embedding space.|
|||3D from images [43] Covariant features [32] Spatial Transformer [22] Ours   x|                    x|,x                    |x CNN MLP *                                                     Interpretable  Supervised                                 Image size 150x150  64x64  128x128  96x96 96x96  30x30x30  57x57 Any  150x150  Table 1.|
||2 instances in total. (in iccv2017)|
|1731|cvpr18-Sparse Photometric 3D Face Reconstruction Guided by Morphable Models|[43] tailored a deep CNN to estimate face normal map in the [44] wild and then inferred the face shape.|
|||Large pose 3d face reconstruction from a single image via direct volumetric cnn regression.|
||2 instances in total. (in cvpr2018)|
|1732|Wang_Multi-Label_Image_Recognition_ICCV_2017_paper|The CNN is first pre-trained on the ImageNet, a large scale single label classification dataset, and further fine-tuned on the target multi-label classification dataset.|
|||Hcp: A flexible cnn framework for multi-label image classification.|
||2 instances in total. (in iccv2017)|
|1733|Knowing When to Look_ Adaptive Attention via a Visual Sentinel for Image Captioning|The encoder uses a CNN to get the representation of images.|
|||better CNN features or better optimization).|
||2 instances in total. (in cvpr2017)|
|1734|cvpr18-Deformable Shape Completion With Graph Convolutional Autoencoders|Thus, tackling such data with a standard CNN requires many network parameters and a prohibitively large amount of training.|
|||Additionally, voxel representations are often memory intensive and suffer from poor resolution [61], although recent models have been proposed to address these issues: implicit surface representation [12], sparse octree networks [57, 44], encoderdecoder CNN for patch-level geometry refinement [19], and a long-term recurrent CNN for upsampling coarse shapes [58].|
||2 instances in total. (in cvpr2018)|
|1735|cvpr18-Practical Block-Wise Neural Network Architecture Generation|Indeed, most modern CNN architectures such as Inception [30, 14, 31] and ResNet Series [10, 11] are assembled as the stack of basic block structures.|
|||As a CNN contains a feed-forward computation procedure, we represent it by a directed acyclic graph (DAG), where each node corresponds to a layer in the CNN while directed edges stand for data flow from one layer to another.|
||2 instances in total. (in cvpr2018)|
|1736|Li_Scene_Graph_Generation_ICCV_2017_paper|We provide a dynamic graph construction layer in the CNN to construct such a graph.|
|||Recently, CNN plus RNN has been adopted as the standard pipeline for image captioning task [5, 11, 12, 22, 41].|
||2 instances in total. (in iccv2017)|
|1737|cvpr18-Reconstruction Network for Video Captioning|The encoder-decoder framework is equipped with different CNN structures such as AlexNet, GoogleNet, VGG19 and Inception-V4.|
|||Besides, we also introduced Inception-V4 as an alternative CNN for feature extraction in the encoder.|
||2 instances in total. (in cvpr2018)|
|1738|cvpr18-Light Field Intrinsics With a Deep Encoder-Decoder Network|Note that the single image CNN [31] does not perform decomposition for the background, thus it appears black in the visualization.|
|||A 4D light-field dataset and CNN architectures for material recognition.|
||2 instances in total. (in cvpr2018)|
|1739|Zhang_PPR-FCN_Weakly_Supervised_ICCV_2017_paper|In particular, the input feature map for WSPP is the same as WSOD, which is the base CNN feature map followed by a trainable conv-layer as in R-FCN [23].|
|||Both VTransE-MIL and PPR-FCN adopts ResNet50 as the base CNN and 100 detected object proposals, i.e., 10,000 region pairs for predicate prediction.|
||2 instances in total. (in iccv2017)|
|1740|Ramanathan_Detecting_Events_and_CVPR_2016_paper|Also, we do not back-propagate into the CNN extracting the frame-level features to be consistent with our model.|
|||2  [74] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and Exploiting image-trained cnn arclassification.|
||2 instances in total. (in cvpr2016)|
|1741|Xiaojun_Chang_RCAA_Relational_Context-Aware_ECCV_2018_paper|have demonstrated that CNN embeddings can be used as a set of objects for an RN.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based CNN with improved triplet loss function.|
||2 instances in total. (in eccv2018)|
|1742|cvpr18-3D Semantic Segmentation With Submanifold Sparse Convolutional Networks|In this work,  9224  Method  Average IoU  NN matching with Chamfer distance Synchronized Spectral CNN [11]  Pd-Network (extension of Kd-Network [10]) Densely Connected PointNet (extension of [17]) PointCNN  77.57% 84.74%  85.49% 84.32% 82.29%  Submanifold SparseConvNet (Section 6.5)  85.98 %  Table 1: Average intersection-over-union (IoU) of six approaches on the test set of a recent part-based segmentation competition on ShapeNet [23].|
|||SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation.|
||2 instances in total. (in cvpr2018)|
|1743|Junho_Jeon_Reconstruction-based_Pairwise_Depth_ECCV_2018_paper|Instead, in our work, we use a deep CNN that can adaptively handle the noise by extracting multi-scale features from a given depth image.|
|||Downsampling the input depth image naturally reduces noise and holes, and the receptive field size of CNN becomes larger.|
||2 instances in total. (in eccv2018)|
|1744|Multimodal Transfer_ A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer|Our contribution is fourfold: (1) We introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) Our hierarchical training scheme and end-to-end CNN network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) Instead of taking  15239  (a) Style  (b) Gatys et al.|
|||Thus, shallower CNN structure can be used for latter subnets, which saves both computing memory and running time.|
||2 instances in total. (in cvpr2017)|
|1745|Ze_Yang_Learning_to_Navigate_ECCV_2018_paper|Specifically, we choose ResNet-50 [17] pre-trained on ILSVRC2012 [39] as the CNN feature extractor, and Navigator, Scrutinizer, Teacher network all share parameters in feature extractor.|
|||Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||2 instances in total. (in eccv2018)|
|1746|High-Resolution Image Inpainting Using Multi-Scale Neural Patch Synthesis|[32] trained an encoderdecoder CNN (Context Encoder) with combined l2 and adversarial loss [17] to directly predict missing image regions.|
|||To overcome the limitations of aforementioned methods, we propose a hybrid optimization approach that leverages the structured prediction power of encoderdecoder CNN and the power of neural patches to synthesize realistic, high-frequency details.|
||2 instances in total. (in cvpr2017)|
|1747|Not All Pixels Are Equal_ Difficulty-Aware Semantic Segmentation via Deep Layer Cascade|[25] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [7].|
|||[18] used CNN cascade for face detection, which rejects false detections quickly in early stages and carefully refines detections in later stages.|
||2 instances in total. (in cvpr2017)|
|1748|Tao_Kong_Deep_Feature_Pyramid_ECCV_2018_paper|The SPP-Net [20] and Fast R-CNN [17] speed up the R-CNN approach with RoI-Pooling that allows the classification layers to reuse the CNN feature maps.|
|||Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic segmentation-aware cnn model.|
||2 instances in total. (in eccv2018)|
|1749|Discover and Learn New Objects From Documentaries|Recall that we use the RPN [35] for proposals generation and a CNN for objectness scoring and feature extraction in the bootstrap stage.|
|||The CNN is later used as a classifier in the joint analysis stage by adding fullyconnected and softmax layers.|
||2 instances in total. (in cvpr2017)|
|1750|Guorun_Yang_SegStereo_Exploiting_Semantic_ECCV_2018_paper|Zbontar and LeCun [43] are the first to use CNN for matching cost computation.|
|||Garg, R., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation:  Geometry to the rescue.|
||2 instances in total. (in eccv2018)|
|1751|Hao_Cheng_Evaluating_Capability_of_ECCV_2018_paper|But when the number of neurons in the hidden layer is large (it happens when we visualize CNN layers), I(X; T ) and I(T ; Y ) barely change.|
|||This evaluation framework allows us to visualize any CNN or FC in the information plane.|
||2 instances in total. (in eccv2018)|
|1752|cvpr18-Soccer on Your Tabletop|At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games.|
|||[33] uses a CNN person segmentation per camera and fuses the estimations in 3D.|
||2 instances in total. (in cvpr2018)|
|1753|cvpr18-Motion-Appearance Co-Memory Networks for Video Question Answering|For each video unit, we use two-stream CNN models [34] to extract unit-level motion and appearance features.|
|||Specifically, the two-direction dense optical flows [6] which are calculated between two adjacent frames in a six-consecutive-frame unit are fed into the pre-trained flow CNN model, which is a BN-Inception network[15].|
||2 instances in total. (in cvpr2018)|
|1754|cvpr18-Multi-Scale Location-Aware Kernel Representation for Object Detection|R-CNN [11] introduces a Region of Interest (RoI) pooling layer to generate representations of all object proposals on feature map with only one CNN pass, which avoids passing separately each object proposal through deep CNNs, leading much faster training/testing process.|
|||Specifically, we first pass an input image through the convolution layers in a basic CNN model (e.g., VGG-16 [35] or ResNet [16]).|
||2 instances in total. (in cvpr2018)|
|1755|Mallya_Recurrent_Models_for_ICCV_2017_paper|We then turn on fine-tuning for the CNN with an initial learning rate of 1e-5 and use Adam with the same learning rate decay scheme for an additional 100k iterations.|
|||We train the RNN and VGG16 CNN using Adam, with an initial learning rate of 4e-4 and 1e-5 respectively.|
||2 instances in total. (in iccv2017)|
|1756|Liong_Cross-Modal_Deep_Variational_ICCV_2017_paper|Experimental Results  Comparisons with State-of-the-art Cross-Modal Hashing Methods: We compared our CMDVH with the different state-of-the-art cross-modal hashing methods which can be grouped to unsupervised (CVH, PDH,  8W =  [ 6  +  , 6  + ] where W  R  CCA-ITQ, LSSH, CMFH) and supervised (SCM, SePH, DisCMH).9 To have a fair comparison because they are shallow methods, we make use of CNN features extracted at the FC7 layer for the images from the pre-trained model initially used by our CMDH method.|
|||Cross-modal retrieval with cnn visual features: A new baseline.|
||2 instances in total. (in iccv2017)|
|1757|Joint Gap Detection and Inpainting of Line Drawings|Here, we show that line drawings have enough structures that can be learned by the CNN to allow automatic detection and completion of the gaps without any such input.|
|||In deep CNN models, typically millions of training data is needed for learning well from scratch.|
||2 instances in total. (in cvpr2017)|
|1758|cvpr18-Lose the Views  Limited Angle CT Reconstruction via Implicit Sinogram Completion|In the 1D CNN architecture, we use multiple filters with varying window sizes, in order to capture information across different sized neighborhoods.|
|||This latent representation is decoded into its corresponding CT image using a 2D CNN to predict the desired CT image.|
||2 instances in total. (in cvpr2018)|
|1759|Bo_Peng_Extreme_Network_Compression_ECCV_2018_paper|For several commonly used CNN models, including VGG and ResNet, our method can reduce over 80% floatingpoint operations (FLOPs) with less accuracy drop than state-of-the-art methods on various image classification datasets.|
|||(6)  4 Experiments  In this section, a set of experiments are performed on standard datasets with commonly used CNN networks to evaluate our method.|
||2 instances in total. (in eccv2018)|
|1760|Zhang_Multi-Oriented_Text_Detection_CVPR_2016_paper|In addition, it is worth mentioning that both of the recent approaches [25, 8, 6] and our method, which used the deep convolutional neural network, have achieved superior performance over conventional approaches in several aspects: 1) learn a more robust component representation by pixel labeling with CNN [8]; 2) leverage the powerful discrimination ability of CNN for better eliminating false positives [6, 35]; 3) learn a strong character/word recognizer with CNN for end-to-end text detection [25, 7].|
|||Recently, some works [6, 8] have achieved great performance, adopting CNN as a character detector.|
||2 instances in total. (in cvpr2016)|
|1761|Wolf_Unsupervised_Creation_of_ICCV_2017_paper|10,000 training images were created and used in order to train a CNN e that maps the three parameters to the output, with very little loss (MSE of 0.1).|
|||We sample random parameterizations and automatically align their frontally-rendered avatars into 6464 RGB images to form the training set t. We then train a CNN e to mimic this engine and generate such images given their parameterization.|
||2 instances in total. (in iccv2017)|
|1762|Heo_Shortlist_Selection_With_CVPR_2016_paper|We have extensively evaluated our method on a diverse set of large-scale benchmarks consisting of up to one billion data with SIFT, GIST, VLAD, and CNN features.|
||| CNN-1M and CNN-11M: 1 and 11 million of 4096dimensional image features from the last fully connected layer (fc7) in the CNN [18].|
||2 instances in total. (in cvpr2016)|
|1763|Liu_Jointly_Recognizing_Object_ICCV_2017_paper|(1) CNN hit@1.|
|||Table 5 shows that the CNN hit@1 method achieves a better accuracy than the LSTM method incorporating the motion information.|
||2 instances in total. (in iccv2017)|
|1764|Xuecheng_Nie_Pose_Partition_Networks_ECCV_2018_paper|Given an image, PPN first uses a CNN to predict (a) joint confidence maps and (b) dense joint-centroid regression maps.|
|||10  X. Nie, J. Feng, J. Xing and S. Yan  4 Learning Joint Detector and Dense Regressor with  CNNs  PPN is a generic model and compatible with various CNN architectures.|
||2 instances in total. (in eccv2018)|
|1765|Zhengqi_Li_CGIntrinsics_Better_Intrinsic_ECCV_2018_paper| indicates that CNN predictions are post-processed with a guided filter [45].|
|||Comparing direct CNN predictions, our CGI-trained model is significantly better than the best learning-based method [45], and similar to [44], even though [45] was directly trained on IIW.|
||2 instances in total. (in eccv2018)|
|1766|MIML-FCN+_ Multi-Instance Multi-Label Learning via Fully Convolutional Networks With Privileged Information|Particularly, we utilize ROI-pooled CNN features as features for proposals as in [23].|
|||We stack our MIMLFCN+ framework on top of ROI-pooled CNN and train the entire system end-to-end.|
||2 instances in total. (in cvpr2017)|
|1767|Huang_Predicting_Gaze_in_ECCV_2018_paper|In [31][8], class labels were used to compute the partial derivatives of CNN response with respect to input image regions to obtain a class-specific saliency map.|
|||Predicting Gaze in Egocentric Video  5  The feature encoding network of S-CNN and T-CNN follows the base architecture of the first five convolutional blocks in Two Stream CNN [32], while omitting the final max pooling layer.|
||2 instances in total. (in eccv2018)|
|1768|Nie_Monocular_3D_Human_ICCV_2017_paper|Another inspiration of our work is the effectiveness that deep CNN has demonstrated in depth map prediction and segmentation from monocular image [8, 35, 16, 36] instead of stereo images.|
|||Liu [16] learn a continuous CRF and a deep CNN jointly.|
||2 instances in total. (in iccv2017)|
|1769|cvpr18-LayoutNet  Reconstructing the 3D Room Layout From a Single RGB Image|In the second step, corner (layout junctions) and boundary probability maps are predicted directly on the image using a CNN with an encoder-decoder structure and skip connections (Sec.|
|||3.1), corner and boundary prediction with a CNN (Sec.|
||2 instances in total. (in cvpr2018)|
|1770|cvpr18-Representing and Learning High Dimensional Data With the Optimal Transport Map From a Probabilistic Viewpoint|For our task,   # Train # Test Default  /w Jittering  /w PCA  # Synthesized Train /w added train set  MNIST ADNI PET NUCLEI  80 20  89.90% 99.00% 95.00%  900  211 53  94.34% 92.45% 93.40%  200  400 100  77.50% 80.00% 84.50%  500  100.00%  94.86%  85.00%  Table 2: Classification Accuracy with and without the data augmentation for CNN classifier   MNIST ADNI PET NUCLEI  # Train # Test Logistic  Regression Bayesian  Classification  800 200  211 53  400 100  89.00%  92.06%  70.20%  97.00%  96.23%  72.00%  Table 3: Classification Accuracy with Bayesian Classifier  7869  Figure 2: Sample Eigenvectors (from deformation maps) of Digit 0-9  Figure 3: Synthesized images of MNIST (left top) and FERET (right top) images, synthesized and true images of ADNI pet scans and Thyroid nuclei images.|
|||a CNN with two convolutional (conv.)|
||2 instances in total. (in cvpr2018)|
|1771|Orekondy_Towards_a_Visual_ICCV_2017_paper|We make the following observations: (i) The CNN performs well in attributes such as tickets, passports, medical treatment that correlated well with scenes (e.g.|
|||Method: Privacy Risk CNN (PR-CNN) We propose a Privacy Risk CNN (PR-CNN) that does not directly use the user profiles privacy preferences  but only indirectly via the ground-truth.|
||2 instances in total. (in iccv2017)|
|1772|Unsupervised Adaptive Re-Identification in Open World Dynamic Camera Networks|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
|||Multi-scale triplet cnn for person re-identification.|
||2 instances in total. (in cvpr2017)|
|1773|cvpr18-The Power of Ensembles for Active Learning in Image Classification|The measure consists of the entropy over predictions minus the conditional entropy over predictions given the weights, approximated for the CNN case ([16]).|
|||For CIFAR-10 we use the Keras CIFAR CNN implementation ([12]) with four convolutional layers and one dense layer, which we refer to as K-CNN.|
||2 instances in total. (in cvpr2018)|
|1774|Zhang_SemiContour_A_Semi-Supervised_CVPR_2016_paper|Recently, CNN has shown its strengths in contour detection [29, 16, 4], and its success is attributed to the complex and deep networks with new losses to capture contour structure.|
|||In fact, this CNN classifier itself is a weak contour detector used to generate better gradient features.|
||2 instances in total. (in cvpr2016)|
|1775|cvpr18-Learning Patch Reconstructability for Accelerating Multi-View Stereo|We then use these labels to train a CNN to predict score maps directly from input images.|
|||We refer to this CNN as the Image-to-Reconstructability network (I2RNet).|
||2 instances in total. (in cvpr2018)|
|1776|cvpr18-Future Frame Prediction for Anomaly Detection – A New Baseline|Thus, by taking both advantages of CNN and RNN, [5][23] leverage a Convolutional LSTMs Auto-Encoder (ConvLSTM-AE) to model normal appearance and motion patterns at the same time, which further boosts the performance of the Conv-AE based solution.|
|||Recently, a CNN based approach has been proposed for optical flow estimation [10].|
||2 instances in total. (in cvpr2018)|
|1777|Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper|In this field [25, 2, 9] inferred matching costs by training a CNN to compare image patches.|
|||Poggi and Mattoccia [17] and Seki and Pollefeys [21] propose two different strategies to train a CNN to predict confidence measures directly from disparity maps.|
||2 instances in total. (in iccv2017)|
|1778|cvpr18-A Robust Method for Strong Rolling Shutter Effects Correction Using Lines With Automatic Feature Selection|first developed a learning-based single RS correction method using CNN in [14].|
|||Unrolling the shutter: Cnn to correct motion distortions.|
||2 instances in total. (in cvpr2018)|
|1779|Amir_Sadeghian_CAR-Net_Clairvoyant_Attentive_ECCV_2018_paper|Fine-tuning VGG on scene segmentation enables the CNN to extract image features that can identify obstacles, roads, sidewalks, and other scene semantics that are essential for trajectory prediction.|
|||The CNN outputs L = N  N feature vectors, A = {a1, ..., aL}, of dimension D, where N and D are the size and the number of feature maps outputted by the 5th convolutional layer, respectively.|
||2 instances in total. (in eccv2018)|
|1780|cvpr18-InverseFaceNet  Deep Monocular Inverse Face Rendering|[38] introduces a self-augmented procedure for training a CNN to regress the spatially varying surface appearance of planar exemplars.|
|||Large pose 3D face reconstruction from a single image via direct volumetric CNN regression.|
||2 instances in total. (in cvpr2018)|
|1781|A Domain Based Approach to Social Relation Recognition|We experiment with two types of models: the first type of models are CNN models trained end-to-end; the second type trains CNN models for semantic attributes derived from the social domain theory, then uses the concatenated feature to learn linear SVM.|
|||5  [40] Cheng, D., Gong, Y., Zhou, S., Wang, J. and Zheng, N.: Person re-identification by multi-channel parts-based CNN with improved triplet loss function.|
||2 instances in total. (in cvpr2017)|
|1782|Hajimirsadeghi_Learning_Ensembles_of_ICCV_2015_paper|These methods jointly train an MRF and a CNN by maximizing likelihood via back-propagation and stochastic gradient descent.|
|||the parameters w via gradient ascent and backpropagation, where f (X, Y, w) is a CNN parameterized by w. However, HCRF-Boost with CNNs extends these algorithms by (1) incorporating the structured hidden variables and (2) learning via functional gradient ascent (i.e.|
||2 instances in total. (in iccv2015)|
|1783|Kim_A_Lightweight_Approach_ICCV_2017_paper|3.3.1 Hemisphere-based CNN (HemiCNN)  For HemiCNN, as shown in the top row of Fig.|
|||A 4d light-field dataset and cnn architectures for material recognition.|
||2 instances in total. (in iccv2017)|
|1784|Xiaohang_Zhan_Consensus-Driven_Propagation_in_ECCV_2018_paper|The CNN architectures for the two tasks are exactly the same as the base model, and the weights are shared.|
|||To create a committee with high heterogeneity, we employ popular CNN architectures including ResNet18 [10], ResNet34, ResNet50, ResNet101, DenseNet121 [12], VGG16 [23], Inception V3 [28], Inception-ResNet V2 [27] and a smaller variant of NASNet-A [37].|
||2 instances in total. (in eccv2018)|
|1785|cvpr18-MoCoGAN  Decomposing Motion and Content for Video Generation|Different from DI, which is based on vanilla CNN architecture, DV is based on a spatio-temporal CNN architecture.|
|||To compute MCS, we first trained a spatio-temporal CNN classifier for action recognition using the labeled training dataset.|
||2 instances in total. (in cvpr2018)|
|1786|Zhao_Open_Vocabulary_Scene_ICCV_2017_paper|Our core CNN in the image stream is adapted from VGG-16 by taking away pool4 and pool5 and then making all the following convolution layers dilated (or Atrous) [3, 31].|
|||We have trained some models in the references and several variants of our proposed model, all of which share the same core CNN to make fair comparisons.|
||2 instances in total. (in iccv2017)|
|1787|cvpr18-ROAD  Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes|Pioneered by [25], the power of CNN has been transferred to semantic segmentation and we have witnessed a rapid boost in semantic segmentation performance.|
|||DeepLab [1] incorporates Conditional random filed(CRF) with CNN to reason about spatial relationship.|
||2 instances in total. (in cvpr2018)|
|1788|Weidi_Xie_Comparator_Networks_ECCV_2018_paper|brown eyes, a bilinear CNN [28] was proposed for fine-grained classification problems, the descriptor of one image is obtained from the outer product of the feature maps.|
|||Lin, T.J., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual  recognition.|
||2 instances in total. (in eccv2018)|
|1789|cvpr18-Human Pose Estimation With Parsing Induced Learner|We implement it by a small CNN with learnable parameters .|
|||Different from traditional CNN based features, F a is extracted in an efficient way by the dynamic parameters  based on parsing information for a given input image, rather than previous hand-crafted parsing based features for human pose estimation.|
||2 instances in total. (in cvpr2018)|
|1790|Yunlong_Wang_End-to-end_View_Synthesis_ECCV_2018_paper|A blur-restoration-deblur framework is presented that consists of three steps: firstly, the input EPI is convolved with a predefined blur kernel; secondly, a CNN is applied to restore the angular detail of the EPI damaged by the undersampling; finally, a non-blind deconvolution is operated to recover the spatial detail suppressed by the EPI blur.|
|||Besides, it is rather difficult to work on the high dimensional data with current CNN frameworks.|
||2 instances in total. (in eccv2018)|
|1791|Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper|Comparisons with four state-of-the-art approaches  Method iDT [54] C3D [49] Two-Stream CNN [13] Two-Stream 3D-CNN [22] 74.67% PM-GANs  Accuracy (%) 72.33% 69.67% 68%  78%  Table 4 presents the accuracy of the competing approaches.|
|||Wei, Y., Zhao, Y., Lu, C., Wei, S., Liu, L., Zhu, Z., Yan, S.: Cross-modal retrieval with cnn visual features: A new baseline.|
||2 instances in total. (in eccv2018)|
|1792|Xinkun_Cao_Scale_Aggregation_Network_ECCV_2018_paper|only three branches in multi-column CNN in [3]).|
|||Walach, E., Wolf, L.: Learning to count with cnn boosting.|
||2 instances in total. (in eccv2018)|
|1793|cvpr18-Weakly Supervised Phrase Localization With Multi-Scale Anchored Transformer Network|[36] formulated the top-down attention of a CNN classifier as a probabilistic Winner-Take-ALL process and utilized an excitation backprop scheme to pass along top-down signals downwards in the network hierarchy.|
|||To build such a correspondence network, given an input image of size W  H, we first use the base CNN to obtain its feature map of size W   H   C. Such a feature map encodes appearance of the image and preserves valuable spatial information.|
||2 instances in total. (in cvpr2018)|
|1794|Baosheng_Yu_Correcting_the_Triplet_ECCV_2018_paper|Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn archi tecture for weakly supervised place recognition.|
|||Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||2 instances in total. (in eccv2018)|
|1795|cvpr18-Learning Superpixels With Segmentation-Aware Affinity Loss|However, most CNN architectures for semantic segmentation generates lower resolution outputs which are then upsampled using post-processing techniques such as DenseCRF.|
|||[9] propose the Bilateral Inception (BI) networks, where superpixels are used for long-range and edge-aware propagation across CNN units, thereby alleviating the need for post-processing CRF tech 574  20040060080010001200Number of Superpixels0.920.940.960.98ASASNICSLICSEEDSLSCERSSEAL-ERS20040060080010001200Number of Superpixels0.50.60.70.80.91BRSNICSLICSEEDSLSCERSSEAL-ERS0.840.860.880.90.92BR0.940.9450.950.9550.960.9650.97ASASEAL-ERSERSLSCSNICSLICSEEDSFigure 7: Results on Cityscapes.|
||2 instances in total. (in cvpr2018)|
|1796|Dario_Rethage_Fully-Convolutional_Point_Networks_ECCV_2018_paper|Yi, L., Su, H., Guo, X., Guibas, L.: Syncspeccnn: Synchronized spectral cnn for  3d shape segmentation.|
||1 instances in total. (in eccv2018)|
|1797|cvpr18-MegDet  A Large Mini-Batch Object Detector|Object detection via a multiregion and semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2018)|
|1798|Yiran_Zhong_Stereo_Computation_for_ECCV_2018_paper|Garg, R., Kumar, B.V., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|1799|Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper|Bondi, L., Lameri, S., G uera, D., Bestagini, P., Delp, E.J., Tubaro, S.: Tampering detection and localization through clustering of camera-based cnn features.|
||1 instances in total. (in eccv2018)|
|1800|Qing_Li_VQA-E_Explaining_Elaborating_ECCV_2018_paper|Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image  captioning.|
||1 instances in total. (in eccv2018)|
|1801|Zhang_Fast_Zero-Shot_Image_CVPR_2016_paper|The main idea is to approximate the principal direction by learning a mapping function f (), between the visual space and the word vector space, such that  f (xm)  wm,  (2)  where xm is the visual feature representation of the image m. Therefore, given a test image x, we can immediately suggest a list of tags by ranking the word vectors of the tags along the direction f (x), namely, by the ranking scores,  Figure 4: The neural network used in our approach for implementing the mapping function f (x; ) from the input image, which is represented by the CNN features x, to its corresponding principal direction in the word vector space.|
||1 instances in total. (in cvpr2016)|
|1802|Doumanoglou_Recovering_6D_Object_CVPR_2016_paper|The authors in [32] tackle the 3D object pose estimation problem by learning discriminative feature  3584  descriptors via a CNN and then passing them to a scalable Nearest Neighbor method to efficiently handle a large number of objects under a large range of poses.|
||1 instances in total. (in cvpr2016)|
|1803|Zhang_FCN-rLSTM_Deep_Spatio-Temporal_ICCV_2017_paper|Thus such work is essentially multiple convolutional neural networks, rather than the combination of CNN and LSTM.|
||1 instances in total. (in iccv2017)|
|1804|Lipeng_Ke_Multi-Scale_Structure-Aware_Network_ECCV_2018_paper|[27] designed a pyramid residual module (PRM) to enhance the deep CNN invariance across scales, by learning the convolutional filters on various feature scales.|
||1 instances in total. (in eccv2018)|
|1805|cvpr18-Motion-Guided Cascaded Refinement Network for Video Object Segmentation|In the proposed system composed by these two components, motion information and deep CNN can well complement each other for the task of VOS.|
||1 instances in total. (in cvpr2018)|
|1806|cvpr18-Through-Wall Human Pose Estimation Using Radio Signals|For each setting, we train a 10-layer vanilla CNN to identify people based on 50 consecutive frames of skeleton heatmaps.|
||1 instances in total. (in cvpr2018)|
|1807|Yang_Latent_Dictionary_Learning_2014_CVPR_paper|The experimental results of DL methods are listed  in Table 5, where the state-of-the-art S(U)-SC methods [35]  and  the  CNN  method  [34]  are  also  reported.|
||1 instances in total. (in cvpr2014)|
|1808|Pourian_Weakly_Supervised_Graph_ICCV_2015_paper|It is worth noting that [20] achieves its performance using a pre-trained CNN on a large auxiliary dataset and then fine-tuned for VOC 2011.|
||1 instances in total. (in iccv2015)|
|1809|Yujun_Cai_Weakly-supervised_3D_Hand_ECCV_2018_paper|Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns.|
||1 instances in total. (in eccv2018)|
|1810|cvpr18-HSA-RNN  Hierarchical Structure-Adaptive RNN for Video Summarization|Besides, the deep feature is extracted from the fc7 layer of the VGGnet16, which is the most popular CNN feature in computer vision tasks [28].|
||1 instances in total. (in cvpr2018)|
|1811|cvpr18-BPGrad  Towards Global Optimality in Deep Learning via Branch and Pruning|We follow the default implementation to train an individual CNN similar to LeNet-5 [22] on each dataset.|
||1 instances in total. (in cvpr2018)|
|1812|WSISA_ Making Survival Prediction From Whole Slide Histopathological Images|This makes training a CNN based survival model very challenging.|
||1 instances in total. (in cvpr2017)|
|1813|cvpr18-Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering|,a1  3.1.2  Image Representation  As in many previous studies, we use a pretrained CNN (i.e., a ResNet [9] with 152 layers pretrained on ImageNet) to extract visual features of multiple image regions, but our extraction method is slightly different.|
||1 instances in total. (in cvpr2018)|
|1814|cvpr18-Revisiting Knowledge Transfer for Training Object Class Detectors|Gated  bi-directional cnn for object detection.|
||1 instances in total. (in cvpr2018)|
|1815|Spatial-Semantic Image Search by Visual Feature Synthesis|Joint embeddings of shapes and images via cnn image purification.|
||1 instances in total. (in cvpr2017)|
|1816|Wang_Studying_Very_Low_CVPR_2016_paper|The authors of [37] concatenated another specific denoising CNN to handle the complex and signal-dependent outliers, at the cost of the growing parameter volume due to the merge.|
||1 instances in total. (in cvpr2016)|
|1817|cvpr18-Augmented Skeleton Space Transfer for Depth-Based Hand Pose Estimation|For comparison, we adopt several state-of-the-art methods that share the same evaluation protocol: For Big Hand 2.2M, we compare with CNN estimator employed by Yuan et al.|
||1 instances in total. (in cvpr2018)|
|1818|Daniel_Maurer_Structure-from-Motion-Aware_PatchMatch_for_ECCV_2018_paper|Similarly, Hur and Roth [16] make use of a CNN to integrate semantic information into a joint approach for estimating the flow and a temporally consistent semantic segmentation.|
||1 instances in total. (in eccv2018)|
|1819|Miika_Aittala_Burst_Image_Deblurring_ECCV_2018_paper|Our proposed network is a U-Net-inspired [25] CNN architecture that maps an unordered set of images into a single output image in a perfectly permutation-invariant manner, and facilitates repeated back-and-forth exchanges of feature information between the frames during the network evaluation.|
||1 instances in total. (in eccv2018)|
|1820|Learning Detection With Diverse Proposals|On the other hand, inference in Region-based CNN models as well as other state-of-the-art networks is done through NMS which selects boxes with highest detection scores for each category.|
||1 instances in total. (in cvpr2017)|
|1821|Liu_Material_Editing_Using_ICCV_2017_paper|[24] propose a CNN architecture to directly predict albedo and shading from an image.|
||1 instances in total. (in iccv2017)|
|1822|Shrivastava_Training_Region-Based_Object_CVPR_2016_paper|Experimental setup  We conduct experiments with two standard ConvNet architectures: VGG CNN M 1024 (VGGM, for short) from [5], which is a wider version of AlexNet [19], and VGG16 from [28].|
||1 instances in total. (in cvpr2016)|
|1823|Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper|The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [25], and by backpropagation to identify or generate salient image features [16, 21].|
||1 instances in total. (in eccv2018)|
|1824|Gkioxari_Using_k-Poselets_for_2014_CVPR_paper|It enables a unified approach to person detection and keypoint prediction which, barring contemporaneous approaches based on CNN features [14], achieves state-of-the-art keypoint prediction while maintaining competitive detection performance.|
||1 instances in total. (in cvpr2014)|
|1825|Boundary-Aware Instance Segmentation|The first module consists of a deep CNN (in practice, the VGG16 [31] architecture) to extract a feature representation from an input image, followed by an RPN [28], which generates a set of bounding box proposals.|
||1 instances in total. (in cvpr2017)|
|1826|Zhe_Chen_Context_Refinement_for_ECCV_2018_paper|Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic  segmentation-aware cnn model.|
||1 instances in total. (in eccv2018)|
|1827|IM2CAD|Both of [37] and [25] use handcrafted features, while our proposed method uses CNN feature which is learned end-to-end on just image data.|
||1 instances in total. (in cvpr2017)|
|1828|PoseTrack_ Joint Multi-Person Pose Estimation and Tracking|As joint detector, we use the publicly available pre-trained CNN [16] trained on the MPII MultiPerson Pose dataset [30].|
||1 instances in total. (in cvpr2017)|
|1829|Jiang_Combination_Features_and_2015_CVPR_paper|Recently, based on the region proposals and deep CNN features, Girshick et al.|
||1 instances in total. (in cvpr2015)|
|1830|Bo_Xiong_Snap_Angle_Prediction_ECCV_2018_paper|We train a multi-class CNN classifier to distinguish the four activity categories in our 360 dataset (Disney, Parade, etc.).|
||1 instances in total. (in eccv2018)|
|1831|Wang_Improving_Human_Action_CVPR_2016_paper|As shown in the last three columns of Table 5, our method as well as the baselines can benefit from the addition of CNN features.|
||1 instances in total. (in cvpr2016)|
|1832|Liu_Stepwise_Metric_Promotion_ICCV_2017_paper|In [5], the CNN model is initialized by directly deploying a  fine-tuned CNN model on a source dataset.|
||1 instances in total. (in iccv2017)|
|1833|Ian_Cherabier_Learning_Priors_for_ECCV_2018_paper|This enables efficient integration of these operations into a CNN with shared weights across the primal and dual updates and across the different iterations of the algorithm.|
||1 instances in total. (in eccv2018)|
|1834|Littwin_The_Multiverse_Loss_CVPR_2016_paper|We observed during experiments performed on a number of datasets, that training of a CNN using a single cross entropy loss produces a representation that has a rapidly decreasing Fisher spectrum, and is highly discriminative in only a few directions.|
||1 instances in total. (in cvpr2016)|
|1835|Li_Fast_Algorithms_for_CVPR_2016_paper|During training, the CNN features extracted from the original images are treated as privileged information, and the CNN features extracted from the downsampled images as the main features.|
||1 instances in total. (in cvpr2016)|
|1836|Jieru_Mei_Online_Dictionary_Learning_ECCV_2018_paper|Zhou, X., Zhu, M., Pavlakos, G., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Monocap: Monocular human motion capture using a CNN coupled with a geometric prior.|
||1 instances in total. (in eccv2018)|
|1837|cvpr18-Translating and Segmenting Multimodal Medical Volumes With Cycle- and Shape-Consistency Generative Adversarial Network|It is an active research area by using synthetic data to overcome the insufficiency of labeled data in CNN training.|
||1 instances in total. (in cvpr2018)|
|1838|Geometric Loss Functions for Camera Pose Regression With Deep Learning|Particular object retrieval with inte gral max-pooling of cnn activations.|
||1 instances in total. (in cvpr2017)|
|1839|Anirudh_Som_Perturbation_Robust_Representations_ECCV_2018_paper|Yi, L., Su, H., Guo, X., Guibas, L.: Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation.|
||1 instances in total. (in eccv2018)|
|1840|Dongwoo_Lee_Joint_Blind_Motion_ECCV_2018_paper|[30] achieves comparable performance to the proposed algorithm in which CNN is trained with MSE loss.|
||1 instances in total. (in eccv2018)|
|1841|Liang_Reversible_Recursive_Instance-Level_CVPR_2016_paper|Object detection via a multiregion & semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2016)|
|1842|Mahmud_Joint_Prediction_of_ICCV_2017_paper|Therefore, we compare with a recent recognition approach which estimates the labels of the observed activities using a combination of CNN and LSTM [28].|
||1 instances in total. (in iccv2017)|
|1843|Gaze Embeddings for Zero-Shot Image Classification|As image embeddings, we extract 1, 024-dim CNN features from an ImageNet pre-trained GoogLeNet [44] model.|
||1 instances in total. (in cvpr2017)|
|1844|Szegedy_Going_Deeper_With_2015_CVPR_paper|R-CNN decomposes the overall detection problem into two subproblems: utilizing lowlevel cues such as color and texture in order to generate object location proposals in a category-agnostic fashion and using CNN classifiers to identify object categories at those locations.|
||1 instances in total. (in cvpr2015)|
|1845|Liu_Active_Learning_for_ICCV_2017_paper|DeepPose [48] takes the first step towards adopting CNN [23] for human pose estimation, where CNN is used to directly regress joint locations in Cartesian coordinates repeatedly.|
||1 instances in total. (in iccv2017)|
|1846|Zhou_Unsupervised_Learning_of_ICCV_2017_paper| We develop a CNN framework with branches for spe cific tasks in stereo matching learning.|
||1 instances in total. (in iccv2017)|
|1847|Mun_MarioQA_Answering_Questions_ICCV_2017_paper|Face attribute prediction using off-the-shelf cnn features.|
||1 instances in total. (in iccv2017)|
|1848|cvpr18-Learning to Extract a Video Sequence From a Single Motion-Blurred Image|[32] propose a CNN that deblurs videos by incorporating information accumulated across frames.|
||1 instances in total. (in cvpr2018)|
|1849|Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper|For CIFAR100, we use a CNN with four convolutional layers followed by a single dense layer (see supplementary for more details).|
||1 instances in total. (in eccv2018)|
|1850|Zhang_Learning_Uncertain_Convolutional_ICCV_2017_paper|[50] take global and local context into account and model the saliency prediction in a multi-context deep CNN framework.|
||1 instances in total. (in iccv2017)|
|1851|Kaiyue_Pang_Deep_Factorised_Inverse-Sketching_ECCV_2018_paper|This can be solved by learning a joint sketch-photo embedding using a CNN f [43, 35].|
||1 instances in total. (in eccv2018)|
|1852|Taigman_Web-Scale_Training_for_2015_CVPR_paper|In [21], optimization is used to trick the CNN to misclassify clear input images and to compute the stability of each layer.|
||1 instances in total. (in cvpr2015)|
|1853|Chen_Surface_Normals_in_ICCV_2017_paper|Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in iccv2017)|
|1854|Bertasius_Semantic_Segmentation_With_CVPR_2016_paper|Several recent papers [15, 10] address this issue by proposing to use deep per-pixel CNN features and then  3603  |{z}classify each pixel as belonging to a certain class.|
||1 instances in total. (in cvpr2016)|
|1855|cvpr18-Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features|Object detection via a multiregion and semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2018)|
|1856|Maksai_Non-Markovian_Globally_Consistent_ICCV_2017_paper|Plug-And-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection.|
||1 instances in total. (in iccv2017)|
|1857|Lan_Action_Recognition_by_ICCV_2015_paper|INRIA [29] uses a mixture of IDTF [41], SIFT [23], color features [6] and the CNN features [14].|
||1 instances in total. (in iccv2015)|
|1858|Gueguen_Large-Scale_Damage_Detection_2015_CVPR_paper|CNN features rank second in the unsupervised set ting, while B-SIFT, B-LLC, and CNN all give comparable accuracies in the supervised setting.|
||1 instances in total. (in cvpr2015)|
|1859|Kar_Category-Specific_Object_Reconstruction_2015_CVPR_paper|During inference, we first detect and segment the object in the image [20] and then predict viewpoint (rotation matrix) and subcategory for the object using a CNN based system similar to [34] (augmented to predict subcategories).|
||1 instances in total. (in cvpr2015)|
|1860|Feature Pyramid Networks for Object Detection|Object detection via a multiIn  region & semantic segmentation-aware CNN model.|
||1 instances in total. (in cvpr2017)|
|1861|Martinez_A_Simple_yet_ICCV_2017_paper|Learning camera viewpoint using cnn to improve 3d body pose estimation.|
||1 instances in total. (in iccv2017)|
|1862|Zhirong_Wu_Improving_Embedding_Generalization_ECCV_2018_paper|We learn a CNN to embed images into a low-dimensional feature space, where the distance metric between images preserves the semantic structure of categorical labels according to the NCA criterion.|
||1 instances in total. (in eccv2018)|
|1863|cvpr18-Semantic Visual Localization|NetVLAD: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2018)|
|1864|Deep Network Flow for Multi-Object Tracking|Learning by tracking: Siamese CNN for robust target association.|
||1 instances in total. (in cvpr2017)|
|1865|Harandi_When_VLAD_Met_CVPR_2016_paper|[26] introduces an approach to employing kernels within a CNN framework.|
||1 instances in total. (in cvpr2016)|
|1866|Polyhedral Conic Classifiers for Visual Object Detection and Classification|[22] CNN on images resized to 256256, producing 4096-dimensional feature vectors for each of the methods shown.|
||1 instances in total. (in cvpr2017)|
|1867|Laptev_Transformation-Invariant_Convolutional_Jungles_2015_CVPR_paper|For the Neuronal membrane segmentation dataset we achieve the same F-score as Convolutional Neural Networks approach, but we train TICJ within 3 hours in a single CPU, comparing to about one week CNN training on a GPU cluster.|
||1 instances in total. (in cvpr2015)|
|1868|Boyu_Chen_Real-time_Actor-Critic_Tracking_ECCV_2018_paper|These outstanding results are partly attributed to the strength of CNN features, which makes our tracker effectively depict the appearance of the tracked object compared with low-level handcrafted features.|
||1 instances in total. (in eccv2018)|
|1869|cvpr18-Deep Mutual Learning|[29] for action recognision: the more expensive optical flow field is treated as privileged information and an optical flow CNN is used to teach a motion vector CNN.|
||1 instances in total. (in cvpr2018)|
|1870|Spatially Adaptive Computation Time for Residual Networks|Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||1 instances in total. (in cvpr2017)|
|1871|End-To-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering|[29] use a CNN to learn a mapping between an image and semantic attributes.|
||1 instances in total. (in cvpr2017)|
|1872|Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper|Object detection via a multiIn  region & semantic segmentation-aware CNN model.|
||1 instances in total. (in cvpr2016)|
|1873|Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose|Learning camera viewpoint using CNN to improve 3D body pose estimation.|
||1 instances in total. (in cvpr2017)|
|1874|Relationship Proposal Networks|The latter has become increasingly popular since proposal generation can be simply performed using one CNN forward pass with near real-time running speed.|
||1 instances in total. (in cvpr2017)|
|1875|Shihao_Wu_Specular-to-Diffuse_Translation_for_ECCV_2018_paper|These methods include training a CNN to reconstruct rendering parameters, e.g., material [48, 49], reflectance maps [50], illumination [51], or some combination of those components [13, 48, 52].|
||1 instances in total. (in eccv2018)|
|1876|cvpr18-Extreme 3D Face Reconstruction  Seeing Through Occlusions|Large pose 3D face reconstruction from a single image via direct volumetric CNN regression.|
||1 instances in total. (in cvpr2018)|
|1877|He_Single_Shot_Text_ICCV_2017_paper|Hierarchical Inception Module  In a CNN model, convolutional features in a lower layer often focus on local image details, while the features in a deeper layer generally capture more high-level abstracted information.|
||1 instances in total. (in iccv2017)|
|1878|cvpr18-Zero-Shot Kernel Learning|Our non-linear mapping is obtained via a non-linear kernel [22, 19, 20] and subspace learning rather than via non-linear layers of CNN per se.|
||1 instances in total. (in cvpr2018)|
|1879|Doersch_Multi-Task_Self-Supervised_Visual_ICCV_2017_paper|Previous work applied self-supervision tasks over a variety of CNN architectures (usually relatively shallow), and often evaluated the representations on different tasks; and even where the evaluation tasks are the same, there are often differences in the fine-tuning algorithms.|
||1 instances in total. (in iccv2017)|
|1880|On Compressing Deep Models by Low Rank and Sparse Decomposition|[29] presented an effective CNN compression approach in the frequency domain using the discrete cosine transform and quantization strategies.|
||1 instances in total. (in cvpr2017)|
|1881|Conditional Similarity Networks|Bilinear cnn models for fine-grained visual recognition.|
||1 instances in total. (in cvpr2017)|
|1882|Krishna_Kumar_Singh_Transferring_Common-Sense_Knowledge_ECCV_2018_paper|Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic  segmentation-aware cnn model.|
||1 instances in total. (in eccv2018)|
|1883|Fanello_Low_Compute_and_ICCV_2017_paper|each pixel is independently processed, and low-compute, with a model complexity an order of magnitude less than existing CNN and CRFbased approaches.|
||1 instances in total. (in iccv2017)|
|1884|cvpr18-CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM|Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|1885|Aggeliki_Tsoli_Joint_3D_tracking_ECCV_2018_paper|Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns.|
||1 instances in total. (in eccv2018)|
|1886|Cordts_The_Cityscapes_Dataset_CVPR_2016_paper|Furthermore, a novel CNN architecture explicitly designed for dense prediction has been proposed recently by [79].|
||1 instances in total. (in cvpr2016)|
|1887|Are You Smarter Than a Sixth Grader_ Textbook Question Answering for Multimodal Machine Comprehension|Cloze datasets (where the system is asked to fill in words that have been removed from a passage) including CNN and DailyMail [10] as well as Childrens Book Test [11] are a good proxy to the traditional MC tasks and have the added benefit of being automatically produced.|
||1 instances in total. (in cvpr2017)|
|1888|Haoshuo_Huang_Domain_transfer_through_ECCV_2018_paper|1: Given a pretrained CNN in the source domain, we seek to adapt it to a target domain.|
||1 instances in total. (in eccv2018)|
|1889|Infinite Variational Autoencoder for Semi-Supervised Learning|Our deep CNN architecture consists of two convolutional layers with 32 filters of 55 and Rectified Linear Unit (ReLU) activation and max-pooling of 2  2 after each one.|
||1 instances in total. (in cvpr2017)|
|1890|Bulat_Binarized_Convolutional_Landmark_ICCV_2017_paper|We are the first to study the effect of binarization on state-of-the-art CNN architectures for the problem of localization, namely human pose estimation and face alignment.|
||1 instances in total. (in iccv2017)|
|1891|cvpr18-Geometry-Aware Learning of Maps for Camera Localization|NetVLAD: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2018)|
|1892|cvpr18-Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation|Feature extraction for each region, region selection, and classification of the selected region are performed through multiple instance learning (MIL) [30, 11, 31, 1, 18] or two-stream CNN [2, 15, 33].|
||1 instances in total. (in cvpr2018)|
|1893|Zhu_TORNADO_A_Spatio-Temporal_ICCV_2017_paper|Implementation Details  The CNN feature extractor we use is based on YOLO [27] which has 24 convolution layers and 2 fullyconnected layer.|
||1 instances in total. (in iccv2017)|
|1894|Supervising Neural Attention Models for Video Captioning by Human Gaze Data|We first extract three types of CNN features for scene, motion, and fovea per frame (section 3.1).|
||1 instances in total. (in cvpr2017)|
|1895|Tatarchenko_Octree_Generating_Networks_ICCV_2017_paper|Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation.|
||1 instances in total. (in iccv2017)|
|1896|Amodal Detection of 3D Objects_ Inferring 3D Bounding Boxes From 2D Ones in RGB-Depth Images|Computation Speed Our 3D detection system is developed based on the open source Caffe CNN library [15].|
||1 instances in total. (in cvpr2017)|
|1897|Li_Combining_Markov_Random_CVPR_2016_paper|Learning FRAME models using CNN filters for knowledge visualization, 2015.|
||1 instances in total. (in cvpr2016)|
|1898|Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper|Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: CNN features off-the-shelf: An astounding baseline for recognition.|
||1 instances in total. (in eccv2018)|
|1899|Qingnan_Fan_Learning_to_Learn_ECCV_2018_paper|Recently, many CNN based methods [16, 25, 44] have been proposed to ap proximate, accelerate or improve these parameterized image operators and achieved significant progress.|
||1 instances in total. (in eccv2018)|
|1900|Kan_Multi-View_Deep_Network_CVPR_2016_paper|A vanilla CNN taking all samples from multiple views as one view may be applicable for cross-view problems with homogeneous view representations (e.g.|
||1 instances in total. (in cvpr2016)|
|1901|Fengting_Yang_Recovering_3D_Planes_ECCV_2018_paper|: Unsupervised CNN for single  view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|1902|Lillo_A_Hierarchical_Pose-Based_CVPR_2016_paper|[5] introduce P-CNN as a framework for incorporating pose-centered CNN features extracted from optical flow and color.|
||1 instances in total. (in cvpr2016)|
|1903|PointNet_ Deep Learning on Point Sets for 3D Classification and Segmentation|correspondences between shapes, as well as our own 3D CNN baseline.|
||1 instances in total. (in cvpr2017)|
|1904|Zheng_Shou_Online_Detection_of_ECCV_2018_paper|Xu, Z., Yang, Y., Hauptmann, A.G.: A discriminative cnn video representation for  event detection.|
||1 instances in total. (in eccv2018)|
|1905|Kyoungoh_Lee_Propagating_LSTM_3D_ECCV_2018_paper|We employed the conventional CNN [2] for 2D pose estimation.|
||1 instances in total. (in eccv2018)|
|1906|Predictive-Corrective Networks for Action Detection|Concretely, our memory is simply the convolutional activations at the previous time step, and, thus is as interpretable as the activations of an image-based CNN (using e.g., [59]).|
||1 instances in total. (in cvpr2017)|
|1907|Chen_Minds_Eye_A_2015_CVPR_paper|The CNN is initialized with the weights from the BVLC reference net.|
||1 instances in total. (in cvpr2015)|
|1908|cvpr18-DensePose  Dense Human Pose Estimation in the Wild|Learning camera viewpoint using cnn to improve 3d body pose estimation.|
||1 instances in total. (in cvpr2018)|
|1909|Zhu_Flow-Guided_Feature_Aggregation_ICCV_2017_paper|In [11], a multistage pipeline called Regions with Convolutional Neural Networks (R-CNN) is proposed for training deep CNN to classify region proposals for object detection.|
||1 instances in total. (in iccv2017)|
|1910|Girshick_Fast_R-CNN_ICCV_2015_paper|to this CaffeNet as model S, for small. The second network is VGG CNN M 1024 from [3], which has the same depth as S, but is wider.|
||1 instances in total. (in iccv2015)|
|1911|Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper|Multi-view CNN (MVCNN) [18] was designed on top of image-based classification networks, which integrated the views taken around a 3D meshed object through view pooling.|
||1 instances in total. (in eccv2018)|
|1912|cvpr18-Surface Networks|In the context of computer graphics, [25] developed the first CNN model on meshed surfaces using intrinsic patch representations, and further generalized in [4] and [27].|
||1 instances in total. (in cvpr2018)|
|1913|cvpr18-Object Referring in Videos With Language and Human Gaze|For depth, we convert depth maps to HHA images [17] and extract the CNN features with the RGB[17] before passing to LSTMdepth local and D network of LSTMdepth global.|
||1 instances in total. (in cvpr2018)|
|1914|Deep Mixture of Linear Inverse Regressions Applied to Head-Pose Estimation|A CNN Regression Approach for Real-Time 2D/3D Registration.|
||1 instances in total. (in cvpr2017)|
|1915|Johannes_Schoenberger_Learning_to_Fuse_ECCV_2018_paper|Sinha, M. Pollefeys  accurate scanline optimization by using a CNN to predict the parameters of the underlying scanline optimization objective.|
||1 instances in total. (in eccv2018)|
|1916|cvpr18-Efficient Diverse Ensemble for Discriminative Co-Tracking|Researchers make ensembles of CNNs that shares convolutional layers [40], different loss functions for each output of the feature map [54], and repeatedly subsampling different nodes and layers in fully connected  4815  layers on CNN to build an ensemble [21, 34].|
||1 instances in total. (in cvpr2018)|
|1917|Learning Fully Convolutional Networks for Iterative Non-Blind Deconvolution|Effect of the Loss Function  Most of existing CNN based low-level vision methods use the L2 norm based reconstruction error as the loss function e.g.|
||1 instances in total. (in cvpr2017)|
|1918|Qi_Semantic_Segmentation_With_ICCV_2015_paper|Method SDS [11] CFM [3]  FCN-8s [24] DeepLab [2]  Ours-crop  Proposal CNN CRF 34.3s [1] 34.3s [1]   0.50s 0.52s  17.9s 2.10s 0.21s 0.13s 1.77s   Table 4.|
||1 instances in total. (in iccv2015)|
|1919|Klein_Associating_Neural_Word_2015_CVPR_paper|Similar to the previous work, we are using a CNN that takes an image as input and embeds it into a single vector by taking the representation of the last layer.|
||1 instances in total. (in cvpr2015)|
|1920|Ramanathan_Learning_Semantic_Relationships_2015_CVPR_paper|The feature fI is obtained through a linear projection of the Convolutional Neural Network (CNN) feature, obtained from the last fully connected layer of a CNN architecture [40]: fI = WimCNN(I) + bim,  (1) where CNN(I) represents the CNN feature of image I.|
||1 instances in total. (in cvpr2015)|
|1921|Fixed-Point Factorized Networks|Experiments are conducted on three commonly used CNN models, i.e., AlexNet [20], VGG-16 [29] and ResNet-50 [10].|
||1 instances in total. (in cvpr2017)|
|1922|cvpr18-Recurrent Pixel Embedding for Instance Grouping|However, most CNN pixel-labeling architectures are trained with loss functions that decompose into a simple (weighted) sum of classification or regression losses over individual pixel labels.|
||1 instances in total. (in cvpr2018)|
|1923|cvpr18-Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference|The second category quan tizes the weights and / or activations of a CNN from 32 bit floating point into lower bit-depth representations.|
||1 instances in total. (in cvpr2018)|
|1924|Xu_Directionally_Convolutional_Networks_ICCV_2017_paper|Pooling on Mesh  Classic pooling layers in CNN make use of the natural multi-scale clustering of grid: they input all the feature maps over a cluster, and output a single feature for that cluster [5].|
||1 instances in total. (in iccv2017)|
|1925|Jain_Objects2action_Classifying_and_ICCV_2015_paper|A discriminative CNN video  representation for event detection.|
||1 instances in total. (in iccv2015)|
|1926|A Hierarchical Approach for Generating Descriptive Image Paragraphs|We use the publically available implementation of [12], which uses the 16-layer VGG network [28] to extract CNN features and projects them as input into an LSTM [9], training the whole model jointly end-to-end.|
||1 instances in total. (in cvpr2017)|
|1927|cvpr18-Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization|NetVLAD: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2018)|
|1928|cvpr18-Focal Visual-Text Attention for Visual Question Answering|For image/video embedding, we extract fixed-size features using the pre-trained CNN model, Inception-ResNet [21], by concatenating the pool5 layer and classification layers output before softmax.|
||1 instances in total. (in cvpr2018)|
|1929|Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper|Method  zoom-out (ours)  pixel accuracy class accuracy 86.1  80.9  Multiscale convnet [10] 81.4  Recurrent CNN [30]  Pylon [22]  Recursive NN [34]  Multilevel [28]  80.2  81.9  78.1  78.4  76.0  69.9  72.4      Table 5.|
||1 instances in total. (in cvpr2015)|
|1930|cvpr18-Video Based Reconstruction of 3D People Models|Therefore, we adopt the CNN based video segmentation method of [9] and train it with 3-4 manual segmentations per sequence.|
||1 instances in total. (in cvpr2018)|
|1931|Bidirectional Multirate Reconstruction for Temporal Modeling in Videos|Exploiting image-trained CNN architectures for unconstrained video classification.|
||1 instances in total. (in cvpr2017)|
|1932|Parikshit_Sakurikar_Single_Image_Scene_ECCV_2018_paper|Perceptual loss is L2-loss between the CNN feature maps of the generated deblurred image and the target image:  LX =  1  WijHij Xx Xy  (ij(I S)xy  ij(GG (I B))xy)2,  (6)  where ij is the feature map in VGG19 trained on ImageNet [5] after the jth convolution and the ith max-pooling layer and W and H denote the size of the feature maps.|
||1 instances in total. (in eccv2018)|
|1933|Angjoo_Kanazawa_Learning_Category-Specific_Mesh_ECCV_2018_paper|Unlike recent CNN based 3D prediction methods which require either ground-truth 3D or multi-view supervision, we only rely on an annotated image collection, with only one available view per training instance, to learn our prediction model.|
||1 instances in total. (in eccv2018)|
|1934|Beyond Instance-Level Image Retrieval_ Leveraging Captions to Learn a Global Visual Representation for Semantic Retrieval|Particular object retrieval with integral max-pooling of cnn activations.|
||1 instances in total. (in cvpr2017)|
|1935|Fangneng_Zhan_Verisimilar_Image_Synthesis_ECCV_2018_paper|[55] use the CNN features trained on synthetic faces to regress face pose parameters.|
||1 instances in total. (in eccv2018)|
|1936|Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper|[12] are most similar to our work, as they use a cycle consistency loss as a way of using transitivity to supervise CNN training.|
||1 instances in total. (in iccv2017)|
|1937|Deep Multimodal Representation Learning From Temporal Data|We compute a 1024-dimensional CNN feature representation for each video frame using GoogLeNet [24].|
||1 instances in total. (in cvpr2017)|
|1938|Purkait_Rolling_Shutter_Correction_ICCV_2017_paper|A CNN based approach [36] is exploited to learn the prior knowledge of the cardinal directions.|
||1 instances in total. (in iccv2017)|
|1939|Kim_Online_Video_Deblurring_ICCV_2017_paper|[31], which trains a CNN to remove blur stemming from both ego and object motions.|
||1 instances in total. (in iccv2017)|
|1940|Scalable Person Re-Identification on Supervised Smoothed Manifold|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in cvpr2017)|
|1941|Xu_Learning_to_Super-Resolve_ICCV_2017_paper|Note that the fine-tuned model of [18] and [15] is a deep CNN model with 35 layers that has been trained with a mean squared error (MSE) loss function.|
||1 instances in total. (in iccv2017)|
|1942|Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper|We  1743  Pixel annotationsImageDeep Convolutional Neural NetworkLossAlgorithm 1 Weakly-Supervised EM (fixed bias version) Input: Initial CNN parameters , potential parameters bl,  l  {0, .|
||1 instances in total. (in iccv2015)|
|1943|Yizhen_Lao_Rolling_Shutter_Pose_ECCV_2018_paper|Rengarajan, V., Balaji, Y., Rajagopalan, A.: Unrolling the shutter: Cnn to correct motion distortions.|
||1 instances in total. (in eccv2018)|
|1944|Yang_Multivariate_Regression_on_CVPR_2016_paper|Setup: We use the state-of-the-art CNN model VGG-full [2] as feature extractor.|
||1 instances in total. (in cvpr2016)|
|1945|Yang_End-To-End_Learning_of_CVPR_2016_paper| An end-to-end deep CNN framework for human pose estimation is proposed.|
||1 instances in total. (in cvpr2016)|
|1946|Veeriah_Differential_Recurrent_Neural_ICCV_2015_paper|In particular, [3] developed a 3D convolutional neural network that extends the conventional CNN by taking spacetime volume as input.|
||1 instances in total. (in iccv2015)|
|1947|Goyal_The_Something_Something_ICCV_2017_paper|The last hidden state of the LSTM is then taken  5848  Method  10 classes  40 classes  174 classes  top-1  top-2  top-1  top-2  top-1  top-2  top-5  Error rate (%)  2D CNN + Avg  76.5 54.7 Pre-2D CNN + LSTM 52.3  Pre-2D CNN + Avg  3D CNN + Stack  Pre-3D CNN + Avg  2D+3D-CNN  58.1 47.5  44.9  58.9 39.0 34.1  38.7 29.2  27.1  88.0 79.2 77.8  70.3 66.2  63.8  78.5 70.0 68.0  57.3 52.7  50.7        88.5  81.5  70.0     Table 4: Error rates on different subsets of the data.|
||1 instances in total. (in iccv2017)|
|1948|Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper|Object detection via a multiregion & semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2016)|
|1949|cvpr18-Dynamic-Structured Semantic Propagation Network|Their loss constraints can only indirectly guide visual features to be hierarchy-aware, which is hard to be guaranteed and often leads to inferior results compared to generic CNN models.|
||1 instances in total. (in cvpr2018)|
|1950|cvpr18-Im2Pano3D  Extrapolating 360° Structure and Semantics Beyond the Field of View|Unsupervised cnn for In single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|1951|Forecasting Interactive Dynamics of Pedestrians With Fictitious Play|Body Orientation: Since a pedestrians body orientation is a strong cue of the direction in which she will walk [10], we train a CNN (described in details in Section 3.4) to predict the initial walking direction of a pedestrian.|
||1 instances in total. (in cvpr2017)|
|1952|Analyzing Computer Vision Data - The Good, the Bad and the Ugly|textures/noise  Traffic scenes  Outdoor scenes for SLAM  Fast food items (61 categories)  R1 Wide baseline still lifes  RN  RN  R2  RN  SN  S1  RN  RN  RN  R1  R1  R2  SN  SN  RN  SN  SN  RN  SLAM, dynamic environments  Difficult road scenes  Suburbs w. little traffic day time  Traffic day time  Office cubicle still life  Texture challenges  Highway w. good/bad weather  Urban city scenes  Dynamic environments real traffic  Cluttered indoor still life  Urban scenes daytime  Road scenes with traffic  Adventure movie scenes  Road scene, animation movie  Difficult road scenes  Diverse driving scenes  Suburban roads, currently RGBD  Driving under varying weather and seasons  316 x 25  640 x 480  960 x 540  20 / 50  1800 /   <1/256  12 /   1/256  1024 x 333  2988 /   Daimler Urban [59]  2014  1024 x 440   / 70000  Malaga Urban [2]  Middlebury [56]  Cityscapes [10]  KITTI 2015 [40]  MPI Sintel [5]  Freiburg CNN [47]  HCI Training [26]  SYNTHIA [55]  Virtual KITTI [14]  2014  2014  2015  2015  2015  2016  2016  2016  2016  Oxford Car [35]  Robot To appear  1024 x 768  / >100000  1328 x 1108  28 / 15  <1/256  2048 x 1024   / 20000  *  1242 x 375  200 / 200  1024 x 436  1064 /   960 x 540  35454 /   2560 x 1080  1023 /   1/256  <1/256  <1/256  <1/256  960 x 720  >100000 /  <1/256  1242 x 375  2126 /   <1/256  1280 x 960  >100000 /  <1/256  Figure 2.|
||1 instances in total. (in cvpr2017)|
|1953|cvpr18-Im2Flow  Motion Hallucination From Static Images for Action Recognition|For the latter, we adopt the popular and effective two-stream CNN architecture [65] that is now widely used for CNN-based action recognition with videos [5, 8, 21, 86, 68, 12, 81, 19].|
||1 instances in total. (in cvpr2018)|
|1954|Kim_DCTM_Discrete-Continuous_Transformation_ICCV_2017_paper|Compared to methods based on handcrafted features [41, 59, 21], CNN based methods [53, 24] provide higher accuracy even though they do not consider geometric variations.|
||1 instances in total. (in iccv2017)|
|1955|Yatskar_Situation_Recognition_Visual_CVPR_2016_paper|To support future work on the SituNet data, we provide results for a baseline model  a Conditional Random Field (CRF) which includes CNN [43] features (fine tuned by backpropagating the CRF error).|
||1 instances in total. (in cvpr2016)|
|1956|Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper|We first trained CNN only using source samples, then, use it as a feature extractor.|
||1 instances in total. (in eccv2018)|
|1957|cvpr18-Efficient and Deep Person Re-Identification Using Multi-Level Similarity|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in cvpr2018)|
|1958|cvpr18-Visual Question Generation as Dual Task of Visual Question Answering|VQG shares the visual CNN with VQA part.|
||1 instances in total. (in cvpr2018)|
|1959|Generalized Deep Image to Image Regression|Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||1 instances in total. (in cvpr2017)|
|1960|Qianru_Sun_A_Hybrid_Model_ECCV_2018_paper|Recently, a model-based face autoencoder (MoFA) has been introduced [4] which combines a trainable CNN encoder with an expert-designed differentiable rendering layer as decoder, which allows for end-to-end training on real images.|
||1 instances in total. (in eccv2018)|
|1961|cvpr18-Adversarial Complementary Learning for Weakly Supervised Object Localization|Hcp: A flexible cnn framework for multi-label image classification.|
||1 instances in total. (in cvpr2018)|
|1962|Liu_Improved_Image_Captioning_ICCV_2017_paper|Another line of work enriches the image encoding beyond just using a CNN that was trained for image classification.|
||1 instances in total. (in iccv2017)|
|1963|Lip Reading Sentences in the Wild|A very recent work [2] uses a CNN and LSTMbased network and Connectionist Temporal Classification (CTC) [15] to compute the labelling.|
||1 instances in total. (in cvpr2017)|
|1964|Shen_High-Quality_Correspondence_and_ICCV_2017_paper|The second line is to adopt an end-to-end trainable CNN model from input images to segmentation labels with the fully convolutional networks (FCN) [29].|
||1 instances in total. (in iccv2017)|
|1965|Hongyang_Li_Neural_Network_Encapsulation_ECCV_2018_paper|From an engineering perspective, the original design for capsules in CNN structure (see Fig.|
||1 instances in total. (in eccv2018)|
|1966|cvpr18-Learned Shape-Tailored Descriptors for Segmentation|CRF learning with CNN features  for image segmentation.|
||1 instances in total. (in cvpr2018)|
|1967|Oliver_Groth_ShapeStacks_Learning_Vision-Based_ECCV_2018_paper|Our experiments suggest that AlexNet provides a useful baseline for CNN performance on this task.|
||1 instances in total. (in eccv2018)|
|1968|Marras_Deep_Globally_Constrained_ICCV_2017_paper|For example, in [26] a MRF that models pairwise relations between different joints is encoded in a single CNN layer.|
||1 instances in total. (in iccv2017)|
|1969|cvpr18-Learning From Synthetic Data  Addressing Domain Shift for Semantic Segmentation|Given an input X, we denote the output of a CNN as Y  RM N Nc , where Nc is the number  3753  the embedding.|
||1 instances in total. (in cvpr2018)|
|1970|cvpr18-M3  Multimodal Memory Modelling for Video Captioning|For the extensive comparisons, we extract features from both pretrained 2D CNN networks, e.g., GoogleNet [26], VGG-19 [23], Inception-V3 [27], ResNet50 [13], and 3D CNN networks, e.g., C3D [29].|
||1 instances in total. (in cvpr2018)|
|1971|Hong_Xuan_Randomized_Ensemble_Embeddings_ECCV_2018_paper|Guo, J., Gould, S.: Deep cnn ensemble with data augmentation for object detection.|
||1 instances in total. (in eccv2018)|
|1972|Cherian_Learning_Discriminative_ab-Divergences_ICCV_2017_paper|Higher-order pooling of CNN features via kernel linearization for action recognition.|
||1 instances in total. (in iccv2017)|
|1973|Jie_Zhang_Geometric_Constrained_Joint_ECCV_2018_paper|Xingang Pan, Jianping Shi, P.L.X.W., Tang, X.: Spatial as deep: Spatial cnn for traffic scene understanding.|
||1 instances in total. (in eccv2018)|
|1974|Xia_Sparse_Projections_for_2015_CVPR_paper|Image Retrieval  In Krizhevsky et al.s work [20], the responses of the second fc layer in the CNN model are used as holistic image features for semantic image retrieval.|
||1 instances in total. (in cvpr2015)|
|1975|Sultani_What_If_We_CVPR_2016_paper|For all other experiments, we used CNN features [35], computed within image/video proposals bounding boxes.|
||1 instances in total. (in cvpr2016)|
|1976|Xie_Deep_Determinantal_Point_ICCV_2017_paper|MLC Performance (%) on the Youtube-8M Validation Set   CNN with independent labels (CNN-IL): replace the softmax output layer in Inception v3 network with 6012way sigmoid layer.|
||1 instances in total. (in iccv2017)|
|1977|Wayne_Wu_Learning_to_Reenact_ECCV_2018_paper|Kumar, A., Chellappa, R.: Disentangling 3d pose in a dendritic cnn for uncon strained 2d face alignment.|
||1 instances in total. (in eccv2018)|
|1978|FCSS_ Fully Convolutional Self-Similarity for Dense Semantic Correspondence|Network Configuration for Dense Descriptor  Multi-Scale Convolutional Self-Similarity Layer In building the descriptor through a CNN architecture, there is a trade-off between robustness to semantic variations and fine-grained localization precision [32, 21].|
||1 instances in total. (in cvpr2017)|
|1979|Global Hypothesis Generation for 6D Object Pose Estimation|In [33] this is done via a so-called robust projective data association procedure, in [3] via a hand-crafted, robust energy, and in [19] via a CNN that scores every hypothesis.|
||1 instances in total. (in cvpr2017)|
|1980|Hou_DualNet_Learn_Complementary_ICCV_2017_paper|Bilinear cnn mod els for fine-grained visual recognition.|
||1 instances in total. (in iccv2017)|
|1981|Song_Deep_Metric_Learning_CVPR_2016_paper|Joint embeddings of shapes and images via cnn image purification.|
||1 instances in total. (in cvpr2016)|
|1982|Tekin_Direct_Prediction_of_CVPR_2016_paper|Motion Compensation CNN architecture.|
||1 instances in total. (in cvpr2016)|
|1983|Rohit_Pandey_Efficient_6-DoF_Tracking_ECCV_2018_paper|Other approaches like [16] treat pose estimation as a regression problem, and used a combination of a CNN based feature network and an object specific pose network to regress 3D pose directly.|
||1 instances in total. (in eccv2018)|
|1984|Learning Residual Images for Face Attribute Manipulation|Ordinal regression with multiple output cnn for age estimation.|
||1 instances in total. (in cvpr2017)|
|1985|Fatih_Cakir_Hashing_with_Binary_ECCV_2018_paper|With these insights in mind, we implement our novel two-stage hashing method with standard CNN architectures, and conduct experiments on multiple image retrieval datasets.|
||1 instances in total. (in eccv2018)|
|1986|Souly__Semi_Supervised_ICCV_2017_paper|For example, in [23] the error of MRF inference is passed backward into CNN in order to train jointly CNN and MRF.|
||1 instances in total. (in iccv2017)|
|1987|Wu_Self-Organized_Text_Detection_ICCV_2017_paper|[24] proposed the Text Flow method that sequences character CNN candidate detection, false character removal, text line extraction and text line verification using minimum cost flow networks.|
||1 instances in total. (in iccv2017)|
|1988|cvpr18-Towards Universal Representation for Unseen Action Recognition|Visual and Semantic Representation For all three datasets, we use a single CNN model to obtain the video features.|
||1 instances in total. (in cvpr2018)|
|1989|Mottaghi_See_the_Glass_ICCV_2017_paper|The CNN part of the network has the same architecture as that of CRC (shown in Figure 3) with two differences.|
||1 instances in total. (in iccv2017)|
|1990|Liu_Neural_Person_Search_ICCV_2017_paper|Method  mAP(%) top-1(%)  ACF [4]+DSIFT [39]+Euclidean ACF+DSIFT+KISSME [12] ACF+BoW [40]+Cosine ACF+LOMO+XQDA [17] ACF+IDNet [33]  CCF [36]+DSIFT+Euclidean CCF+DSIFT+KISSME CCF+BoW+Cosine CCF+LOMO+XQDA CCF+IDNet  CNN [24]+DSIFT+Euclidean CNN+DSIFT+KISSME CNN+BoW+Cosine CNN+LOMO+XQDA CNN+IDNet OIM [33](Baseline)  NPSM  21.7 32.3 42.4 55.5 56.5  11.3 13.4 26.9 41.2 50.9  34.5 47.8 56.9 68.9 68.6 75.5  77.9  25.9 38.1 48.4 63.1 63.0  11.7 13.9 29.3 46.4 57.1  39.4 53.6 62.3 74.1 74.8 78.7  81.2  4.4.1 Results on CUHK-SYSU  We report the person search performance on CUHK-SYSU with 100 gallery size setting in Table 2, where CNN represents the detector part (Faster-RCNN [24] with ResNet50) and IDNet denotes the re-identification part in the framework of OIM [33].|
||1 instances in total. (in iccv2017)|
|1991|Yan_Deep_Correlation_for_2015_CVPR_paper|In contrast, our TF-IDF based text features and CNN based visual features capture global properties of the two modalities respectively.|
||1 instances in total. (in cvpr2015)|
|1992|Chao_HICO_A_Benchmark_ICCV_2015_paper|A total of 106 heatmaps (80 object categories plus 26 body parts) are stacked together as the input to a CNN architecture (Fig.|
||1 instances in total. (in iccv2015)|
|1993|cvpr18-A Common Framework for Interactive Texture Transfer|Neural doodle [8] based on the combination of CNN and MRF methods [29] does not guarantee a high-quality image with low-level details (the first row).|
||1 instances in total. (in cvpr2018)|
|1994|Diversified Texture Synthesis With Feed-Forward Networks|Based on this, a noise map is gradually optimized to a desired output that matches the texture example in the CNN feature space.|
||1 instances in total. (in cvpr2017)|
|1995|Yihua_Cheng_Appearance-Based_Gaze_Estimation_ECCV_2018_paper|The face image can be used to compute the head pose [31, 6] or input to the CNN for gaze regression [29, 30].|
||1 instances in total. (in eccv2018)|
|1996|cvpr18-GeoNet  Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose|Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|1997|Semantic Segmentation via Structured Patch Prediction, Context CRF and Guidance CRF|As it is shown in Algorithm 1, all of these steps can  43251957  be described by CNN layers.|
||1 instances in total. (in cvpr2017)|
|1998|Zhang_Scale-Adaptive_Convolutions_for_ICCV_2017_paper|Note that the performance of our model is slightly lower than the very recent models reported on Arxiv [33, 28], which employ an effective optimization strategy [33] or a wider CNN architecture [28].|
||1 instances in total. (in iccv2017)|
|1999|cvpr18-Viewpoint-Aware Video Summarization|Given that our method considers the discrimination of the generated summary, not all clips, it worked better even when using CNN with large parameters.|
||1 instances in total. (in cvpr2018)|
|2000|Lu_Adaptive_Object_Detection_CVPR_2016_paper|Object detection via a multiregion and semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2016)|
|2001|Pengyuan_Lyu_Mask_TextSpotter_An_ECCV_2018_paper|use CNN and RNN to model image features and output the recognized sequences with CTC [11].|
||1 instances in total. (in eccv2018)|
|2002|Shih_Reflection_Removal_Using_2015_CVPR_paper|The captions below each image are the labels predicted by a trained CNN [16].|
||1 instances in total. (in cvpr2015)|
|2003|cvpr18-Image Super-Resolution via Dual-State Recurrent Networks|The first is that increasing the depth of a CNN model improves SR performance; a deeper model with more parameters can represent a more complex map Authors contributed equally to this work Ding Liu and Thomas Huangs research works are supported in part  by US Army Research Office grant W911NF-15-1-0317.|
||1 instances in total. (in cvpr2018)|
|2004|cvpr18-AttnGAN  Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks|The intermediate layers of the CNN learn local features of different sub-regions of the image, while the later layers learn global features of the image.|
||1 instances in total. (in cvpr2018)|
|2005|Learning Discriminative and Transformation Covariant Local Feature Detectors|Another solution is to reapply the CNN to slightly shifted images.|
||1 instances in total. (in cvpr2017)|
|2006|Wang_Orientation_Invariant_Feature_ICCV_2017_paper|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in iccv2017)|
|2007|cvpr18-End-to-End Recovery of Human Shape and Pose|Monocular 3d human pose estimation in the wild using improved cnn supervision.|
||1 instances in total. (in cvpr2018)|
|2008|Irie_Alternating_Co-Quantization_for_ICCV_2015_paper|In this evaluation, we use 4, 096D CNN activation features extracted from the fc6 layer of VGGNet [31] and 300D skip-gram word vectors for image and text representations, respectively.|
||1 instances in total. (in iccv2015)|
|2009|LCR-Net_ Localization-Classification-Regression for Human Pose|These pose proposals are efficiently sampled, scored and refined using an end-to-end CNN architecture inspired by the latest work on object detection [22].|
||1 instances in total. (in cvpr2017)|
|2010|Tete_Xiao_Unified_Perceptual_Parsing_ECCV_2018_paper|[32] that although the theoretical receptive field of deep CNN is large enough, the empirical receptive field of deep CNN is relatively much smaller [33], we apply a Pyramid Pooling Module (PPM) from PSPNet [16] on the last layer of the backbone network before feeding it into the top-down branch in FPN.|
||1 instances in total. (in eccv2018)|
|2011|Gupta_Aligned_Image-Word_Representations_ICCV_2017_paper|Our recognition model  is related to previous openvocabulary recognition/localization models [55, 48, 18], which learn to map visual CNN features to continuous word vector representations.|
||1 instances in total. (in iccv2017)|
|2012|Deep Pyramidal Residual Networks|Feature Map Dimension Configuration  Most deep CNN architectures [7, 8, 13, 25, 31, 35] utilize an approach whereby feature map dimensions are increased by a large margin when the size of the feature map decreases, and feature map dimensions are not increased until they encounter a layer with downsampling.|
||1 instances in total. (in cvpr2017)|
|2013|cvpr18-Deep Adversarial Metric Learning|Difference with Existing Data Augmentation Methods: Data augmentation aims to apply transformation to the images without altering the labels, which have been widely used to improve the performance of CNN and prevent from overfitting [21].|
||1 instances in total. (in cvpr2018)|
|2014|Gu_Convolutional_Sparse_Coding_ICCV_2015_paper|In [16], the author proposed to use a convolutional neural network (CNN) to approximate the CSC model for SR, the model is actually a CNN based SR model and the authors also did not compare the proposed method with state-of-the-art SR algorithms.|
||1 instances in total. (in iccv2015)|
|2015|cvpr18-Attentive Generative Adversarial Network for Raindrop Removal From a Single Image|Its CNN consists of 3 layers, where each has 512 neurons.|
||1 instances in total. (in cvpr2018)|
|2016|Convolutional Random Walk Networks for Semantic Image Segmentation|However, in the context of prior segmentation methods [27, 19, 17, 5], such complex losses often require: 1) modifying the network structure (casting CNN into an RNN) [27, 5], or 2) using a complicated multi-stage learning scheme, where different layers are optimized during a different training stage [19, 17].|
||1 instances in total. (in cvpr2017)|
|2017|Lee_Unsupervised_Representation_Learning_ICCV_2017_paper|Our hypothesis is that successfully solving the sequence sorting task will allow the CNN to learn useful visual representation to recover the temporal coherence of video by observing how objects move in the scene.|
||1 instances in total. (in iccv2017)|
|2018|cvpr18-Creating Capsule Wardrobes From Fashion Images|All distances are computed on the 2048-D CNN features and normalized by the  of all distances.|
||1 instances in total. (in cvpr2018)|
|2019|cvpr18-Shift  A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions|In the pre-processing step, we detect and align all faces using a multi-task CNN [28].|
||1 instances in total. (in cvpr2018)|
|2020|Deep Video Deblurring for Hand-Held Cameras|In this work, we introduce a deep learning solution to video deblurring, where a CNN is trained end-toend to learn how to accumulate information across frames.|
||1 instances in total. (in cvpr2017)|
|2021|cvpr18-Gaze Prediction in Dynamic 360° Immersive Videos|Then we combine the CNN features and LSTM features for gaze displacement prediction between gaze point at a current time and gaze point at an upcoming time.|
||1 instances in total. (in cvpr2018)|
|2022|cvpr18-Interleaved Structured Sparse Convolutional Neural Networks|Deep roots: Improving CNN efficiency with hierarchical filter groups.|
||1 instances in total. (in cvpr2018)|
|2023|Di_Lin_Multi-Scale_Context_Intertwining_ECCV_2018_paper|Hazirbas, C., Ma, L., Domokos, C., Cremers, D.: FuseNet: Incorporating depth into semantic segmentation via fusion-based CNN architecture.|
||1 instances in total. (in eccv2018)|
|2024|cvpr18-Direct Shape Regression Networks for End-to-End Face Alignment|Method CelebA MAFL  TCDCN [50] Cascaded CNN [34] CFAN [48] RCPR [6] SDM [42] CFSS [54] DSRN   4.12 4.35 3.95 3.08  7.95 9.73 15.84   3.15  as the most challenging subset.|
||1 instances in total. (in cvpr2018)|
|2025|Chen_Query-Guided_Regression_Network_ICCV_2017_paper|PGN generates proposals and extracts their CNN features via a RoI pooling operation [28].|
||1 instances in total. (in iccv2017)|
|2026|Attention-Aware Face Hallucination via Deep Reinforcement Learning|SRCNN[3] is a 3-layers CNN and it is the fastest among the compared methods.|
||1 instances in total. (in cvpr2017)|
|2027|A Joint Speaker-Listener-Reinforcer Model for Referring Expressions|Here, a pre-trained CNN model is used to define a visual representation for the target object and other visual context.|
||1 instances in total. (in cvpr2017)|
|2028|Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features|Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||1 instances in total. (in cvpr2017)|
|2029|Low-Rank Embedded Ensemble Semantic Dictionary for Zero-Shot Learning|There are three kinds of features: AlexNet CNN features [ALEX], VGG-VeryDeep-19 CNN features [VGG] and GoogLeNet features [GGL].|
||1 instances in total. (in cvpr2017)|
|2030|cvpr18-Visual to Sound  Generating Natural Sound for Videos in the Wild|Comparison with [15]: [15] presents a CNN stacked with RNN structure to predict sound features (cochleagrams) at each time step, and audio samples are reconstructed by example-based retrieval.|
||1 instances in total. (in cvpr2018)|
|2031|End-To-End 3D Face Reconstruction With Deep Neural Networks|Joint head pose estimation and face alignment framework using global and local CNN feaIn Proc.|
||1 instances in total. (in cvpr2017)|
|2032|Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper|Tolias, G., Sicre, R., , Jegou, H.: Particular object retrieval with integral max pooling of cnn activations.|
||1 instances in total. (in eccv2018)|
|2033|T_M_Feroz_Ali_Maximum_Margin_Metric_ECCV_2018_paper|Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identification by multi-channel  parts-based cnn with improved triplet loss function.|
||1 instances in total. (in eccv2018)|
|2034|Zeng_Learning_by_Associating_2013_CVPR_paper|[ News From CNN ]  President  Barack  Obama,  Italian  Prime  Minister  Silvio  Berlusconi,  center,  and  Russian  President  Dmitry  Medvedev,  right,  smile  during  a  group  photo  at  the  G20  Summit in London.|
||1 instances in total. (in cvpr2013)|
|2035|cvpr18-Tags2Parts  Discovering Semantic Regions From Shape Tags|SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation.|
||1 instances in total. (in cvpr2018)|
|2036|Yan_Wang_Spatial_Pyramid_Calibration_ECCV_2018_paper|Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: Cnn features off-the-shelf: an astounding baseline for recognition.|
||1 instances in total. (in eccv2018)|
|2037|Temporal Attention-Gated Model for Robust Sequence Classification|For the sake of computational efficiency, we extract CNN features with a sampling rate 1/8  6736  Event: biking  Event: birthday  Event: baseball  Figure 6.|
||1 instances in total. (in cvpr2017)|
|2038|Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper|Then CNN features of region proposals are extracted.|
||1 instances in total. (in cvpr2015)|
|2039|cvpr18-IQA  Visual Question Answering in Interactive Environments|The first deep architectures designed for VQA involved using an RNN to encode the question, using a CNN to encode the image and combining them using fully connected layers to yield the answer [2, 42].|
||1 instances in total. (in cvpr2018)|
|2040|cvpr18-Person Transfer GAN to Bridge Domain Gap for Person Re-Identification|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in cvpr2018)|
|2041|Dinesh_Jayaraman_ShapeCodes_Self-Supervised_Feature_ECCV_2018_paper|Access to synthetic object models is especially valuable to train a generative CNN [14].|
||1 instances in total. (in eccv2018)|
|2042|Song_SUN_RGB-D_A_2015_CVPR_paper|We use both linear SVM and RBF kernel SVM with this CNN feature.|
||1 instances in total. (in cvpr2015)|
|2043|Surveillance Video Parsing With Single Frame Supervision|Hcp: A flexible cnn framework for multi-label image classification.|
||1 instances in total. (in cvpr2017)|
|2044|Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper|In [41, 40], a CNN was trained to map to and from spherical mesh representations without supervision.|
||1 instances in total. (in eccv2018)|
|2045|Hu_WordSup_Exploiting_Word_ICCV_2017_paper|Recently, some component based methods [49, 44, 8, 38, 35] attempt to learn text components by CNN feature learning.|
||1 instances in total. (in iccv2017)|
|2046|cvpr18-Towards Human-Machine Cooperation  Self-Supervised Sample Mining for Object Detection|lj(xi, W) means the softmax loss of the proposal  1  1607  xi in the j-th classifier:  1 + y(j)  i  2  log j(xi; W)+  lj(xi, W) = (cid:0)  1  y(j)  i  2  log(1  j(xi; W))(cid:1),  where W represents the parameter of the CNN for all m categories (including background), j(xi; W) denotes the probability of belonging to the j-th category for each region proposal xi.|
||1 instances in total. (in cvpr2018)|
|2047|Xu_Tell_Me_What_2014_CVPR_paper|Rows 2 and 3 show segmentation accuracy for each class when a CNN tag predictor or the ground truth tags are used respectively.|
||1 instances in total. (in cvpr2014)|
|2048|Luo_Switchable_Deep_Network_2014_CVPR_paper|For the convolutional layers and the logistic regression layer W are calculated in the same as shown in Fig.3 (a) (c), Err way as the traditional CNN [14].|
||1 instances in total. (in cvpr2014)|
|2049|Jiang_Learning_Discriminative_Latent_ICCV_2017_paper|For all the datasets, we utilize the 4096-dimension CNN feature vectors extracted by the imagenet-vgg-verydeep-19 pre-trained model [36].|
||1 instances in total. (in iccv2017)|
|2050|cvpr18-Learning Time Memory-Efficient Deep Architectures With Budgeted Super Networks|Efficient architectures: Architecture improvements have been widely used in CNN to improve cost efficiency of network components, some examples are the bottleneck units in the ResNet model [11], the use of depthwise separable convolution in Xception [3] and the lightweight MobileNets[13] or the combinaison of pointwise group convolution and channel shuffle in ShuffleNet[30].|
||1 instances in total. (in cvpr2018)|
|2051|cvpr18-Self-Supervised Learning of Geometrically Stable Features Through Probabilistic Introspection|Following the success of CNN architectures for recognition tasks like image classification [18], these architectures have been used as feature extractors for other tasks, including semantic matching.|
||1 instances in total. (in cvpr2018)|
|2052|Cho_Unsupervised_Object_Discovery_2015_CVPR_paper|Also note that the best  performing method [56] uses CNN features pretrained on the ImageNet dataset [11], thus additional supervised data (A).|
||1 instances in total. (in cvpr2015)|
|2053|cvpr18-FoldingNet  Point Cloud Auto-Encoder via Deep Grid Deformation|To encode, it sorts the 3D points using the lexicographic order and applies a 1D CNN on the point sequence.|
||1 instances in total. (in cvpr2018)|
|2054|cvpr18-Language-Based Image Editing With Recurrent Attentive Models|Given a source image of size H  W , the CNN encoder produces a M  N spatial feature map, with each position on the feature map containing a D-dimensional feature vector (D channels), V = {vi : i = 1, .|
||1 instances in total. (in cvpr2018)|
|2055|cvpr18-High Performance Visual Tracking With Siamese Region Proposal Network|The two branches share parameters in CNN so that the two patches are implicitly encoded by the same transformation which is suitable for the subsequent tasks.|
||1 instances in total. (in cvpr2018)|
|2056|Santhosh_Kumar_Ramakrishnan_Sidekick_Policy_Learning_ECCV_2018_paper|Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|2057|Safa_Messaoud_Structural_Consistency_and_ECCV_2018_paper|[18] uses a CNN network to learn an embedding of a gray-level images, which is then used as input for a PixelCNN network.|
||1 instances in total. (in eccv2018)|
|2058|A Combinatorial Solution to Non-Rigid 3D Shape-To-Image Matching|g  Score Image: In order to perform image segmentation with our method, we use a data term that is based on the recently proposed 3D U-Net CNN [9].|
||1 instances in total. (in cvpr2017)|
|2059|cvpr18-CLIP-Q  Deep Network Compression Learning by In-Parallel Pruning-Quantization|7879   Accuracy Network Size  AlexNet on ImageNet Uncompressed Data-Free Pruning [47] (CaffeNet) Deep Fried Convnets [57] Less Is More [60] Dynamic Network Surgery [13] Circulant CNN [3] Quantized CNN [53] Binary-Weight-Networks [41] Deep Compression [16] [16] + Weighted-Entropy Quantization [38] CLIP-Q GoogLeNet on ImageNet Uncompressed Weighted-Entropy Quantization [38] CLIP-Q ResNet-50 on ImageNet Uncompressed ThiNet [34] Weighted-Entropy Quantization [38] CLIP-Q    -2.2% -0.3% -0.6% -0.2% 1 -0.4% -1.4% +0.1% +0.0% -0.8% +0.7%    +0.2% +0.0%    -1.9% -1.8% +0.6%  243.9 MB 159 MB 68 MB 57 MB 13.8 MB 12.7 MB 12.6 MB 7.6 MB 8.9 (6.9) MB 8.3 (6.5) MB 4.8 MB  28.0 MB 4.4 MB 2.8 MB  102.5 MB 49.5 MB 16.0 MB 6.7 MB  Table 4.|
||1 instances in total. (in cvpr2018)|
|2060|Rochan_Weakly_Supervised_Localization_2015_CVPR_paper|These object models are trained from a subset of the ImageNet images with bounding box annotations using the Caffe-based CNN features.|
||1 instances in total. (in cvpr2015)|
|2061|cvpr18-Weakly Supervised Action Localization by Sparse Temporal Pooling Network|Two(cid:173)stream CNN Models  We employ the recently proposed I3D model [5] to compute feature representations for the sampled video segments.|
||1 instances in total. (in cvpr2018)|
|2062|Xu_R-C3D_Region_Convolutional_ICCV_2017_paper|per-frame CNN features or 16-frame C3D features).|
||1 instances in total. (in iccv2017)|
|2063|Product Split Trees|Deep descriptors are formed as normalized and PCA-compressed outputs of fully-connected layer of a CNN pretrained on the Imagenet dataset[1].|
||1 instances in total. (in cvpr2017)|
|2064|Wu_Sampling_Matters_in_ICCV_2017_paper|A lightened cnn for deep face representa tion.|
||1 instances in total. (in iccv2017)|
|2065|Sevilla-Lara_Optical_Flow_With_CVPR_2016_paper|The unaries are the CNN output and the pairwise potentials are a position kernel and a bilateral kernel with both position and RGB values.|
||1 instances in total. (in cvpr2016)|
|2066|GMS_ Grid-based Motion Statistics for Fast, Ultra-Robust Feature Correspondence|Such research is still on-going, the most recent example being CNN trained LIFT descriptors [47].|
||1 instances in total. (in cvpr2017)|
|2067|Rhinehart_Learning_Action_Maps_CVPR_2016_paper|The office and corridor layers correspond to the features from the scene classification CNN, and the sit layer corresponds to the object detection CNN features aggregated across all sit-able objects, which is also one of the baselines as described in Section 3.2.|
||1 instances in total. (in cvpr2016)|
|2068|Deep Representation Learning for Human Motion Prediction and Classification|Due to the structure of the data, we hypothesize that fully-connected encoders are more expressive than state-of-the-art CNN architectures.|
||1 instances in total. (in cvpr2017)|
|2069|The Amazing Mysteries of the Gutter_ Drawing Inferences Between Panels in Comic Book Narratives|In particular, we use Faster R-CNN [45] initialized with a pretrained VGG CNN M 1024 model [9] and alternatingly optimize the region proposal network and the detection network.|
||1 instances in total. (in cvpr2017)|
|2070|Visual Dialog|Deeper LSTM and Normalized CNN Visual Question Answering model.|
||1 instances in total. (in cvpr2017)|
|2071|Ding_Probabilistic_Label_Relation_ICCV_2015_paper|First we pre-train a CNN with a HEX graph as the top layer until convergence.|
||1 instances in total. (in iccv2015)|
|2072|Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper|Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition.|
||1 instances in total. (in eccv2018)|
|2073|Issam_Hadj_Laradji_Where_are_the_ECCV_2018_paper|Walach, E., Wolf, L.: Learning to count with cnn boosting.|
||1 instances in total. (in eccv2018)|
|2074|cvpr18-Unsupervised Learning of Depth and Ego-Motion From Monocular Video Using 3D Geometric Constraints|Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2075|Context-Aware Captions From Context-Agnostic Supervision|First, easy confusion: For each image in the validation (test) set, we find its nearest neighbor in the FC7 space of a pre-trained VGG-16 CNN [31], and repeat this process of neighbor finding for 1000 randomly chosen source images.|
||1 instances in total. (in cvpr2017)|
|2076|Yang_He_Diverse_Conditional_Image_ECCV_2018_paper|They first leveraged a CNN to generate a coarse image, and then search patch level nearest neighbors with perceptual similarity [12] from the training data for replacing.|
||1 instances in total. (in eccv2018)|
|2077|Sergio_Silva_License_Plate_Detection_ECCV_2018_paper|To take advantage of its shape, we proposed a novel CNN called Warped Planar Object Detection Network.|
||1 instances in total. (in eccv2018)|
|2078|Bertasius_High-for-Low_and_Low-for-High_ICCV_2015_paper|N 4 fields rely on dictionary learning and the use of the Nearest Neighbor algorithm within a CNN framework while DeepNet uses a traditional CNN architecture to predict contours.|
||1 instances in total. (in iccv2015)|
|2079|Zhou_GridFace_Face_Rectification_ECCV_2018_paper|[20] propose the face representation by fusing multiple pose-aware CNN models.|
||1 instances in total. (in eccv2018)|
|2080|Look Into Person_ Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing|To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 35] and the adoptions of multi-scale feature representations [4, 5, 28].|
||1 instances in total. (in cvpr2017)|
|2081|cvpr18-ShuffleNet  An Extremely Efficient Convolutional Neural Network for Mobile Devices|Deep roots: Improving cnn efficiency with hierarchical filter groups.|
||1 instances in total. (in cvpr2018)|
|2082|Rastegari_Computationally_Bounded_Retrieval_2015_CVPR_paper|We used CNN features extracted by Caffe [8] with 4096 dimensions.|
||1 instances in total. (in cvpr2015)|
|2083|Interpretable Structure-Evolving LSTM|Each node v0 is represented by the deep features f (0) learned from the underlying CNN model with D dimensions.|
||1 instances in total. (in cvpr2017)|
|2084|cvpr18-Geometry-Aware Scene Text Detection With Instance Transformation Network|lar representation learnt in standard CNN models is incapable of well encoding the unique geometric distributions of scene text.|
||1 instances in total. (in cvpr2018)|
|2085|Ouyang_Factors_in_Finetuning_CVPR_2016_paper|At the finetuning stage, aside from replacing the CNNs 1000-way pretrained classification layer with a randomly initialized (200 + 1)-way softmax classification layer (plus 1 for background), the CNN architecture is unchanged.|
||1 instances in total. (in cvpr2016)|
|2086|cvpr18-Classifier Learning With Prior Probabilities for Facial Action Unit Recognition|[34] proposed a patch-based CNN model for region learning and AU detection.|
||1 instances in total. (in cvpr2018)|
|2087|Hu_Natural_Language_Object_CVPR_2016_paper|We nor We use VGG-16 net [30] trained on ILSVRC-2012 dataset [29] as the CNN architecture for CNNlocal and CNNglobal and extract 1000-dimensional fc8 outputs as  4557  CNNword embeddingword predictionS=man in middle with blue shirt and blue shortsscorebox=p(S=man in middle with blue shirt and blue shorts | Ibox, Iim, xspatial)CNNIboxIimxspatialquery=man in middle with blue shirt and blue shortsLSTMlocalLSTMglobalLSTMlanguagespatial configurationlocalglobalxbox and xcontext, and use the same LSTM implementation as in [4], where the gates are computed as  it = (Wxixt + Whiht1 + bi) ft = (Wxf xt + Whf ht1 + bf ) ot = (Wxoxt + Whoht1 + bo) gt = tanh(Wxgxt + Whght1 + bg)  (3)  (4)  (5)  (6)  All the three LSTM units have 1000-dimensional state ht.|
||1 instances in total. (in cvpr2016)|
|2088|cvpr18-Burst Denoising With Kernel Prediction Networks|Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||1 instances in total. (in cvpr2018)|
|2089|Liu_Efficient_Global_2D-3D_ICCV_2017_paper|Netvlad: Cnn architecture for weakly supervised place recognition.|
||1 instances in total. (in iccv2017)|
|2090|Xu_Deep_Interactive_Object_CVPR_2016_paper|Other work has looked at improving the boundary localization of CNN semantic segmentation approaches.|
||1 instances in total. (in cvpr2016)|
|2091|Ranjan_Multi-Label_Cross-Modal_Retrieval_ICCV_2015_paper|Table 2 also reports the NDCG score using CNN based image features.|
||1 instances in total. (in iccv2015)|
|2092|cvpr18-Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis|We implement this idea using perceptual loss [11, 3, 31, 2], which measures the distance of real and fake images in the feature space of a pre-trained CNN by  Lrec = X  l  (cid:13)(cid:13)l(Gglobal)  l(Mglobal)(cid:13)(cid:13),  (8)  where l is the feature extracted from the l-th layer of a CNN.|
||1 instances in total. (in cvpr2018)|
|2093|Yifan_Sun_Beyond_Part_Models_ECCV_2018_paper|Li, Y., Liu, L., Shen, C., van den Hengel, A.: Mining mid-level visual patterns with  deep CNN activations.|
||1 instances in total. (in eccv2018)|
|2094|cvpr18-Controllable Video Generation With Sparse Trajectories|Some earlier methods focus on predicting semantic information in the future, such as action categories [12], pedestrian trajectories [11], car trajectories [33], deep CNN features [29], or optical flows [34, 20].|
||1 instances in total. (in cvpr2018)|
|2095|Sujoy_Paul_W-TALC_Weakly-supervised_Temporal_ECCV_2018_paper|Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn architecture for  weakly supervised place recognition.|
||1 instances in total. (in eccv2018)|
|2096|Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild|To address this ambiguity of unconstrained emotion, we further propose a novel Deep Locality-preserving CNN (DLP-CNN).|
||1 instances in total. (in cvpr2017)|
|2097|From Motion Blur to Motion Flow_ A Deep Learning Solution for Removing Heterogeneous Motion Blur|The most relevant work is a method based on CNN and patch-level blur type classification [33], which also focuses on estimating the motion flow from single blurry image.|
||1 instances in total. (in cvpr2017)|
|2098|cvpr18-3D Human Pose Estimation in the Wild by Adversarial Learning|Monocular 3d human pose estimation in the wild using improved cnn supervision.|
||1 instances in total. (in cvpr2018)|
|2099|Chenglong_Li_Cross-Modal_Ranking_with_ECCV_2018_paper|Attribute-based Precision Rate and Success Rate (PR/SR %) on RGBT210 dataset with 9 trackers, including CSR [4], DSST [32], MEEM [33], CNN [22], SOWP [6], KCF [22], SGT [5], CFnet [27] and ECO [28].|
||1 instances in total. (in eccv2018)|
|2100|cvpr18-Bidirectional Attentive Fusion With Context Gating for Dense Video Captioning|Following [20], each video frame is encoded by the 3D CNN [36], which was pretrained on Sports-1M video dataset [17].|
||1 instances in total. (in cvpr2018)|
|2101|cvpr18-Two-Step Quantization for Low-Bit Neural Networks|Experiments are conducted on two of the mostly used CNN models, i.e., AlexNet [20] and VGG-16 [29].|
||1 instances in total. (in cvpr2018)|
|2102|Xin_Wang_Look_Before_You_ECCV_2018_paper|ResNet-152 CNN features [13] are extracted for all the images without fine-tuning.|
||1 instances in total. (in eccv2018)|
|2103|Gratianus_Wesley_Putra_Data_Interpolating_Convolutional_Neural_ECCV_2018_paper|: NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNN by Neural Discriminative Dimensionality Reduction.|
||1 instances in total. (in eccv2018)|
|2104|Pixelwise Instance Segmentation With a Dynamically Instantiated Network|The category-level segmentation, along with CNN features, was used to predict instance-level bounding boxes.|
||1 instances in total. (in cvpr2017)|
|2105|Hsueh-Fu_Lu_Toward_Scale-Invariance_and_ECCV_2018_paper|Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic  segmentation-aware cnn model.|
||1 instances in total. (in eccv2018)|
|2106|Jingwei_Ji_End-to-End_Joint_Semantic_ECCV_2018_paper|Here we resort to 3D CNN as the ingredient to achieve temporal aggregation.|
||1 instances in total. (in eccv2018)|
|2107|Hosang_Taking_a_Deeper_2015_CVPR_paper|Surrogate tasks for improved detections  The R-CNN approach (Regions with CNN features) wraps the large network previously trained for the ImageNet classification task [27], which we refer to as AlexNet (see figure 4).|
||1 instances in total. (in cvpr2015)|
|2108|Awesome Typography_ Statistics-Based Text Effects Transfer|The third method, Neural Doodle [3], is based on the combination of MRF and CNN [17] and incorporates semantic maps for analogy guidance.|
||1 instances in total. (in cvpr2017)|
|2109|cvpr18-Learning Pose Specific Representations by Predicting Different Views|Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2110|cvpr18-Unsupervised Discovery of Object Landmarks as Structural Representations|Method  Cascaded CNN [55]  RCPR [5] CFAN [77] TCDCN [82]  MAFL ALFW 11.60 15.84 10.94 07.95 07.65 08.97 09.73 07.23 05.39 06.90 Thewlis et al.|
||1 instances in total. (in cvpr2018)|
|2111|Soomro_Predicting_the_Where_CVPR_2016_paper|The difference in performance is attributed to the online vs. offline nature of the methods, as well as the use of CNN features by [7].|
||1 instances in total. (in cvpr2016)|
|2112|Masana_Domain-Adaptive_Deep_Network_ICCV_2017_paper|Since this network excelled on the ImageNet Large-Scale Visual Recognition Challenge in 2014 (ILSVRC-2014), it is a strong candidate as a pre-trained CNN source for the transfer learning.|
||1 instances in total. (in iccv2017)|
|2113|cvpr18-GAGAN  Geometry-Aware Generative Adversarial Networks|Going forward, we are currently working on extending our method in several ways by, i) applying it to the generation of larger images, ii) exploring more complex geometric transformations that have the potential to alleviate the deformations induced by the piecewise-affine warping and iii) augmenting traditional CNN architectures with our method for facial landmark detection.|
||1 instances in total. (in cvpr2018)|
|2114|cvpr18-Textbook Question Answering Under Instructor Guidance With Memory Networks|We use BiLSTM and CNN respectively to extract essential facts from the input, and use  the attention mechanisms to merge the latent features for reasoning.|
||1 instances in total. (in cvpr2018)|
|2115|Fabien_Baradel_Object_Level_Visual_ECCV_2018_paper|Effect of the CNN architecture (choice of kernel inflations) on a single head ResNet-18 network.|
||1 instances in total. (in eccv2018)|
|2116|Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks|Low-level representations, limited by the local receptive field, are not aware of objectlevel semantic information; on the other hand, high-level features are not necessarily aligned consistently with object boundaries because CNN models are invariant to translation.|
||1 instances in total. (in cvpr2017)|
|2117|CityPersons_ A Diverse Dataset for Pedestrian Detection|The diversity of CityPersons allows us for the first time to train one single CNN model that generalizes well over multiple benchmarks.|
||1 instances in total. (in cvpr2017)|
|2118|cvpr18-Bilateral Ordinal Relevance Multi-Instance Regression for Facial Action Unit Intensity Estimation|Furthermore, we also compare our method to the state-of-the-art supervised deep models such as CNN [9], CCNN-IT [42], and 2DC [17].|
||1 instances in total. (in cvpr2018)|
|2119|Deng_Structure_Inference_Machines_CVPR_2016_paper|Hence, connectivity of the model should  4776  Algorithm 1 Structure Inference Machine  mij is:  Inputs: frame, detected person bounding boxes Pass image through CNN to get xs Pass person bounding boxes through CNN to get {xi}M i=1 Initialize m(0) for each iteration t do  si by xs; m(0)  ij by {xi}M i=1  is, m(0)  for edge (i, j) do  Compute messages m(t)  ij and m(t)  ji by Eq.|
||1 instances in total. (in cvpr2016)|
|2120|From Zero-Shot Learning to Conventional Supervised Classification_ Unseen Visual Data Synthesis|For deep learning features, we adopt CNN features released by[54] for the four datasets using the VGG-19 model.|
||1 instances in total. (in cvpr2017)|
|2121|Humayun_The_Middle_Child_ICCV_2015_paper|[29] introduced a CNN trained on COCO to generate segment proposals.|
||1 instances in total. (in iccv2015)|
|2122|Gupta_Characterizing_and_Improving_ICCV_2017_paper|FlowNet [20, 7] matches the performance of variational methods and introduces novel CNN architectures for optical flow estimation.|
||1 instances in total. (in iccv2017)|
|2123|Yin_Li_In_the_Eye_ECCV_2018_paper|Poleg, Y., Ephrat, A., Peleg, S., Arora, C.: Compact CNN for indexing egocentric  videos.|
||1 instances in total. (in eccv2018)|
|2124|Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in cvpr2017)|
|2125|Xiao_Recurrent_3D-2D_Dual_ICCV_2017_paper|CDM [26], SDM [25], ESR [6] are 2D-based methods; 3DDFA [29] and D3PF [11] are the most recent 3D face alignment methods based on cascaded CNN regression.|
||1 instances in total. (in iccv2017)|
|2126|cvpr18-A PID Controller Approach for Stochastic Optimization of Deep Networks|First, our proposed PID optimizer achieves lower test errors than SGD-Momentum for all the used CNN architectures on both the two CIFAR datasets, except for ResNet with depth 1202.|
||1 instances in total. (in cvpr2018)|
|2127|cvpr18-Importance Weighted Adversarial Nets for Partial Domain Adaptation|Baselines and Settings The proposed method is compared with the baseline that finetuning the CNN using source data only (AlexNet+bottleneck) and several stateof-the-art deep learning-base domain adaptation methods: RevGrad [7], RTN [18], ADDA-grl [24], and SAN [4].|
||1 instances in total. (in cvpr2018)|
|2128|He_Mask_R-CNN_ICCV_2017_paper|Related Work  R-CNN: The Region-based CNN (R-CNN) approach [10] to bounding-box object detection is to attend to a manageable number of candidate object regions [33, 16] and evaluate convolutional networks [20, 19] independently on each RoI.|
||1 instances in total. (in iccv2017)|
|2129|cvpr18-PoseTrack  A Benchmark for Human Pose Estimation and Tracking|We adopt the DeeperCut CNN architecture from [18] as our pose estimation method.|
||1 instances in total. (in cvpr2018)|
|2130|Dvornik_BlitzNet_A_Real-Time_ICCV_2017_paper|On the left, CNN denotes a feature extractor, here ResNet-50 [9]; it is followed by the downscale-stream (in blue) and the last part of the net is the upscale-stream (in purple), which consists of a sequence of deconvolution layers interleaved with ResSkip blocks (see Figure 3).|
||1 instances in total. (in iccv2017)|
|2131|cvpr18-FaceID-GAN  Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis|Unconstrained face verification using deep cnn features.|
||1 instances in total. (in cvpr2018)|
|2132|Context-Aware Correlation Filter Tracking|Major improvements to MOSSE include the incorporation of kernels and HOG features [10], the addition of color name features [18] or color histograms [1], integration with sparse tracking [30], adaptive scale [2, 5, 18], mitigation of boundary effects [6], and the integration of deep CNN features [21].|
||1 instances in total. (in cvpr2017)|
|2133|cvpr18-MovieGraphs  Towards Understanding Human-Centric Situations From Videos|2, xage , xgen ) and extract features from another CNN trained to predict emotions [21] (Eq.|
||1 instances in total. (in cvpr2018)|
|2134|Tian_WeText_Scene_Text_ICCV_2017_paper|[30] propose a Connectionist Text Proposal Network that combines CNN and long short-term memory (LSTM) architecture to detect text lines directly.|
||1 instances in total. (in iccv2017)|
|2135|cvpr18-Feature Quantization for Defending Against Distortion of Images|For instance, XNORNet [37] is a type of CNN that uses mostly bitwise operations to approximate convolutions, where both filters and features are binary.|
||1 instances in total. (in cvpr2018)|
|2136|cvpr18-Learning to Act Properly  Predicting and Explaining Affordances From Images|[9] designed a hash function such that CNN can be applied to graphs.|
||1 instances in total. (in cvpr2018)|
|2137|Wu_Deep_Multiple_Instance_2015_CVPR_paper|4.1.3  Implementation Details  Following previous works [3, 9, 36], we first conduct pretraining of CNN on the ILSVRC dataset [6].|
||1 instances in total. (in cvpr2015)|
|2138|Gated Feedback Refinement Network for Dense Image Labeling|The encoder network is typically a CNN with alternating layers of convolution, pooling, non-linear activation, etc.|
||1 instances in total. (in cvpr2017)|
|2139|Bowen_Cheng_Revisiting_RCNN_On_ECCV_2018_paper|2 Related Work  Object Detection Recent CNN based object detectors can generally be categorized into two-stage and single stage.|
||1 instances in total. (in eccv2018)|
|2140|Kalogeiton_Joint_Learning_of_ICCV_2017_paper|For example, the triplet human kicks ball becomes two pairs: (i) one with object human and action kick, and (ii) another pair with ob 4169  Modality  Method  V  VP [36]  Joint CNN [39]  VRD [24] Baseline  Ours Multitask  V+L+F  VRD [24]  Phrase detection R@100 R@50 0.04 0.07 2.2 7.7 14.5 16.2  0.07 0.09 2.6 11.9 18.3 17.0  Relationship detection R@100  R@50   0.09 1.9 7.1 11.3 14.7   0.07 1.6 4.5 8.6 13.9  Table 7: Comparison to different architectures and to the state-of-the-art visual relationships on the VRD dataset for phrases and relationship detection.|
||1 instances in total. (in iccv2017)|
|2141|Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos|The proposed method addresses an important problem of video understanding: how to build a video representation that incorporates the CNN features over the entire video.|
||1 instances in total. (in cvpr2017)|
|2142|Mattyus_HD_Maps_Fine-Grained_CVPR_2016_paper|We exploit the CNN for segmentation [23, 19] trained  3613  l l  a c e r  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0  Precision-Recall Curves  road road* sidewalk parking  0.1  0.2  0.3  0.4  0.5  0.6  0.7  precision  Figure 4.|
||1 instances in total. (in cvpr2016)|
|2143|Gay_Probabilistic_Structure_From_ICCV_2017_paper|A semantic engine extracts and matches bounding boxes from objects in multiple frames using a CNN detector [47].|
||1 instances in total. (in iccv2017)|
|2144|Commonly Uncommon_ Semantic Sparsity in Situation Recognition|Bilinear cnn models for fine-grained visual recognition.|
||1 instances in total. (in cvpr2017)|
|2145|Wang_Motionlets_Mid-level_3D_2013_CVPR_paper|Method  Accuracy (%)  Harris3D [20] + HOG/HOF [21] (from [30])  Cuboids [9] + HOF3D [17] (from [30])  Dense + HOF [21] (from [30] )  Hessian [32]+ ESURF [32] (from [30])  HMAX(C2) [15]  3D CNN [16] GRBM [29]  ISA (dense sampling) [22] ISA (norm thresholding) [22]  ActionBank [27] Motionlet (1000) Motionlet (3000)  91.8 90.0 88.0 81.4 91.7 90.2 90.0 91.4 93.9 98.01 92.1 93.3  Table 2.|
||1 instances in total. (in cvpr2013)|
|2146|cvpr18-Geometry-Aware Deep Network for Single-Image Novel View Synthesis|Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2147|cvpr18-Generate to Adapt  Aligning Domains Using Generative Adversarial Networks|The abundance of labeled data has resulted in remarkable improvements for tasks such as the Imagenet challenge: beginning with the CNN framework of AlexNet [12] and more recently ResNets [9] and its variants.|
||1 instances in total. (in cvpr2018)|
|2148|Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper|Spanhel, J., Sochor, J., Jur anek, R., Herout, A., Mars k, L., Zemc k, P.: Holistic recognition of low quality license plates by cnn using track annotated data.|
||1 instances in total. (in eccv2018)|
|2149|Andreas_Veit_Convolutional_Networks_with_ECCV_2018_paper|Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||1 instances in total. (in eccv2018)|
|2150|Bryan_Plummer_Conditional_Image-Text_Embedding_ECCV_2018_paper|Radenovi, F., Tolias, G., Chum, O.: Cnn image retrieval learns from bow: Unsu pervised fine-tuning with hard examples.|
||1 instances in total. (in eccv2018)|
|2151|cvpr18-Learning to Estimate 3D Human Pose and Shape From a Single Color Image|Monocular 3D human pose estimation in the wild using improved CNN supervision.|
||1 instances in total. (in cvpr2018)|
|2152|Noise-Blind Image Deblurring|[33] design a CNN to handle saturation and nonlinearities of the model.|
||1 instances in total. (in cvpr2017)|
|2153|Fabian_Caba_What_do_I_ECCV_2018_paper|We first extract framelevel features using a CNN and then aggregate these representations into a single feature vector oi.|
||1 instances in total. (in eccv2018)|
|2154|Zixin_Luo_Learning_Local_Descriptors_ECCV_2018_paper|Radenovic, F., Tolias, G., Chum, O.: CNN Image Retrieval Learns from BoW  Unsupervised Fine-Tuning with Hard Examples.|
||1 instances in total. (in eccv2018)|
|2155|Wang_Deep_Growing_Learning_ICCV_2017_paper|Among the comparing deep learning models, NN and CNN easily go into overfitting and yield steadily worse test error using the labeled set for training only.|
||1 instances in total. (in iccv2017)|
|2156|Hengshuang_Zhao_Compositing-aware_Image_Search_ECCV_2018_paper|[7] used off-the-shelf deep CNN features to capture local surrounding context particularly for person compositing.|
||1 instances in total. (in eccv2018)|
|2157|Yinda_Zhang_Active_Stereo_Net_ECCV_2018_paper|Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|2158|cvpr18-Attentional ShapeContextNet for Point Cloud Recognition|This type of conversion from 3D points to 3D volumetric data can facilitate the extension from 2D CNN to 3D CNN but it also loses the intrinsic geometric property of the point cloud.|
||1 instances in total. (in cvpr2018)|
|2159|Siqi_Yang_Using_LIP_to_ECCV_2018_paper|Qin, H., Yan, J., Li, X., Hu, X.: Joint training of cascaded cnn for face detection.|
||1 instances in total. (in eccv2018)|
|2160|Vondrick_Anticipating_Visual_Representations_CVPR_2016_paper|Exploiting image-trained cnn architectures for  unconstrained video classification.|
||1 instances in total. (in cvpr2016)|
|2161|Chen_Zhu_Fine-grained_Video_Categorization_ECCV_2018_paper|Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for fine-grained visual recognition.|
||1 instances in total. (in eccv2018)|
|2162|cvpr18-SO-Net  Self-Organizing Network for Point Cloud Analysis|Unsupervised cnn for In  single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2163|Lassner_A_Generative_Model_ICCV_2017_paper|In [6] an encoder-decoder CNN in voxel space is used for 3D shape completion, which requires 3D ground truth.|
||1 instances in total. (in iccv2017)|
|2164|Zhang_Interleaved_Group_Convolutions_ICCV_2017_paper|Deep roots: Improving CNN efficiency with hierarchical filter groups.|
||1 instances in total. (in iccv2017)|
|2165|cvpr18-Link and Code  Fast Indexing With Graphs and Compact Regression Codes|Particular object retrieval In ICLR,  with integral max-pooling of CNN activations.|
||1 instances in total. (in cvpr2018)|
|2166|cvpr18-DocUNet  Document Image Unwarping via a Stacked U-Net|[6] applied a CNN to detect paper creases for rectification.|
||1 instances in total. (in cvpr2018)|
|2167|cvpr18-Factoring Shape, Pose, and Layout From the 2D Image of a 3D Scene|Joint embeddings of shapes and images via cnn image purification.|
||1 instances in total. (in cvpr2018)|
|2168|cvpr18-SeGAN  Segmenting and Generating the Invisible|The segmentation part of the network is a CNN that takes a four-channel tensor as input, where three channels correspond to the RGB image, and there is a single channel for the segmentation mask of the visible region of an object.|
||1 instances in total. (in cvpr2018)|
|2169|Discovering Causal Signals in Images|Related work  The experiments described in this paper depend crucially on the properties of the features computed by the convolutional layers of a CNN [14].|
||1 instances in total. (in cvpr2017)|
|2170|Byeon_Scene_Labeling_With_2015_CVPR_paper|[13] improved CNNs by combining two CNN models which learn context information and visual features in separate networks.|
||1 instances in total. (in cvpr2015)|
|2171|cvpr18-Event-Based Vision Meets Deep Learning on Steering Prediction for Self-Driving Cars|More recently, NVIDIA used a CNN to learn a driving policy from video frames [15].|
||1 instances in total. (in cvpr2018)|
|2172|Dibeklioglu_Visual_Transformation_Aided_ICCV_2017_paper|Repulsed Metric [23] Local Large-Margin Multi-Metric [14] Similarity Metric Based CNN [17] Neighborhood Repulsed Correlation Metric [34] Ensemble Similarity [40] Scalable Similarity [41] Asymmetric Metric [18] Neighborhood Repulsed Metric [20]  80.5 78.7   72.7 66.3 78.6 77.6 78.4 64.3  82.3 80.6 80.0 79.3 78.7 75.7 74.8 80.9 75.7  art image-based methods in terms mean verification accuracy.|
||1 instances in total. (in iccv2017)|
|2173|Misra_Cross-Stitch_Networks_for_CVPR_2016_paper|Object detection via a multiregion & semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2016)|
|2174|Multi-View 3D Object Detection Network for Autonomous Driving|Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.|
||1 instances in total. (in cvpr2017)|
|2175|cvpr18-From Lifestyle Vlogs to Everyday Interactions|A multi-scale CNN for affordance  segmentation in RGB images.|
||1 instances in total. (in cvpr2018)|
|2176|cvpr18-Learning Facial Action Units From Web Images With Scalable Weakly Supervised Clustering|Temporal modeled as the Long ShortTerm Memory (LSTM) is aggregated to CNN architecture to construct fusion models (e.g., [8, 19]).|
||1 instances in total. (in cvpr2018)|
|2177|Dwibedi_Cut_Paste_and_ICCV_2017_paper|Particular object retrieval with integral max-pooling of cnn activations.|
||1 instances in total. (in iccv2017)|
|2178|cvpr18-Min-Entropy Latent Model for Weakly Supervised Object Detection|Experimental Settings  MELM was implemented based on the widely used VGG16 CNN model [35] pre-trained on the ILSVRC 2012 dataset [21].|
||1 instances in total. (in cvpr2018)|
|2179|Hossam_Isack_K-convexity_shape_priors_ECCV_2018_paper|1  Introduction  Regularization is common in computer vision problems/applications such as photo or video editing, biomedical image analysis, weakly-supervised training of semantic CNN segmentation, etc.|
||1 instances in total. (in eccv2018)|
|2180|cvpr18-Domain Generalization With Adversarial Feature Learning|[19] proposed a low-rank parameterized CNN model based on domain shift-robust deep learning methods.|
||1 instances in total. (in cvpr2018)|
|2181|Re-Ranking Person Re-Identification With k-Reciprocal Encoding|Multi-scale triplet cnn for person re-identification.|
||1 instances in total. (in cvpr2017)|
|2182|Fried Binary Embedding for High-Dimensional Visual Features|For example, with 2048 bits or more, FBE performs the best of embedding the 4096-dimensional CNN features, when compared with SP [23], KBE [28], BP [3], CBE [25] and ITQ [4].|
||1 instances in total. (in cvpr2017)|
|2183|Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper|Exploiting that learned features transition from general to specific within the network, they train the first layers of a CNN commonly for source and target domain, then train individual task-specific layers while minimizing the multiple kernel maximum mean discrepancies between these layers.|
||1 instances in total. (in iccv2017)|
|2184|cvpr18-Beyond Holistic Object Recognition  Enriching Image Understanding With Part States|For baseline 3, we adopt the conventional computer vision scheme: concatenate the CNN feature from  6961  Iteration #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 71.3 mIoU mAP 46.5  63.6 43.3  70.1 45.6  60.8 41.6  57.6 40.7  65.4 44.0  67.1 44.8  62.0 42.5  70.8 45.9  71.2 46.2  71.3 46.3  71.2 46.3  Table 2: Average part state mAP (second row) and segmentation accuracy (mean IoU) of part categories (first row) of as iterations are proceeding.|
||1 instances in total. (in cvpr2018)|
|2185|cvpr18-Avatar-Net  Multi-Scale Zero-Shot Style Transfer by Feature Decoration|[8, 9] discovered that multilevel feature statistics extracted from a trained CNN notably represent the characteristics of visual styles, which boosts the development of style transfer approaches, either by iterative optimizations [9, 16, 30, 20] or feed-forward networks [5, 13, 18, 20, 31, 28, 29, 25, 3].|
||1 instances in total. (in cvpr2018)|
|2186|Learning Spatial Regularization With Image-Level Supervisions for Multi-Label Image Classification|Because of their strong capability in learning discriminative features, deep CNN models pretrained on large datasets can be easily transferred to solve other tasks and boost their performance.|
||1 instances in total. (in cvpr2017)|
|2187|cvpr18-BlockDrop  Dynamic Inference Paths in Residual Networks|While a recurrent model (e.g., LSTM) could also serve as the policy network, we found a CNN to be more efficient with similar performance.|
||1 instances in total. (in cvpr2018)|
|2188|Yi_Zhou_Single-view_Hair_Reconstruction_ECCV_2018_paper|Jackson, A.S., Bulat, A., Argyriou, V., Tzimiropoulos, G.: Large pose 3d face reconstruction from a single image via direct volumetric CNN regression.|
||1 instances in total. (in eccv2018)|
|2189|cvpr18-A Generative Adversarial Approach for Zero-Shot Learning From Noisy Texts|MCZSL directly uses part annotations as strong supervision to extract CNN representation of each semantic part in the test phase.|
||1 instances in total. (in cvpr2018)|
|2190|UntrimmedNets for Weakly Supervised Action Recognition and Detection|In the experi 4327  ments, we try out two architectures: Two-Stream CNN [40] with deeper architecture [18] and Temporal Segment Network [50] with the same architecture.|
||1 instances in total. (in cvpr2017)|
|2191|cvpr18-Learning Strict Identity Mappings in Deep Residual Networks|In this paper, we will show that when a residual block produces zero responses using our proposed variant, then the weights in the CNN filters of the corresponding residual block will be pushed to zeros by the training loss function that consists of cross-entropy term and L2 norm of the weight parameters with momentum optimization [20].|
||1 instances in total. (in cvpr2018)|
|2192|cvpr18-PhaseNet for Video Frame Interpolation|The prediction function, F is a CNN with parameters .|
||1 instances in total. (in cvpr2018)|
|2193|Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization_|NetVLAD: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2017)|
|2194|Guanan_Wang_Semi-Supervised_Generative_Adversarial_ECCV_2018_paper|Traditional methods with CNN features achieve better performance, which shows that features learned from deep neural networks capture more semantic information.|
||1 instances in total. (in eccv2018)|
|2195|Mao_Learning_Like_a_ICCV_2015_paper|We remove the final SoftMax layer of the deep CNN and connect the top fully connected layer (a 4096 dimensional layer) to our model.|
||1 instances in total. (in iccv2015)|
|2196|Recurrent 3D Pose Sequence Machines|This may be due to that Human3.6m 2D pose data, compared with MPII dataset, is less challenging for CNN to learn a rich 2D pose presentation.|
||1 instances in total. (in cvpr2017)|
|2197|Lu_SafetyNet_Detecting_and_ICCV_2017_paper|It is clear that a CNN can encode bars and weighted sums of bars, and that for at least k  2  Figure 5: Simple example bar functions on the x, y plane, where black is 0 and white is 1.|
||1 instances in total. (in iccv2017)|
|2198|Jetley_End-To-End_Saliency_Mapping_CVPR_2016_paper|DeepGaze [23] employs an existing network architecture, the 5-layer deep AlexNet [21] trained for object classification on ImageNet, to demonstrate that off-theshelf CNN features can significantly outperform non-deep and shallower models, even if not trained explicitly on the task of saliency prediction.|
||1 instances in total. (in cvpr2016)|
|2199|XU_JUN_A_Trilateral_Weighted_ECCV_2018_paper|Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a Gaussian denoiser: Residual learning of deep cnn for image denoising.|
||1 instances in total. (in eccv2018)|
|2200|Deeply Aggregated Alternating Minimization for Image Restoration|The DJF [17] avoids the texture copying artifacts thanks to faithful CNN responses extracted from both color image and depth map (Fig.|
||1 instances in total. (in cvpr2017)|
|2201|Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper|The task is to perform pixel segmentation into two classes: cell membranes and inner parts of the neu 295  Method  Error, %  MIL over CNN [28] CNN with augmentation [12] TI-POOLING dropout TI-POOLING + dropout  8.9 8.1 7.4 7.0  Table 4.|
||1 instances in total. (in cvpr2016)|
|2202|Workman_Understanding_and_Mapping_ICCV_2017_paper|We represent our CNN as a function, G(I; g), where I is an image and the output is a probability distribution over the 10 scenicness levels.|
||1 instances in total. (in iccv2017)|
|2203|Dong_Semantic_Image_Synthesis_ICCV_2017_paper|Therefore, we further propose an alternative model that employs a much deeper pretrained CNN to perform the encoding function.|
||1 instances in total. (in iccv2017)|
|2204|cvpr18-Demo2Vec  Reasoning Object Affordances From Online Videos|A multi-scale cnn for affordance In European Conference on  segmentation in rgb images.|
||1 instances in total. (in cvpr2018)|
|2205|Olivia_Wiles_X2Face_A_network_ECCV_2018_paper|In the second training stage (II), we make use of a CNN pre-trained for face identification to add additional constraints based on the identity of the faces in the source and driving frames to finetune the model following training stage (I).|
||1 instances in total. (in eccv2018)|
|2206|cvpr18-Dual Skipping Networks|Starting with the notable victory of AlexNet [21], ImageNet [8] classification contest has boomed the exploration of deep CNN architectures.|
||1 instances in total. (in cvpr2018)|
|2207|Fitsum_Reda_SDC-Net_Video_prediction_ECCV_2018_paper|4 Conclusions  We present a 3D CNN and a novel spatially-displaced convolution (SDC) module that achieves state-of-the-art video frame prediction.|
||1 instances in total. (in eccv2018)|
|2208|Jyh-Jing_Hwang_Adaptive_Affinity_Field_ECCV_2018_paper|We develop new affinity field matching loss functions to learn a CNN that automatically outputs a segmentation respectful of spatial structures and small details.|
||1 instances in total. (in eccv2018)|
|2209|cvpr18-Multi-Label Zero-Shot Learning With Structured Knowledge Graphs|For comparison, we consider WSABIE [47], WARP [18], and logistic regression (all with the above CNN features) as baseline approaches.|
||1 instances in total. (in cvpr2018)|
|2210|cvpr18-Towards High Performance Video Object Detection|From Image to Video Object Detection  Object detection in static images has achieved significant progress in recent years using deep CNN [17].|
||1 instances in total. (in cvpr2018)|
|2211|cvpr18-Deep Marching Cubes  Learning Explicit Surface Representations|The result of the grid pooling operation is fed into a standard 3D encoder-decoder CNN for increasing the size of the receptive field.|
||1 instances in total. (in cvpr2018)|
|2212|cvpr18-MaskLab  Instance Segmentation by Refining Object Detection With Semantic and Direction Features|Object detection via a multiregion and semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2018)|
|2213|Yandong_Li_How_Local_is_ECCV_2018_paper|From each video frame, we extract 4,096D deep CNN features as the activation of the last fully connected layer of the VGG19 network [44] pretrained on ImageNet [45].|
||1 instances in total. (in eccv2018)|
|2214|cvpr18-Neural 3D Mesh Renderer|Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation.|
||1 instances in total. (in cvpr2018)|
|2215|Fu_Semi-Supervised_Vocabulary-Informed_Learning_CVPR_2016_paper|Different types of CNN and hand-crafted low-level feature are used by different methods.|
||1 instances in total. (in cvpr2016)|
|2216|cvpr18-Sliced Wasserstein Distance for Learning Gaussian Mixture Models|In addition, we trained a CNN classifier on the MNIST training data.|
||1 instances in total. (in cvpr2018)|
|2217|Avrithis_Web-Scale_Image_Clustering_ICCV_2015_paper|We thank Clayton Mellina and the Flickr Vision Team for their help with CNN features and the distributed k-means experiment.|
||1 instances in total. (in iccv2015)|
|2218|cvpr18-Generating Synthetic X-Ray Images of a Person From the Surface Geometry|Large pose 3d face reconstruction from a single image via direct volumetric CNN regression.|
||1 instances in total. (in cvpr2018)|
|2219|Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks|To capture complementary information between appearance and motion, a two-stream CNN architecture is developed for RGB based action recognition [39].|
||1 instances in total. (in cvpr2017)|
|2220|cvpr18-Data Distillation  Towards Omni-Supervised Learning|Object detection via a multiregion & semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2018)|
|2221|Aggregated Residual Transformations for Deep Neural Networks|Deep roots: Improving cnn efficiency with hierarchical filter groups.|
||1 instances in total. (in cvpr2017)|
|2222|Chuhui_Xue_Accurate_Scene_Text_ECCV_2018_paper|Jiang, Y., Zhu, X., Wang, X., Yang, S., Li, W., Wang, H., Fu, P., Luo, Z.: R2cnn: Rotational region cnn for orientation robust scene text detection.|
||1 instances in total. (in eccv2018)|
|2223|Alayrac_Unsupervised_Learning_From_CVPR_2016_paper|To capture the depicted objects in the video, we apply the VGG-verydeep-16 CNN [29] over each frame in a sliding window manner over multiple scales.|
||1 instances in total. (in cvpr2016)|
|2224|Liu_Referring_Expression_Generation_ICCV_2017_paper|Traditional approaches adopt CNN and CNN/LSTM to encode the image and texts/sentences in their feature space, then neural models like MLP embed them into a common space.|
||1 instances in total. (in iccv2017)|
|2225|cvpr18-Ring Loss  Convex Feature Normalization for Face Recognition|Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection.|
||1 instances in total. (in cvpr2018)|
|2226|Chu_Structured_Feature_Learning_CVPR_2016_paper|[27, 26] are based on CNN features as well.|
||1 instances in total. (in cvpr2016)|
|2227|CASENet_ Deep Category-Aware Semantic Edge Detection|Three CNN architectures designed in this paper are shown in (a)-(c).|
||1 instances in total. (in cvpr2017)|
|2228|Gkioxari_Finding_Action_Tubes_2015_CVPR_paper|[40] CNN (1/3 spatial, 2/3 motion) Action Tubes J-HMDB  62.5  56.5  56.6  Table 3: Classification accuracy on J-HMDB (averaged over the three splits).|
||1 instances in total. (in cvpr2015)|
|2229|Kendall_End-To-End_Learning_of_ICCV_2017_paper|Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue.|
||1 instances in total. (in iccv2017)|
|2230|cvpr18-Conditional Probability Models for Deep Image Compression|[9] instead by using an importance map to help the CNN attend to different regions of the image with different amounts of bits.|
||1 instances in total. (in cvpr2018)|
|2231|Jain_Structural-RNN_Deep_Learning_CVPR_2016_paper|[58] jointly train CNN and MRF for human pose estimation.|
||1 instances in total. (in cvpr2016)|
|2232|Bappy_Exploiting_Spatial_Structure_ICCV_2017_paper|Even though CNN has shown very promising performance in understanding visual concepts such as object detection and recognition, the detection of manipulated regions with CNNs may not be best strategy because well manipulated images usually do not leave any visual clue of alteration [50], and resemble genuine images.|
||1 instances in total. (in iccv2017)|
|2233|Maria_Klodt_Supervising_the_new_ECCV_2018_paper|Garg, R., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|2234|On Human Motion Prediction Using Recurrent Neural Networks|[53] have shown that a simple baseline that concatenates features from questions words and CNN image features performs comparably to approaches based on deep RNNs.|
||1 instances in total. (in cvpr2017)|
|2235|Konstantin_Shmelkov_How_good_is_ECCV_2018_paper|We verify if our findings depend on the type of classifier by using random forests [23, 43] instead of CNN for classification.|
||1 instances in total. (in eccv2018)|
|2236|cvpr18-Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal|[50] used a stacked CNN with separated steps, including first generating the image level shadow-prior and training a patch-based CNN which produces shadow masks for local patches.|
||1 instances in total. (in cvpr2018)|
|2237|End-To-End Learning of Driving Models From Large-Scale Video Datasets|In the 1-Frame configuration, we only feed in a single image at each timestep and use a CNN as the visual encoder.|
||1 instances in total. (in cvpr2017)|
|2238|Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet|Learning FRAME models  using cnn filters.|
||1 instances in total. (in cvpr2017)|
|2239|cvpr18-Eliminating Background-Bias for Robust Person Re-Identification|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in cvpr2018)|
|2240|Chen_DCAN_Deep_Contour-Aware_CVPR_2016_paper|Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning.|
||1 instances in total. (in cvpr2016)|
|2241|Fine-Grained Recognition of Thousands of Object Categories With Single-Example Training|We design a CNN that admits the detected object bounding boxes together with their associated scored short-lists of possible classifications produced by Phase 1, and produces improved classifications for these bounding boxes.|
||1 instances in total. (in cvpr2017)|
|2242|Shiyao_Wang_Fully_Motion-Aware_Network_ECCV_2018_paper|[7] is a typical proposal based CNN detector by using Selective Search [28] to extract proposals.|
||1 instances in total. (in eccv2018)|
|2243|cvpr18-Occlusion-Aware Rolling Shutter Rectification of 3D Scenes|Unrolling the shutter: Cnn to correct motion distortions.|
||1 instances in total. (in cvpr2018)|
|2244|Guangyu_Robert_Yang_A_dataset_and_ECCV_2018_paper|The baseline network still contains a CNN for visual processing, a LSTM network for semantic processing, and a GRU network as the controller.|
||1 instances in total. (in eccv2018)|
|2245|cvpr18-PieAPP  Perceptual Image-Error Assessment Through Pairwise Preference|Image quality assessment by comparing CNN features between images.|
||1 instances in total. (in cvpr2018)|
|2246|Wu_Robust_Video_Segment_2015_CVPR_paper|The CNN features perform better on Airplane because videos in the airplane category include airplanes with different colors and shapes; CNN features generalize better across different shapes.|
||1 instances in total. (in cvpr2015)|
|2247|Damien_Teney_Visual_Question_Answering_ECCV_2018_paper|The image embedding uses features from a CNN (Convolutional Neural Network) with bottom-up attention [3] and question-guided attention over those features.|
||1 instances in total. (in eccv2018)|
|2248|Procedural Generation of Videos to Train Deep Action Recognition Networks|These partial predictions are then condensed into a video-level decision using a segmental consensus function G. We use the same parameters as [75]: a number of segments K = 3, and the consensus function: G = 1 k=1 F(Tk; W ), where F(Tk; W ) is a function representing a CNN architecture with weight parameters W operating on short snippet Tk from video segment k.  K PK  4.2.|
||1 instances in total. (in cvpr2017)|
|2249|Yongqiang_Zhang_SOD-MTGAN_Small_Object_ECCV_2018_paper|As shown in Table 1 and Figure 2, we adopt a deep CNN architecture which has shown effectiveness for image de-blurring in [13] and face detection in [1].|
||1 instances in total. (in eccv2018)|
|2250|Matrix Tri-Factorization With Manifold Regularizations for Zero-Shot Learning|Datasets  Images  Attributes  AwA CUB aPY SUN  24,295 / 6,180 8,855 / 2,933 12,695 / 2,644 14,140 / 200  85 312 64 102  Classes 40 / 10 150 / 50 20 / 12 707 / 10  for all the datasets, we utilize the deep features extracted from popular CNN architecture.|
||1 instances in total. (in cvpr2017)|
|2251|Panna_Felsen_Where_Will_They_ECCV_2018_paper|[19] proposed using a CNN to personalize pose estimation to a persons appearance over time.|
||1 instances in total. (in eccv2018)|
|2252|Feedback Networks|Bilinear cnn models for fine-grained visual recognition.|
||1 instances in total. (in cvpr2017)|
|2253|cvpr18-Unsupervised Training for 3D Morphable Model Regression|Large pose 3d face reconstruction from a single image via direct volumetric cnn regression.|
||1 instances in total. (in cvpr2018)|
|2254|cvpr18-Multimodal Visual Concept Learning With Weakly Supervised Techniques|Contrary to this, we use deep features extracted by the last fully connected layer of the VGG-face pre-trained CNN [33], while a single kernel is computed on each pair of face tracks regardless to the faces poses.|
||1 instances in total. (in cvpr2018)|
|2255|cvpr18-ISTA-Net  Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing|In particular and inspired by the powerful representation power of CNN [9] and its universal approximation property [14], we propose to design F() as a combination of two linear convolutional operators (without bias terms) separated by a rectified linear unit (ReLU).|
||1 instances in total. (in cvpr2018)|
|2256|cvpr18-OATM  Occlusion Aware Template Matching by Consensus Set Maximization|Increasing cnn robustness to occlusions by reducing filter support.|
||1 instances in total. (in cvpr2018)|
|2257|cvpr18-Look at Boundary  A Boundary-Aware Face Alignment Algorithm|CALE [4] is a two-stage convolutional aggregation model to aggregate score maps predicted by detection stage along with early CNN features for final heatmap regression.|
||1 instances in total. (in cvpr2018)|
|2258|cvpr18-Learning by Asking Questions|This is the same as our default answering module v. FiLM [40] uses question features from a GRU [10] to modulate the image features in each CNN layer.|
||1 instances in total. (in cvpr2018)|
|2259|Yasutaka_Inagaki_Learning_to_Capture_ECCV_2018_paper|(2013) 27862793  16  Y. Inagaki, Y. Kobayashi, K. Takahashi, T. Fujii and H. Nagahara  18. andJun Yan Zhu, T.C.W., Hiroaki, E., Chandraker, M., Efros, A., Ramamoorthi, In:  R.: A 4d light-field dataset and cnn architectures for material recognition.|
||1 instances in total. (in eccv2018)|
|2260|Dong_Su_Is_Robustness_the_ECCV_2018_paper|This result also echoes with [49], where the authors use a larger model to increase the l robustness of a CNN based MNIST model.|
||1 instances in total. (in eccv2018)|
|2261|cvpr18-PlaneNet  Piece-Wise Planar Reconstruction From a Single RGB Image|With the surge of deep neural networks, numerous CNN based approaches have been proposed [8, 23, 27].|
||1 instances in total. (in cvpr2018)|
|2262|cvpr18-Feature Super-Resolution  Make Machine See More Clearly|To achieve this purpose, we design a simple deep CNN network.|
||1 instances in total. (in cvpr2018)|
|2263|Galteri_Deep_Generative_Adversarial_ICCV_2017_paper|Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising.|
||1 instances in total. (in iccv2017)|
|2264|Singh_A_Multi-Stream_Bi-Directional_CVPR_2016_paper|To model long-term temporal dynamics within and between actions, the multi-stream CNN is followed by a bi-directional Long Short-Term Memory (LSTM) layer.|
||1 instances in total. (in cvpr2016)|
|2265|Knowledge Acquisition for Visual Question Answering via Iterative Querying|Among these models, one of the most popular choices is to use CNN to encode images and LSTM to encode words [3, 26, 31].|
||1 instances in total. (in cvpr2017)|
|2266|cvpr18-Revisiting Dilated Convolution  A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation|Hcp: A flexible cnn framework for multi-label image classification.|
||1 instances in total. (in cvpr2018)|
|2267|Siyuan_Huang_Monocular_Scene_Parsing_ECCV_2018_paper|4.1 Room Geometry Estimation  Although recent approaches [3335] are capable of generating a relatively robust prediction of the 2D room layout using CNN features, 3D room layout estimations are still inaccurate due to its sensitivity to camera parameter estimation in clusttered scenes.|
||1 instances in total. (in eccv2018)|
|2268|Zhu_Visual7W_Grounded_Question_CVPR_2016_paper|We extract the activations from the last fully connected layer (fc7) of a pre-trained CNN model VGG-16 [39].|
||1 instances in total. (in cvpr2016)|
|2269|Luo_A_Revisit_of_ICCV_2017_paper|To better understand the differences between our dataset and existing anomaly detection datasets, we briefly summarize all anomaly detection datasets as follows:  Similar to the work in Conv-AE [11], we also find that the motion feature, such as optical flow or CNN feature extracted from optical flow [12] does not help the anomaly prediction.|
||1 instances in total. (in iccv2017)|
|2270|cvpr18-Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation|Moreover, the responses at the final CNN layer have been combined with a fully connected Conditional Random Fields (CRF), that has been applied at the post-processing stage.|
||1 instances in total. (in cvpr2018)|
|2271|Pathak_Context_Encoders_Feature_CVPR_2016_paper|We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks.|
||1 instances in total. (in cvpr2016)|
|2272|Mahasseni_Regularizing_Long_Short_CVPR_2016_paper|Exploiting image-trained cnn architectures for unconstrained video classification.|
||1 instances in total. (in cvpr2016)|
|2273|Oh_Fast_Randomized_Singular_2015_CVPR_paper|Let X  Cmn, m  n. There exists a matrix W  Cmn and a unique Hermitian positive semi-definite matrix P  Cnn such  that  X = WP, WW = I,  where I is the identity matrix.|
||1 instances in total. (in cvpr2015)|
|2274|cvpr18-Video Captioning via Hierarchical Reinforcement Learning| be the parameters of the whole model and a T be the  2, ..., a  1, a  Require: Training pairs <video, GT caption> 1: Randomly initialize the model parameters  2: Load the pretrained CNN model and internal critic 3: for iteration=1,M do 4:  Randomly sample a minibatch if Train-Worker then  5:  6:  7:  8:  9:  10:  11:  12:  13:  14:  15:  16:  Disable the goal exploration Run a forward pass to get the sampled caption  a1a2...aT  Calculate R(at) for each at Freeze the manager Update the worker policy using Equation 14  else if Train-Manager then  Initialize a random process N for goal explo ration  Run a forward pass to get the greedily decoded  caption e1e2...en  Calculate R(et) for each et Freeze the worker Update the manager policy using Equation 22  end if  17: 18: end for  ground-truth word sequence, then the cross-entropy loss is defined as  L() =   T  X  t=1  log((a  t ; a  1, ..., a  t1))  (26)  4.|
||1 instances in total. (in cvpr2018)|
|2275|Gidaris_LocNet_Improving_Localization_CVPR_2016_paper|Object detection via a multiregion & semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2016)|
|2276|Varas_Region-based_Particle_Filter_2014_CVPR_paper|The score associated with X is computed as:  tr(X T QX) =  xT l Qxl  (16)  l=1  where Q  Cnn is a matrix that measures affinities between regions.|
||1 instances in total. (in cvpr2014)|
|2277|cvpr18-Monocular Relative Depth Perception With Web Stereo Data Supervision|2  [29] R. Garg and I. Reid, Unsupervised cnn for single view depth estimation: Geometry to the rescue, in Proc.|
||1 instances in total. (in cvpr2018)|
|2278|Split-Brain Autoencoders_ Unsupervised Learning by Cross-Channel Prediction|If F is a CNN of a desired fixed size, e.g., AlexNet [25], we can design the subnetworks F1, F2 by splitting each layer of the network F in half, along the channel dimension.|
||1 instances in total. (in cvpr2017)|
|2279|Christos_Sakaridis_Semantic_Scene_Understanding_ECCV_2018_paper|The optimization of (6) is implemented by mixing images from D u in a proportion of 1 : w and feeding the stream of hybrid data to a CNN for standard supervised training.|
||1 instances in total. (in eccv2018)|
|2280|cvpr18-What Makes a Video a Video  Analyzing Temporal Information in Video Understanding Models and Datasets|Exploiting image-trained cnn architectures for unconstrained video classification.|
||1 instances in total. (in cvpr2018)|
|2281|cvpr18-CondenseNet  An Efficient DenseNet Using Learned Group Convolutions|In Table 1, we show the results of experiments comparing a 160-layer CondenseNetlight and a 182-layer CondenseNet with alternative state-of-the-art CNN architectures.|
||1 instances in total. (in cvpr2018)|
|2282|Paul_Hongsuck_Seo_Attentive_Semantic_Alignment_ECCV_2018_paper|Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn architec ture for weakly supervised place recognition.|
||1 instances in total. (in eccv2018)|
|2283|Generating Holistic 3D Scene Abstractions for Text-Based Image Retrieval|We evaluate the occurrence baseline (H), 2D relation baseline (2D), CNN baseline, the proposed hard version, proposed soft versions, and a combination between our soft version and the 2D baseline.|
||1 instances in total. (in cvpr2017)|
|2284|Fast-At_ Fast Automatic Thumbnail Generation Using Deep Neural Networks|A CNN designed to generate thumbnails in real time was trained using this set.|
||1 instances in total. (in cvpr2017)|
|2285|Liangyan_Gui_Few-Shot_Human_Motion_ECCV_2018_paper|Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: CNN features off-the-shelf: An astounding baseline for recognition.|
||1 instances in total. (in eccv2018)|
|2286|cvpr18-Image-Image Domain Adaptation With Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification|We use mini-batch SGD to train CNN models on a Tesla K80 GPU.|
||1 instances in total. (in cvpr2018)|
|2287|cvpr18-Recurrent Saliency Transformation Network  Incorporating Multi-Stage Visual Cues for Small Organ Segmentation|Efficient MultiScale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation.|
||1 instances in total. (in cvpr2018)|
|2288|Joel_Janai_Unsupervised_Learning_of_ECCV_2018_paper|: Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|2289|Pierre_Stock_ConvNets_and_ImageNet_ECCV_2018_paper|1: Top: Performance evolution of various CNN architectures on Imagenet.|
||1 instances in total. (in eccv2018)|
|2290|Yufei_Wang_ConceptMask_Large-Scale_Segmentation_ECCV_2018_paper|As shown in Figure 2 stage 1, each image I is passed through a CNN feature extractor.|
||1 instances in total. (in eccv2018)|
|2291|Eddy_Ilg_Uncertainty_Estimates_and_ECCV_2018_paper|[28] presented a depth estimation CNN that internally uses a predictor for the deviation of the estimated optical flow from the ground-truth.|
||1 instances in total. (in eccv2018)|
|2292|cvpr18-MegaDepth  Learning Single-View Depth Prediction From Internet Photos|Unsupervised CNN for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2293|cvpr18-Group Consistent Similarity Learning via Deep CRF for Person Re-Identification|Person re-identification by multi-channel parts-based cnn with improved triplet loss function.|
||1 instances in total. (in cvpr2018)|
|2294|cvpr18-Pseudo Mask Augmented Object Detection|Object detection via a multiregion & semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2018)|
|2295|Wang_Actions__Transformations_CVPR_2016_paper|A discriminative cnn video representation for event detection.|
||1 instances in total. (in cvpr2016)|
|2296|cvpr18-Local Descriptors Optimized for Average Precision|We use the CNN architecture recently proposed in L2Net [32], which consists of seven convolution layers, and is regularized with Batch Normalization and Dropout.|
||1 instances in total. (in cvpr2018)|
|2297|Multi-Context Attention for Human Pose Estimation|Object detection via a multiIn  region and semantic segmentation-aware cnn model.|
||1 instances in total. (in cvpr2017)|
|2298|Xu_Tang_PyramidBox_A_Context-assisted_ECCV_2018_paper|Zhu, C., Zheng, Y., Luu, K., Savvides, M.: Cms-rcnn: contextual multi-scale regionarXiv preprint arXiv::1606.05413  based cnn for unconstrained face detection.|
||1 instances in total. (in eccv2018)|
|2299|Zhiding_Yu_SEAL_A_Framework_ECCV_2018_paper|Deep active contour [40] uses learned CNN features to steer contour evolution given the input of an initialized contour.|
||1 instances in total. (in eccv2018)|
|2300|cvpr18-Distort-and-Recover  Color Enhancement Using Deep Reinforcement Learning|Given an image I(t) at a sequential adjustment step t, we extract a contextual feature Fcontext with a pre-trained CNN and a color feature Fcolor.|
||1 instances in total. (in cvpr2018)|
|2301|cvpr18-Real-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer|Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2302|Aashish_Sharma_Into_the_Twilight_ECCV_2018_paper|Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||1 instances in total. (in eccv2018)|
|2303|Zhang_Mining_And-Or_Graphs_ICCV_2015_paper|Just like in [27], we can extract CNN features for local parts as unary ARG attributes.|
||1 instances in total. (in iccv2015)|
|2304|Gang_Zhang_Generative_Adversarial_Network_ECCV_2018_paper|The two classiiers  SaGAN  5  are both designed as a CNN with softmax function, denoted as Dsrc and Dcls respectively.|
||1 instances in total. (in eccv2018)|
|2305|SouYoung_Jin_Unsupervised_Hard-Negative_Mining_ECCV_2018_paper|Sonntag, D., Barz, M., Zacharias, J., Stauden, S., Rahmani, V., F othi,  A., L orincz, A.: Fine-tuning deep cnn models on specific ms coco categories.|
||1 instances in total. (in eccv2018)|
|2306|cvpr18-Fast End-to-End Trainable Guided Filter|The Role of Guided Filtering Layer To verify the effectiveness of the proposed guided filtering layer, we replace it with DJF [30], which is a complex CNN designed for joint image upsampling and achieves the state-of-the-art performance among many filters.|
||1 instances in total. (in cvpr2018)|
|2307|Liu_Infant_Footprint_Recognition_ICCV_2017_paper|The CNN model is first trained on millions of hard samples of minutiae pairs selected from NIST SD14 [18] by a state of the art fingerprint matcher [4], then fine-tuned on a small set of infant footprint minutiae pairs.|
||1 instances in total. (in iccv2017)|
|2308|Fast Video Classification via Adaptive Cascading of Deep Models|We investigated this and found that the specialized compact CNN repeatedly made mistakes on one person in the video, which led to a high cascade rate.|
||1 instances in total. (in cvpr2017)|
|2309|cvpr18-Visual Question Reasoning on General Dependency Tree|The size of images in Sort-of-CLEVR is 7575, we used a four-layer CNN and each layer has a 3  3 kernel and 24channel outputs to extract the image features.|
||1 instances in total. (in cvpr2018)|
|2310|cvpr18-Recognize Actions by Disentangling Components of Dynamics|Image based CNN models are firstly used to roughly filter the negative samples.|
||1 instances in total. (in cvpr2018)|
|2311|cvpr18-Dimensionality's Blessing  Clustering Images by Underlying Distribution|Netvlad: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2018)|
|2312|Cheng_Wang_Mancs_A_Multi-task_ECCV_2018_paper|Zheng, Z., Zheng, L., Yang, Y.: A discriminatively learned cnn embedding for  person reidentification.|
||1 instances in total. (in eccv2018)|
|2313|cvpr18-Sketch-a-Classifier  Sketch-Based Photo Classifier Generation|Figure 5: CNN photo classification decision process visualized by GradCAM [21].|
||1 instances in total. (in cvpr2018)|
|2314|cvpr18-PointNetVLAD  Deep Point Cloud Based Retrieval for Large-Scale Place Recognition|NetVLAD: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2018)|
|2315|cvpr18-Fast Spectral Ranking for Similarity Search|Particular object retrieval with  integral max-pooling of cnn activations.|
||1 instances in total. (in cvpr2018)|
|2316|Unsupervised Pixel-Level Domain Adaptation With Generative Adversarial Networks|3 For all our experiments, the CNN topologies used for the task classifier T are identical to the ones used in [14, 5] to be comparable to previous work in unsupervised domain adaptation.|
||1 instances in total. (in cvpr2017)|
|2317|cvpr18-Learning Descriptor Networks for 3D Shape Synthesis and Analysis|Learning FRAME models using CNN filters.|
||1 instances in total. (in cvpr2018)|
|2318|cvpr18-Customized Image Narrative Generation via Interactive Visual Question Generation and Answering|[14] generated region-level descriptions by implementing alignment model of region-level CNN and bidirectional recurrent neural network (RNN).|
||1 instances in total. (in cvpr2018)|
|2319|Morerio_Curriculum_Dropout_ICCV_2017_paper|In particular, we used two different CNN architectures: LeNet  4Code  at curriculum-dropout.|
||1 instances in total. (in iccv2017)|
|2320|cvpr18-Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics|Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2321|Hoffman_Learning_With_Side_CVPR_2016_paper|Deep CNN ensemble with data augmentation for object detection.|
||1 instances in total. (in cvpr2016)|
|2322|Han_Automatic_Spatially-Aware_Fashion_ICCV_2017_paper|Spatially(cid:173)aware Concept Discovery  The training process of a joint visual-semantic embedding will lead to a discriminative CNN model, which contains not only the semantic information (i.e., the last embedding layer) but also important spatial information that is hidden in the network.|
||1 instances in total. (in iccv2017)|
|2323|cvpr18-Enhancing the Spatial Resolution of Stereo Images Using a Parallax Prior|[39] found that deep CNN cannot handle relatively large disparities even with an enlarged receptive field.|
||1 instances in total. (in cvpr2018)|
|2324|Zhao_Single_Image_Action_ICCV_2017_paper|VGG16&19 [22] combines a 16-layer CNN and a 19-layer CNN, and train SVMs on fc7 features.|
||1 instances in total. (in iccv2017)|
|2325|cvpr18-Global Versus Localized Generative Adversarial Nets|In experiments, the local generator network G(x, z) was constructed by first using a CNN to map the input image x to a feature vector added with a noise vector of the same dimension.|
||1 instances in total. (in cvpr2018)|
|2326|Spatio-Temporal Naive-Bayes Nearest-Neighbor (ST-NBNN) for Skeleton-Based Action Recognition|Recently, the combination of NBNN and CNN [10], as well as the effort to speeding up NN search [9], revive the possibility of NBNNs return in computer vision.|
||1 instances in total. (in cvpr2017)|
|2327|Learning Deep Match Kernels for Image-Set Classification|We choose the last fully connected layer of the CNN [57, 48] as the descriptor for each frame and reduce the dimensionality of the CNN features from 1183 to 400 by PCA.|
||1 instances in total. (in cvpr2017)|
|2328|cvpr18-Universal Denoising Networks   A Novel CNN Architecture for Image Denoising|Universal Denoising Networks : A Novel CNN Architecture for Image Denoising  Stamatios Lefkimmiatis  Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russia  s.lefkimmiatis@skoltech.ru  Abstract  We design a novel network architecture for learning discriminative image models that are employed to efficiently tackle the problem of grayscale and color image denoising.|
||1 instances in total. (in cvpr2018)|
|2329|Discriminative Covariance Oriented Representation Learning for Face Recognition With Image Sets|More precisely, since () is a CNN which is parameterized by , we seek to find a value of  to meet such optimization objective.|
||1 instances in total. (in cvpr2017)|
|2330|Xin_Yu_Face_Super-resolution_Guided_ECCV_2018_paper|Our CNN has two branches: one for super-resolving face images and the other branch for predicting salient regions of a face coined facial component heatmaps.|
||1 instances in total. (in eccv2018)|
|2331|He_Exemplar-Driven_Top-Down_Saliency_CVPR_2016_paper|In [28], large-scale knowledge in ImageNet is transferred to locate objects, while in [29], a weakly-supervised CNN is used to predict object locations.|
||1 instances in total. (in cvpr2016)|
|2332|Xin_Yuan_Towards_Optimal_Deep_ECCV_2018_paper|Note that the deep hashing methods sustainably outperform the conventional hash learning methods on both datasets by a large margin even though the conventional ones utilize the CNN features, which suggests the end-to-end learning scheme is advantageous.|
||1 instances in total. (in eccv2018)|
|2333|cvpr18-Super-FAN  Integrated Facial Landmark Localization and Super-Resolution of Real-World Low Resolution Faces in Arbitrary Poses With GANs|Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution.|
||1 instances in total. (in cvpr2018)|
|2334|Liren_Chen_The_Devil_of_ECCV_2018_paper|5 Conclusion  Beyond existing efforts of developing sophisticated losses and CNN architectures, our study has investigated the problem of face recognition from the data perspective.|
||1 instances in total. (in eccv2018)|
|2335|Law_Quadruplet-Wise_Image_Similarity_2013_ICCV_paper|The performance gain is particularly noticeable compared to the Euclidean distance, especially in New York Times (+14.3%) and CNN (+11.6%).|
||1 instances in total. (in iccv2013)|
|2336|cvpr18-Real-World Anomaly Detection in Surveillance Videos|NetVLAD: CNN architecture for weakly supervised place recognition.|
||1 instances in total. (in cvpr2018)|
|2337|cvpr18-Image to Image Translation for Domain Adaptation|Training a CNN based on such synthetic data and applying it to real-world images (i.e.|
||1 instances in total. (in cvpr2018)|
|2338|cvpr18-A Prior-Less Method for Multi-Face Tracking in Unconstrained Videos|The datasets that are used to train these CNN models are generally chosen from good conditions, e.g., high image resolution, frontal faces, rectified faces, and full faces.|
||1 instances in total. (in cvpr2018)|
|2339|cvpr18-Geometric Robustness of Deep Networks  Analysis and Improvement|The experiment was done using a baseline CNN with 2 convolutional layers.|
||1 instances in total. (in cvpr2018)|
|2340|Ham_Proposal_Flow_CVPR_2016_paper|The CNN features in our comparison come from AlexNet [30] trained for ImageNet classification.|
||1 instances in total. (in cvpr2016)|
|2341|Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper|Recent advances in computer graphics make it possible to train CNN models on photo-realistic synthetic data with computer-generated annotations.|
||1 instances in total. (in iccv2017)|
|2342|Liu_Multi-Scale_Patch_Aggregation_CVPR_2016_paper|Object detection via a multiregion & semantic segmentation-aware CNN model.|
||1 instances in total. (in cvpr2016)|
|2343|Yuan_Temporal_Dynamic_Graph_ICCV_2017_paper|The network is trained on the Charades training set by using fine-tuning on all layers, including those of the pre-trained base CNN model.|
||1 instances in total. (in iccv2017)|
|2344|Nimisha_Blur-Invariant_Deep_Learning_ICCV_2017_paper|The most relevant work to handle space-variant blur is a method based on CNN for patch-level classification of the blur type [28], which focuses on estimating the blur kernel at all locations from a single observation.|
||1 instances in total. (in iccv2017)|
|2345|Deep Matching Prior Network_ Toward Tighter Multi-Oriented Text Detection|Conclusion and future work  In this paper, we have proposed an CNN based method, named Deep Matching Prior Network (DMPNet), that can effectively reduce the background interference.|
||1 instances in total. (in cvpr2017)|
|2346|cvpr18-Tangent Convolutions for Dense Prediction in 3D|SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation.|
||1 instances in total. (in cvpr2018)|
|2347|Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper|[34] Synthesized Queries [9] Strokelets [41] *PhotoOCR [5] (in house training data) Deep CNN [13] *Deep CNN [12] (synthetic training data) [SIFT] + FV + Atts [4] [Prop.|
||1 instances in total. (in cvpr2015)|
|2348|Chenyang_Si_Skeleton-Based_Action_Recognition_ECCV_2018_paper|The comparison results on NTU RGB+D dataset with Cross-Subject and Cross-View settings in accuracy (%)  Skeleton-Based Action Recognition  11  Methods  Cross-Subject  Cross-View  HBRNN-L [4] (2015) Part-aware LSTM [24] (2016) Trust Gate ST-LSTM [18] (2016) Two-stream RNN [28] (2017) STA-LSTM [26] (2017) Ensemble TS-LSTM [16] (2017) Visualization CNN [19] (2017) VA-LSTM [32] (2017) ST-GCN [31] (2018) SR-TSL (Ours)  59.1 62.9 69.2 71.3 73.4 74.6 76.0 79.4 81.5 84.8  64.0 70.3 77.7 79.5 81.2 81.3 82.6 87.6 88.3 92.4  5.2 Experimental Results  We compare the performance of our proposed model against several state-ofthe-art approaches on the NTU dataset and SYSU dataset in Table 1 and Table 2.|
||1 instances in total. (in eccv2018)|
|2349|cvpr18-Geometry-Aware Network for Non-Rigid Shape Prediction From a Single View|Unsupervised CNN for In  single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in cvpr2018)|
|2350|Liang_Human_Parsing_With_ICCV_2015_paper|Besides the above mentioned limitations, there are still two technical hurdles in the application of existing CNN architectures to pixel-wise prediction for the human parsing task.|
||1 instances in total. (in iccv2015)|
|2351|Rozantsev_Flying_Objects_Detection_2015_CVPR_paper|The DPM and CNN methods perform the worst on average.|
||1 instances in total. (in cvpr2015)|
|2352|Tian_Feng_Urban_Zoning_Using_ECCV_2018_paper|Inspired by the relevance of the information conveyed by deep networks for scene recognition, we choose the Places-CNN, a CNN trained on Places Database [43].|
||1 instances in total. (in eccv2018)|
|2353|Zorah_Laehner_DeepWrinkles_Accurate_and_ECCV_2018_paper|In DeepGarment [13] the global shape and low frequency details are reconstructed from a single segmented image using a CNN but no retargeting is possible.|
||1 instances in total. (in eccv2018)|
|2354|Zhou_Multi-Image_Matching_via_ICCV_2015_paper|More specifically, we use the publicly available deep learning toolbox Caffe [17] and the pre-trained CNN Alexnet [19].|
||1 instances in total. (in iccv2015)|
|2355|Li_Is_Second-Order_Information_ICCV_2017_paper|Higher-order pooling of CNN features via kernel linearization for action recognition.|
||1 instances in total. (in iccv2017)|
|2356|Johnson_Inferring_and_Executing_ICCV_2017_paper|CNN+LSTM+SA [45]: Questions and images are encoded using a CNN and LSTM as above, then combined using two rounds of soft spatial attention; a linear transform of the attention output predicts the answer.|
||1 instances in total. (in iccv2017)|
|2357|Li_Towards_End-To-End_Text_ICCV_2017_paper|Firstly, the input image is fed into a CNN that is modified from the VGG-16 net [25].|
||1 instances in total. (in iccv2017)|
|2358|Pramod_Do_Computational_Models_CVPR_2016_paper|The best individual model was still the CNN model, which explained 62.6% of the explainable variance (r = 0.72, p < 0.00005; Figure 3B).|
||1 instances in total. (in cvpr2016)|
|2359|cvpr18-SSNet  Scale Selection Network for Online 3D Action Prediction|An empirical study of  language cnn for image captioning.|
||1 instances in total. (in cvpr2018)|
|2360|Generating Descriptions With Grounded and Co-Referenced People|Several works also produce grounding while generating the description: [53] propose an attention mechanism to ground each word to spatial CNN image features, [55] extend this to bounding boxes, [54] to video frames, and [59] to spatial-temporal proposals.|
||1 instances in total. (in cvpr2017)|
|2361|Xiaofeng_Han_Single_Image_Water_ECCV_2018_paper|Recently, CNN based semantic segmentation methods have demonstrated a superior performance.|
||1 instances in total. (in eccv2018)|
|2362|Michael_Moeller_Lifting_Layers_Analysis_ECCV_2018_paper|Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.|
||1 instances in total. (in eccv2018)|
|2363|Zhu_Structured_Attentions_for_ICCV_2017_paper|We take the question feature q  RnQ from the last time step of a GRU such as [20], and the image feature map X = [x1, ..., xM ]  RnI M from one of the convolution layers of a CNN such as [10].|
||1 instances in total. (in iccv2017)|
|2364|EAST_ An Efficient and Accurate Scene Text Detector|[12] employed both a CNN and an ACF to hunt word candidates and further refined them using regression.|
||1 instances in total. (in cvpr2017)|
|2365|Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper|For k  (w, h, d), pk is encoded as:  pk = (k  ak)/ak  pk = log(k/ak)  (5)  (6)  8  M. Liang, B. Yang, S. Wang and R. Urtasun  Method  Input  Time (s)  3D AP (%)  BEV AP (%)  LIDAR LIDAR LIDAR LIDAR  MV3D [6] VxNet [39] NVLidarNet PIXOR [37]  easy moderate hard easy moderate hard 68.94 66.77 77.39 77.49 74.31 n/a n/a 76.01 F-PC CNN [8] LIDAR+Img 70.17 60.06 LIDAR+Img 68.49 71.09 AVOD-FPN [18] LIDAR+Img 77.90 81.94 F-PointNet [26] LIDAR+Img 75.33 81.20 LIDAR+Img 73.59 77.73 Our Cont Fuse LIDAR+Img 77.33 82.54 Table 1: Evaluation on KITTI 3D and Birds-Eye-View (BEV) Object Detection Benchmark (Car).|
||1 instances in total. (in eccv2018)|
|2366|Miech_Learning_From_Video_ICCV_2017_paper|[28] which is a strong baseline using the same CNN face descriptors as in our method.|
||1 instances in total. (in iccv2017)|
|2367|cvpr18-Discriminability Objective for Training Descriptive Captions|In our case, the image embedding network  is a pre-trained CNN and the parameters are fixed during training.|
||1 instances in total. (in cvpr2018)|
|2368|cvpr18-4D Human Body Correspondences From Panoramic Depth Maps|Starting from the second column, we first show the results [14] among the state-of-the-art non-rigid surface alignment methods [32, 3, 10, 26], previous CNN approach [50], our technique before refinement, and ours after refinements respectively; the second row shows the corresponding error maps using various techniques.|
||1 instances in total. (in cvpr2018)|
|2369|Xu_Multi-Channel_Weighted_Nuclear_ICCV_2017_paper|Beyond a  gaussian denoiser: Residual learning of deep cnn for image de noising.|
||1 instances in total. (in iccv2017)|
|2370|Inverse Compositional Spatial Transformer Networks|We compare IC-STN to several network architectures, including a baseline CNN with no spatial transformations, the original STN from Jaderberg et al., and c-STNs.|
||1 instances in total. (in cvpr2017)|
|2371|cvpr18-Towards Dense Object Tracking in a 2D Honeybee Hive|We implemented the CNN using Caffe2.|
||1 instances in total. (in cvpr2018)|
|2372|PolyNet_ A Pursuit of Structural Diversity in Very Deep Networks|Systematic evaluation of cnn advances on the imagenet.|
||1 instances in total. (in cvpr2017)|
|2373|Manen_PathTrack_Fast_Trajectory_ICCV_2017_paper|Learning In  by tracking: Siamese cnn for robust target association.|
||1 instances in total. (in iccv2017)|
|2374|Mengshi_Qi_stagNet_An_Attentive_ECCV_2018_paper|The node  8  M. Qi, J. Qin, A. Li, Y. Wang, J. Luo and L. Van Gool  Scene nodeRNN  Person nodeRNN  Spatial CNN   Temporal Link  Spatial Link  Visual Feature Link  T e m p o r a l    A  t t e n t i o n  T e m p o r a l    A  t t e n t i o n  Spatial Attention  Spatial Attention  Spatial Attention  T-1  T  T+1  Fig.|
||1 instances in total. (in eccv2018)|
|2375|cvpr18-Robust Physical-World Attacks on Deep Learning Visual Classification|We use a publicly available implementation [39] of a multi-scale CNN architecture that has been known to perform well on road sign recognition [31].|
||1 instances in total. (in cvpr2018)|
|2376|cvpr18-Unsupervised Correlation Analysis|Similarly to the CCA literature, we assume the data in the two domains is already encoded, e.g., by a preexisting deep CNN for images, or a thought-vector technique for text.|
||1 instances in total. (in cvpr2018)|
|2377|Unsupervised Part Learning for Visual Recognition|Particular object retrieval  with integral max-pooling of CNN activations.|
||1 instances in total. (in cvpr2017)|
|2378|cvpr18-Feature Mapping for Learning Fast and Accurate 3D Pose Inference From Synthetic Images|[32] proposed a semi-supervised approach that incorporates a semantic segmentation of the hand; DeepModel [53] integrates a 3D hand model into a Deep Network; DISCO [3] learns the posterior distribution of hand poses; Feedback [34] uses an additional Deep Network to improve results of an initial prediction; Hand3D [9] uses a volumetric CNN to process a point cloud.|
||1 instances in total. (in cvpr2018)|
|2379|Wang_Recurrent_Face_Aging_CVPR_2016_paper|We also compare our method with two popular Convolution Neural Networks (VGG-S [3], Super-Resolution CNN [6]).|
||1 instances in total. (in cvpr2016)|
|2380|Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper|Cross-modal retrieval with cnn visual features: A new baseline.|
||1 instances in total. (in iccv2017)|
|2381|Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper|We use a CNN model based on the VGG-16 architecture [36] pretrained on ImageNet.|
||1 instances in total. (in eccv2018)|
|2382|Adrian_Bulat_To_learn_image_ECCV_2018_paper|Huang, H., He, R., Sun, Z., Tan, T.: Wavelet-srnet: A wavelet-based cnn for multi scale face super resolution.|
||1 instances in total. (in eccv2018)|
|2383|Pan_Shallow_and_Deep_CVPR_2016_paper|We used transfer learning to initialize the weights for the first three convolutional layers with the pre-trained weights from the VGG CNN M network from [3].|
||1 instances in total. (in cvpr2016)|
|2384|He_Convolutional_Neural_Networks_2015_CVPR_paper|This paper investigates the accuracy of CNN architectures at constrained time cost during both training and testing stages.|
||1 instances in total. (in cvpr2015)|
|2385|Qian_Fine-Grained_Visual_Categorization_2015_CVPR_paper|Note that there has been some difficulties  4321  27DML3145689100121123314665789101112in training CNN directly on FGVC datasets because the existing FGVC benchmarks are often too small [12] (only several tens of thousands of training images or less).|
||1 instances in total. (in cvpr2015)|
|2386|cvpr18-Wide Compression  Tensor Ring Nets|Complexity: We employ the ratio between complexity in CNN layer and the complexity in tensor ring layer to quantify the capability of TRN in reducing computation (Cconv) and parameter (Pconv) costs, D2IO  (14)  Pconv =  D2R2 + IR2 + OR2 ,  Cconv =  IO  D2  R2I + R3D2 + R2O  .|
||1 instances in total. (in cvpr2018)|
|2387|cvpr18-Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs|Previous attempts at using deep learning for large 3D data were trying to replicate successful CNN architectures used for image segmentation.|
||1 instances in total. (in cvpr2018)|
|2388|Jiangxin_Dong_Learning_Data_Terms_ECCV_2018_paper|Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image  restoration.|
||1 instances in total. (in eccv2018)|
|2389|cvpr18-Pulling Actions out of Context  Explicit Separation for Effective Combination|Compared to other state-of-the-art methods such as Dense Trajectory Descriptors (DTD) [32] or Two-stream CNN [25], C3D achieves a good balance between efficiency and simplicity.|
||1 instances in total. (in cvpr2018)|
|2390|Heng_Wang_Scenes-Objects-Actions_A_Multi-Task_ECCV_2018_paper|ResNet [11] is among the most successful CNN models for image classification.|
||1 instances in total. (in eccv2018)|
|2391|A Deep Regression Architecture With Two-Stage Re-Initialization for High Performance Facial Landmark Detection|We employ a CNN structure, e.g., TCDCN[36], as the localization model to predict the transformation parameter g.|
||1 instances in total. (in cvpr2017)|
|2392|cvpr18-Fast and Furious  Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net|Nam and Han [21] finetuned a CNN at inference time to track object within the same video.|
||1 instances in total. (in cvpr2018)|
|2393|cvpr18-Convolutional Sequence to Sequence Model for Human Dynamics|The hierarchical structure of CNN makes it capable of capturing both spatial and temporal correlations effectively.|
||1 instances in total. (in cvpr2018)|
|2394|Yu_Multi-Modal_Factorized_Bilinear_ICCV_2017_paper|bilinear pooling [28] has recently been used to integrate different CNN features for fine-grained image recognition [17].|
||1 instances in total. (in iccv2017)|
|2395|Unsupervised Video Summarization With Adversarial LSTM Networks|3: Initialize all parameters {s, e, d, c} 4: for max number of iterations do 5:  6:  7:  8:  9:  10:  11:  12:  13:  14:  X  mini-batch from CNN feature sequences S  sLSTM(X) % select frames E = eLSTM(X, S) % encoding X = dLSTM(E) % reconstruction Sp  draw samples form the uniform distribution Ep = eLSTM(X, Sp) % encoding Xp = dLSTM(ESp ) % reconstruction % Updates using Stochastic Gradient: {s, e} + (Lreconst + Lprior + Lsparsity) {d} + (Lreconst + LGAN) {c} + +(LGAN) % maximization update  15: 16: end for  two hidden states in eLSTM, I is an identity matrix and L(s) is a smaller square matrix, cut down from L given s. Let et be the hidden state of eLSTM at time t. For time steps t and t the pairwise similarity values are defined as Lt,t = stst etet .|
||1 instances in total. (in cvpr2017)|
|2396|Zhao_Chen_Estimating_Depth_from_ECCV_2018_paper|Garg, R., BG, V.K., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue.|
||1 instances in total. (in eccv2018)|
|2397|Rishabh_Dabral_Learning_3D_Human_ECCV_2018_paper|Monocular 3d human pose estimation in the wild using improved cnn supervision.|
||1 instances in total. (in eccv2018)|
|2398|Kafle_An_Analysis_of_ICCV_2017_paper|Almost all systems use CNN features to represent the image and either a recurrent neural network (RNN) or a bag-of-words model for the question.|
||1 instances in total. (in iccv2017)|
|2399|Siqi_Liu_Remote_Photoplethysmography_Correspondence_ECCV_2018_paper|It is noted that the CNN exceeds MS-LBP on generalizability due to the property of deep features.|
||1 instances in total. (in eccv2018)|
|2400|Murdock_Blockout_Dynamic_Model_CVPR_2016_paper|Bilinear cnn models for fine-grained visual recognition.|
||1 instances in total. (in cvpr2016)|
|2401|Han_High-Resolution_Shape_Completion_ICCV_2017_paper| A novel patch-level 3D CNN for local geometry refinement under the guidance of our global structure inference network.|
||1 instances in total. (in iccv2017)|
|2402|cvpr18-Context Contrasted Feature and Gated Multi-Scale Aggregation for Scene Segmentation|An empirical study of language cnn for image captioning.|
||1 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
