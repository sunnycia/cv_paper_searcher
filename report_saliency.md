|Index|Title|sentence|
|---|---|---|
|1|Matthias_Kummerer_Saliency_Benchmarking_Made_ECCV_2018_paper|...Here we show that no single saliency map can perform well under all metrics....|
|||...Instead, we propose a principled approach to solve the benchmarking problem by separating the notions of saliency models, maps and metrics....|
|||...Inspired by Bayesian decision theory, we define a saliency model to be a probabilistic model of fixation density prediction and a saliency map to be a metric-specific prediction derived from the model...|
|||...We derive these optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC, NSS, CC, SIM, KL-Div) and show that they can be computed analytically or approximated with high precision....|
|||...We show that this leads to consistent rankings in all metrics and avoids the penalties of using one saliency map for all metrics....|
|||...A long-standing account of bottom-up attentional guidance posits the existence of a saliency map (or maps) in the human brain [48,26]....|
|||...Over time, the concept of a saliency map has moved away from its origins in low-level feature integration, and can now refer more generally to a map that predicts fixations....|
|||...In practice, saliency maps are now synonymous with saliency models....|
|||...The community uses these metrics in benchmarks to keep track of the progress: the MIT Saliency Benchmark [9,23] and the LSUN Challenge [53,54,52,21]....|
|||...ng that much of the disagreement between different metrics can be removed via postprocessing of the saliency maps by optimizing the saliency scale and smoothing kernel for information gain (IG, essent...|
|||...In fact, we show in this paper that even with knowledge of the true fixation distribution, no single saliency map can perform well in all metrics....|
|||...In practice however, researchers must still decide on a particular saliency map to submit to the benchmark....|
|||... because the model is intrinsically bad on those metrics, but because different metrics require the saliency maps to look different, independent of the encoded information about fixation placement (se...|
|||...As long as one evaluates all saliency metrics on the same saliency maps, it is impossible to solve the benchmarking problem....|
|||...Here, we argue that the fundamental problem is that saliency models and saliency maps are considered to be the same....|
|||...In the saliency setting, one decides on a saliency map to submit to a certain metric....|
|||...Correspondingly, saliency models should be defined as metric-independent probability densities over possible fixations and subsequently many different metric-dependent saliency maps can be derived fro...|
|||...We show that saliency maps for the most influential metrics AUC, sAUC, NSS, CC, SIM, and KL-Div can be derived from fixation densities in a principled way....|
|||...By decoupling the notions of saliency models and saliency maps, saliency models can be meaningfully compared on all metrics in their original scale, and the MIT saliency benchmark will implement our s...|
|||...2 Theory  Motivated by the line of thoughts presented above we here propose to use the following definitions:  1. a saliency model predicts a fixation probability density p(x, y   I) given an  image I....|
|||...2. a saliency metric is a performance measure for a saliency map on ground  truth data....|
|||...3. a saliency map sp,metric(x, y, I) is a metric-specific prediction derived from  the model density....|
|||...It has been argued before that formulating saliency models as probabilistic models is advantageous (e.g....|
|||...In this definition, a saliency model predicts a fixation probability density, that is, the probability p(x, y   I) of observing a fixation at a given pixel in a given image3....|
|||...1: No single saliency map can perform best in all metrics even when the true fixation distribution is known....|
|||...This problem can be solved by separating saliency models from saliency maps....|
|||...b) This ground truth density predicts different saliency maps depending on the intended metric....|
|||...The saliency maps differ dramatically due to the different properties of the metrics but always reflect the same underlying model....|
|||...c) Performances of the saliency maps from b) under seven saliency metrics on a large number of fixations sampled from the model distribution in a)....|
|||...The predicted saliency map for the specific metric (framed bar) yields best performance in all cases....|
|||...above follow the rationale of Bayesian decision theory: the saliency model is a posterior density over all possible events and the saliency metric is a utility function....|
|||...Based on the posterior density and the utility function, a saliency map is then chosen to maximize the expected utility....|
|||...2.1 Predicting saliency maps from saliency models  From the predicted fixation density of a model, one can use expected utility maximization to derive the saliency map which the model expects to yield...|
|||...0.60.80.60.8012010.00.51.00.00.50.00.51.0Saliency Benchmarking Made Easy  5  Evaluating a saliency metric involves a saliency map s(x, y   I) for a stimulus I and ground truth fixation data (xi, yi)....|
|||...Note that some metrics as CC or SIM use an empirical saliency map instead of ground truth fixations (distribution-based metrics,richeSaliency2013 )....|
|||...Assuming that the fixations are distributed according to some distribution (xi, yi)  p(x, y   I) and therefore D  Qn 1 p(x, y), the expected performance of the metric on a saliency map is EDM [s(x, y   I); D]....|
|||...One should choose the saliency map which is expected to yield highest performance for the metric M : that is, the solution of  max s(x,y I)  EDM [D, s(x, y   I)]  Solving this optimization problem for...|
|||...For a metric M the solutions to the optimization problem give rise to a transformation p(x, y   I) 7 sM (x, y   I) from fixation densities to derived metric-specific saliency maps....|
|||...While the optimization problem might be hard in general, for most commonly-used saliency metrics it can be solved exactly or approximately, as we show below....|
|||...nd three distribution-based metrics which first convert the ground truth fixations into a empirical saliency map (CC, SIM, KL-Div)....|
|||...Additionally we include the IG metric introduced in [32] since we use this metric for converting existing saliency map models to probabilistic models....|
|||...Therefore the model should expect the saliency map pfix(x, y)/pnonfix(x, y) to yield highest performance....|
|||...In the special case of the standard AUC metric, pnonfix is constant and the saliency map boils down to pfix....|
|||...To compensate for this limited precision and possible JPEG-artefacts, one should additionally histogram-equalize the saliency map (see Supplementary Material)....|
|||...NSS The Normalized Scanpath Saliency (NSS, [37]) performance of a saliency map model is defined to be the average saliency value of fixated pixels in the normalized (zero mean, unit variance) saliency...|
|||...Then the expected NSS of a saliency map q = (q1, ....|
|||...Finding the saliency map with the best possible NSS is equivalent to finding the solution of the problem  N Pi qi =  q = 0, kqk2  maxhp, qi  s.t....|
|||... q =  p, kqk2 = kpk2 instead (and normalize q afterwards to get the normalized saliency map)....|
|||...hese conditions is identical with the minimum of kp  qk2, which is p.  Therefore, the best possible saliency map with respect to NSS is the density  of the fixation distribution....|
|||...IG The information gain (IG, [32]) metric requires the saliency map to be a probability distribution and compares the average log-probability of fixated pixels to that given by a baseline model (usual...|
|||...The optimal saliency map for IG depends on how the metric interprets saliency maps as probability densities....|
|||...We normalize the saliency maps to be probability vectors (nonnegative, unit sum) and in this case the predicted density itself yields the highest expected performance: Let p = (p1, ....|
|||...inq KL[p, q] = p.  CC The correlation coefficient (CC, [22]) measures the correlation between model saliency map and empirical saliency map after normalizing both saliency maps to have zero mean and u...|
|||...This is equivalent to measuring the euclidean distance between the predicted saliency map and the normalized empirical saliency map....|
|||...Therefore the optimal saliency map with respect to CC is the expected normalized empirical saliency map....|
|||...This shows that predicting the optimal saliency map for CC crucially depends on how the empirical saliency maps are computed....|
|||...Empirical saliency maps are typically computed by blurring observed fixation positions from eye movement data with a Gaussian kernel of a certain size....|
|||...In this case the expected empirical N PN i G  saliency map would be Exip p = G  p, that is, the density blurred with a Gaussian kernel of size .  ExpG(x) = 1  i G(x) = 1  1  N PN  N PN  i  Unfortunate...|
|||...Effectively, normalizing the variance just changes the weight by which the different empirical saliency maps are averaged in the expectation value....|
|||...Therefore, as an approximation to the expected normalized empirical saliency map, we use the expected saliency map in this paper, which is computed by convolving the expected density by a Gaussian....|
|||...cross validation of the kernel size as in [32]), then the expected empirical saliency map is harder or impossible to calculate analytically....|
|||...However, one can still approximate it numerically by sampling normalized empirical saliency maps from the expected fixation distribution and averaging them....|
|||...KL-Div The KL-Div metric computes the Kullback-Leibler divergence between the empirical saliency maps and the model saliency maps after converting both of them into probability distributions (by makin...|
|||...We can show that for the KL-Div metric, the expected empirical saliency map expects the best performance: let e = (e1, ....|
|||..., eN ) with e  0, Pi ei = 1 denote the random variable which represents the empirical saliency map and q with q  0, Pi qi = 1 the model saliency map....|
|||...As for CC, this is the density blurred by the same kernel size as used for the empirical saliency map....|
|||...[ei]Pi  Ep[ei] log qi,  Ep[ei] log qi =  SIM The Similarity (SIM, [23]) metric normalizes the model saliency map and the empirical saliency map to be probability vectors (in the same way as KLDiv) and...|
|||... opposed to the CC-metric, which can be interpreted as measuring the l2-distance between normalized saliency maps, this effectively measures the l1-distance between saliency maps (Pi min(pi, qi) = Pi ...|
|||...Note that the optimal saliency map for SIM, unlike all other saliency maps presented here, depends on the number of fixations per image (see the Supplement for details on this effect)....|
|||...2 (pi + qi   pi  qi ) = 1  1  1  3 Experiments and Results  We use the pysaliency toolbox [29] to compute saliency metrics (see Supplement for details)....|
|||...From a probability density over an image we compute five types of saliency maps: AUC saliency maps are created by equalizing the probability density to yield a uniform histogram over all pixels....|
|||...sAUC saliency maps are created by dividing the probability density by the center bias density and again equalizing the saliency map to yield a uniform histogram over all pixels....|
|||...NSS/IG saliency maps are simply the probability density....|
|||...CC/KLDiv saliency maps are calculated by convolving the probability density with a Gaussian kernel with  = 35px (corresponding to 1dva, as commonly used on the MIT1003 dataset)....|
|||...SIM saliency maps: We divide the CC saliency map by its sum to normalize it....|
|||...3.1 No saliency map to rule them all  Here we illustrate using simulated data that even if the true fixation density is known, no single saliency map can win in all saliency metrics....|
|||...From a fictional fixation density (Figure 1a) we compute the saliency maps that we predict to be optimal for the seven saliency metrics AUC, sAUC, NSS/IG, CC/KL-Div and SIM (Figure 1b)....|
|||...100 fixations from the fixation density  Saliency Benchmarking Made Easy  9  and evaluate all five saliency maps using the seven different saliency metrics on this dataset (Figure 1c, raw data in the...|
|||...Although the saliency maps in Figure 1b all are predicted by the same model, they appear visually different: while the AUC saliency map is essentially just the normalized density, the sAUC saliency ma...|
|||...The NSS/IG saliency map is exactly the density and shows large areas with very low values....|
|||...The CC/KL-Div saliency map, being a blurred version of the density, is much smoother than the NSS saliency map....|
|||...The SIM saliency map looks mostly like the CC/KL-Div saliency map but is slightly more sparse....|
|||...The ranking of the five saliency maps is highly inconsistent across metrics (Figure 1c): even with knowledge of the real fixation distribution, no saliency map can be optimal for all saliency metrics....|
|||...However, each saliency map is optimal for exactly those metrics for which it has been predicted to be optimal (framed bars)....|
|||...3.2 MIT1003  In our main experiment, we use our approach to evaluate six saliency models on the popular benchmarking dataset MIT1003 (freeviewing fixations of 15 subjects on 1003 images, [24])....|
|||...The included models are AIM [6], Boolean Map-based Saliency (BMS) [55], the Ensemble of Deep Networks (eDN) [49], OpenSALICON [47], SalGAN [36] and DeepGaze II [31]....|
|||...Converting existing models that produce arbitrary saliency maps into probabilistic models is not straightforward [32]....|
|||...For showing the original saliency map we use the log density in this case....|
|||...Example saliency maps....|
|||...In Figure 2, we show the probability distribution and the predicted saliency maps (columns) for the saliency models (rows) for one example stimulus....|
|||...Comparing the saliency maps within and between columns, i.e....|
|||...metrics, one notices that the process of predicting saliency maps for certain metrics has a strong effect on the shape of the saliency maps that is consistent across models....|
|||...It influences the visual appearance of the saliency map to a larger degree than the actual model does: the AUC and sAUC maps are very high contrast, while the NSS and CC saliency maps have large areas...|
|||...The CC and SIM saliency maps are much smoother than all other  10  M. K ummerer, T.S.A....|
|||...2: The predicted saliency map for various metrics according to different models, for the same stimulus....|
|||...For six models (rows) we show their original saliency map (first column), the probability distribution after converting the model into a probabilistic model (second column) and the saliency maps predi...|
|||...In particular, note the inconsistency of the original models (what are typically compared on the benchmark) relative to the per-metric saliency maps....|
|||...intended for the same saliency metric)....|
|||...In Figure 3 we evaluate the saliency maps of the saliency models (AIM, BMS, eDN, OpenSALICON, SalGAN, DeepGaze II; x-axis) on the seven saliency metrics (subplots, raw data in the Supplement)....|
|||...Each line indicates the models performances in the evaluated metric when using a specific type of saliency map....|
|||...The dashed lines indicate performance using the models original saliency maps (i.e....|
|||...3: We reformulated several saliency models in terms of fixation densities and evaluated AUC, sAUC, NSS, IG, CC, KL-Div and SIM on the original saliency maps (dashed line) and the saliency maps derived...|
|||...that metric(thick line), and for each metric the model ranking is consistent when using the correct saliency maps  unlike for the original saliency maps and some other derived saliency maps....|
|||...Note that AUC metrics yield identical results on AUC saliency maps, NSS saliency maps and log-density saliency maps, therefore the blue and purple lines are hidden by the red line in the AUC and sAUC plots....|
|||...Also, the CC metric yields only slightly worse results on the SIM saliency map than on the CC saliency map, therefore the orange line is hidden by the green line in the CC plot....|
|||...The performances are very inconsistent between the different metrics on the original saliency maps....|
|||...The solid lines indicate the metric performances on the five types of derived saliency maps (red: AUC, pink: sAUC, blue: NSS and IG, green: CC and KL-Div, orange: SIM)....|
|||...For each metric, the saliency map predicted for that metric (thick line in each sub plot) yields highest performance for all models....|
|||...eDNOpnSSalGANDGII0.20.40.60.8AIMBMSeDNOpnSSalGANDGII0.40.81.2AIMBMSeDNOpnSSalGANDGII0.40.50.60.7AUC saliency mapssAUC saliency mapsNSS/IG saliency mapsCC/KL saliency mapsSIM saliency mapsoriginal sali...|
|||...Wallis and M. Bethge  metrics on each saliency map type are much more consistent than on the original saliency maps, there is still disagreement between metrics left when evaluating all metrics on the...|
|||...Interestingly, the AIM model reaches better NSS performance with the CC saliency map than with the NSS saliency map....|
|||...For example, DeepGaze II reaches significantly higher NSS scores with the NSS saliency map than with the CC saliency map and vice versa for the CC metric....|
|||...The SIM metric seems to show only slighly better performance on the SIM saliency map than on the CC saliency map, with the average difference being just 0.006....|
|||...trate a key difference between the metric unification proposed in [32] and our method of predicting saliency maps from fixation densities: the metric results presented in [32] correspond to the purple...|
|||...As reported in [32], the model rankings are more consistent for those lines than for the original saliency maps....|
|||...To summarize, Figure 3 illustrates the main result of this paper: No matter what saliency map type you decide for, even state-of-the-art models will perform suboptimally in some metrics and rankings w...|
|||...Only by using the right saliency map for each metric given the model density, every model performs as well as it can theoretically and all model rankings agree....|
|||...Here we argue that benchmarking can be simplified by considering saliency models to be probability density predictors, saliency metrics to be performance measures that assess saliency maps against gro...|
|||...We have shown that probabilistic models can predict good saliency maps for the most common saliency metrics: good models perform well in many metrics....|
|||...In this way it is possible to obtain optimal predictions from a given saliency density for arbitrary metrics without retraining....|
|||...The fact that metrics impose strong constraints on saliency maps means that it is misleading to visually compare saliency maps intended for different metrics (see Figure 2)but this is commonly done in...|
|||...While the saliency maps we have derived give the optimal metric-specific saliency map for a given fixation density, it is nevertheless still possible that a given model could do better on a metric wit...|
|||...does not reflect the data-generating density), then the derived saliency maps can be suboptimal....|
|||...If the models density is especially bad, some metrics might even perform better on saliency maps not predicted for this metric than on the one predicted for this metric....|
|||...For example: if a models density prediction is too sparse, the AUC metric will perform better on the smoothed CC saliency map than it will perform on the actual AUC saliency map....|
|||...The fact that we dont observe this effect on the original saliency maps (which were trained in the case of eDN, OpenSALICON, SalGAN and DeepGaze II: Figure 3, dashed lines) suggests any improvement is...|
|||...Finally, we would like to note that the distinction between saliency models and saliency maps we draw here does not contradict ideas that a saliency map or maps may be instantiated in the human brain,...|
|||...Our nomenclature is rather independent and intended for saliency model benchmarking....|
|||...The code for evaluating saliency models as demonstrated in this work has been released as part of the pysaliency python library (available at https: //github.com/matthias-k/pysaliency)....|
|||...Conclusion Our work solves the problem that one saliency model cannot reach state-of-the-art performance in all relevant saliency metrics....|
|||...Our key theoretical contribution is to decouple the notions of saliency models and saliency maps....|
|||...For benchmarking practice, this means that saliency models can be meaningfully compared on all metrics in their original scale....|
|||...The MIT saliency benchmark will implement this option....|
|||...Borji, A., Sihite, D.N., Itti, L.: Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Bylinskii, Z., Judd, T., Durand, F., Oliva, A., Torralba, A.: MIT saliency bench mark....|
|||...Bylinskii, Z., Judd, T., Oliva, A., Torralba, A., Durand, F.: What do different evaluation metrics tell us about saliency models?...|
|||...Bylinskii, Z., Recasens, A., Borji, A., Oliva, A., Torralba, A., Durand, F.: Where should saliency models look next?...|
|||...Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: Predicting human eye fixations via an LSTM-based saliency attentive model....|
|||...Huang, X., Shen, C., Boix, X., Zhao, Q.: SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Itti, L.: Quantifying the contribution of  low-level saliency to human eye movements in dynamic scenes....|
|||...Jetley, S., Murray, N., Vig, E.: End-to-end saliency mapping via probability distribution prediction....|
|||...Jiang, M., Huang, S., Duan, J., Zhao, Q.: SALICON: Saliency in context....|
|||...Judd, T., Durand, F.d., Torralba, A.: A Benchmark of Computational Models of Saliency to Predict Human Fixations....|
|||...: What do saliency models predict?...|
|||...K ummerer, M., Theis, L., Bethge, M.: Deep gaze i: Boosting saliency prediction with feature maps trained on ImageNet....|
|||...K ummerer, M., Wallis, T.S.A., Bethge, M.: Information-theoretic model comparison unifies saliency metrics....|
|||...Li, Z.: A saliency map in primary visual cortex....|
|||...Nuthmann, A., Einh auser, W., Sch utz, I.: How well can saliency models predict fixation selection in scenes beyond central bias?...|
|||...Pan, J., Ferrer, C.C., McGuinness, K., OConnor, N.E., Torres, J., Sayrol, E., Giro-i Nieto, X.: SalGAN: Visual saliency prediction with generative adversarial networks....|
|||...Riche, N.: Metrics for saliency model validation....|
|||...Riche, N.: Saliency model evaluation....|
|||...Yu, F., Kontschieder, P., Song, S., Jiang, M., Zhang, Y., Zhao, C.Q., Funkhouser, T., Xiao, J.: SALICON saliency prediction challenge....|
|||...Zhang, J., Sclaroff, S.: Saliency detection: A Boolean map approach....|
|||...: SUN: A Bayesian framework for saliency using natural statistics....|
||154 instances in total. (in eccv2018)|
|2|Quanlong_Zheng_Task-driven_Webpage_Saliency_ECCV_2018_paper|...In this paper, we present an end-to-end learning framework for predicting task-driven visual saliency on webpages....|
|||...omponents on a webpage (e.g., images, buttons and input boxes), our network explicitly disentangles saliency prediction into two independent sub-tasks: task-specific attention shift prediction and tas...|
|||...The task-specific branch estimates task-driven attention shift over a webpage from its semantic components, while the task-free branch infers visual saliency induced by visual features of the webpage....|
|||...Such a task decomposition framework allows us to efficiently learn our model from a small-scale task-driven saliency dataset with sparse labels (captured under a single task condition)....|
|||...Experimental results show that our method outperforms the baselines and prior works, achieving state-of-the-art performance on a newly collected benchmark dataset for task-driven webpage saliency detection....|
|||...Keywords: Webpage analysis  Saliency detection  Task-specific saliency  1  Introduction  Webpages are a ubiquitous and important medium for information communication on the Internet....|
|||...1: Given an input webpage (a), our model can predict a different saliency map under a different task, e.g., information browsing (b), form filling (c) and shopping (d)....|
|||...There are two main obstacles for this problem: 1) Lack of powerful features for webpage saliency prediction: while existing works have investigated various features for natural images, effective featu...|
|||...Inspired by this, we propose to disentangle task-driven saliency prediction into two sub-tasks: task-specific attention shift prediction and task-free saliency prediction....|
|||...The task-specific branch estimates task-driven global attention shift over the webpage from its semantic components, while the task-free branch predicts visual saliency independent of the task....|
|||...task decomposition framework allows efficient network training using only a small-scale task-driven saliency dataset captured under the single task condition, i.e., each webpage in the dataset contain...|
|||...train our model effectively, we first pre-train the task-free subnet on a large-scale natural image saliency dataset and task-specific subnet on synthetic data generated by our proposed data synthesis...|
|||...We then train our network end-to-end on a small-scale task-driven webpage saliency dataset....|
|||...To evaluate our model, we create a benchmark dataset of 200 webpages, each with visual saliency maps captured under one or more tasks....|
|||...Our main contributions are:  Task-driven Webpage Saliency  3   We address webpage saliency prediction under the multi-task condition....|
|||...2 Related Work  2.1 Saliency Detection on Natural Images  Saliency detection on natural images is an active research topic in computer vision....|
|||...Some works [17, 18, 40] produce high-quality saliency maps using different CNNs to extract multi-scale features....|
|||...[23] propose shallow and deep CNNs for saliency prediction....|
|||...More recent works [10, 16, 19, 31] apply fully convolutional networks for saliency detection, in order to reduce the number of parameters of the networks and preserve spatial information of internal r...|
|||...In contrast, our work focuses on predicting saliency on webpages, which are very different from natural images in visual, structural and semantic characteristics [27]....|
|||...2.2 Saliency Detection on Webpages  Webpages have well-designed configurations and layouts of semantic components, aiming to direct viewer attention effectively....|
|||...[28] propose a saliency model based on hand-crafted features (face, positional bias, etc.)...|
|||...However, all these methods assume a free-viewing condition, without considering the effect of tasks upon saliency prediction....|
|||...[4] propose deep learning based models to predict saliency for data visualization and graphics....|
|||...In contrast, we aim for a unified, task-conditional framework, where our model will output different saliency maps depending on the given task label....|
|||...4  Q.Zheng, J.Jiao, Y.Cao and Rynson.Lau  2.3 Task-driven Visual Saliency  There are several works on analyzing or predicting visual saliency under taskdriven conditions....|
|||...Recent works attempt to drive saliency prediction using various high-level signals, such as example images [8] and image captions [35]....|
|||...There is also a line of research on visualizing object-level saliency using imagelevel supervision [25, 29, 37, 39, 41]....|
|||...In contrast, as it is expensive to collect the task-driven webpage saliency data, we especially design our network architecture so that it can be trained efficiently on a small-scale dataset with spar...|
|||...Sparse annotations in our context means that each image in our dataset only has ground-truth saliency for a single task, but our goal is to predict saliency under the multiple tasks....|
|||...First, we perform a data analysis to understand the relationship between task-specific saliency and semantic components on webpages, which motivates the design of our network and inspires our data syn...|
|||...3.1 Task-driven Webpage Saliency Dataset  To train our model, we use a publicly available, state-of-the-art task-driven webpage saliency dataset presented in [24]....|
|||...To compute a saliency map for a webpage, they aggregated the data gaze data from all the viewers and convolved the result with a Gaussian filter, as in [13]....|
|||...Note that the size of the dataset is small and we only have saliency data of the webpages captured under the single task condition....|
|||...2: Accumulative saliency of each semantic component (row) under a specific task (column)....|
|||...From left to right, each column represents the saliency distribution under the Signing-up, Form filling, Information browsing, Shopping or Community joining task....|
|||...Here, we explore the relationship between task-driven saliency and semantic components by analyzing the task-driven webpage saliency dataset in Sec....|
|||...2 shows the accumulative saliency on each semantic component under different tasks....|
|||...Both the input field and button components have higher saliency under Form filling, relative to other tasks....|
|||...6  Q.Zheng, J.Jiao, Y.Cao and Rynson.Lau  Table 1: Component saliency ratio for each semantic component (column) under each task (row)....|
|||...hip quantitatively, for each semantic component c under a task t, we define a within-task component saliency ratio, which measures the average saliency of c under t compared with the average saliency ...|
|||...Our component saliency ratio tells whether a semantic component under a particular task is more salient (> 1), equally salient (= 1) or less salient (< 1), as compared with the average saliency....|
|||...er semantic components are low ( 1), which is consistent with our observation from the accumulative saliency maps above....|
|||...Based on these component saliency scores, for each task, we identify two semantic components with higher scores as the key components (the shaded components in Table 1) that people tend to focus on un...|
|||...These key components are used to synthesize task-driven saliency data for pre-training the task-specific subnet of our network, as introduced in Section 3.5....|
|||...he task-dependent attention shift (upper), while the task-free subnet predicts the task-independent saliency (lower)....|
|||...The task-specific attention shift and task-free saliency are combined to obtain the final saliency map under the input task....|
|||...To simplify our network, this subnet uses the output layer of the FCN [26] to directly output a saliency map, which works well in our experiments....|
|||...Discussion: Our network architecture can be efficiently trained, even with small amounts of training data, to produce reasonable saliency predictions given different tasks....|
|||...This is because our framework has the task-specific branch to model the task-related saliency shift from task-free saliency....|
|||...5: Synthetic saliency data....|
|||...(a) Saliency map from the webpage dataset [24]....|
|||...(b)(f) Synthesized saliency maps from (a) for 5 different tasks....|
|||...etween the predicted and ground-truth semantic segmentation maps, Lseg, and train it on the webpage saliency dataset with ground-truth semantic annotations....|
|||...Finally, we train the entire model end-to-end using L2 loss between the ground-truth and predicted saliency maps given a task label....|
|||...3.5 Task-driven Data Synthesis  Pre-training the task-specific subnet requires a lot of saliency data on webpages under the multi-task condition, which is not available and expensive to collect....|
|||...Given a webpage in our dataset, we take its existing task-driven saliency map....|
|||...For each task of the five tasks, we only preserve the saliency of the saliency map on the corresponding key components of the task, by zeroing out the saliency in other regions....|
|||...In this way, we generate 5 task-specific saliency maps for each webpage....|
|||...5 shows an example of the synthesized saliency maps under different tasks....|
|||...With our data synthesis approach, we generate a dataset with dense annotations (i.e., the saliency data under all tasks are available for all webpages), which is sufficient for pre-training our task-s...|
|||...5.1 Evaluation Dataset and Metrics  To evaluate our method, a task-driven webpage saliency dataset is required, where each webpage has ground-truth saliency under different tasks....|
|||...To the best of our knowledge, the newly collected dataset, containing 200 webpages, is the largest task-driven webpage saliency evaluation dataset (vs. 30 webpages in [24, 28])....|
|||...Similar to previous works [3, 12, 14], we use the following metrics for evaluation: Kullback-Leibler divergence (KL), shuffled Area Under Curve (sAUC) and Normalized Scanpath Saliency (NSS)....|
|||...This shows that having a one-branch network to directly predict saliency from a webpage is not a promising solution and our task-decomposition framework is essential for the task-driven saliency predi...|
|||...This implies that while task-driven human attention mainly  12  Q.Zheng, J.Jiao, Y.Cao and Rynson.Lau  Table 3: Performances of different saliency detection approaches on our evaluation dataset....|
|||...With only the task-specific subnet (i.e., no task-free subnet), the model tends to put saliency mainly on task-relevant semantic components, but ignores the regions that people do look at (although wi...|
|||...In contrast, our full model learns to optimally allocate saliency between high-saliency task-relevant semantic components and other low-saliency regions....|
|||...In addition, learning with only synthetic saliency data does not perform well, due to the gap between the statistics of real and synthetic saliency data....|
|||...th a recent classification-driven concept localization model that is adapted to predict task-driven saliency by treating our task labels as class  Task-driven Webpage Saliency  13  labels....|
|||...Unfortunately, we did not get the code for the free-viewing webpage saliency prediction method [28] for comparison....|
|||...For each webpage under each task, we run each method to get a saliency map....|
|||...Since the free-viewing saliency detection methods do not take a task label as input, thus always producing the same results under different task conditions....|
|||...This is perhaps because that those free-viewing saliency models tend to fire at almost all the salient regions in a webpage, thereby generating a more uniform saliency distribution that is more likely...|
|||...However, such uniform saliency predictions certainly result in more false positives, making the performance of these models worse than ours in sAUC and NSS....|
|||...The task-driven saliency method, Grad-CAM [25] performs worst in our evaluation dataset....|
|||...Grad-CAM fails to locate salient regions for each task.The free viewing saliency models (i.e., SalNet, SALICON, VIMGD) simply highlight all the salient regions, oblivious to task conditions....|
|||...6 Conclusion  We have presented a learning framework to predict webpage saliency under multiple tasks....|
|||...Our framework disentangles the saliency prediction into a taskspecific branch and a task-free branch....|
|||...Such disentangling framework allows us to learn our model efficiently, even from a relatively small task-driven webpage saliency dataset....|
|||...6: Saliency prediction results of our method and prior methods under different task conditions....|
|||...Bylinskii, Z., Judd, T., Oliva, A., Torralba, A., Durand, F.: What do different  evaluation metrics tell us about saliency models?...|
|||...He, S., Lau, R.: Exemplar-driven top-down saliency detection via deep association....|
|||...Jiang, M., Huang, S., Duan, J., Zhao, Q.: Salicon: Saliency in context....|
|||...Kruthiventi, S., Gudisa, V., Dholakiya, J., Venkatesh Babu, R.: Saliency unified: A deep architecture for simultaneous eye fixation prediction and salient object segmentation....|
|||...Kuen, J., Wang, Z., Wang, G.: Recurrent attentional networks for saliency detec tion....|
|||...K ummerer, M., Theis, L., Bethge, M.: Deep gaze I: Boosting saliency prediction  with feature maps trained on imagenet....|
|||...Lee, G., Tai, Y., Kim, J.: Deep saliency with encoded low level distance map and  high level features....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...Liu, N., Han, J.: DHSNet: Deep hierarchical saliency network for salient object  detection....|
|||...N.Liu, J.Han: A deep spatial contextual long-term recurrent convolutional network  for saliency detection....|
|||...Pan, J., Sayrol, E., Giro-i Nieto, X., McGuinness, K., OConnor, N.: Shallow and  deep convolutional networks for saliency prediction....|
|||...: Deep inside convolutional networks: visualising image classifica tion models and saliency maps....|
|||...Tang, Y., Wu, X.: Saliency detection via combining region-level and pixel-level  predictions with cnns....|
|||...Wang, L., Lu, H., Ruan, X., Yang, M.: Deep networks for saliency detection via  local estimation and global search....|
|||...Wang, L., Wang, L., Lu, H., Zhang, P., Ruan, X.: Saliency detection with recurrent  fully convolutional networks....|
|||...Xu, Y., Wu, J., Li, N., Gao, S., Yu, J.: Personalized saliency and its prediction....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional  features for accurate saliency detection....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep  learning....|
||99 instances in total. (in eccv2018)|
|3|cvpr18-Deep Unsupervised Saliency Detection  A Multiple Noisy Labeling Perspective|...ity, Canberra, Australia  3DATA61,CSIRO, Canberra, Australia  Abstract  The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the for...|
|||...By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could...|
|||...To this end, we present a novel perspective to unsupervised 1 saliency detection through learning from multiple noisy labeling generated by weak and noisy unsupervised handcrafted saliency methods....|
|||...Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly....|
|||...Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way....|
|||...ults on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-...|
|||...dense saliency maps in our task....|
|||...Unsupervised saliency learning from weak noisy saliency maps....|
|||...Given an input image xi and its corresponding j unsupervised saliency maps y i , our framework learns the latent saliency map  yi by jointly optimizing the saliency prediction module and the noise mod...|
|||...Compared with SBF [35] which also learns from unsupervised saliency but with different strategy, our model achieves better performance....|
|||...Depending on whether human annotations have been used, saliency detection methods can be roughly divided as: unsupervised methods and supervised methods....|
|||...The former ones compute saliency directly based on various priors (e.g., center prior [9], global contrast prior [6], background connectivity prior [43] and etc....|
|||...The later ones learn direct mapping from color images to saliency maps by exploiting the availability of large-scale human annotated database....|
|||...The success of these deep saliency methods strongly depend on the availability of large-scale training dataset with pixel-level human annotations, which is not only labor-intensive but also could hind...|
|||...By contrast, the unsupervised saliency methods, even though have been outperformed by the deep supervised methods, are generally dataset-independent and could be applied in the wild....|
|||...9029  In this paper, we present a novel end-to-end deep learning framework for saliency detection that is free from human annotations, thus unsupervised (see Fig....|
|||...Our framework is built upon existing efficient and effective unsupervised saliency methods and the powerful capacity of deep neural network....|
|||...The unsupervised saliency methods are formulated with human knowledge and different unsupervised saliency methods exploit different human designed priors for saliency detection....|
|||...By utilizing existing unsupervised saliency maps, we are able to remove the need of labor-intensive human annotations, also by jointly learn different priors from multiple unsupervised saliency method...|
|||...To effectively leverage these noisy but  informative saliency maps, we propose a novel perspective to the problem: Instead of removing the noise in saliency labeling from unsupervised saliency methods...|
|||...2, our framework consists of two consecutive modules, namely a saliency prediction module that learns the mapping from a color image to the latent saliency map based on current noise estimation and th...|
|||...r method takes advantages of both probabilistic methods and deterministic methods, where the latent saliency prediction module works in a deterministic way while the noise modeling module fits the noi...|
|||...To the best of our knowledge, the idea of considering unsupervised saliency maps as learning from multiple noisy labels is brand new and different from existing unsupervised deep saliency methods (e.g., [35])....|
|||...Our main contributions can be summarized as:  1) We present a novel perspective to unsupervised deep saliency detection, and learn saliency maps from multiple noisy unsupervised saliency methods....|
|||...2) Our deep saliency model is trained in an end-to-end manner without using any human annotations, leading to an extremely cheap solution....|
|||...methods with a wide margin while achieving comparable results with state-of-the-art deep supervised saliency detection methods [11, 40]....|
|||...Related Work  Depending on whether human annotations are used or not, saliency detection techniques can be roughly grouped as unsupervised and supervised methods....|
|||...Unsupervised Saliency Detection  Prior to the deep learning revolution, saliency methods mainly relied on different priors and handcrafted features [43, 7, 6, 9]....|
|||...Shen and Wu [27] formulated saliency detection as a low-rank matrix decomposition problem by exploiting the sparsity prior for salient objects....|
|||...Supervised Saliency Detection  Conventional supervised techniques, such as [14, 17], formulate saliency detection as a regression problem, and a classifier is trained to assign saliency at pixel or su...|
|||...Recently, deep neural networks have been adopted successfully for saliency detection [40, 26, 41, 29, 11, 22, 42, 19, 28, 20, 38, 39, 37]....|
|||...Deep networks can encode highlevel semantic features and hence capture saliency more effectively than both unsupervised saliency methods and nondeep supervised methods....|
|||...Deep saliency detection methods generally train a deep neural network to assign saliency to each pixel or superpixel....|
|||...[11] proposed a deep supervised framework with multi-branch short connections embed both highand low-level features for accurate saliency detection....|
|||...Though deep techniques are methods of choice in saliency detection, very few studies have explicitly addressed the problem of saliency learning with unreliable  9030  Figure 2....|
|||...Conceptual illustration of our saliency detection framework, which consists of a latent saliency prediction module and a noise modeling module....|
|||...Given an input image, noisy saliency maps are generated by handcrafted feature based unsupervised saliency detection methods....|
|||...The saliency prediction module targets at learning latent saliency maps based on current noise estimation and the noisy saliency maps....|
|||...The noise modeling module updates the noise estimation in different saliency maps based on updated saliency prediction and the noisy saliency maps....|
|||...To the best of our knowledge, [35] is the first and only deep method that learns saliency without human annotations, where saliency maps from unsupervised saliency methods are fused with manually desi...|
|||...The method iteratively replaces inter-image saliency map of low reliability with its corresponding saliency map....|
|||...Different from [35], we formulate unsupervised saliency learning as the joint optimization of latent saliency and noise modeling....|
|||...Our method is not only simpler and easier to implement, but also outperforms [35] and existing unsupervised saliency methods....|
|||...Furthermore, our method produces competitive performances as compared to the most recent deep supervised saliency detection methods....|
|||...Our Framework  Targeting at achieving deep saliency detection without human annotations, we propose an end-to-end noise model  integrated deep framework, which builds upon existing efficient and effec...|
|||...Given a color image xi, we would like to learn a better saliency map from its M noisy saliency maps yj i , j = 1,    , M using different unsupervised saliency methods [32, 13, 21, 43]....|
|||...A trivial and direct solution would be using the noisy saliency maps as proxy human annotations and train a deep model with these noisy saliency maps as supervision....|
|||...While there could be many other potentials in utilizing the noisy saliency maps, they are all based on human-designed pipelines, thus cannot effectively exploit the best manner....|
|||...Instead, we propose a principled way to infer the saliency maps from using multiple noisy labels and simultaneously estimate the noise....|
|||... based pipeline [35], we propose a new perspective toward the problem of learning from unsupervised saliency....|
|||...2, our framework consists of two consecutive modules, namely a saliency prediction module that learns the mapping from a color image to the latent saliency map, and a noise modeling module that fits the noise....|
|||...These two modules work collaboratively toward fitting the noisy saliency maps....|
|||...By explicitly modeling noise, we are able to train a deep saliency prediction model without any human annotations and thus achieve unsupervised deep saliency detection....|
|||..., N } and a set of M different saliency maps of these images, denoted as Y = {yj i , i = 1, ....|
|||...We propose a neural network with parameter  for saliency detection, which computes a saliency map  yi = f (xi, ) of each image....|
|||...These two losses are described below:  Saliency Prediction: For the latent saliency prediction module, we use a fully convolutional neural network (FCN) due to its superior capability in feature learn...|
|||...pred(, ) =  N  M  X  X  X  i=1  j=1  m,n  LCE(yj  i,mn, yj  i,mn),  (4)  where yj i,mn is our noisy saliency map prediction at pixel (m, n) which can be easily computed by (1) element-wise, and yj  i,...|
|||...Noise Modeling To effectively handle noisy saliency maps from different unsupervised saliency map labelers, we build a probabilistic model to approximate the noise, and connect it with our determinist...|
|||...Based on the variance of the saliency prediction and noisy labels, we then update the noise variance for each image and retrain the network....|
|||...Deep Noise Model based Saliency Detector  Network Architecture We build our latent saliency prediction module upon the DeepLab network [4], where a deep CNN (ResNet-101 [10] in particular) originally ...|
|||...For training, the noise model is used to iteratively update saliency prediction yj i , and its excluded in testing stage, where the latent saliency prediction output  yi in Fig....|
|||...2 is our predicted saliency map....|
|||...Setup  Dataset: We evaluated performance of our proposed model on 7 saliency benchmarking datasets....|
|||...The SOD saliency dataset [14] contains 300 images, where many images contain multiple salient objects with low contrast....|
|||...Unsupervised Saliency Methods:  In this paper, we learn unsupervised saliency from existing unsupervised saliency detection methods....|
|||...Competing methods: We compared our method against 10 state-of-the-art deep saliency detection methods (with clean labels): DSS [11], NLDF [26], Amulet [40], UCF [41], SRM [35], DMT [22], RFCN [28], De...|
|||...MAE can provide a better estimate of the dissimilarity between the estimated and ground truth saliency map....|
|||...It is the average per-pixel difference between the ground truth and the estimated saliency map, normalized to [0, 1], which is defined as:  M AE =  1  W  H  W  H  X  X  x=1  y=1   S(x, y)  GT (x, y) ,...|
|||...From DSS to DC are deep learning based supervised methods, from DRFI to HS are the handcrafted feature based unsupervised methods, SBF and OURS are deep learning based unsupervised saliency detection methods....|
|||...The PR curves are obtained by thresholding the saliency map in the range of [0, 255]....|
|||...Baseline Experiments  As there could be different ways to utilize the multiple noisy saliency maps, and for fair comparisons with straightforward solutions for our task, we run the following three bas...|
|||...Baseline 1 using noisy unsupervised saliency pseudo ground truth: For a given input image xi and its M handcrafted feature based saliency map yj i , j = 1, ..., M , we get M image pairs with noisy lab...|
|||...Baseline 2: using averaged unsupervised saliency as pseudo ground truth: Instead of using all the four unsupervised saliency as ground truth, we use the averaged saliency map of those unsupervised sal...|
|||...Baseline 3: supervised learning with ground truth supervision: Our proposed framework consists of the saliency prediction module and the noise modeling module to effectively leverage the noisy saliency maps....|
|||...odel can achieve as well as to provide a baseline comparison for our framework, we train our latent saliency module directly with clean labels, which naturally gives an upper bound of the saliency det...|
|||...This is because: 1) For BL1, we have 12,000 training image pairs (four unsupervised saliency methods), while for BL2, we have 3,000 averaged noisy labels; 2) as those unsupervised saliency methods ten...|
|||...Simply averaging those saliency maps results in even worse proxy saliency map supervision....|
|||...This demonstrates that by jointly learning the latent saliency maps and modeling the noise in a unified framework, we are able to learn the desired reliable saliency maps even without any human annotations....|
|||...Comparison with the State-of-the-art  Quantitative Comparison We compared our method with eleven most resent deep saliency methods and five conventional methods....|
|||...rform traditional methods with 2%-12% decrease in MAE, which further proves the superiority of deep saliency detection....|
|||...The most recent deep supervised saliency methods [11] [26] [40] can achieve the highest mean F-measure of 0.8970, and our unsupervised method without human annotations can achieve a mean F-measure of ...|
|||...age confidence map as pseudo ground truth to train an unsupervised deep model based on unsupervised saliency, which is quite different from our formulation of predicting saliency from unsupervised sal...|
|||...Qualitative Comparison Figure 4 demonstrates several visual comparisons, where our method consistently outperforms the competing methods, especially those four unsupervised saliency we used to train our model....|
|||...With proper noisy labels, we achieve the best results compared with both unsupervised saliency methods and deep saliency methods....|
|||...The fourth image is in very low-contrast, where most of the competing methods failed to capture the whole salient objects with the last penguin mis-detected, especially for those unsupervised saliency methods....|
|||...Ablation Studies:  In this paper, we propose to iteratively update the noise modeling module and the latent saliency prediction model to achieve accurate saliency detection....|
|||...As the two modules work collaboratively to optimize the overall loss function, it is interesting to see how the saliency prediction results evolves with respect to the increase of updating round....|
|||...5, we illustrate both the performance metric (MAE) with respect to updating round and an example saliency detection results....|
|||...Starting with the zero noise initialization, our method consistently improves the performance of saliency detection with the updating of noise modeling....|
|||...Conclusions  In this paper, we propose an end-to-end saliency learning framework without the need of human annotated saliency maps in network training....|
|||...We represent unsupervised saliency learning as learning from multiple noisy saliency maps generated by various efficient and effective conventional unsupervised saliency detection methods....|
|||...Our framework consists of a latent saliency prediction module and an explicit noise modeling models, which work collaboratively....|
|||...In the future, we plan to investigate the challenging scenarios of multiple saliency object detection and small salient object detection under our  r o r r  E e     t  l  u o s b A n a e M     0.2  0....|
|||...Salientshape: group saliency in image collections....|
|||...Visual saliency based on multiscale deep In Proc....|
|||...Hierarchical saliency detection....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||97 instances in total. (in cvpr2018)|
|4|Mai_Saliency_Aggregation_A_2013_CVPR_paper|...This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one....|
|||...Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images....|
|||...Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between ne...|
|||...Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods....|
|||...Introduction  Visual saliency measures low-level stimuli to the human vision system that grabs a viewers attention in the early stage of visual processing [17]....|
|||...There is a rich literature on image saliency analysis [1, 2, 46, 813, 15, 1720, 22, 2431, 33, 3545]....|
|||...Individual saliency methods, such as GC [6], FT [2], and CA [12], often complement each other....|
|||...(e) Ours  (d) CA  data-driven approaches to compute a saliency map from an image....|
|||...More interestingly, different saliency methods can often complement each other....|
|||...Therefore, the aggregation of these saliency analysis results can likely outperform each individual one, as reported in a recent study [5]....|
|||...Our method combines saliency maps from various methods with diverged properties and large performance gaps....|
|||...Specifically, we use a Conditional Random Field (CRF) framework [21] for  saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neig...|
|||...Therefore, saliency aggregation should be customized to each individual image....|
|||...First, our method considers the performance gaps among individual saliency analysis methods and better determines their contribution in aggregation....|
|||...Second, our method considers that the performance of each individual saliency analysis method varies over images and is able to customize an appropriate aggregation model to each input image....|
|||...As more and more saliency analysis methods have been developed recently, our research provides a way to best use the existing and forthcoming saliency methods and allows the possibility for pushing fo...|
|||...Saliency Aggregation  Our method starts from running a set of m saliency analysis algorithms, {Mi(cid:2)1  i  m}, on a given image I, and produces m saliency maps, {Si(cid:2)1  i  m}, one for each algorithm....|
|||...Each element Si(p) in a saliency map encodes the saliency value at pixel p. The saliency value in each map is normalized to [0, 1]....|
|||...Our goal is to take these m saliency maps as input and produce a final saliency map S. This section begins with the standard aggregation methods from previous work that use pre-defined combination fun...|
|||...Standard Saliency Aggregation  To serve as our baseline method, we first apply the combination strategies from [5] to saliency aggregation....|
|||...Given a set of m saliency maps {Si(cid:2)1  i  m} computed from an image I, the aggregated saliency value S(p) at pixel p of I is modeled as the probability  S(p) = P (yp = 1 S1(p), S2(p), .., Sm(p)) ...|
|||...(2)  1 log(x)  We used these standard aggregation methods to combine a range of saliency analysis methods and tested them on two public saliency benchmarks FT [2] and SS [31]....|
|||...Data-driven Saliency Aggregation  We observe that while various saliency analysis methods often complement each other, there are performance gaps among them....|
|||...Therefore, saliency aggregation should be individual method-aware and individual image-aware....|
|||...We design data-driven approaches to achieve such saliency aggregation....|
|||...2.2.1 Pixel-wise Aggregation  Our first method associates each pixel p with a feature vector x(p) = (S1(p), S2(p), , Sm(p)), where Si(p) is the saliency value at p in the saliency map Si....|
|||...We compute the final saliency value S(p) as the posterior probability P (yp = 1 x(p))....|
|||...denotes the sigmoid function  (z) =  1  1 + exp(z)  (4)  where Si(p) represents the saliency value of pixel p in the saliency map Si, yp is a binary random variable taking the  The parameter  can be l...|
|||....2     0  1  GBVS  IT  GC  EXP  LIN  LOG  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  (a) Standard saliency aggregation using the best-performed three individual methods on the FT benchmark (left) an...|
|||...Standard saliency aggregation using pre-defined combination functions....|
|||...can then appropriately account for the performance gaps among individual saliency methods....|
|||...2.2.2 Aggregation using Conditional Random Field  One potential problem with estimating the saliency value for each pixel individually is its ignorance of the interaction between neighboring pixels....|
|||...Our second method addresses this problem by modeling saliency estimation using binary Conditional Random Field (CRF) [21]....|
|||...for saliency analysis [24]....|
|||...Their method estimates saliency map directly using image features....|
|||...In contrast, our method uses CRF to aggregate saliency analysis results from multiple methods....|
|||...The saliency label of each pixel depends not only on its feature vector, but also the labels of neighboring pixels....|
|||...We define the feature function fd(xp, yp) based on only  the input saliency maps Si....|
|||...m(cid:2)  i=1  fd(xp, yp) =  iSi(p)yp + m+1yp  (6) where {i} is a subset of the CRF model parameters and Si(p) is the saliency value at pixel p in the saliency map Si....|
|||...)  The first component fe(xp, xq, yp, yq) encodes the observation that if two pixels have different saliency values according to an individual saliency method, they are likely to have different salien...|
|||...Particularly, if a pixel takes a high saliency value than its neighbor in an individual saliency map, it is also likely to take a more salient label after aggregation....|
|||...fc(xp, xq, yp, yq) follows the idea from [24] to incorporate the observation that neighboring pixels with similar colors should have similar saliency labels....|
|||...The saliency aggregation result for each pixel is the posterior probability of being labeled as salient, which is computed using a standard inference procedure according to the trained CRF aggregation...|
|||...2.2.3 Image-Dependent Saliency Aggregation  The above CRF-based saliency aggregation model considers the performance gaps among individual methods and captures the interaction between neighboring pixels....|
|||...It, however, does not consider the fact that the performance of a saliency analysis method varies over images....|
|||...y are applied to all the new images consistently without considering the performance variation of a saliency analysis method on individual images....|
|||...We can further improve the above global saliency aggregation method....|
|||...We train such an imagedependent saliency aggregation model based on the observation that a saliency analysis method has similar performances on similar images....|
|||...Specifically, given an input image, our method first finds its k nearest neighbors in the training set and then trains a saliency aggregation model using these k images....|
|||...Experiments  We experimented with our saliency aggregation approaches on two public image saliency benchmarks....|
|||...The first one is the FT image saliency benchmark from [2]....|
|||...This dataset contains 1000 images from [24] and includes a manually segmented saliency object mask for each image....|
|||...The second dataset is the Stereo Saliency dataset (SS) from [31]....|
|||...This dataset has 1000 stereoscopic images along with the manually segmented saliency masks....|
|||...In our experiments, we extract all the left images from the stereoscopic images along with their saliency masks and use them in the same way as the FT dataset....|
|||...For each image in the FT benchmark, we obtained ten saliency maps using saliency analysis methods, including IT [18], MZ [26], LC [46], GBVS [14], SR [15], AC [1], FT [2], HC [6], GC [6], and CA [12]....|
|||...Specifically, the saliency maps for MZ and AC methods were downloaded together with the FT benchmark [2]....|
|||...For the SS benchmark, we created eight saliency maps using the same set of methods as the FT benchmark except MZ and AC as their implementations are not available....|
|||...Specifically, for each image in the dataset, we use our pixel-wise saliency aggregation approach (PW) described  1132 1132 1132 1134 1134  (a) Input  (b) Ground truth  (c) GBVS  (d) HC  (e) GC  (f) C...|
|||...Image-dependent saliency aggregation....|
|||...in Section 2.2.1 to train the corresponding saliency aggregation model using the rest of the images in the dataset....|
|||...The trained model is then used to produce the aggregation saliency map....|
|||...For the image-dependent saliency aggregation method (CRFGIST) in Section 2.2.3, our method first finds an image its k = 50 nearest neighbors in the rest of the dataset, and  then uses these neighbors ...|
|||...Figure 4 shows that all of our three aggregation methods consistently outperform each individual saliency analysis method....|
|||...Precision-recall curves of our saliency aggregation approaches, including PW, CRF, and CRF-GIST....|
|||...One important reason is that some saliency analysis methods already consider the smoothness of saliency map....|
|||...For example, the GC method computes saliency values for regions, instead of pixels....|
|||...Robustness of Saliency Aggregation  We now examine how the individual saliency methods that are aggregated together affect our aggregation approaches....|
|||...In our first test,  we selected the three best-performed saliency methods and aggregated them together using our image-dependent CRF aggregation method....|
|||...These two tests show the capability of our aggregation approach in making the best use of individual saliency analysis methods....|
|||...To further examine the robustness of our approach, we add a random map as one of the basic saliency maps used in aggregation....|
|||...This faked saliency detector randomly assigns each pixel a saliency value in the range [0, 1] according to a uniform distribution....|
|||...We find that our aggregation results with/without this random saliency map are almost the same, as shown in the last column of Figure 6....|
|||...Discussions  Our experiments show that saliency aggregation can consistently improve the performance of each individual saliency analysis method....|
|||...Because aggregation is based solely on the saliency maps from individual methods, when all the individual methods fail to identify a salient region in an image, saliency aggregation will usually fail too....|
|||...On the other hand, our aggregation result can benefit from the progress of the research on individual saliency analysis methods....|
|||...Our image-dependent saliency aggregation method currently uses the GIST descriptor to find similar images to an input one....|
|||...Conclusion  In this paper, we presented data-driven approaches to saliency aggregation that integrate saliency analysis results from multiple individual saliency analysis methods....|
|||...We designed and discussed three saliency aggregation approaches....|
|||...We show the input images (a), the ground-truth (b), individual saliency maps (c  j), and our aggregation results using our image-dependent CRF aggregation method....|
|||...ter than each individual saliency method....|
|||...F-based approach that considers the interaction among pixels, the performance gaps among individual saliency analysis methods, and the dependent of saliency analysis on individual image, works the bes...|
|||...Our work provides a robust way to combine individual saliency analysis methods into a more powerful one....|
|||...Boosting bottom-up and top-down visual features  for saliency estimation....|
|||...Random walks on graphs to model saliency in images....|
|||...Effect of individual saliency maps on saliency aggregation....|
|||...The first column shows the precision-recall curves from aggregations using all the saliency maps and the best three ones....|
|||...The second column shows the aggregation results when one saliency map is removed at each time....|
|||...The third column shows the test when a random map is added into the aggregation process as a faked saliency map....|
|||...A saliency map in primary visual cortex....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Geodesic saliency using  background priors....|
|||...Top-down visual saliency via joint  crf and dictionary learning....|
||93 instances in total. (in cvpr2013)|
|5|Kuen_Recurrent_Attentional_Networks_CVPR_2016_paper|...Recurrent Attentional Networks for Saliency Detection  Jason Kuen, Zhenhua Wang, Gang Wang  School of Electrical and Electronic Engineering,  Nanyang Technological University....|
|||...{jkuen001,wzh,wanggang}@ntu.edu.sg  Abstract  input image  initial saliency map  re ned saliency map  Convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection....|
|||...Using spatial transformer and recurrent network units, RACDNN is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively....|
|||...Besides tackling the scale problem, RACDNN can also learn context-aware features from past iterations to enhance saliency refinement in future iterations....|
|||...Experiments on several challenging saliency detection datasets validate the effectiveness of RACDNN, and show that RACDNN outperforms state-of-the-art saliency detection methods....|
|||...Despite that it has been studied for years, saliency detection still remains an unsolved research problem due to its tough goal to model high-level subjective human perceptions....|
|||...Traditionally, methods in saliency detection leverage low-level saliency priors such as contrast prior and center prior to model and approximate human saliency....|
|||...on about the objects and its surroundings: the traditional methods are still very far away from how saliency works in the context of human perceptions....|
|||...To incorporate high-level visual concepts into a saliency detection framework, it is  Corresponding author  Figure 1....|
|||...An example of applying recurrent attention-based saliency refinement to an initial saliency map produced by convolutional-deconvolutional network....|
|||...Compared to the initial saliency map, the refined saliency map has significantly sharper edges and preserves more object details....|
|||...It is also the first learning algorithm to achieve human-competitive performances [18] in large-scale image classification task, which is a high-level vision task like saliency detection....|
|||...Although there have been works on developing CNNs for visual saliency modeling, they either focus on predicting eye fixations [31], or applying CNNs to predict just the saliency value of visual sub-units (e.g....|
|||...For this framework, the input is an image, and the output is its corresponding saliency map....|
|||...To overcome this limitation, we propose a recurrent attentional network (RACDNN) to refine the saliency maps generated by CNNDeCNN....|
|||...ss  3668  network units to iteratively attend to flexibly-sized image sub-regions, and refines the saliency predictions on those sub-regions....|
|||...Another advantage of RACDNN is that the attended sub-regions in the previous iterations can provide contextual information for the saliency refinement of the sub-region in the current iteration....|
|||...For example, in Figure 1, RACDNN can make use of the more visible front legs of the deers to help at refining the saliency values of the less-visible back legs....|
|||...We perform experiments on several challenging saliency detection benchmark datasets, and compare the proposed method with state-of-the-art saliency detection methods....|
|||...Instead of manually defining and tuning saliencyspecific features, these methods can learn both low-level features and high-level semantics useful for saliency detection straight from minimally proces...|
|||...However, these works employ neither attention mechanism nor RNN to improve saliency detection....|
|||...To the best of our knowledge, ours is the first work to exploit recurrent attention along with deep learning for saliency detection....|
|||...ism used in our work is much more complex, as it is tied with a large CNN-DecNN for dense pixelwise saliency refinement....|
|||...Proposed Method  In this section, we describe our proposed saliency detection method in detail....|
|||...In our method, initial saliency maps are first generated by a convolutional-deconvolutional network (CNN-DecNN) which takes entire images as input, and outputs saliency maps....|
|||...The saliency maps are then refined iteratively via another CNN-DecNN operated under a recurrent attentional framework....|
|||...Unlike the initial saliency map prediction which is done through single feedforward passes on the entire images, the saliency refinement is done locally on selected image sub-regions in a progressive way....|
|||...The attentional saliency refinement helps to alleviate the inability of CNNDecNN to deal with multiscale saliency detection....|
|||...from past iterations to enhance the representation of the attended sub-region, hence to improve the saliency detection performance....|
|||...Then, z is transformed to a raw saliency map r through the DecNN, as r = DecNN(z)....|
|||...To obtain the final saliency map  S that lies within the probability range of [0, 1], we perform  S = (r), passing the raw saliency map r into element-wise sigmoid activation function ()....|
|||...The resulting network can be trained in end-toend fashion to perform saliency detection....|
|||...Furthermore, long-distance contextual information which is important for saliency detection, cannot be well captured by the locally applied convolution filters in DecNN....|
|||...Attentional Inputs and Outputs with Spatial  Transformer  To realize the attention mechanism for saliency refinement, we adopt the spatial transformer network proposed in [22]....|
|||...However, our saliency refinement method (see Section 3.3) via DecNN demands that the input and output ends point to the same image sub-region....|
|||...To this end, we propose an inverse spatial transformer which can map refined saliency output back to the same sub-region attended at input end....|
|||...As illustrated in Figure 4, given an intiail saliency map produced by the initial CNN-DeCNN, RACDNN iteratively uses spatial transformer to attend to a sub-region, and applies its CNN-DecNN to perform...|
|||...Since RACDNN is attentional, the already attended sub-regions can help to guide saliency refinement for the upcoming sub-regions....|
|||...This is beneficial for the task of saliency detection, as the saliency of an object is highly dependable on its surrounding regions....|
|||...3671  initial saliency mapCNNCNNCNNDecNNDecNNDecNNSpatial Transformerinput imageSpatial TransformerSpatial TransformerInverse Spatial Transformerrefined saliency mapRNN Layer 1RNN Layer 2CNNCNNInverse...|
|||...Instead of replacing the values of initial saliency map with the output of RACDNN at each iteration, the initial saliency map r0 is refined cumulatively for N number of iterations....|
|||...At iteration i, the saliency map ri is refined as  ri = ri1 + ST (DecNNr(h1  i1),  1  i  )  (7)  Before being added to ri, the saliency output of DecNNr is spatially transformed back to the attended s...|
|||...For the unattended regions, the saliency refinement values are set as zero and thus those regions do not affect ri....|
|||...After N number of iterations, as in Section 3.1, sigmoid activation function () is applied to rN , resulting in the final saliency map  Sr....|
|||...Besides saliency refinement outputs, at every iteration, RACDNN should generate  to determine which sub-region to attend to in the next iteration....|
|||...0, h2  Similar to the CNN-DecNN used for saliency detection, the loss function of RADCNN is the binary cross-entropy between the final saliency output  Sr and the groundtruth saliency map  G....|
|||...Implementation Details  For initial saliency detection, we use a CNN-DecNN independent from the CNN-DecNN used in the saliency refinement stage....|
|||...At the end of the initial CNN-DecNN, the DecNN outputs a 56  56 saliency map....|
|||...The output size of 56  56 achieves a good balance between computational complexity and saliency pixels details....|
|||...For performance evaluation, the 56  56 saliency map is resized to the input images original size....|
|||...The number of recurrent iterations of RACDNN (inclusive of the 0-th iteration) is set to 9 for all saliency detection experiments....|
|||...Most of the saliency detection methods employ object segmentation techniques which can output image segments with consistent saliency values within each segment....|
|||...However, most of the saliency detection datasets are too small....|
|||...Here, we follow the dataset procedure in one recent deep learningbased saliency detection work [49]....|
|||...We train the deep models (initial CNN-DecNN and RADCNN) in our proposed method on saliency datasets different from the datasets used for experimental evaluations....|
|||...Although the training set is considered large in saliency detection context, it is still small for deep learning methods, and may cause overfitting....|
|||...HKUIS [29] is a recently released saliency detection dataset with 4,447 annonated images....|
|||...ECSSD [41] is a challenging saliency detection dataset with many semantically meaningful but structurally complex images....|
|||...SED2 [2] is a small saliency dataset having only 100 images....|
|||...We evaluate the proposed method based on precisionrecall curves, which is the most commonly used evaluation metric for saliency detection....|
|||...The saliency output is thresholded at integer values within the range of [0, 255]....|
|||...At each threshold value, the binarized saliency output is compared to the binary groundtruth mask to obtain a pair of precision-recall values....|
|||...Another popular evaluation metric for saliency detection is F-measure, which is a combination of precision and recall values....|
|||...Following the recent saliency detection benchmark paper [4], we use a weighted F-measure F that favors precision more than recall: (1+2)PrecisionRecall , where 2 is set as 0.3....|
|||...2Precision+Recall  Even though F-measure is the most commonly used evaluation metric for saliency detection, it is not comprehensive enough as it does not consider true negative saliency labeling....|
|||...s given    S(n, m)   G(n, m) , where W and H  by:  W  H  1  WH  Pn=1  Pm=1  are width and height of saliency map;  S is the real-valued saliency map output normalized to the range of [0, 1], and  G is...|
|||...Saliency map binarization is not needed in MAE as it measures the mean of absolute differences between groundtruth saliency pixels and given saliency pixels....|
|||...Datasets and Evaluation Metrics  We evaluate our proposed on a number of challenging saliency detection datasets: MSRA10K [9] is by far the largest publicly available saliency detection dataset, conta...|
|||...Compared to the proposed method, the baseline CNN-DecNN has no recurrent attention mechanism to perform iterative saliency refinement....|
|||...By removing the recurrent connections, NACDNN cannot learn context-aware features useful for saliency refinement despite having attention mechanism....|
|||...This shows that the RACDNN can help to improve the saliency map outputs of CNN-DecNN, using a recurrent attention mechanism to alleviate the scale issues  of CNN-DecNN, and to learn region-based conte...|
|||...However, due to the lack of recurrent connections, NRACDNN is inferior to RACDNN because it does not exploit contextual information from past iterations for saliency refinement....|
|||...e baseline methods, we compare the proposed method CNN-DecNN + RACDNN with several state-of-the-art saliency detection methods: RRWR [28], BSCA [39], DRFI [24], RBD [50], DSR [30], MC [23], and HS [41...|
|||...DRFI, RBD, DSR, MC, and HS are the top-performing methods evaluated in [4], while RRWR and BSCA are two very recent saliency detection works....|
|||...Qualitative saliency results of some evaluated images....|
|||...From the leftmost column: input image, saliency groundtruth, the saliency output maps of our proposed method (CNN-DecNN + RACDNN) with mean-shift post-processing, MCDL [49], MDF [29], RRWR [28], BSCA ...|
|||...compute the curves based on the saliency maps generated by the proposed method....|
|||...To further evaluate the proposed method CNN-DecNN + RACDNN, we compare it with two recent deep learningbased saliency detection methods (MCDL [49] and MDF [29]) on HKUIS, ECSSD, and SED2 datasets....|
|||...Conclusion  and  attention  In this paper, we introduce a novel method of using recurrent convolutional-deconvolutional network to tackle the saliency detection problem....|
|||...Still, the performance of proposed method may be limited by the quality of the initial saliency maps....|
|||...To overcome such limitation, the recurrent attentional network can be potentially revamped to detect saliency from scratch in end-to-end manner....|
|||...Salientshape: group saliency in image collections....|
|||...Visual saliency based on multiscale deep features....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
|||...Regionbased saliency detection and its application in object recognition....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Top-down visual saliency via joint crf and dictionary learning....|
||88 instances in total. (in cvpr2016)|
|6|Wang_Deep_Networks_for_2015_CVPR_paper|...of Technology OMRON Corporation University of California at Merced  Abstract  This paper presents a saliency detection algorithm by integrating both local estimation and global search....|
|||...In the local estimation stage, we detect local saliency by using a deep neural network (DNN-L) which learns local patch features to determine the saliency value of each pixel....|
|||...The estimated local saliency maps are further refined by exploring the high level object concepts....|
|||...In the global search stage, the local saliency map together with global contrast and geometric information are used as global features to describe a set of object candidate regions....|
|||...The final saliency map is generated by a weighted sum of salient object regions....|
|||...First, local features learned by a supervised scheme can effectively capture local contrast, texture and shape information for saliency detection....|
|||...Second, the complex relationship between different global saliency cues can be captured by deep networks and exploited principally rather than heuristically....|
|||...Existing methods mainly formulate saliency detection by a computational model in a bottom-up fashion with either a local or a global view....|
|||...(b) Ground truth saliency maps....|
|||...(c) Saliency maps by a local method [13]....|
|||...(d) Saliency maps by a global method [7]....|
|||...cterized by holistic rarity and uniqueness, and thus help detect large objects and uniformly assign saliency values to the contained regions....|
|||...lor histograms and other handcrafted features are utilized in a simple and heuristic way to compute saliency maps....|
|||...(b) Local saliency map (Section 3.1)....|
|||...(g) Final saliency map (Section 4.2)....|
|||...In this paper, we propose a novel saliency detection algorithm by combining local estimation and global search (LEGS) to address the above-mentioned issues....|
|||...In the local estimation stage, we formulate a deep neural network (DNN) based saliency detection method to assign a local saliency value to each pixel by considering its local context....|
|||...The saliency maps generated by DNN-L are further refined by exploring the high level objectness (i.e., generic visual information of objects) to ensure label consistency and serve as local saliency me...|
|||...These extracted feature vectors are used to train another deep neural network, DNN-G, to predict the saliency value of each object candidate region from a global perspective....|
|||...The final saliency map is generated by the sum of salient object regions weighted by their saliency values....|
|||...However, the use of DNNs in saliency detection is still limited, since DNNs, mainly fed with image patches, fail to capture the global relationship of image regions and maintain label consistency in a...|
|||...Our main contribution addresses these issues by proposing an approach to apply DNNs to saliency detection from both local and global perspectives....|
|||...The proposed DNN-G can effectively detect global salient regions by using various saliency cues through a supervised learning scheme....|
|||...Related Work  In this section, we discuss the related saliency detection methods and their connection to generic object proposal methods....|
|||...In [11], the saliency values are measured by the equilibrium distribution of Markov chains over different feature maps....|
|||...On the other hand, global methods detect saliency by using holistic contrast and color statistics of the entire image....|
|||...[1] estimate visual saliency by computing the color difference between each pixel with respect to its mean....|
|||...[24] propose a set of features from both local and global views, which are integrated by a conditional random field to generate a saliency map....|
|||... 1x1  contrast measures based on the uniqueness and spatial distribution of regions are defined for saliency detection....|
|||...[40] propose a multi-layer approach to analyze saliency cues....|
|||...A random forest based regression model is proposed in [16] to directly map regional feature vectors to saliency scores....|
|||...[42] present a background measurement scheme to utilize boundary prior for saliency detection....|
|||...Although significant advances have been made, most of the abovementioned methods integrate hand-crafted features heuristically to generate the final saliency map, and do not perform well on challenging images....|
|||...In [2], saliency is utilized as objectness measurement to generate object candidates....|
|||...In [23], a random forest model is trained to predict the saliency score of an object candidate....|
|||...In this work, we propose a DNN-based saliency detection method combining both local saliency estimation and global salient object candidate search....|
|||...Since DNNs mainly take image patches as inputs, they tend to fail in capturing long range label dependencies for scene parsing as well as saliency detection....|
|||...We propose to utilize DNNs in both local and global perspectives for saliency detection, where the DNN-L estimates local saliency of each pixel and the DNN-G searches for salient object regions based ...|
|||...By incorporating object level concepts into local estimation, we present a refinement method to enhance the spatial consistency of local saliency maps....|
|||...DNN based Local Saliency Estimation  Architecture of DNN-L....|
|||...To label the training patches, we mainly consider the ground truth saliency values of their central pixels as well as the overlaps between the patches and the ground truth saliency mask....|
|||...At test stage, we apply DNN-L in a sliding window fashion to the entire image and predict the probability P (l = 1 ) for each pixel as its local saliency value....|
|||...Figure 4(c) demonstrates the generated local saliency maps....|
|||...Both Figure 3 and Figure 4 show that the proposed local estimation method can effectively learn, rather than design, useful features characterizing local saliency by training DNN-L with local image patches....|
|||...Refinement  The local estimation method detects saliency by considering the color, contrast and texture information within a  (a)  (b)  (c)  Figure 3....|
|||...(b) Input image (top) and the local saliency map (bottom) generated by DNN-L. (c) Output feature maps of the first layer by applying DNN-L to the input image in a sliding window manner....|
|||...(c) Local saliency maps predicted by DNN-L. (d) Local saliency maps after refinement....|
|||...On the other hand, saliency is closely correlated with the object-level concepts, i.e., interesting objects easily attract human attention....|
|||...Based on this observation, we propose to refine the local saliency map by combining low level saliency and high level objectness....|
|||...Figure 4 shows the local saliency maps before and after refinement....|
|||...However, these saliency cues are considered independently, and combined based on heuristics....|
|||...Instead, we formulate a DNN-based regression method for saliency detection, where various saliency cues are considered simultaneously and their complex dependencies are learned automatically through a...|
|||...For each input image, we first detect local saliency using the proposed local estimation method....|
|||...The proposed deep network DNN-G takes the extracted features as inputs and predicts the saliency values of the candidate regions through regression....|
|||...Top row (left to right): source image, ground truth, local saliency map output by DNN-L, local saliency map after refinement....|
|||...To determine the confidence of each segment, we mainly consider two measurements based on the local saliency map, accuracy score A and coverage score C, defined by  Ai = Px,y Oi(x, y)  SL(x, y)  Px,y ...|
|||...Geometric information and local saliency measurements of object regions....|
|||...Geometric Information  Local Saliency Measurement  Feature  Definition  Feature  Definition  Feature  Definition  g1 g2 g3  g4  g5  Bounding box aspect ratio  Bounding box height Bounding box width Ce...|
|||...Local saliency measurements evaluate the saliency value of each candidate region based on the saliency map produced by the local estimation method....|
|||...Given the refined local saliency map and the object candidate mask, we compute the accuracy score A and the coverage score C using (2)-(3)....|
|||...The overlap rate between the object mask and the local saliency map is also computed (See Table 3 for details)....|
|||...The final saliency map is computed by a weighted sum of the top K candidate masks,  1 , ....|
|||...thod utilizes DNNs to learn the complex dependencies among different visual cues and determines the saliency of a candidate region in a global view, whereas [10] applies DNN to a bounding box to extra...|
|||...Both [16] and [23] use random forests to predict region saliency based on regional features, where [23] trains the model for each data set....|
|||...In contrast, we use DNNs for saliency detection and conduct training in one  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0    0     1     0.35     0.25     1  Foreground Background  0.2  0.4  0.6  0.8  1 ...|
|||... feature spaces, including global contrast features (c1 and c50), geometric information (g4), local saliency measurements (s1 and s2) and the global confidence scores (conf G) generated by DNN-G.  Tab...|
|||...Global search is integrated with local estimation in our work, which facilitates more robust saliency detection from both perspectives....|
|||...The MSRA-5000 data set is widely used for saliency detection and covers a large variety of image contents....|
|||...The PASCAL-S data set is arguably one of the most challenging saliency data sets without various design biases (e.g., center bias and color contrast bias)....|
|||...All the data sets contain manually annotated ground truth saliency maps....|
|||...The precision and recall of a saliency map are computed by segmenting a salient region with a threshold, and comparing the binary map with the ground truth....|
|||...The PR curves demonstrate the mean precision and recall of saliency maps at different thresholds....|
|||...The F-measure is defined as F = (1+ 2)P recisionRecall , where P recision and Recall are obtained using twice the mean saliency value of saliency maps as the threshold, and 2 is set to 0.3....|
|||...The MAE is the average per-pixel difference between saliency maps and the ground truth....|
|||...Feature Analysis  Our global search method exploits various saliency cues to describe each object candidate....|
|||...PR curves of saliency detection methods on four benchmark data sets....|
|||...Our global search method trains a deep network to learn complex feature dependencies and achieves accurate confidence scores for saliency detection....|
|||...We use either the implementations or the saliency maps provided by the authors for fair comparison3....|
|||...Figure 7 shows that our method generates more accurate saliency maps in various challenging scenarios....|
|||...The robust performance of our method can be attributed to the use of DNNs for complex feature and model learning, and the integration of local/global saliency estimation....|
|||...Conclusions  In this paper, we propose DNNs for saliency detection by combining local estimation and global search....|
|||...In the global search stage, the proposed DNN-G effectively exploits the complex relationships among global saliency cues and predicts the saliency value for each object region....|
|||...Our method integrates low level saliency and high level objectness through a supervised DNN-based learning schemes....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Visual saliency detection based on  bayesian model....|
|||...Bayesian saliency via low and mid level cues....|
|||...Hierarchical saliency detec tion....|
||87 instances in total. (in cvpr2015)|
|7|Cholakkal_Backtracking_ScSPM_Image_CVPR_2016_paper|...g Technological University Singapore {hisham002, jubin001, asdrajan}@ntu.edu.sg  Abstract  Top-down saliency models produce a probability map that peaks at target locations specified by a task/goal su...|
|||...We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence/absence of an object in an image....|
|||...First, the probabilistic contribution of each image patch to the confidence of an ScSPM-based classifier produces a Reverse-ScSPM (R-ScSPM) saliency map....|
|||...Neighborhood information is then incorporated through a contextual saliency map which is estimated using logistic regression learnt on patches having high R-ScSPM saliency....|
|||...Both the saliency maps are combined to obtain the final saliency map....|
|||...We evaluate the performance of the proposed weakly supervised top-down saliency and achieves comparable performance with fully supervised approaches....|
|||...Introduction  A saliency map can be thought of as a probability map in which the probabilities of pixels belonging to salient regions are mapped to intensities....|
|||...Bottom-up saliency aims to locate regions in an image that capture human fixations within first few milliseconds after the stimulus is presented [4, 14]....|
|||...For example, the saliency maps produced by recent bottom-up approaches [10, 39] cannot discriminate between bus, person and bicycle in Fig....|
|||...Comparison of our top-down saliency with bottom-up methods....|
|||...(a) Input image, (b) saliency map of [10], (c) [39]; proposed top-down saliency maps for (d) bus (e) bicycle and (f) person categories....|
|||...The top-down saliency models produce a probability map that peaks at target/object locations [5, 36]....|
|||...Our objective is to generate top-down saliency maps like those shown in Fig....|
|||...Most methods for top-down saliency detection learn object classes in a fully supervised manner, where an exact object annotation is available....|
|||...The learning component enables either discrimination between object classes or generation of saliency models of objects....|
|||...The top-down saliency method using WSL [25] employs iterative refinement of object hypothesis on a training image....|
|||...The proposed weakly supervised top-down saliency approach (Fig....|
|||...2 (e)) does not require these iterative steps, but produces a saliency map that is even comparable to fully supervised approaches as shown in Fig....|
|||...Our weakly supervised top-down saliency map in comparison with fully supervised methods....|
|||...(a) Input images, person and bicycle saliency maps of (b) [36], (c) [20], (d) [5] and (e) proposed method are shown in row 1 and row 2 respectively....|
|||...The patches having high R-ScSPM saliency are generally from object regions, but they lack contextual information....|
|||...Hence, we incorporate a contextual saliency module that computes the probability of object presence in a patch using logistic regression trained on contextual max-pooled vectors [5]....|
|||...The training of contextual saliency needs a set of positive patches from the object region and a set of random negative patches from images that do not contain the object....|
|||...Since a patch-level annotation is not available, we use patches from the positive training images having high R-ScSPM saliency to train the contextual saliency....|
|||...The contextual saliency inferred on a test image is combined with the R-ScSPM saliency to form the final saliency map....|
|||...R-ScSPM saliency considers the spatial location of patch through backtracking max-pooled vector whereas contextual saliency considers its spatial neighborhood information, thereby complementing one another....|
|||...Besides illustrating the accuracy of saliency maps produced by the proposed method, we demonstrate its effectiveness in applications like weakly supervised object annotation and class segmentation....|
|||...[17] proposed a top-down saliency approach which uses object appearance cues along with location prior....|
|||...Closer to our framework, [36] proposed a fully supervised top-down saliency model that jointly learns a conditional random field (CRF) and dictionary using sparse codes of SIFT features as latent variables....|
|||...In [32], the task of image classification is improved using discriminative spatial saliency to weight visual features....|
|||...The use of weak supervision in top-down saliency has largely been left unexamined....|
|||...classifier and top-down saliency is used for object categorization by sampling representative windows containing the object....|
|||...A saliency model is proposed in [33] to address the object annotation task [26, 34]....|
|||...Illustration of our R-ScSPM saliency estimation and patch selection for dog category....|
|||...For a patch A  , R-ScSPM saliency P (rA, cA, pA) is evaluated by setting the sparse codes of all patches except A to ~0 forming UA followed by a scalar product () with the classifier weight W ....|
|||...We then introduce contextual saliency (Sec....|
|||...Training of contextual saliency requires object patches that are selected using the R-ScSPM saliency map....|
|||...3.3), our framework combines both the saliency maps to generate the final saliency map....|
|||...3.1.2 R-ScSPM saliency formulation  In an ScSPM image classifier, both the linear-SVM and multi-scale max-pooling operations can be traced back to the patch level....|
|||...This enables us to analyze the contribution of each patch towards the final classifier score which is then utilized to generate the R-ScSPM saliency map for an object....|
|||...The probability of a patch m belonging to an object, which in turn indicates the saliency G of the object, depends on the three parametersrm, cm and pm as  G = P (rm, cm, pm) = P (pm rm, cm)P (cm rm)P (rm)....|
|||...The R-ScSPM saliency of a patch m is given by  P (rA, cA, pA) =(W F (~0.., uA, ..,~0) + b if A  ,  otherwise....|
|||...Contextual saliency training  The purpose of contextual saliency is to include neighborhood information of a patch....|
|||...From positive training images, patches with RScSPM saliency G > 0.5 are selected as positive patches with label l = +1, while random patches are selected from negative images with patch label l = 1....|
|||...A logistic regression model with weight v and bias bv is learned using positive and negative patches from the training images to form the contextual saliency model [5]....|
|||...wed by logistic regression learning is the only additional computation required for this contextual saliency model....|
|||...Saliency Inference  On a test image, the contextual saliency L is inferred  using the logistic regression by  P (l = 1   , v) =  1  1 + exp((vT  + bv)  ,  (10)  where P (l = 1   , v) indicates the pro...|
|||...For each patch, the contextual and RScSPM saliency values are combined as GL + 0.5(G + L) and normalized to values in [0, 1] to form the saliency S. We choose this combination criteria instead of a pr...|
|||...The saliency map is refined using the same image classifier used for R-ScSPM saliency having SVM parameters (W, bias)....|
|||...(a) Input image, (b) patches in  and (c) patches selected by thresholding (d) R-ScSPM saliency map....|
|||...(e) contextual saliency map and (f) final saliency map....|
|||...However, if W Z > thO, the saliency of the patch is retained as S. Pixel-level saliency maps are generated from patch-level saliency S using Gaussian-weighted interpolation as in [5]....|
|||...Experimental evaluation  We evaluate the performance of our weakly supervised top-down saliency model on 5 challenging datasets across three applications....|
|||...We compare with other topdown saliency approaches using Graz-02 [27] and PASCAL VOC-07 [6] segmentation datasets....|
|||...4(b) and the R-ScSPM saliency map is shown in Fig....|
|||...This saliency map is thresholded at 0.5 to obtain the most relevant patches weeding out the false detection in  as shown in Fig....|
|||...4(e)) and RScSPM saliency maps....|
|||...At non-textured patches of the car (top), the lower R-ScSPM saliency is boosted by high con 5282  Table 1....|
|||...textual saliency in the final saliency map....|
|||...The smearing of saliency in the contextual saliency map for small objects (bottom) is removed when combined with the R-ScSPM saliency map....|
|||...Table 1 analyzes the contribution of each component of the proposed saliency model on Graz-02 dataset (dataset details are in Sec....|
|||...The contribution of R-ScSPM saliency is studied by removing the contextual saliency component from the framework....|
|||...The results are poorer compared to R-ScSPM trained contextual saliency due to lack of contextual information....|
|||...Our complete framework gives 60.52% which shows that both the contextual and R-ScSPM saliency maps complement one another....|
|||...R-ScSPM utilizes the patch location and contextual saliency utilizes its neighborhood information and hence they complement each other....|
|||...We analyze the influence of negative patches extracted from positive images on the performance of contextual saliency using different supervision settings in Graz-02....|
|||...The contextual saliency model is learned using the additional label information....|
|||...First, pixellevel results of the proposed saliency model and recent topdown saliency models [36, 20, 5] are evaluated on all 600 test images of the dataset....|
|||...Table 2 compares our pixel-level results with recent topdown saliency approaches [5, 36, 20] and related object segmentation and localization approaches [2, 24]....|
|||...As expected, recent bottomup saliency model [39] produces poor performance when compared to our result....|
|||...Table 3 compares the patch-level precision at EER of the proposed saliency model on 300 test images with other representative patch-level methods....|
|||...Knowledge about object presence inferred from the classifier refinement helped the saliency map to outperform [36, 5] in classes like aeroplane and train....|
|||...By simple thresholding at EER, the proposed saliency maps outperform segmentation approaches [11, 2]....|
|||...Applications of the proposed saliency maps for (a) class segmentation, (b) object annotation and (c) action-specific patch selection....|
|||...In our framework, all the saliency models share a common sparse code and contextual max-pooled vector....|
|||...Applications  4.6.1 Object class segmentation  The saliency maps obtained for a particular class are thresholded as in [13] followed by Grab-Cut [28]....|
|||...We generated rectangular boxes from our saliency maps using coherent sampling [33] to annotate PASCAL VOC-07 detection images....|
|||...Conclusion  In this paper, a weakly supervised top-down saliency approach is presented that requires just a binary label indicating the presence/absence of the object in an image for training....|
|||...A novel R-ScSPM framework produces a saliency map that enables selection of representative patches for contextual saliency which is shown to improve the final saliency map....|
|||...Extensive experimental evaluations show that the proposed method performs comparably with that of fullysupervised top-down saliency approaches....|
|||...Analysis of scores, datasets, and  models in visual saliency prediction....|
|||...Sun: Top-down saliency using natural statistics....|
|||...Top-down visual saliency via joint  crf and dictionary learning....|
||83 instances in total. (in cvpr2016)|
|8|Xin_Li_Contour_Knowledge_Transfer_ECCV_2018_paper|...fferent tasks: 1) predicting contours with the original contour branch, and 2) estimating per-pixel saliency score of each image with the newlyadded saliency branch....|
|||...In this scheme, the contour branch generates saliency masks for training the saliency branch, while the saliency branch, in turn, feeds back saliency knowledge in the form of saliency-aware contour la...|
|||...Keywords: Saliency detection  deep learning  transfer learning  1  Introduction  Salient object detection, which aims at locating the most visually conspicuous object(s) in natural images, is critical...|
|||...Saliency maps produced by currently best deep saliency models (DSS [8], UCF [38], and Amulet [37]) and ours....|
|||...On the other hand, saliency knowledge helps remove background clutter, and thus improves contour detection results....|
|||...Our goal is to convert a trained contour detection model (CEDN) [35] into a saliency detection model without using any manually labeled salient object masks....|
|||...Then, we employ the well-trained contour branch to generate contour maps for all images and use a novel contour-to-saliency transferring method to produce the corresponding saliency masks....|
|||...The newly-added branch is trained under the strong supervision of these automatically generated saliency masks....|
|||...After that, the trained branch in turn transfers the learned saliency knowledge,  OursInputAmuletGTUCFDSSContour Knowledge Transfer for Salient Object Detection  3  in the form of saliency-aware cont...|
|||...ranches (i.e., Contour-toSaliency procedure and Saliency-to-Contour procedure), becoming a powerful saliency detection model, where one branch focuses on salient object contour detection and the other...|
|||...Despite not using manually annotated salient object labels for training, our proposed method is capable of generating a reliable saliency map for each input (See Fig....|
|||... salient object detection by automatically converting a well-trained contour detection model into a saliency detection model, without requiring any groudtruth salient object labels....|
|||...Therefore, the results generated by the well-trained contour branch can be used to generate reliable saliency masks for training the saliency branch....|
|||...Although these methods can produce accurate saliency maps in most simple cases, they are unable to deal with complex images due to the lack of semantic knowledge....|
|||...These methods can be categorized into two groups: region-based methods and pixel-wise saliency prediction methods....|
|||...Region-based methods predict saliency score in a region-wise manner....|
|||...[39] integrate both global and local context into a multi-context CNN framework for saliency detection....|
|||...In [13], a multi-layer fully connected network is proposed for estimating the saliency score of each super pixel....|
|||...[28] proposed the integration of both local estimation and global search for patch-wise saliency score estimation....|
|||...To overcome these drawbacks, pixel-wise saliency prediction methods directly map an input image to the corresponding saliency map by using a trained deep Fully Convolutional Network (FCN)....|
|||...[30] proposed a recurrent FCN to encode saliency prior knowledge for salient object detection....|
|||...Notable previous attempts at detecting salient object(s), while using no saliency mask for training, are Weakly Supervised Saliency (WSS) [29] and Supervision by Fusion (SBF) [37] methods....|
|||...WSS takes advantage of image-level tags to generate pixel-wise annotations for training a deep saliency model....|
|||...SBF trains the desirable deep saliency model by automatically generating reliable supervisory signals from the fusion process of weak saliency models....|
|||...In the contour-to-saliency procedure, we use the generated saliency masks to train the newly-added saliency branch....|
|||...into an accurate deep saliency detection model without using any manually labeled saliency mask....|
|||...In this architecture, the original contour branch and the newly-added saliency branch share the same feature extractor (or encoder)....|
|||...The feature extractor and contour branch are initialized using CEDN, and the saliency branch is randomly initialized....|
|||...Then, we train the saliency branch and update the parameters of the contour branch on two different unlabeled image sets through a novel alternating training pipeline....|
|||...These generated masks are used to simulate strong supervision over the saliency branch....|
|||...With cross-domain connections (the dashed line), the saliency branch is naturally capable of consolidating both saliency and contour knowledge....|
|||...We update the network by grafting a new decoder for saliency detection onto the original encoder....|
|||...By doing this, our C2S-Net is made of three major components: encoder (fenc), contour decoder (fcont) and saliency decoder (fsal)....|
|||...tations from an input image, the contour decoder identifies contours of the salient region, and the saliency decoder estimates the saliency score of each pixel....|
|||...The saliency decoder fsal share the same encoder fenc with the contour decoder fenc....|
|||...Similarly, it takes the feature map Fi as input and produces a single-channel saliency map S(Fi, s), where s is the model parameter of saliency decoder....|
|||...Because salient object detection is a more difficult task than contour detection, we add another convolutional layer in each saliency decoder group....|
|||...The objective of the saliency branch is to minimize the per-pixel error between the ground-truths and estimated saliency maps....|
|||...Specifically, in the saliency decoder stage, the feature learning of the second from contour branch si of its previous layer....|
|||...Therefore, the second si on the i-th level in the saliency branch is formally  convolutional layer encodes both the learned features f cont and the convolutional features f sal convolutional feature m...|
|||...hem to generate corresponding salient object masks, so as to simulate strong human supervision over saliency branch training....|
|||...B  C  (4)  where S() is the data term that encourages the selection of region proposals with a higher saliency score....|
|||...Update of contour labels and saliency masks....|
|||...In addition, the saliency map obtained in the previous stage provides useful prior knowledge....|
|||...Therefore, we also use it to estimate the saliency score of a given region mask....|
|||...Formally, the saliency score of each region proposal can be formally written as:  Si = K(cnt(bi), C er ) +   K(bi, Ser )  (6)  where cnt(bi) denotes a function that extracts contour map from a given r...|
|||...C er and Ser denote the detected contour and saliency map after the r-th training epoch, respectively....|
|||...As the parameters of saliency branch are randomly initialized and our network cannot generate saliency maps at the very beginning, we set the combination weight  = 0 in the first epoch, and  = 1 in th...|
|||...3.4 Alternating Training  Our C2S-Net has three important components: encoder fenc, contour decoder fcont and newly-added saliency decoder fsal....|
|||...To avoid the poor local optimum problem, we use two different sets of unlabeled images (M and N ) to interactively train the saliency branch and contour branch....|
|||...After that, we use the proposed contour-to-saliency transfer method to produce salient object masks Lsal as training samples for updating the saliency decoder parameters s....|
|||...In the saliency-to-contour procedure, we fix the network parameters e and s , and use the learned C2S-Net to generate both contour maps and saliency maps....|
|||...On one hand, the contour branch is able to learn saliency knowledge, and thus it can focus more on the contours of those attention-grabbing objects....|
|||...More importantly, the training samples generated by saliency branch are not limited to a small number of predefined categories....|
|||...Therefore, the contour branch can learn saliency properties from a large set of images to detect contours of unseen objects....|
|||...The parameters of saliency decoder are initialized randomly....|
|||...During testing, the input RGB image is forwarded through our C2S-Net to generate a saliency map with the same size as the output....|
|||...MAE is defined as the average pixel-wise absolute difference between the ground-truth mask and estimated saliency map....|
|||...me encoder, our CDC enables the proposed model to better explore the intrinsic correlations between saliency detection and contour detection, and results in a better performance....|
|||...Here, we take three different approaches to generate saliency masks for training our model....|
|||...Specifically, for AVG-P, we first simply take an average of all proposals (generated from detected contours) to form a saliency map for each image, and then use SalCut [3] to produce its salient object mask....|
|||...The third, fourth and fifth lines of Tab.1 show the corresponding results of using AVG-P, WTA and CTS to generate saliency masks for training our C2S-Net, respectively....|
|||...This is because the estimated saliency masks and contour maps have already become reliable enough after three AT iterations....|
|||...In addition, to show the superiority of our alternating training scheme, we use the same 10K images with estimated labels (including both saliency and contour labels) to train our C2S-Net....|
|||...One loss is for contour branch and another loss is for saliency branch....|
|||...This indicates that data size is a big influencing factor for saliency models performance....|
|||...parison to Other Methods  We compare the proposed method with nine top-ranked fully deep supervised saliency detection models including MC [39], MDF [13], DS [19], ELD [12], DHS [23], DCL [15], DSS [8...|
|||... 10K training images) consistently outperforms the exsiting weakly supervised and unsupervised deep saliency models with a large margin, and compares favorably with the top-ranked fully supervised dee...|
|||...To bridge the gap between contours and salient object regions, we propose a novel transferring method that can automatically generate a saliency mask for each image from its contour map....|
|||...These generated masks are then used to train the saliency branch of C2SNet....|
|||...Extensive experiments on five datasets show that our method surpasses the current top saliency detection approaches....|
|||...Lee, G., Tai, Y.W., Kim, J.: Deep saliency with encoded low level distance map  and high level features....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...Li, X., Yang, F., Chen, L., Cai, H.: Saliency transfer: An example-based method  for salient object detection....|
|||...Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object  detection....|
|||...Ramanishka, V., Das, A., Zhang, J., Saenko, K.: Top-down visual saliency guided  by captions....|
|||...: Deep networks for saliency detection via  local estimation and global search....|
|||...Wang, L., Wang, L., Lu, H., Zhang, P., Ruan, X.: Saliency detection with recurrent  fully convolutional networks....|
|||...: Bayesian saliency via low and mid level cues....|
|||...: Saliency detection via graph based manifold ranking....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional  features for accurate saliency detection....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep  learning....|
||83 instances in total. (in eccv2018)|
|9|Tong_Salient_Object_Detection_2015_CVPR_paper|...First, a weak saliency map is constructed based on image priors to generate training samples for a strong model....|
|||...Results from multiscale saliency maps are integrated to further improve the detection performance....|
|||...Extensive experiments on six benchmark datasets demonstrate that the proposed bootstrap learning algorithm performs favorably against the state-of-the-art saliency detection methods....|
|||...Introduction  As an important preprocessing step in computer vision problems to reduce computational complexity, saliency detection has attracted much attention in recent years....|
|||...In contrast, top-down saliency models are able to detect objects of certain sizes and categories based on more representative features from training samples....|
|||...Brighter pixels indicate higher saliency values....|
|||...Left to right: input, ground truth, weak saliency map, strong saliency map, and final saliency map....|
|||...First, we compute a weak contrast-based saliency map based on superpixels of an input image....|
|||...This coarse saliency map is smoothed by a graph cut method, where a set of training samples is collected, where positive samples are pertaining to the salient objects while negative samples are from t...|
|||...As the weak saliency model tends to detect fine details and the strong saliency model focuses on global shapes, these two are combined to generate the final saliency map....|
|||...A weak saliency map is constructed based on image priors to generate training samples for a strong model....|
|||...A strong classifier based on multiple kernel boosting is learned to measure saliency where three feature descriptors are extracted and four kernels are used to exploit rich feature representations....|
|||...The weak and strong saliency maps are weighted combined to generate the final saliency map....|
|||...Figure 1 shows some saliency maps generated by the proposed method where brighter pixels indicate higher saliency values....|
|||...Numerous bottom-up saliency detection methods have been proposed in recent years....|
|||...A graph-based saliency measure is proposed by Harel et al....|
|||...However, this method focuses on eye fixation prediction and generates a low resolution saliency map similar to [16]....|
|||...present a contrast-based saliency filter and measure saliency by the uniqueness and spatial distribution of regions over an image....|
|||...Shen and Wu [30] construct a unified model combining lower-level features and higherlevel priors for saliency detection based on the theory of low rank matrix recovery....|
|||...focus on the background instead of the foreground and build a saliency detection model based on two background priors, i.e., boundary and connectivity....|
|||...Compared to bottom-up approaches, considerable efforts have been made on top-down saliency models....|
|||...construct a Bayesian-based top-down model by integrating both the top-down and bottom-up information where saliency is computed locally....|
|||...A saliency model based on the Conditional Random Field is formulated with latent variables and a discriminative dictionary in [40]....|
|||...[19] propose a learning-based method by regarding saliency detection as a regression problem where the saliency detection model is constructed based on the integration of numerous descriptors extracte...|
|||... propose a bootstrap learning approach which exploits the strength of both bottom-up contrast-based saliency models and top-down learning methods....|
|||...Bootstrap Saliency Model  Figure 2 shows the main steps of the proposed salient object detection algorithm....|
|||...The final saliency map is the weighted integration of the weak and strong maps for accurate detection results....|
|||...Weak Saliency Model  The center-bias prior has been shown to be effective in salient object detection [5, 25]....|
|||...Based on this assumption, we develop a method to construct a weak saliency model by exploiting the contrast between each region and the regions along the image border....|
|||...In this paper, we exploit the center-bias and dark channel priors to better estimate saliency maps....|
|||...Therefore, we exploit the dark channel property to estimate saliency of pixels....|
|||...Left to right: input, dark channel map and dark channel prior (the opposite of dark channel map and brighter pixels indicate higher saliency values)....|
|||...The coarse saliency value for the region ci is constructed by  Nci Ppci  1  1 NB  NB  Xj=1     d(ci, nj) ,  f0(ci) = g(ci)Sd(ci) X{F1,F2,F3}  (2) where d(ci, nj) is the Euclidean distance between regi...|
|||...Thus the saliency value of the region closer to the image center is assigned a higher weight....|
|||...We generate a pixel-wise saliency map M0 using (2), where the saliency value of each superpixel is assigned to the contained pixels....|
|||...Most existing methods usually use Gaussian filtering to generate smoothed saliency maps at the expense of accuracy....|
|||...The weight of each node (pixel) p connected to the foreground terminal is assigned with the saliency value in the pixelwise map M0....|
|||...Thus we consider both the binary map M1 and the map M0 to construct the continuous and smoothed weak saliency map Mw by  Mw =  M0 + M1  2  ....|
|||...The training set for the strong classifier is selected from the weak saliency map....|
|||...We compute the average saliency value for each superpixel and set two thresholds to generate the training set containing both positive and negative samples....|
|||...The superpixels with saliency values larger than the high threshold are labeled as the positive samples with +1 while those with saliency values smaller than the low threshold as the negative samples ...|
|||...Strong Saliency Model  One of the main difficulties using a Support Vector Machine (SVM) is to determine the appropriate kernel for the given dataset....|
|||...While numerous saliency detection methods based on various features have been proposed, it is still not clear how these features can be well integrated....|
|||...For each image, we have the training sami=1 from the weak saliency map Mw (See Secples {ri, li}H tion 3.2) where ri is the i-th sample, li represents the binary label of the sample and H indicates the...|
|||... for the combination, and  = 0.7 to weigh the strong map more than the weak map, and M is the final saliency map via bootstrap learning....|
|||...Experimental Results  j(i)ej lizj (ri)  2poj(oj  1)  After J iterations, all the j and zj(r) are computed and we have a boosted classifier (8) as the saliency model learned directly from an input image....|
|||...We apply this strong saliency model to the test samples (based on all the superpixels of an input image), and a pixel-wise saliency map is thus generated....|
|||...To improve the accuracy of the map, we first use the Graph Cut method to smooth the saliency detection results. Next, we obtain the strong saliency map Ms by further enhancing the saliency map with th...|
|||...Multiscale Saliency Maps  The accuracy of the saliency map is sensitive to the number of superpixels as salient objects are likely to appear at different scales....|
|||...We represent the weak saliency map (See Section 3.2) at each scale as { Mwi } and the multiscale weak saliency map is computMwi ....|
|||...Next, the training sets from ed by Mw = 1 the four scales are used to train one strong saliency model and the test sets (based on all the superpixels from four scales) are tested by the learned model ...|
|||...Four strong saliency maps from four scales are constructed (See Section 3.3), denoted as { Msi }, i = 1, 2, 3, 4....|
|||...obtain the final strong saliency map as Ms = 1 As such, the proposed method is robust to scale variation....|
|||...Integration  The proposed weak and strong saliency maps have complementary properties....|
|||...Thus we integrate these two maps by a weighted  We present experimental results of 22 saliency detection methods including the proposed algorithms on six benchmark datasets....|
|||...Comparison of our saliency maps with ten state-of-the-art methods....|
|||...Qualitative Results  We present some results of saliency maps generated by twelve methods for qualitative comparison in Figure 5, where wCO-bootstrapped means the wCO model bootstrapped by the propose...|
|||...g both weak (effective for picking up details) and strong (effective for discriminating boundaries) saliency maps, the proposed bootstrap learning algorithm performs well for images containing multipl...|
|||...We set the fixed threshold from 0 to 255 with an increment of 5 for a saliency map with consistent gray value, thus producing 52 binary masks....|
|||...In addition, we measure the quality of the saliency maps using the F-Measure by adaptively setting a segmentation threshold for binary segmentation [1]....|
|||...Each image is segmented with superpixels and masked out if the mean saliency values are lower than the adaptive threshold....|
|||...Analysis of the Bootstrap Saliency Model  Every component in the proposed algorithm contributes to the final saliency map....|
|||... the performance of each step in the proposed method, i.e., the dark channel prior, graph cut, weak saliency map, and strong saliency map, among which the dark channel prior appears to contribute leas...|
|||...The proposed weak saliency model may generate less accurate results than several state-of-the-art methods, but it is efficient with less computational complexity....|
|||...01 0.552295 0.609845  0.198502 0.206821  0.7  0.8  n o i s i c e r P  0.6  0.5  0.4  0.3  0.2  Weak saliency map without dark channel prior  Weak saliency map without graph cut  Weak saliency map  Str...|
|||...Bootstrapping State-of-the-Art Methods  The performance of the proposed bootstrap learning method hinges on the quality of the weak saliency model....|
|||...If a weak saliency model does not perform well, the proposed algorithm is likely to fail as an insufficient number  of good training samples can be collected for constructing the strong model for a sp...|
|||...Figure 8 shows examples where the weak saliency model does not perform well, thereby affecting the overall performance of the proposed algorithm....|
|||...The two rows named ASD (b) show the AUC of the saliency results by taking other state-of-the-art saliency maps as the weak saliency maps in the proposed approach on the ASD dataset....|
|||... .9904 .9817  .9593 .8886 .7810 .9199 .8121 .8566 .9825  of-the-art methods (i.e., with better weak saliency maps)....|
|||...Thus, we generate different weak saliency maps by applying the graph cut method on the results generated by the state-of-the-art methods....|
|||...Note that we only use two scales instead of four scales for efficiency and use equal weights in (11) (to better use these weak saliency maps) in the experiments....|
|||...image and is unsupervised since the training examples for the strong model are determined by a weak saliency map based on contrast and image priors....|
|||...Left to right: input, ground truth, weak saliency map, strong saliency map and the bootstrap saliency map generated by the proposed algorithm....|
|||...In addition, the proposed bootstrap learning algorithm can be applied to other saliency models for significant improvement....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Geodesic saliency using  background priors....|
|||...Visual saliency detection based on  Bayesian model....|
|||...Bayesian saliency via low  and mid level cues....|
|||...Hierarchical saliency detec tion....|
|||...Top-down visual saliency via joint  CRF and dictionary learning....|
|||...Sun: A Bayesian framework for saliency using natural statistics....|
||82 instances in total. (in cvpr2015)|
|10|Borji_Analysis_of_Scores_2013_ICCV_paper|...EEE International Conference on Computer Vision  Analysis of scores, datasets, and models in visual saliency prediction  Ali Borji  Hamed R. Tavakoli+  Dicky N. Sihite  Laurent Itti   Department of Co...|
|||...In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy....|
|||...Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations....|
|||...Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling....|
|||...Modeling visual saliency broadens our understanding of a highly complex cognitive behavior, which may lead to subsequent findings in other areas (object and scene recognition, visual search, etc.)...|
|||...We offer 3 main contributions: (1) discussing current challenges and directions in saliency modeling (evaluation metrics, dataset bias, model parameters, etc.)...|
|||... classic and emotional stimuli) as well as scanpath prediction, and (3) stimuli/task decoding using saliency and fixation statistics....|
|||...Hopefully, our study will open new directions and conversations and help better organize the saliency literature....|
|||...ter of an image, such that a trivial model that predicts salience near the center may surpass other saliency models) [28, 25], have not been addressed in previous benchmarks....|
|||...We provide the latest update on saliency modeling, with the most comprehensive set of models, challenges/parameters, datasets, and measures....|
|||...There is often confusion between saliency and atten tion....|
|||...any cognitive factors of very high-level such as strategy for image search and interactions between saliency and search strategy, as well as subjective factors such as age and experience....|
|||...Bottom-up attention is reflexive, fast, likely feed-forward, and mainly deployed by stimulus saliency (e.g., pop-out)....|
|||...Similar observations indicate that visual attention is essentially guided by recognized objects, with low-level saliency contributing only indirectly [50]....|
|||...A closely related field to saliency modeling is salient region detection....|
|||...Evaluation is often done by measuring precision-recall of saliency maps of a model against ground truth data (explicit saliency judgments of subjects by annotating salient objects or clicking on locations)....|
|||...Annoyingly, due to CB in data, a trivial saliency model that just consists of a Gaussian blob at the center of the image, often scores higher than almost all saliency models [2]....|
|||...These analyses show that smoothing the saliency maps and the size of the central Gaussian affect scores and should be accounted for fair model comparison....|
|||...All scores are invariant to saliency map shifting and scaling....|
|||...When using the method in [25] (i.e., saliency from other images but at fixations of the current image), this type of AUC leads to the exact value of 0.5 for the central Gaussian (See Supp.)....|
|||...Features for saliency detection....|
|||...Traditionally, intensity, orientation, and color (in LAB and RGB spaces) have been used for saliency derivation over static images....|
|||...Furthermore, several other low-level features have been used to estimate saliency (size, depth, optical flow, etc.)....|
|||...1st column: scores of a saliency map made by placing a variable Gaussian (1) at fixated locations, 2nd column: scores of the central Gaussian blob (2), and the 3rd column: scores of the image with var...|
|||...Traditionally, saliency models have been evaluated against eye movement datasets....|
|||...We use three popular metrics for saliency evaluation: (1) Correlation Coefficient (CC) between a model (s) and human (h) saliency maps: CC(s,h) = cov(s,h) , sh (2) Normalized Scanpath Saliency (NSS): ...|
|||...The saliency map is then treated as a binary classifier to separate the positive samples from negatives....|
|||...2 shows analysis of how the above scores are affected by smoothness of the saliency map and possible center bias in the reference data....|
|||...We generated some random eye fixations (sampled from a Gaussian distribution) and made a saliency map by convolving it with a Gaussian filter with variable sigma 1....|
|||...A sample saliency map smoothed by convolving with a variable-size Gaussian kernel (for the AWS model over an image of the Toronto dataset)....|
|||...We resized saliency maps to the size of the original images onto which eye movements have been recorded....|
|||...We smoothed saliency map of each model by convolving it with a Gaussian kernel (Fig....|
|||...Compared visual saliency models....|
|||...Abbreviations are: M: Matlab, C: C/C++, E: Executables, S: Sent saliency maps....|
|||...Numbers are maximum shuffled AUC scores of models by optimizing the saliency map smoothness (Fig....|
|||...In ITTI98, each feature maps contribution to the saliency map is weighted by the squared difference between the globally most active location and the average activity of all other local maxima in the ...|
|||...This gives rise to smooth saliency maps, which tend to correlate better with noisy human eye movement data....|
|||...In the ITTI model [33], the spatial competition for saliency is much stronger, and is implemented in each feature map as 10 rounds of convolution by a large difference-of-Gaussians followed by half-wa...|
|||...This gives rise to much sparser saliency maps, which are more useful than the ITTI98 maps when trying to decide on the single next location to look at (e.g., in machine vision and robotics applications)....|
|||...Sample emotional images with positive, negative, and neutral emotional valence from NUSEF dataset along with saliency maps of the AWS model....|
|||...Note that in some cases saliency misses fixations....|
|||...gest that only a fraction of fixations landed on emotional image regions, possibly due to bottom-up saliency (interaction between saliency and emotion; AWS on emotional = 0.59, non-emotional = 0.69)....|
|||...6 shows saliency maps of some emotional images....|
|||...In the context of saliency modeling, few models have aimed to predict scanpath sequence, partly due to difficulty in measuring  Figure 5. sAUC scores over emotional images of the NUSEF dataset....|
|||...A separate analysis over the Kootstra dataset showed that models have difficulty in saliency detection over nature stimuli where there are less distinctive and salient objects (See supplement)....|
|||...This means that much progress remains to be done in saliency detection over stimuli containing conceptual stimuli (e.g, images containing interacting objects, actions such as grasping, living vs. non-...|
|||...r centers Store the string sequence   PHASE 2: Model evaluation on a given test image  Take a model saliency map Load the sequence strings and clusters corresponding to that image for each subject do ...|
|||...Then for a saliency map, an inhibition of return (IOR) mechanism is used to generate a sequence....|
|||...An application of saliency modeling is to decode task (illustrated by the classic study of Yarbus [63]), stimulus category, or different populations of human subjects (e.g., control vs. ADHD patients ...|
|||...Here, as an example we intend to decode the category of the stimulus from features augmented from statistics of fixations, saccades, and saliency at fixations....|
|||...36 bins), saccade duration (60 bins), saccade velocity (50 bins), and saccade slope (30 bins), from saliency: saliency map (300D), saliency histogram at fixated locations (10 bins), and Top 10 salient...|
|||...Saliency-based features include histogram of saliency values at fixation points (10 bins), vectorized saliency map of size 2015 (a vector of size 1300) and coordinates of ten most salient points obtai...|
|||...Smoothing saliency maps affects performance and should be taken into account for fair model comparison....|
|||...This could be because of two reasons: similar saliency patterns across scenes of a category and/or systematic/semantic biases of fixations in each category....|
|||...In this regard, it will also be interesting to test the feasibility of predicting whether a scene is natural or man-made from saliency and fixations....|
|||...Some saliency models implicitly emphasize the central parts of objects (e.g, [17])....|
|||...10.a shows decoding results using saliency maps of four models and the MEP map....|
|||...Using all features (and ITTI98 saliency maps), the best decoding accuracy is 0.512 followed by AIM (+ all features = 0.435)....|
|||...Fixation histogram, saliency (here MEP), and  Figure 10. a) Stimulus category decoding for five categories of the NUSEF dataset, b) Breakdown of performance over different features....|
|||...Another direction is finding better ways to combine local and global saliences (i.e., regions with the same global saliency could have different local saliences)....|
|||...We showed that, from statistics of fixations, saccades, and saliency at fixations, it is possible to decode the stimulus category....|
|||...An eye  fixation database for saliency detection in images....|
|||...A novel multiresolution spatiotemporal saliency detection model and Its applications in image and video compression....|
|||...Visual saliency based on conditional  entropy....|
|||...Visual saliency via sparsity rank decomposi tion....|
|||...Static and space-time visual saliency detection by  self-resemblance....|
|||...Modeling saliency to predict gaze direction for short videos....|
|||...928928  [25] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cottrell, SUN: A  Bayesian framework for saliency using natural statistics....|
|||...Probabilistic multi-task learning for  visual saliency estimation in video....|
|||...Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study....|
|||...Fast and efficient saliency detection  using sparse sampling and kernel density estimation....|
|||...Predicting human gaze using  low-level saliency combined with face detection....|
|||...Learning a saliency map using fixated locations in  natural scenes....|
|||...B. Huang and N. Ahuja, Saliency Detection via Divergence Analysis: A  Unified Perspective, ICPR, 2012....|
|||...A Benchmark of Computational Models  of Saliency to Predict Human Fixations....|
|||...A unified method for comparison of algorithms of saliency extraction, SPIE, 2012....|
|||...Salience of the lambs: A test of the saliency map hypothesis with pictures of emotive objects, J. of vision, 2012....|
|||...Comparative evaluation of visual saliency models  for quality assessment task, 6th Int....|
|||...[58] J.B Huang and N. Ahuja, Saliency Detection via Divergence Analysis: A  Unified Perspective, International Conference on Pattern Recognition, 2012....|
|||...[59] B. Schauerte and R. Stiefelhagen, Quaternion-based spectral saliency detection  for eye fixation prediction, ECCV, 2012....|
|||...[60] E. Erdem and A. Erdem, Visual saliency estimation by nonlinearly integrating  features using region covariances, Journal of Vision, 13:4, 1-20, 2013....|
||81 instances in total. (in iccv2013)|
|11|Li_Visual_Saliency_Based_2015_CVPR_paper|...epartment of Computer Science, The University of Hong Kong  https://sites.google.com/site/ligb86/mdfsaliency/  Abstract  Visual saliency is a fundamental problem in both cognitive and computational sc...|
|||...For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for feature extraction at three different scales....|
|||...We then propose a refinement method to enhance the spatial coherence of our saliency results....|
|||...Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation....|
|||...To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotations....|
|||...Introduction  Visual saliency attempts to determine the amount of attention steered towards various regions in an image by the human visual and cognitive systems [6]....|
|||...Visual saliency has been incorporated in a variety of computer vision and image processing tasks to improve their performance....|
|||...In this paper, we develop a new computational model for visual saliency using multiscale deep features computed by convolutional neural networks....|
|||...By definition, saliency is resulted from visual contrast as it intuitively characterizes certain parts of an image that appear to stand out relative to their neighboring regions or the rest of the image....|
|||...Thus, to compute the saliency of an image region, our model should be able to evaluate the contrast between the considered region and its surrounding  1  area as well as the rest of the image....|
|||...Concatenated multiscale CNN features are fed into these layers trained using a collection of labeled saliency maps....|
|||...Thus, these fully connected layers play the role of a regressor that is capable of inferring the saliency score of every image region from the multiscale CNN features extracted from nested windows sur...|
|||...We have extensively evaluated our CNN-based visual saliency model over existing datasets, and meanwhile noticed a lack of large and challenging datasets for training and testing saliency models....|
|||...To facilitate research and evaluation of advanced saliency models, we have created a large dataset where an image likely contains multiple salient objects, which have a more general spatial distributi...|
|||...Our proposed saliency model has significantly outperformed all existing saliency models over this new dataset as well as all existing datasets....|
|||...In summary, this paper has the following contributions:  A new visual saliency model is proposed to incorporate multiscale CNN features extracted from nested windows with a deep neural network with mu...|
|||...The deep neural network for saliency estimation is trained using regions from a set of labeled saliency maps....|
|||... A complete saliency framework is developed by further integrating our CNN-based saliency model with a spatial coherence model and multi-level image segmentations....|
|||...Our proposed saliency model has been successfully validated on this new dataset as well as on all existing datasets....|
|||...Related Work  Visual saliency computation can be categorized into bottom-up and top-down methods or a hybrid of the two....|
|||...Bottom-up models are primarily based on a center-surround scheme, computing a master saliency map by a linear or non-linear combination of low-level visual attributes such as color, intensity, texture...|
|||...Recently, much effort has been made to design discriminative features and saliency priors....|
|||...Saliency priors, such as the center prior [26, 35, 23] and the boundary prior [22, 40], are widely used to heuristically combine low-level cues and improve saliency estimation....|
|||...These saliency priors are either directly combined with other saliency cues as weights [8, 9, 20] or used as features in learning based algorithms [22, 23, 25]....|
|||...While these empirical priors can improve saliency results for many images, they can fail when a salient object is off-center or significantly overlaps with the image boundary....|
|||...Nevertheless, CNN features have not yet been explored in visual saliency research primarily because saliency cannot be solved using the same framework considered in [11, 30]....|
|||...It is the contrast against the surrounding area rather than the content inside an image region that should be learned for saliency prediction....|
|||...1, the architecture of our deep feature based model for visual saliency consists of one output layer and two fully connected hidden layers on top of three deep convolutional neural networks....|
|||...Our saliency model requires an input image to be decomposed into a set of nonoverlap crafted features on other visual computing tasks....|
|||...As we know, a very important cue in saliency computation is the degree of (color and content) uniqueness of a region with respect to the rest of the image....|
|||...These three feature vectors obtained at different scales together define the features we adopt for saliency model training and testing....|
|||...This network plays the role of a regressor that infers the saliency score of every image region from the multiscale CNN features extracted for the image region....|
|||...The output of the second fully-connected layer is fed into the output layer, which performs two-way softmax that produces a distribution over binary saliency labels....|
|||...When generating a saliency map for an input image, we run our trained saliency model repeatedly over every region of the image to produce a single saliency score for that region....|
|||...This saliency score is further transferred to all pixels within that region....|
|||...network, which is trained using a collection of training images and their labeled saliency maps, that have pixelwise binary saliency scores....|
|||...The saliency label of every image region is further estimated using pixelwise saliency labels....|
|||...During the training stage, only those regions with 70% or more pixels with the same saliency label are chosen as training samples, and their saliency labels are set to either 1 or 0 respectively....|
|||...Traditional regression techniques, such as support vector regression and random forests, can be further trained on this feature vector to generate a saliency score for every image region....|
|||...l layer of our architecture is strong enough to generate state-of-the-art performance on all visual saliency datasets....|
|||...To enable a fair comparison with previous work on saliency estimation, we follow the multi-level region decomposition pipeline in [22]....|
|||...Spatial Coherence  Given a region decomposition of an image, we can generate an initial saliency map with the neural network model presented in the previous section....|
|||...However, due to the fact that image segmentation is imperfect and our model assigns saliency scores to individual regions, noisy scores inevitably appear in the resulting saliency map....|
|||...To enhance spatial coherence, a superpixel based saliency refinement method is used....|
|||...The saliency score of a superpixel is set to the mean saliency score over all pixels in the superpixel....|
|||...The refined saliency map is obtained by minimizing the following cost function, which can be reduced to solving a lin ear system....|
|||... (cid:1)2  (cid:88)  +  wij  (cid:0)aR  i  aR  j  (cid:1)2  ,  (1)  i  i,j  where aI is the initial saliency score at superpixel i, aR i i is the refined saliency score at the same superpixel....|
|||...The first term in (1) encourages similarity between the refined saliency map and the initial saliency map, while the second term is an all-pair spatial coherence term that favors consistent saliency s...|
|||...As a result, we obtain M refined saliency maps,  {A(1), A(2), ..., A(M )}, interpreting salient parts of the input image at various granularity....|
|||...We aim to further fuse them together to obtain a final aggregated saliency map....|
|||...(3)  kA(k)  i  (cid:88)  iIv  Note that there are many options for saliency fusion....|
|||...For example, a conditional random field (CRF) framework has been adopted in [27] to aggregate multiple saliency maps from different methods....|
|||...Nevertheless, we have found that, in our context, a linear combination of all saliency maps can already serve our purposes well and is capable of producing aggregated maps with a quality comparable to...|
|||...We have constructed a more challenging dataset to facilitate the research and evaluation of visual saliency models....|
|||...x } be the binary saliency mask labeled by x = 1 if pixel x is labeled as salient  the p-th user....|
|||...The pixelwise saliency label in the ground truth saliency map, G = {gx gx  {0, 1}}, is determined according to the majority label among the three people as follows,  (cid:32) 3(cid:88)  (cid:33) x  2 ...|
|||...(5)  p=1  At the end, our new saliency dataset, called HKU-IS, contains 4447 images with high-quality pixelwise annotations....|
|||...Dataset  We have evaluated the performance of our method on several public visual saliency benchmarks as well as on our own dataset....|
|||...This dataset has 5000 images, and is widely used for visual saliency estimation....|
|||...Figure 2: Visual comparison of saliency maps generated from 10 different methods, including ours (MDF)....|
|||...MDF consistently produces saliency maps closest to the ground truth....|
|||...We compare MDF against spectral residual (SR[18]), frequency-tuned saliency (FT [1]), saliency filters (SF [29]), geodesic saliency (GS [35]), hierarchical saliency (HS [37]), regional based contrast ...|
|||...Since other existing datasets are too small to train reliable models, we directly applied a trained model to generate their saliency maps as in [22]....|
|||...A continuous saliency map can be converted into a binary mask using a threshold, resulting in a pair of precision and recall values when the binary mask is compared against the ground truth....|
|||...This adaptive threshold is determined to be twice the mean saliency of the image:  W(cid:88)  H(cid:88)  x=1  y=1  Ta =  2  W  H  S(x, y),  (7)  where W and H are the width and height of the saliency ...|
|||...It is defined as the average pixelwise absolute difference between the binary ground truth G and the saliency map S [29],  M AE =  1  W  H   S(x, y)  G(x, y) ....|
|||...S(g)RC(h)MR(i)wCtr*(j)DRFI(k)Our MDF(l) GT(a)  (b)  (c)  (d)  Figure 3: Quantitative comparison of saliency maps generated from 10 different methods on 4 datasets....|
|||...MAE measures the numerical distance between the ground truth and the estimated saliency map, and is more meaningful in evaluating the applicability of a saliency model in a task such as object segmentation....|
|||...Comparison with the State of the Art  Let us compare our saliency model (MDF) with a number of existing state-of-the-art methods, including discriminative regional feature integration (DRFI) [22], opt...|
|||...SRCwCtr*MDF0.050.100.150.200.25(a)  (b)  (c)  (d)  Figure 4: Component-wise efficacy in our visual saliency model....|
|||...Layer1, Layer2 and Layer3 refer to the three segmentation levels that have the highest single-level saliency prediction performance....|
|||...of the MAE measure, which provides a better estimation of the visual distance between the predicted saliency map and the ground truth....|
|||...of our multiscale CNN feature vector are complementary to each other, and the training stage of our saliency model is capable of discovering and understanding region contrast information hidden in our...|
|||...Spatial Coherence In Section 3.2, spatial coherence was incorporated to refine the saliency scores from our CNNbased model....|
|||...To validate its effectiveness, we have evaluated the performance of our final saliency model with and without spatial coherence using the testing images in the MSRA-B dataset....|
|||...We further chose the three segmentation levels that have the highest single-level saliency prediction performance, and compared their performance with spatial coherence turned on and off....|
|||...The aggregated saliency map from 15 levels of image segmentation improves the average precision by 2.15% and at the same time improves the recall rate by 3.47% when it is compared with the result from...|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||81 instances in total. (in cvpr2015)|
|12|Lai_Jiang_DeepVS_A_Deep_ECCV_2018_paper|...Hence, an object-to-motion convolutional neural network (OM-CNN) is developed to predict the intra-frame saliency for DeepVS, which is composed of the objectness and motion subnets....|
|||...We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames....|
|||...Consequently, the interframe saliency maps of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention maps....|
|||...Finally, the experimental results show that DeepVS advances the state-of-the-art in video saliency prediction....|
|||...Keywords: Saliency prediction  Convolutional LSTM  Eye-tracking database  1  Introduction  The foveation mechanism in the human visual system (HVS) indicates that only a small fovea region captures mo...|
|||...The traditional video saliency prediction methods mainly focus on the feature integration theory [16, 19, 20, 26], in which some spatial and temporal features were developed for video saliency prediction....|
|||...sed to learn human attention in an end-to-end manner, significantly improving the accuracy of image saliency prediction....|
|||...long shortterm memory (LSTM) network connected to a mixture density network was learned to generate saliency maps in a Gaussian mixture distribution....|
|||...More importantly, Hollywood focuses on task-driven attention, rather than free-view saliency prediction....|
|||...In this paper, we propose a new DL based video saliency prediction (DeepVS) method....|
|||...Both Figure 1 and the analysis of our database show that the saliency maps are smoothly transited across video frames....|
|||...Accordingly, a saliency-structured convolutional long short-term memory (SS-ConvLSTM) network is developed to predict the pixelwise transition of video saliency across frames, with the output features...|
|||...The traditional LSTM networks for video saliency prediction [2, 23] assume that human attention follows the Gaussian mixture distribution, since these LSTM networks cannot generate structured output....|
|||...Furthermore, since the center-bias (CB) exists in the saliency maps as shown in Figure 1, a CB dropout is proposed in the SS-ConvLSTM network....|
|||...As such, the structured output of saliency considers the CB prior....|
|||...Consequently, the dense saliency prediction of each video frame can be obtained in DeepVS in an end-to-end manner....|
|||...The experimental results show that our DeepVS method advances the state-of-the-art of video saliency prediction in our database and other 2 eye-tracking databases....|
|||...Most early saliency prediction methods [16, 20, 26, 34] relied on the feature integration theory, which is composed of two main steps: feature extraction and feature fusion....|
|||...In the image saliency prediction task, many effective spatial features were extracted to predict human attention with either a top-down [17] or bottom-up [4] strategy....|
|||...Compared to image, video saliency prediction is more challenging because temporal features also play an important role in drawing human attention....|
|||...To achieve this, a countable amount of motion-based features [11, 42] were designed as additional temporal information for video saliency prediction....|
|||...Besides, some methods [16, 40] focused on calculating a variety of temporal differences across video frames, which are effective in video saliency prediction....|
|||...In addition to feature extraction, many works have focused on the fusion strategy to generate video saliency maps....|
|||...Most recently, DL has been successfully incorporated to automatically learn spatial features for predicting the saliency of images [13, 18, 28, 29, 32]....|
|||...However, only a few works have managed to apply DL in video saliency prediction [13, 23, 27, 33, 35]....|
|||...Similarly, in [35], the pair of consecutive frames concatenated with a static saliency map (generated by the static CNN) are fed into the dynamic CNN for video saliency prediction, allowing the CNN to...|
|||...[23] applied LSTM networks to predict video saliency maps, relying on both shortand long-term memory of attention distribution....|
|||...LSTM limit the dimensions of both the input and output; thus, it is unable to obtain the end-to-end saliency map and the strong prior knowledge needs to be assumed for the distribution of saliency in ...|
|||...In our work, DeepVS explores SS-ConvLSTM to directly predict saliency maps in an end-to-end manner....|
|||...Finding 3: There exists a temporal correlation of human attention with a smooth saliency transition across video frames....|
|||...4 Proposed method  4.1 Framework  For video saliency prediction, we develop a new DNN architecture that combines OMCNN and SS-ConvLSTM....|
|||...As such, OM-CNN integrates both regions and motion of objects to predict video saliency through two subnets, i.e., the subnets of objectness and motion....|
|||...Overall architecture of our OM-CNN for predicting video saliency of intra-frame....|
|||...Note that the proposed cross-net mask, hierarchical feature normalization and saliency inference module are highlighted with gray background....|
|||...o ,C 8  o  Finally, the saliency map of each frame is generated from 2 deconvolutional layers of SS-ConvLSTM....|
|||...Moreover, the inference module is developed to generate the crossnet mask or saliency map, based on the learned features....|
|||...es {FTi}4 i=1 from the two subnets of OM-CNN, an inference module If is constructed to generate the saliency map Sf , which models the intra-frame saliency of a video frame....|
|||...Architecture of our SS-ConvLSTM for predicting saliency transition across inter-frame, following the OM-CNN....|
|||...Note that the last 4 convolutional layers are not masked with the cross-net mask for considering the motion of the non-object region in saliency prediction....|
|||...4.4 SS-ConvLSTM  According to Finding 3, we develop the SS-ConvLSTM network for learning to predict the dynamic saliency of a video clip....|
|||...We propose a CB dropout for SS-ConvLSTM, which improves the generalization capability of saliency prediction via incorporating the prior of CB....|
|||...It is because the effectiveness of the CB prior in saliency prediction has been verified [37]....|
|||...Finally, saliency map St l is obtained upon the hidden states of the 2-nd LSTM layer Ht  1, At 1 and Ht  i , Zh  a, Zh  o , Zh  a, Zf  o , Zf  1, Mt  1, At  1, Ot  1, Gt  2 for each frame t.  i , Zf  ...|
|||...Regarding the saliency map as a probability distribution of attention, we can measure the KL divergence DKL between the saliency map Sf of OM-CNN and the ground-truth distribution G of human fixations...|
|||...In (7), a smaller KL divergence indicates higher accuracy in saliency prediction....|
|||...(9)  i=1 are the final saliency maps of T frames generated by SS-ConvLSTM, In (9), {Si l}T and {Gi}T i=1 are their ground-truth attention maps....|
|||...06  Here, the hyper-parameters of OM-CNN and SS-ConvLSTM are tuned to minimize the KL divergence of saliency prediction over the validation set....|
|||...Benefiting from that, our method is able to make real-time prediction for video saliency at a speed of 30 Hz....|
|||...5.2 Evaluation on our database  In this section, we compare the video saliency prediction accuracy of our DeepVS method and to other state-of-the-art methods, including GBVS [11], PQFT [9], Rudoy [31]...|
|||...Among these methods, [11], [9], [31], [12] and [37] are 5 state-of-the-art saliency prediction methods for videos....|
|||...Note that other DNN-based methods on video saliency prediction [1,2,23] are not compared in our experiments, since their codes are not public....|
|||...In our experiments, we apply four metrics to measure the accuracy of saliency prediction: the area under the receiver operating characteristic curve (AUC), normalized scanpath saliency (NSS), CC, and ...|
|||...Note that larger values of AUC, NSS or CC indicate more accurate prediction of saliency, while a smaller KL divergence means better saliency prediction....|
|||...(1) Our method embeds the objectness subnet to utilize objectness information in saliency prediction....|
|||...Mean (standard deviation) of saliency prediction accuracy for our and 8 other methods over all test videos in our database....|
|||...SS-ConvLSTM is leveraged to model saliency transition across video frames....|
|||...Next, we compare the subjective results in video saliency prediction....|
|||...Figure 6 demonstrates the saliency maps of 8 randomly selected videos in the test set, detected by our DeepVS method and 8 other methods....|
|||...Mean (standard deviation) values for saliency prediction accuracy of our and other methods over SFU and DIEM databases....|
|||...This demonstrates the generalization capability of our method in video saliency prediction....|
|||...ectness subnet, motion subnet and OM-CNN, we further analyze the contribution of each component for saliency prediction accuracy in DeepVS, i.e., the combination of OM-CNN and SS-ConvLSTM....|
|||...We further evaluate the saliency prediction performance of the trained SS-ConvLSTM model over  14  L.Jiang et al....|
|||...Note that the smaller KL divergence indicates higher accuracy in saliency prediction....|
|||...This is probably because the well-trained LSTM cell is able to utilize more inputs to achieve a better performance for video saliency prediction....|
|||...Then, the OM-CNN architecture was proposed to explore the spatio-temporal features of the objectness and object motion to predict the intraframe saliency of videos....|
|||...The SS-ConvLSTM architecture was developed to model the inter-frame saliency of videos....|
|||...Bak, C., Kocak, A., Erdem, E., Erdem, A.: Spatio-temporal saliency networks for dynamic  saliency prediction....|
|||...: A video saliency detection model in  compressed domain....|
|||...Guo, C., Zhang, L.: A novel multiresolution spatiotemporal saliency detection model and its  applications in image and video compression....|
|||...: Video saliency detection based on  spatiotemporal feature learning....|
|||...Nguyen, T.V., Xu, M., Gao, G., Kankanhalli, M., Tian, Q., Yan, S.: Static saliency vs. dy namic saliency: a comparative study....|
|||...: Salgan: Visual saliency prediction with generative adversarial networks....|
|||...: Shallow and deep  convolutional networks for saliency prediction....|
|||...Rudoy, D., Goldman, D.B., Shechtman, E., Zelnik-Manor, L.: Learning video saliency from  human gaze using candidate selection....|
|||...Wang, L., Wang, L., Lu, H., Zhang, P., Ruan, X.: Saliency detection with recurrent fully  convolutional networks....|
|||...Wang, W., Shen, J., Shao, L.: Consistent video saliency using local gradient flow optimization and global refinement....|
|||...Xu, M., Jiang, L., Sun, X., Ye, Z., Wang, Z.: Learning to detect video saliency with hevc  features....|
|||...Zhang, J., Sclaroff, S.: Exploiting surroundedness for saliency detection: a boolean map  approach....|
|||...: Sunday: Saliency using natural statistics for dynamic  analysis of scenes....|
||79 instances in total. (in eccv2018)|
|13|cvpr18-Going From Image to Video Saliency  Augmenting Image Salience With Dynamic Attentional Push|...clark@cim.mcgill.ca  Abstract  We present a novel method to incorporate the recent advent in static saliency models to predict the saliency in videos....|
|||...Our model augments the static saliency models with the Attentional Push effect of the photographer and the scene actors in a shared attention setting....|
|||...We propose a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) structure which augments state-of-the-art in static saliency models with dynamic Attentional Push....|
|||...Our network contains four pathways, a saliency pathway and three Attentional Push pathways....|
|||...We evaluate our model by comparing the performance of several augmented static saliency models with state-of-the-art in spatiotemporal saliency on three largest dynamic eye tracking datasets, HOLLYWOO...|
|||...[28] where early visual features across multiple scales are linearly combined into a static saliency map....|
|||...Traditional spatiotemporal saliency models have also benefited from employing early visual features or other static handcrafted features, along with various motion-based features to capture the spatio...|
|||...The resulting performance gain of the static convnet-based models has been to the extent that newer models achieve only marginal improvements over state-of-the-art (the MIT saliency benchmark [6] and [30])....|
|||...However, these advancements are yet to be employed by the spatiotemporal saliency models, many of which only consider simple motion cues and are mere straightforward extension of static models (see [3...|
|||...To the best of our knowledge, the only recent convnet-based spatiotemporal saliency models are: the CMASS method [45], in which shallow neural nets are trained to fuse static hand-crafted features wit...|
|||...iewers as passive participants in a shared attention setting, it becomes possible to augment static saliency models with the gaze direction of the scene actors....|
|||...actors gaze here, is used to enhance the saliency of some  the gazed-at region....|
|||...The fact that even deep convnet-based saliency models such as [36] and [13], which are based on the VGG-16 [58] and the ResNet-50 [24] networks, need to explicitly combine central bias maps with deep ...|
|||...We design a novel spatiotemporal saliency augmentation model which benefits from the recent advent in static saliency to estimate video saliency....|
|||...Here, we extend the model in [21] by including the photographer in the shared attention setting and augment state-of-the-art in static saliency with dynamic Attentional Push....|
|||...Our network contains four pathways, a saliency pathway and three Attentional Push pathways, i.e....|
|||...The saliency pathway embeds static saliency models and captures the temporal dependencies between consecutive video frames by sequentially analyzing the static saliency maps in the ConvLSTM recurrent ...|
|||... propose a novel spatiotemporal visual attention model that incorporates state-of-the-art in static saliency and learns longterm temporal dependencies to estimate video saliency....|
|||...Second, we expand the notion of Attentional Push to dynamic stimuli and show its effectiveness in augmenting static saliency in dynamic scenes....|
|||...Third, we provide comprehensive experimental evaluation on publicly available video saliency datasets which demonstrates significant improvement in predicting viewers fixation patterns on videos conta...|
|||...Related work  We describe closely related work on static and spatiotemporal saliency models and saliency models benefiting from gaze following as a subcomponent....|
|||...The CMASS method [45] uses three-layered fully connected neural nets to fuse static features, ranging from color channels to existing saliency models, with dense optical flow fields....|
|||...ecent work in [39], which uses RGB color planes, dense optical flow map, depth map and the previous saliency map are fed to a seven-layered encoder-decoder structure....|
|||...is gaze related or being saliency driven using a two-state Markov chain....|
|||...Our model is inspired by the Attentional Push model in [21], which augments static saliency models with the actors gaze for static scenes....|
|||...applicable for static images, in this work, we extend the Attentional Push notion to augment static saliency models to deal with dynamic stimuli....|
|||...Although related, there is a major distinction between saliency prediction and gaze following....|
|||...actors gaze, attentional bounce and abrupt scene changes and combine them with static saliency to estimate the fixation patterns on videos....|
|||...In addition, to benefit from the strong temporal correlation of the fixation patterns in consecutive video frames, a ConvLSTM cell is also used in the saliency pathway....|
|||...Saliency Pathway  The saliency pathway embeds state-of-the-art static saliency models....|
|||...Given a video frame I(t)  Rcolsrows3 at time t, the static saliency sstatic(t)  Rcolsrows1 is computed and fed to a ConvLSTMs module....|
|||...+ Whi  ht1 + bi)  (1)  7504  Figure 3: Network Architecture: Our network contains four pathways, a saliency pathway and three Attentional Push pathways, gaze following, rapid scene changes and attent...|
|||...The network computes the augmented saliency map  s(t) for each video frame I(t)....|
|||...We sequentially pass static saliency maps to the ConvLSTM input by setting x(t) = sstatic(t), and obtain a refined sequence of timecorrelated saliency maps as s(t) = h(t)....|
|||...During training, the saliency ConvLSTM learns to estimate video saliency, by leveraging the temporal correlation between consecutive static saliency maps....|
|||...Evaluation protocol  To fuse the saliency and the Attentional Push pathways, we use a set of trainable dilated convolutional layers....|
|||...The last convolutional layer has a (1  1) kernel which effectively maps the deep features of the previous layer into the augmented saliency map,  s(t)....|
|||...The augmenting convnet is trained to learn an optimal combination strategy to fuse the complementary information given by the saliency and Attentional Push ConvLSTMs....|
|||...We provide partial head location and gaze annotations  Static saliency models: We evaluate the performance of the proposed model with several state-of-the-art in spatiotemporal saliency models....|
|||...To illustrate the effectiveness of dynamic Attentional Push in augmenting static saliency models, we use several neural network-based and traditional static saliency models and train and test the perf...|
|||...eDN [62], ML-Net [12], SalNet [46] and SAM-ResNet [13], and two best-performing traditional static saliency models, BMS [67] and RARE [54]....|
|||...For evaluation, we report the performance of the models using three popular evaluation metrics: the Area Under the ROC Curve (AUC), the Normalized Scan-path Saliency (NSS), and the Correlation Coeffic...|
|||...We pre-train the saliency ConvLSTM using all the training samples listed in Table 1 with a learning rate of 1104 and a weight decay of 5  105....|
|||... the Xavier method and are trained by back propagating the KL divergence loss between the augmented saliency maps and the ground truth fixation densities with a learning rate 1  105....|
|||...Results  In this section, we compare the accuracy of the augmented saliency models in predicting video saliency with three state-of-the-art in spatiotemporal saliency models, OBDL [33] , Rudoy [56] an...|
|||...The results clearly show that the augmented saliency consistently improves upon the static saliency models and achieve considerable performance gain over spatiotemporal saliency models on all three test sets....|
|||...Interestingly, although the Rudoy model outperform four of the static saliency models, including the convnet-based ML-Net and SalNet, all the augmented saliency models achieve considerable gain over t...|
|||... or more components of the model in Fig 7507  Table 2: Average evaluation scores for the augmented saliency vs. static and spatiotemporal saliency models on the DIEM, HOLLYWOOD2 and UCF-Sports test s...|
|||...The results are based on eDN saliency and the DIEM test set....|
|||...Conclusion  Augmented saliency No dynamic Attentional Push Saliency and gaze following pathways No gaze following No attentional bounce No rapid scene change No saliency Saliency pathway only Static S...|
|||...We performed extensive experimental tests and found the augmented saliency models to outperform both the static and spatiotemporal saliency models....|
|||...The third entry is the result of augmenting the static saliency model with dynamic gaze following, which achieves considerable performance....|
|||...Two-stream convolutional networks for dynamic saliency prediction....|
|||...Complementary effects of gaze direction and early saliency in guiding fixations during free viewing....|
|||...a study of human explicit saliency judgment....|
|||...Mit saliency benchmark....|
|||...Where should saliency models look next?...|
|||...Transfer learning with deep networks for saliency prediction in natural video....|
|||...A deep multi-level network for saliency prediction....|
|||...A deep multi-level network for saliency prediction....|
|||...Predicting human eye fixations via an lstm-based saliency attentive model....|
|||...Temporal spectral residual: Fast motion saliency detection....|
|||...A video saliency detection model in compressed domain....|
|||...Video saliency incorporating spatiotemporal cues and uncertainty weighting....|
|||...A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...A comparative study for feature integration strategies in dynamic saliency estimation....|
|||...Deep gaze I: boosting saliency prediction with feature maps trained on imagenet....|
|||...Learning gaze transitions from depth to improve video saliency estimation....|
|||...Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition....|
|||...Static saliency vs. dynamic saliency: A comparative study....|
|||...Augmented saliency model using automatic 3D head pose detection and learned gaze following in natural scenes....|
|||...Noveltybased spatiotemporal saliency detection for prediction of gaze in egocentric video....|
|||...Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Learning to detect video saliency with hevc features....|
||77 instances in total. (in cvpr2018)|
|14|Huang_SALICON_Reducing_the_ICCV_2015_paper|...SALICON: Reducing the Semantic Gap in Saliency Prediction  by Adapting Deep Neural Networks  Xun Huang1  ,  2 Chengyao Shen1 Xavier Boix1  ,  3 Qi Zhao1  1Department of Electrical and Computer Enginee...|
|||...Conventional saliency models typically rely on low-level image statistics to predict human fixations....|
|||...Two key components are fine-tuning the DNNs with an objective function based on the saliency evaluation metrics, and integrating information at different image scales....|
|||...We compare our method with 14 saliency models on 6 public eye tracking benchmark datasets....|
|||...Results demonstrate that our DNNs can automatically learn features for saliency prediction that surpass by a big margin the state-of-the-art....|
|||...The pioneering theory of feature integration served as the basis for many of the initial saliency models [37]....|
|||...[14] extracted multi-scale lowlevel features including intensity, color and orientation, and predicted saliency based on graph algorithms....|
|||...Bruce and Tsotsos [2] represented images with Gabor-like features learned from independent component analysis and estimated saliency as self-information....|
|||...The recent Boolean Map Saliency (BMS) model by Zhang and Sclaroff [42] used color as features and computed saliency based on Boolean maps that are generated by randomly thresholding feature maps....|
|||...In fact, object detectors have been shown to play an important role in improving saliency prediction [40], and several computational models have successfully incorporated object detectors into salienc...|
|||...In saliency prediction, the multi-layer sparse network [33] and Ensemble of Deep Networks (eDN) [39] are both early architectures that automatically learn the representations for saliency prediction....|
|||...Since the amount of data  1262  available to learn saliency prediction is scarce, the complexity of the deep architectures cannot be easily scaled to outperform current state-of-the-art....|
|||...In this paper, we investigate saliency prediction using the representational power of the semantic content in DNNs pretrained in ImageNet [6]....|
|||...To address the difference in the task objective between saliency prediction and object recognition, we use saliency evaluation metrics as the objective to fine-tune the DNNs....|
|||...The results demonstrate that our architectures can surpass the state-of-the-art accuracy of saliency prediction by a big margin in all the tested benchmarks....|
|||...DNNs for Saliency Prediction  In this Section, we introduce our method to use DNNs in saliency prediction....|
|||...Thus, learning a DNN only from saliency maps may be difficult, since even the largest saliency dataset does not contain millions of training images, as in the object recognition datasets used to succe...|
|||...A wellknown result is that features based on object detectors can be useful to predict saliency maps [4, 7, 21, 40, 44]....|
|||...Thus,  DNNs pretrained for object recognition, that are known to encode powerful semantics features, might be useful as well for saliency prediction....|
|||...We introduce an architecture that integrates the saliency prediction to a DNN pretrained for object recognition....|
|||...Our architecture is based on one of these DNNs in the context of saliency prediction (see Fig....|
|||...We found that a good compromise between semantic representation and spatial information for saliency prediction is to use the last convolution layer, in any of the three DNNs we use....|
|||...y Objectives (KLD, NSS, CC, SIM)  50  Resizing  7 3  Figure 1: Learning of the DNN architecture for saliency prediction....|
|||...We use objective functions to optimize some common saliency evaluation metrics....|
|||...To use Yc for saliency prediction, we add one convolutional layer after Yc....|
|||...We denote the result of convolving Yc with the saliency detector filter as Ys, and it encodes the saliency prediction information at a resolution of ms  ns....|
|||...The filter of the convolutional layer for saliency prediction is of size 1  1....|
|||...Increasing this size does not improve the accuracy because the receptive field of the neurons of Yc capture enough context for saliency prediction....|
|||...The 11 filter of the convolutional layer may select and discard which objects parts or patterns detected by the DNN are useful for saliency prediction....|
|||...Finally, to obtain the saliency map, we resize the responses of Ys with a linear interpolation to match the image size, and we scale it to take values between 0 and 1....|
|||...Predicting the saliency map at the spatial resolution of Ys is effective, but also, it reduces the computational burden at training time because the number of neural responses to process from Ys is mu...|
|||...A common practice of saliency prediction models is to use information at multiple image scales to improve the accuracy, e.g....|
|||...Stimuli  Human  Fine  Coarse  Fine+Coarse  Figure 2: Saliency prediction with multiple image scales....|
|||...Each scale is processed by a DNN, and the neural responses of all DNNs are used to predict the saliency map with the convolutional layer we previously introduced....|
|||...Note that Y  c for saliency prediction, we upsample Y  c has half the spatial resolution of Yc, i.e....|
|||...Finally, these neural responses feed the 1 1 convolutional layer that generates the saliency map Ys....|
|||...It can be seen that by combining features from both scales, our saliency map correctly highlights salient regions of different sizes....|
|||...Learning with Saliency Evaluation Objectives  In our architecture we have integrated the saliency prediction into the DNN, and the parameters can be learnt with back-propagation [31]....|
|||...In the experiments, we show that adapting the features of the DNN to saliency prediction yields significant improvement over directly using the off-the-shelf features....|
|||...Another advantage of our learning scheme is that backpropagation can be used to optimize the saliency evaluation metric....|
|||...Pixels in the saliency map are evaluated using a groundtruth label that indicates whether the pixel is salient or nonsalient....|
|||...Back-propagation allows directly optimizing the saliency metrics, which may better guide the learning towards the goal of saliency prediction....|
|||...There is a plethora of saliency evaluation metrics available that are complementary to each other....|
|||...area under a curve of true positive rate versus false positive rate for different thresholds on the saliency map, and the shuffled-AUC (sAUC) [36] alleviates the effects of center bias in the AUC scor...|
|||...The Normalized Scanpath Saliency (NSS) [28] computes the average value at all fixations in a normalized saliency map....|
|||...Similarity (Sim) [20] calculates the sum of minimum values of saliency distribution and fixation distribution at each  point....|
|||...Finally, the saliency map can be compared with the human fixation map with the Linear Correlation Coefficient (CC) [19] and the Kullback-Leibler divergence (KLD) [36]....|
|||...We evaluate the saliency map with the metrics previously introduced in Sec....|
|||...It takes 0.27s to predict one saliency map with GPU....|
|||...Recall that we can learn the DNN using different saliency evaluation metrics, i.e....|
|||...0.02 0 Weight Value  0.02  0.04  Figure 5: Histogram of the weights of the convolutional filter for saliency prediction....|
|||...Larger absolute weight value means bigger contribution to saliency prediction....|
|||...This suggests that fine-tuning effectively adapts the DNN representation for saliency prediction....|
|||...owest weight values, which correspond to the neurons that the filter considers more informative for saliency prediction....|
|||...We compare our method with 14 saliency prediction models, which are BMS [42], Adaptive Whitening Saliency [11], Attention based on Information Maxi(AWS) mization (AIM) [2], RARE [30], Local and Global...|
|||...We report the sAUC scores of different saliency models under optimal blurring on 6 datasets....|
|||...Many of them are recently published models that have shown top performance on saliency evaluation datasets....|
|||...We also compare our model with a very recently published saliency model based on DNN [26]....|
|||...We compare our results with 14 saliency prediction models, namely BMS [42], AWS [11], AIM [2], RARE [30], LG [1], eDN [39], Judd [21], SigSal [15], CAS [12], CovSal [8], QDCT [32], SUN [43], GBVS [14]...|
|||...Our saliency maps are very localized in the salient regions compared to the rest of the methods....|
|||...Before our results, it was unclear which method was performing best since BMS [42] ranked top for NSS, CC and Similarity, Deep Gaze [24] ranked top for AUC-Judd and AUCBorji, AWS [11] for sAUC, and Ou...|
|||...Also, note that our saliency maps are very localized in the salient regions compared with other methods, even when images have cluttered backgrounds (row 1 and 3)....|
|||...To reduce the semantic gap between model prediction and human behavior, we re-architect DNNs for object recognition to the task of saliency prediction....|
|||...This leads to a saliency prediction accuracy that significantly outperforms the state-of-the-art....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...Mit saliency benchmark....|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet....|
|||...An eye fixation database for saliency detection in images....|
|||...Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
|||...Learning visual saliency by combining feature maps in a nonlinear manner using adaboost....|
||76 instances in total. (in iccv2015)|
|15|Leifman_Learning_Gaze_Transitions_ICCV_2017_paper|...Learning Gaze Transitions from Depth to Improve Video Saliency Estimation  George Leifman  Amazon  Dmitry Rudoy  Intel Corporation  Tristan Swedish MIT Media Lab  gleifman@amazon.com  dmitry.rudoy@gma...|
|||...Our experiments indicate that it is beneficial to integrate depth into video saliency estimates for content that is viewed on a 2D display....|
|||...Our depth-aware video saliency is more similar to the ground-truth than the state-of-the-art method [34]....|
|||...3D visual information supplies a powerful cue for saliency analysis....|
|||...This has been shown by numerous studies that investigate the effect of depth information for image and video saliency [3, 17, 24, 30, 31]....|
|||...We propose a novel Depth-Aware Video Saliency approach that exploits depth information to establish saliency in video sequences (Figure 1)....|
|||...In other cases, the fact that the object is distant increases its saliency (b)....|
|||...The network predicts a saliency map for a frame, given the estimated map of the previous frame....|
|||...To evaluate the performance of our approach we introduce the Depth-Aware Video Saliency dataset....|
|||...In this paper we show how to get solid performance improvement for video saliency estimation when depth data is available along with the RGB frames....|
|||...Section 4 introduces our depth-aware video saliency approach....|
|||...This section discusses two saliency aspects closely related to our research: video saliency and depth-aware saliency....|
|||...Video Saliency: Most existing motion saliency methods improve image saliency models by taking into account simple motion cues....|
|||...[4] concentrate on motion saliency only by analyzing the Fourier spectrum of the video along X-T and Y-T planes....|
|||...Seo and Milanfar [36] propose using self-resemblance in static and space-time saliency detection....|
|||...[46] detects spatiotemporal visual saliency based on the phase spectrum of the videos....|
|||...We are interested in exploiting depth for saliency estimation, when the stimuli are twodimensional....|
|||...For still images, integrating depth information into the saliency model was first proposed more than a decade ago by Ouerhani et al....|
|||...The recent dramatic improvement of 3D-capable acquisition devices has prompted many researchers to find more effective ways to exploit depth for image saliency calculation....|
|||...[24] present a depth prior for saliency learned from human gaze information....|
|||...This saliency prior produces a saliency map that is then either directly added or multiplied by the saliency results of other methods....|
|||...[32] propose a saliency model, where depth and appearance information from multiple layers is taken into account simultaneously, rather than simply fusing depth-induced saliency with color-produced saliency....|
|||...:  1. dataset of RGBD videos containing ground-truth of  human attention  2. state-of-the-art video saliency estimation algorithm,  extended to take into account depth information  3.1....|
|||...Depth(cid:173)Aware Video Saliency Dataset  An overview of eye-tracking datasets is found in [43]....|
|||...Collecting the videos: The videos in our dataset should represent the scenarios where depth-aware saliency is beneficial....|
|||...racker  1700  fixation map explains itself. This quality measure also serves as an upper bound for saliency prediction....|
|||...DIEM is a well-known dataset, which has been widely used for evaluation of video saliency techniques....|
|||...We think that these clips better represent typical saliency use cases, since is the edited videos the viewers attention is directed by the editor....|
|||...We believe that our dataset represents the wide range of common scenarios where depth-aware saliency is beneficial....|
|||...Finally, the probabilities are integrated into a saliency map....|
|||...Finally, a saliency map is generated for each frame based on transition probabilities....|
|||... Gaussian blob, calculated by applying mean-shift clustering and Gaussian fitting on the normalized saliency maps and on the differences in the optical flow magnitude....|
|||...Depth-aware extension: We incorporate depth information in three key stages: static saliency estimation, optical flow calculation and gaze transition modeling....|
|||...First, depth-aware image saliency is used for generating candidate locations....|
|||...Finally, the saliency of pixel p in the destination frame is given by a sum of constant-size Gaussians around each destination candidate d, scaled up by the probability P (d):  S(p) =  1   ND  XdND  P...|
|||...Our Approach  This section presents our approach for depth-aware video saliency estimation, which is based on the following three principles....|
|||...Third, in addition to the above two principles, which are common to many previous video saliency approaches, we claim that depth perception has an impact on human attention....|
|||...As shown in Figure 6, our networks input is the saliency calculated for the previous frame and additional information from the current frame....|
|||...The input is the saliency calculated for the previous frame and additional information from the current frame....|
|||...Then the data is encoded and only the saliency of the current frame is reconstructed....|
|||...The output of the network is an estimation of a single saliency map S(t) for the current frame....|
|||...We use 2 distance between two distributions of the saliency maps....|
|||...The whole process is recursive, where we start with a saliency map S(0) which consists of a single Gaussian located in the center of  the first frame....|
|||...Then the estimated saliency map S(1) is fed as an input S(1) to the network for the next frame....|
|||...Since our saliency learning is recursive, only frames from different videos are used simultaneously, limiting the batch size to the number of videos in the training set....|
|||...In other words, the first batch consists of all the first frames, the second batch consists of all the second frames, when the input to the second batch is the saliency maps estimated in the first batch....|
|||...mmon frame-rate of 30 fps, in the implementation the S(t  1) is  1703  changed to include a single saliency map of 10 frames back....|
|||...The saliency map is then treated as a binary classifier to separate the positive samples from the negative ones....|
|||...Thresholding over the saliency map and plotting true positive rate vs. false positive rate results in the ROC curve....|
|||...AUC considers the saliency results at the locations of the human fixations....|
|||...Thus, it distinguishes purely between a peaky saliency map and a smooth one....|
|||...To the best of our knowledge, we are the first exploiting depth for video saliency when viewing on 2D screens....|
|||...We also compare our approach to video saliency technique [34], four image saliency methods [9, 18, 39, 45], depth-aware image saliency (RGBD) [32] and a Gaussian in the center [19]....|
|||...The Ground-truth in the bottom row is the upper bound for the saliency prediction, which measures how much the ground-truth fixation map explains itself (Equation 1)....|
|||...We also see that employing depth in video saliency algorithms, which are based on learning, improves their accuracy; both the extended baseline (Sec....|
|||...We compare our method to depth-aware image saliency [32], four image saliency methods [9, 18, 39, 45], a Gaussian placed in the center [19], video saliency [34] and the extended baseline algorithm fro...|
|||...The upper bound (Ground-truth) for the saliency predic2 and AUC meation is given in Equation 1....|
|||...Moreover, employing depth in learning based video saliency algorithms improves their accuracy....|
|||...Qualitative evaluation: Figure 7 demonstrates a qualitative comparison of our approach to the ground-truth and other saliency techniques....|
|||...In addition to the previously used saliency methods we compare our approach to DeepGaze II [22] and to Deep Saliency [7]....|
|||...1704  Ground-truth  GBVS [9]  Deep Saliency [7]  Rudoy et al....|
|||...[34]  Our w/o depth  Depth  RGBD [32]  DeepGaze II [22]  Extended baseline  Our w/ depth  Depth-aware saliency maps are similar to the ground-truth, detecting background motion....|
|||...Ground-truth  GBVS [9]  Deep Saliency [7]  Rudoy et al....|
|||...We compare our results to the ground-truth and to the additional saliency methods: video saliency (Rudoy et al.)...|
|||...[34], three image saliency methods (GBVS [9], DeepGaze II [22], Deep Saliency [7]) and depth-aware image saliency (RGBD) [32]....|
|||...Our method employs a generative convolutional neural network to reconstruct saliency for each frame by implicitly learning the gaze transition from the previous frame....|
|||...The network was trained to predict the saliency of the next frame by learning from depth, color, motion and saliency of the current frame....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...Spatiotemporal saliency detection for video sequences based on random walk with restart....|
|||...Temporal spectral residual: fast motion saliency detection....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Unsupervised video analysis based on a spatiotemporal saliency detector....|
||75 instances in total. (in iccv2017)|
|16|Li_Deep_Contrast_Learning_CVPR_2016_paper|...Resulting saliency maps are typically blurry, especially near the boundary of salient objects....|
|||...The first stream directly produces a saliency map with pixel-level accuracy from an input image....|
|||...The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries....|
|||...Introduction  Visual saliency aims at identifying the most visually distinctive parts in an image, and has received increasing interest in recent years....|
|||...mages, research has shown that salient object detection, which emphasizes object-level integrity of saliency prediction results, is more useful and can serve as a pre-processing step for a variety of ...|
|||...Various conventional saliency detection algorithms based on local or global contrast cues [8, 49] have been successfully developed....|
|||...Although machine learning based saliency models have been developed [32, 21, 30, 34], they are primarily for integrating different handcrafted features [21] or fusing multiple saliency maps generated ...|
|||...However, in all these methods, CNNs are all operated at the patch level instead of the pixel level, and each pixel is simply assigned the saliency value of its enclosing patch....|
|||...As a result, saliency maps are typically blurry without fine details, especially near the boundary of salient objects....|
|||...As a result, these methods usually have to run a CNN at least thousands of times (once for every patch) to obtain a complete saliency map....|
|||...For example, training a patch-oriented CNN model for saliency detection takes over 2 GPU days and requires hundreds of gigabytes of storage for the 5000 images in the MSRA-B dataset....|
|||... an end-to-end deep contrast network to overcome the aforementioned limitations of recent CNN-based saliency detection methods....|
|||...Here, endto-end means that our deep network only needs to be run on the input image once to produce a complete saliency map with the same pixel resolution as the input image....|
|||...In the fully convolutional stream, we design a multi-scale fully convolutional network (MS-FCN), which takes the raw image as input and directly produces a saliency map with pixellevel accuracy....|
|||...Our MS-FCN can not only generate effective semantic features across different scales, but also capture subtle visual contrast among multi-scale feature maps for saliency inference....|
|||...The segment-level spatial pooling stream generates another saliency map at the superpixel level by performing spatial pooling and saliency estimation over superpixels....|
|||...The fused saliency map from these two streams is further refined with a fully connected CRF for better spatial coherence and contour localization....|
|||... We propose a multi-scale fully convolutional network as the first stream in our deep contrast network to infer a pixel-level saliency map directly from the raw input image....|
|||...[26] trained a deep neural network for deriving a saliency map from multiscale features extracted using deep convolutional neural networks....|
|||...In [50, 7], both global context and local context are utilized and integrated into a deep learning based pipeline to produce saliency maps which well preserve object details....|
|||...Since this paper is focused on visual saliency based on deep learning, we discuss relevant work in this context below....|
|||...fully convolutional stream is a multi-scale fully convolutional network (MS-FCN), which generates a saliency map S1 with one eighth resolution of the raw input image by exploiting visual contrast acro...|
|||...The segment-wise spatial pooling stream generates a saliency map at the superpixel level by performing spatial pooling and saliency estimation over individual superpixels....|
|||...The saliency maps from both streams are fused at the end through an extra convolutional layer with 1  1 kernels in our deep network to produce the final saliency map....|
|||...To re-purpose it into a dense image saliency prediction network, the two fully connected layers of VGG16 are first converted into convolutional ones with 11 kernel as described in [31]....|
|||...The stacked feature maps (5 channels) are fed into a final convolutional layer with a 1  1 kernel and a single output channel, which is the inferred saliency map....|
|||...Although the output saliency map is of 8 subsampled resolution, they  480  are smooth enough and allow us to use simple bilinear interpolation to make their resolution the same as that of the origina...|
|||...We call this resized saliency map S1....|
|||...Segment-Level Saliency Inference  Salient objects often have irregular shapes and the corresponding saliency map has discontinuities along object boundaries....|
|||...Our multiscale fully convolutional network operates at a subsampled pixel level without explicitly modeling such saliency discontinuities....|
|||...To better model visual contrast between regions and visual saliency along region boundaries, we design a segment-wise spatial pooling stream in our network....|
|||...The output of the second fully connected layer is fed into the output layer, which uses the sigmoid function to perform logistic regression to produce a distribution over binary saliency labels....|
|||...We call the saliency map generated in this way S2....|
|||...During this process, the weights for fusing the saliency maps (S1 and S2) from the two streams as well as the parameters in the multiscale fully convolutional network are updated using stochastic grad...|
|||...Then we fix the parameters in the first stream and fine-tune the neural network in the second stream for one epoch using groundtruth saliency maps....|
|||...In our experiments, we have found that the final saliency detection performance does not vary much when the number of superpixels is between 200 and 300....|
|||...Spatial Coherence  Since both streams in our deep contrast network assign saliency scores to individual pixels or segments without considering the consistency of saliency scores among neighboring pixe...|
|||...Initially, P (1) = Si and P (0) = 1  Si, where Si is the saliency score at pixel xi from the fused saliency map S. ij (li, lj) is a pairwise potential and defined as follows,  ij =  (li, lj)"1 exp    ...|
|||...At the end of energy minimization, we generate a saliency map using the posterior probability of each pixel being salient....|
|||...We call the generated saliency map Scrf ....|
|||...The proposed saliency refinement model can not only generate smoother results with pixelwise accuracy but also well preserve salient object contours....|
|||...A quantitative study of the effectiveness of the saliency refinement model can be found in Section 5.3.2....|
|||...Comparison of saliency detection results with and without CRF....|
|||...Visual comparison of saliency maps generated from state-of-the-art methods, including our DCL and DCL+....|
|||...DCL+ consistently produces saliency maps closest to the ground truth....|
|||...We have noticed that many saliency annotations in this dataset may be controversial among different human observers....|
|||...As a result, none of the existing saliency models has achieved a high accuracy on this dataset....|
|||...All the datasets contain manually annotated groundtruth saliency maps....|
|||...To test the adaptability of trained saliency models to other different datasets, we use the models trained on the MSRA-B dataset and test them over all other datasets....|
|||...The precision and recall of a saliency map is computed by converting a continuous saliency map to a binary mask using a threshold and comparing the binary mask against the ground truth....|
|||...The PR curve of a dataset is obtained from the average precision and recall over saliency maps of all images in the dataset....|
|||...Comparison of precision-recall curves of 11 saliency detection methods on 3 datasets....|
|||...We also report the average precision, recall and F-measure using an adaptive threshold for generating a binary saliency map....|
|||...MAE is meaningful in evaluating the applicability of a saliency model in a task such as object segmentation....|
|||...We use DCL to denote our saliency model based on deep contrast learning only without CRF-based post-processing, and DCL+ to denote the saliency model that includes CRFbased refinement....|
|||...Note that this is far more efficient than the latest deep learning based methods which treat all image patches as independent data samples for saliency regression....|
|||...Comparison with the State of the Art  We compare our saliency models (DCL and DCL+) including against eight recent state-of-the-art methods, SF [39], GC [8], DRFI [21], PISA [43], BSCA [40], LEGS [44]...|
|||...For fair comparison, we use either the implementations or the saliency maps provided by the authors....|
|||...To train the FCN saliency model, we simply replace its last softmax layer with a sigmoid cross-entropy layer for saliency inference, and finetune the revised model using the training sets in the afore...|
|||...As can be seen, our method generates more accurate saliency maps in various challenging cases, e.g., objects touching the image boundary (the first two rows), multiple disconnected salient objects (th...|
|||....7  0.8  0.9  1.0  Recall   pre  rec  fmea  DCL+  DCL  MSFCN Segment-Level SC_MSFCN  we compare the saliency map S1 generated from the first stream (MS-FCN), the saliency map S2 from the second segmen...|
|||...7, the fused saliency map (DCL) consistently achieves the best performance on average precision, recall and F-measure, and the fully convolutional stream (MS-FCN) has more contribution to the fused re...|
|||...To demonstrate the effectiveness of MS-FCN, we also generate saliency maps from the last scale of MS-FCN (the best performing scale) for comparison....|
|||...5.3.2 Effectiveness of CRF  In Section 4.2, a fully connected CRF is incorporated to improve the spatial coherence of the saliency maps from our deep contrast network....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Disc: Deep image saliency computing via progressive representation learning....|
|||...Does luminance-contrast contribute to a saliency map for overt visual attention?...|
|||...Contextaware saliency detection....|
|||...Visual saliency based on multiscale deep features....|
|||...Bottom-up saliency is a discriminant process....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
|||...Pisa: Pixelwise image saliency by aggregating complementary appearance contrast measures with edge-preserving coherence....|
|||...Deep networks for saliency detection via local estimation and global search....|
||73 instances in total. (in cvpr2016)|
|17|Liu_Adaptive_Partial_Differential_2014_CVPR_paper|...Adaptive Partial Differential Equation Learning for Visual Saliency Detection  Risheng Liu, Junjie Cao, Zhouchen Lin B(cid:93) and Shiguang Shan  Dalian University of Technology  (cid:93)Key Lab....|
|||...We assume that the saliency of image elements can be carried out from the relevances to the saliency seeds (i.e., the most representative salient elements)....|
|||... by optimizing a discrete submodular function constrained with this LESD and a uniform matroid, the saliency seeds (i.e., boundary conditions) can be learnt for this image, thus achieving an optimal P...|
|||...Experimental results on various challenging image sets show the superiority of our proposed learning-based PDEs for visual saliency detection....|
|||...g., image editing [9], segmentation [18], compression [12], object detection and recognition [32]), saliency detection gains much attention in recent years and numerous saliency detectors have been pr...|
|||...The bottomup methods [13, 7, 38, 36, 39, 34, 22, 15] are data-driven and focus more on detecting saliency from image features, such as contrast, location and texture....|
|||...[13] consider local contrast and define  image saliency using center-surround differences of image features....|
|||...So one can utilize low-rank and sparse matrix decomposition methods and their extensions for saliency detection....|
|||...[15] formulate saliency detection as a semi-supervised clustering problem and use the well-studied facility location model to extract cluster centers for salient regions....|
|||...[40] use dictionary learning to extract region features and CRF to generate a saliency map....|
|||...ion from general intuitive considerations) is not suitable for complex vision tasks, such as visual saliency detection....|
|||...This is because saliency is a kind of intrinsic information contained in the image and its description strongly depends on human perception....|
|||...The pipeline of our learning-based LESD for saliency detection on an example image....|
|||...The orange region illustrates the core components (i.e., guidance map and saliency seeds) of our PDE saliency detector, which will be formally introduced in Section 2....|
|||...The bottom row shows the ground truth (GT for short) salient region and saliency maps computed by some state-of-the-art saliency detection methods....|
|||...exactly define a PDE system with fixed formulation and boundary conditions to describe all types of saliency due to the complexity of salient regions in real world images....|
|||...From the top-down view (i.e., object-level structure), high-level human perceptions (e.g., color [34], center [31], and semantic information [16]) are important for saliency detection....|
|||...Overall, traditional PDEs with fixed form and boundary conditions cannot efficiently describe complex visual saliency patterns quantitatively, thus may fail to solve the saliency detection problem....|
|||...We summarize the contributions of this paper as follows:   A novel PDE system is learnt to describe the evolution of visual attention in saliency diffusion....|
|||...We prove that visual attention in our system is a monotone submodular function with respect to saliency seeds....|
|||... We develop an efficient method to incorporate both bottom-up and top-down prior knowledge into the LESD formulation for saliency diffusion....|
|||... We derive a discrete optimization model with PDE and matroid constrains to extract saliency seeds for LESD....|
|||...Firstly, an adaptive PDE system, named Linear Elliptic System with Dirichlet boundary (LESD), is proposed to describe the saliency diffusion....|
|||...Then we develop efficient techniques to incorporate both bottom-up and top-down information into saliency diffusion and learn the specific formulation and boundary condition of LESD from the given image,....|
|||...Similarly, let v be a vector field on V and denote vp as the vector at p.  Input ImageSuperpixel  SetmentationCenter PriorColor PriorGuidance MapSaliency Score MapMasked Salient RegionBackground Prior...|
|||...Saliency Diffusion Using PDE System  This section proposes a diffusion viewpoint to understand visual saliency and establishes a PDE system to model saliency diffusion on an image....|
|||...Visual Attention Evolution  For a given visual scene, saliency detection is to find the regions which are most likely to capture humans attention....|
|||...on is firstly attracted by the most representative salient image elements (this paper names them as saliency seeds) and then the visual attention will be propagated to all salient regions....|
|||...Then we define a real-value visual attention score function f (p) : V  R to measure the saliency of p  V. Suppose we have known a set of saliency seeds (denoted as S) and its corresponding scores (i.e...|
|||...We can mathematically formulate saliency diffusion as an evolutionary PDE with Dirichelet boundary condition:  t  f (p, t)  = F (f,f ), f (g) = 0, f (p) = sp, p  S, where g is an environment point wit...|
|||...As the purpose of above PDE is to propagate visual attention from saliency seeds to other image elements, we adopt a linear diffusion term div(Kpf (p)) for the score function, in which Kp is an inhomo...|
|||...Linear Elliptic System with Dirichlet Boundary For saliency detection purpose, we only consider the situation when the saliency evolution is stable (i.e., no saliency  1Similar discretization scheme i...|
|||...Thus given an image, the saliency detection task reduces to the problem of solving an LESD....|
|||...Till now, we have established a general PDE system for saliency diffusion....|
|||... learnt g and S) can successfully incorporate image structure and high-level knowledge to model the saliency diffusion, thus achieves better saliency detection results than state-of-the-art approaches...|
|||...is the solution to (1) with respect to the saliency seed set S. This implies that the solution to our LESD is inherently combinatorial, thus much more difficult to be handled than the PDEs in conventi...|
|||...As shown in Section 3, these results provide good guarantees for our saliency detector....|
|||...(p;S) be the visual attention score of image element p. Suppose the sources {sp  0} are attached to saliency seed set S, i.e., f (p) = sp for all p  S. Then f is a monotone submodular function with re...|
|||...Learning LESD for Saliency Detection  This section discusses how to adaptively learn a specific LESD for saliency diffusion on a given image....|
|||...Based on the submodularity of the system, we also provide a discrete optimization model for boundary condition (i.e., saliency seeds S) learning....|
|||...(b)-(e) center prior fl, color prior fc, background diffusion prior ff , final guidance map g (top) and their corresponding saliency maps (bottom), respectively....|
|||...(f) optimal seeds (L = 43.8589) and final saliency map....|
|||...Optimizing Saliency Seeds via Submodularity Due to the following two reasons, we cannot choose all nodes in Fc as seeds for saliency diffusion....|
|||...rs (e.g., nodes near object boundary and bright or dark nodes on the object) may also lead to a bad saliency map (Fig....|
|||...Fortunately, by learning a proper guidance map g, we can enforce very small saliency scores (in most case near zero) in background regions (g in Fig....|
|||...In general, the performance of (5) is dependent on the maximum number of saliency seeds n (Fig....|
|||...We have experimentally found that a greedy algorithm with this stopping criterion is efficient for maximizing L in our saliency detector....|
|||...The complete pipeline of our saliency detector on a test image is also illustrated in Fig....|
|||...Discussions  In this section, we would like to discuss and highlight some aspects of our proposed PDE-based saliency detector....|
|||...In [24, 25], they adopt a nonlinear PDE formulation  Algorithm 1 Learning LESD for Saliency Detection Input: Given an image I and necessary parameters....|
|||...Output: Saliency map for the given image....|
|||...3: Initialize saliency seed set S  ....|
|||...4: while  S   n do 5: 6: 7:  Solve (3) with saliency seeds S  {p} for f. Obtain the gain L(p) = L(S  {p})  L(S), or  L(p) = L(S  {p})  L(S)....|
|||...16: Construct the final saliency map from f....|
|||...Therefore, we can successfully handle the more complex saliency detection task....|
|||...Specifically, the submodular optimization model in [15] is used to extract cluster centers8 and graph clustering and saliency map computation steps are required in their framework....|
|||...mization model to learn the Dirichlet boundary condition of the PDE system and directly extract the saliency map by solving the learnt PDE system (no further postprocessing is needed)....|
|||...We compare our methods (denoted as PDE in the comparisons) with seventeen state-of-the-art saliency detectors, such as IT [13], AC [1], CA [9], CB [14], FT [2], GB [10], GS [36], LC [41], LR [34], MZ ...|
|||...We also present ground truth (GT) salient regions and the saliency maps for compared methods....|
|||...These results also verify that  the proposed learning strategy can successfully incorporate both bottom-up and top-down information into saliency diffusion....|
|||...Qualitative Comparisons  We show example saliency maps computed by some typical saliency detectors in Fig....|
|||...As a result, they may share the same saliency value with the background region....|
|||...Conclusions  This paper develops a PDE system for saliency detection....|
|||...We define a Linear Elliptic System with Dirichlet boundary (LESD) to model the saliency diffusion on an image and prove the submodularity of its solution....|
|||...We evaluate our PDE on various challenging image sets and compare with many state-of-the-art techniques to show its superiority in saliency detection....|
|||...Fusing generic objectness  and visual saliency for salient object detection....|
|||...Context-aware saliency detection....|
|||...Static and space-time visual saliency detection by  self-resemblance....|
|||...Boosting color saliency in  image feature detection....|
|||...Geodesic saliency using background priors....|
|||...Bayesian saliency via low and mid level cues....|
|||...Top-down visual saliency via joint CRF and dictionary  learning....|
||72 instances in total. (in cvpr2014)|
|18|Predicting Salient Face in Multiple-Face Videos|...stract  Although the recent success of convolutional neural network (CNN) advances state-of-the-art saliency prediction in static images, few work has addressed the problem of predicting attention in ...|
|||...Existing literature on saliency prediction typically focuses on finding salient face in static images [21]....|
|||...Early work on image saliency prediction uses hand-craft features to predict visual attention for images [2, 10, 20, 26, 35, 42], based on understanding of the human visual system (HVS) [29]....|
|||...The representative method on predicting image saliency is Ittis model [20], which combines centersurround features of color, intensity and orientation together....|
|||...[41] proposed to precisely model saliency of face region, via learning the fixation distributions of face and facial features....|
|||...[14] proposed saliency in context (SALICON) method to learn features for image saliency prediction by incorporating convolutional neural network (CNN)....|
|||...For video saliency prediction, earlier methods [6, 8, 12, 1719] have investigated several dynamic features to model visual attention on videos, in light of the HVS....|
|||...For example, the Ittis image model was extended in [17] for video saliency prediction, by integrating with two dynamic features: motion and flicker contrast....|
|||...Later, several advanced video saliency prediction methods have been proposed, which exploits other dynamic features, such as Bayesian surprise in [18] and motion vector in [6]....|
|||...Recently, learning-based video saliency prediction methods have also emerged [13, 31, 33, 37]....|
|||...[33] proposed a learning-based video saliency prediction method, which explores the top-down information of eye movement patterns, i.e., passive and active states [34],  4420  Figure 1....|
|||...[13] proposed to learn middle-level features, i.e., gists of a scene, as the cue in video saliency prediction....|
|||...[37] proposed to predict saliency of a given frame according to its highand low-level features, conditioned on the detected saliency of the previous reference frame....|
|||...However, to our best knowledge, the existing video saliency prediction methods rely on the handcrafted features, despite CNN being applied to automatically learn features for image saliency prediction...|
|||...-based method to predict salient face in multiple-face videos, which learns both image features and saliency transition for modeling attention on multiple faces across frames....|
|||...To the best of our knowledge, our method is the first aiming at saliency prediction in multiple-face videos....|
|||...The first dataset includes fixations of 39 subjects on 65 multiple-face videos, used as a baseline for testing saliency prediction performance....|
|||...The second dataset is composed of 100 multiple-face videos viewed by 36 subjects, which is utilized for training the saliency prediction model....|
|||...Our experimental results demonstrate that our method achieves significant improvements on saliency prediction in multiple-face videos....|
|||...It is because both training and test utilize the fixations of same subjects are not rationale in existing saliency prediction works [1], despite videos being different....|
|||...Among them, two were experts working in the field of saliency prediction....|
|||...Others did not have any experience on saliency prediction, and meanwhile they were naive to the purpose of our eye tracking experiment....|
|||...We formulate the multiple-face saliency prediction as a regression problem, and build an M-LSTM network to generate the continuous saliency weights of multiple faces....|
|||...Formally, we aim to predict saliency weight of each face defined by wn,t, which is the ground truth (GT) attention proportion of the n-th face to all faces in frame t. For such prediction, M-LSTM netw...|
|||...These outputs are then passed through a softmax layer to produce the final saliency weight predictions:  4424  ,  (5)  wn,t =  n=1(Un,t  sn,t + bn,t)}  exp{n (cid:3)N (cid:3)N n=1 exp{n (cid:3)N where...|
|||...It is because the saliency changing mode in different faces is similar....|
|||...Postprocessing  After obtaining saliency weight of each face from MLSTM, postprocessing is required to generate final saliency map....|
|||...ing 2 has revealed that visual attention is also correlated with the center-bias feature 2Note that saliency produced by the channel of single feature is defined as the conspicuity map, in order to ma...|
|||...In this paper, the saliency prediction results are reported by averaging over those 65 videos....|
|||...Moveover,  of (8) is set to 100.2 for imposing center-bias on saliency of each face, in order to make saliency prediction appropriate....|
|||...The impact of different  on saliency prediction results is to be discussed in Section 4.3....|
|||...Evaluation on saliency prediction  l }L  In this section, we compare our method with 8 conventional saliency prediction methods3, including Xu et al....|
|||...Besides, [41], [14], [21] and [10] are recent image saliency prediction methods....|
|||...To be more specific, [41] and [21] work on saliency prediction of singleface and multiple-face images, respectively....|
|||...We compare our method to these two top-down methods, as there is no multiple-face saliency prediction method for videos....|
|||...On the contrary, [10] is a bottom-up method, which provides background saliency for our method....|
|||...In addition, [14] is another latest DL-based method 3In our experiments, we run the codes provided by the authors with  default parameters, to obtain saliency prediction results....|
|||...In order to consider background in saliency prediction, t with conour method combines face consipicuity map MF sipicuity maps of three saliency-related feature channels of t for color and MO GBVS [10]...|
|||...Let St be the final saliency map of the t-th video frame....|
|||...Next, we can compute (9) to predict saliency maps k=1 are of multiple-face videos, once the values of {k}4 known....|
|||...for saliency detection, which is also compared with our DLbased method....|
|||...The most recent work of [27] has reported that normalized scanpath saliency (NSS) and correlation coefficient (CC) perform the best among all metrics in evaluating saliency prediction accuracy4....|
|||...Table 2 reports the comparison results of saliency prediction, averaged over all 65 test videos of MUFVET-I....|
|||...We can see from this table that our method is much better than all other methods in predicting saliency of multiple-face videos....|
|||...lient 4 [27] has also shown that area under ROC (AUC) is the worst metric  in measuring accuracy of saliency prediction....|
|||...We show in Figure 8 the saliency maps of several frames in a video, generated by our and other 8 methods....|
|||...As a result, the saliency maps of our method are more accurate than those of other methods....|
|||...In addition, Figure 9 provides the saliency maps of the frames selected from 5 videos....|
|||...Performance analysis of saliency prediction  Since our M-LSTM presented in Section 3.3 aims at predicting saliency weights of faces across video frames, it is worth evaluating the prediction error of M-LSTM....|
|||...To this end, Figure 10 plots saliency weights of faces by CNN, MLSTM and GT, for the video sequence of Figure 8....|
|||...Accuracy of saliency prediction by our and other 8 methods, averaged over all test videos of MUFVET-I....|
|||...STM, as a deep RNN proposed in this paper, was utilized to take into account the transition of face saliency from previous frames, either in short-term or long-term....|
|||...As a result, saliency maps of multiple-face videos can be generated upon the predicted salient face....|
|||...In this figure, the mean squared error (MSE) between M-LSTM and GT averaged over 3 faces is 0.0081.  the predicted face saliency weights output by M-LSTM....|
|||...Here, we calculate quantify mean squared error (MSE) of face saliency weights between M-LSTM and GT, averaged over all faces in MUVFET-I....|
|||...This also implies the small gap of M-LSTM in predicting saliency weights of faces....|
|||...Next, it is interesting to see how the gap between our predicted and GT face saliency weights influences saliency prediction performance....|
|||...To this end, we use GT face saliency n=1,t=1 as the input to (7) for generalizing weights {wn,t}N,T the final saliency maps of multiple-face videos....|
|||...It can be found that saliency prediction performance of using estimated (M-LSTM) and target (GT) saliency weights of faces is close, implying that our method is approaching to the upper bound performance....|
|||...To this end, standard deviation  in (8) is traversed, imposing different impact of face center-bias on saliency prediction....|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Visual saliency detection by spatially weighted dissimilarity....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Visual saliency based on scale-space analysis in the frequency domain....|
|||...A data-driven metric In International  for comprehensive evaluation of saliency models....|
|||...Static saliency vs. dynamic saliency: a comparative study....|
|||...Shallow and deep convolutional networks for saliency prediction....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Bottom-up saliency detection with sparse representation of learnt texture atoms....|
|||...Learning to predict saliency on face In International Conference on Computer Vision (ICCV),  images....|
|||...Learning a saliency map using fixated loca tions in natural scenes....|
||71 instances in total. (in cvpr2017)|
|19|Tasse_Cluster-Based_Point_Set_ICCV_2015_paper|...Cluster-based point set saliency  Flora Ponjou Tasse  Jiri Kosinka  Neil Dodgson  The Computer Laboratory, University of Cambridge, UK  Figure 1: Cluster-based point set saliency on a range scan....|
|||...We evaluate cluster uniqueness and spatial distribution of each cluster and combine these values into a cluster saliency function....|
|||...Finally, the probabilities of points belonging to each cluster are used to assign a saliency to each point....|
|||...Our approach detects fine-scale salient features and uninteresting regions consistently have lower saliency values....|
|||...We evaluate the proposed saliency model by testing our saliency-based keypoint detection against a 3D interest point detection benchmark....|
|||...The human perceptual system is able to consistently and quickly estimate saliency for known and new shapes....|
|||...Perceptual research shows that contrast is an important factor in low-level visual saliency [7]....|
|||...Few papers handle saliency of point sets [24, 21, 27]....|
|||...Our proposed saliency algorithm does not use topological information, and thus supports a wider range of 3D surface representations, including polygon soups and 3D range scans....|
|||...We show that even though we do not use topological data, our saliency detection still performs better compared to mesh-based methods....|
|||...te a per-region contrast from cluster uniqueness and spatial distribution, and use this to assign a saliency value to each cluster and then each pixel....|
|||...Application of our proposed saliency detection to key point extraction (Section 7)....|
|||...[14] show that mesh saliency exists and can be useful in mesh simplification....|
|||...Mesh saliency methods are typically based on contrastbased saliency techniques for images [19, 20, 34, 29]....|
|||...For example, multi-scale image saliency [16] has inspired several 3D saliency methods based on the Difference of Gaussian scale space [24, 19, 17, 21, 2]....|
|||...To achieve stable saliency models, robust to noise, several techniques use local shape descriptors to compute contrast....|
|||...[11] compute the Integral Volume Descriptor for each point on a mesh, and saliency of a point is based on the uniqueness of its associated descriptor....|
|||...Saliency regions are grown around the local patches by incrementally adding neighbouring patches that maximize a saliency grade....|
|||...The mesh saliency model of Leifman et al....|
|||...[20] is inspired by context-aware saliency detection in images [13]....|
|||...Relatively few saliency models support point clouds [24, 21, 32, 27]....|
|||...[27] is the most robust point set saliency detection....|
|||...They typically perform 3 steps: an oversegmentation of the image, a per-cluster saliency detection, and a propagation of the cluster saliency to obtain per-pixel saliency....|
|||...Inspired by global contrast-based saliency in images [5], Wu et al....|
|||...Using these patches as queries, patch saliency is obtained by ranking all patches based on their relevance to the queries while respecting the manifold structure of the descriptor space of patches....|
|||...Both of these methods [34, 31] produce stable saliency maps but they only support meshes and the descriptor computation is inefficient....|
|||...Our method is inspired by cluster-based saliency detection in images [25, 4]....|
|||...We first cluster a given point set into similar regions, compute a saliency value per cluster, and then propagate these saliency values to points (Figure 1)....|
|||...The second step ensures that in the saliency propagation stage, the saliency of a point p is most influenced by those clusters p most likely belongs to....|
|||...164  Clustering  Uniqueness  Distribution  Cluster saliency  Point saliency  Figure 2: An overview of our saliency model....|
|||...Our adaptive fuzzy clustering approach to saliency detection is in contrast to other saliency models [34, 31] that oversegment a shape into mutually exclusive clusters based on spatial proximity....|
|||...Cluster-based saliency (Section 5) From the set of clusters described above, we compute cluster uniqueness and distribution....|
|||...Based on this premise, uniqueness and spatial distribution are integrated into a single saliency value per cluster....|
|||... dimensional space, whose points are represented by  f = [x, y, z, h1, ..., h33],  (1)  Point-level saliency Each point p is finally assigned a saliency value which is a linear combination of saliency...|
|||...Figures 1 and 2 illustrate the results of clustering, uniqueness, spatial distribution, cluster saliency, and point saliency on point sets....|
|||...Our saliency detection is therefore more stable and better captures interesting regions, compared to a simple voxel-based spatial clustering, especially with small K....|
|||...More precisely, two clusters are neighbours  Figure 3: Compare saliency results based on a voxel-based spatial clustering (top) and our adaptive clustering (bottom)....|
|||...On the Max Planck model, our saliency is concentrated around facial features, with values close to zero elsewhere, in contrast with Shtrom et al....|
|||...We also note on the Dragon model that our saliency result is less noisy and emphasizes fine-scale features such as the outline of the dragon mouth....|
|||...Cluster saliency We combine the saliency map obtained from the two cues above to construct a cluster saliency map Sj = Uj +Vj, where  is a parameter between 0 and 1 that determines the relative import...|
|||...Point-level saliency Finally, the saliency of a point with  index i is given by si =PK  6....|
|||...j =PK  We now compare our method against previous pointbased and mesh-based saliency methods, followed by a discussion on the influence of the parameter K....|
|||...Comparison with point-based saliency method In Figure 4, we compare results generated by our algorithm against state-of-the-art in point set saliency [27]....|
|||...Note how our saliency is concentrated in interesting regions such as the eyes on the Max Planck model....|
|||...[31] fail to capture salient regions on the human and glasses models, and in general, do not match high saliency regions in the ground-truth....|
|||...[29] saliency model tends to assign different saliency values to similar regions such as the bodies of the vases and arms of the teddy shape....|
|||...Overall, our saliency values closely match ground-truth, especially on the glasses model....|
|||...Comparison with mesh-based saliency method In Figure 5, we compare against the mesh-based methods by Song et al....|
|||...The only mesh-based method that consistently produces correct saliency maps is spectral mesh saliency [29]; it detects salient regions correctly but similar unsalient regions, like the main bodies of ...|
|||...Note that our method may assign high saliency to a flat region (e.g....|
|||...Influence of the number of clusters Figure 6 shows how the number of desired clusters K affects saliency detection....|
|||...The smallest value of K beyond which more clusters do not improve the saliency depends on the input point cloud....|
|||...However, our saliency detection is robust enough that our adaptation of SLIC can be replaced with a faster clustering algorithm such as voxelization, with an expected small quality loss....|
|||...s  Total 0.84s 4s 4s 11s 13s  Table 1: Computation times of a single-threaded implementation of our saliency detection, on an Intel Core i7 CPU with 2.5 Ghz and 16GB RAM....|
|||...[27] saliency time complexity is O(kN log N ), where k is the neighbourhood size used....|
|||...They detect saliency on the Igea model (134K points) in 2 minutes....|
|||... data: local maxima computed from Mesh Saliency [19], salient points based on a combination of mesh saliency with statistical descriptors [2], scale-dependent corners [22], 3DSIFT [12], 3D-Harris [28]...|
|||...[27] also tested their point set saliency detection model against this benchmark by selecting local maxima above a certain saliency threshold as keypoints....|
|||...[27], our keypoints are local maxima over a saliency threshold given by the average saliency over all local maxima....|
|||...Ground truth [6]  Our saliency  Our keypoints  Mesh Saliency [19] Salient points [2]  HKS [30]  Figure 7: Interest point detection: keypoints detected by our algorithm compared to other methods....|
|||...Our algorithm is able to achieve the above competitive results without using any mesh connectivity information during saliency detection or key point extraction....|
|||...Our qualitative results show that our saliency model detects fine-scale salient features better than other state-of-the-art point-based and mesh-based methods....|
|||...We apply our saliency model to keypoint detection and show that it has significantly lower false positive error rates than previous work, with the exception of keypoints based on the Heat Kernel Signa...|
|||...Sparse points matching by combining 3D mesh saliency with statistical descriptors....|
|||...Does luminance-contrast contribute to a saliency map for overt visual attention?...|
|||...Predicting and evaluating saliency for simplified polygonal models....|
|||...Mesh saliency and human eye fixations....|
|||...Mesh saliency via ranking unsalient patches in a descriptor space....|
|||...Mesh saliency with global rarity....|
||69 instances in total. (in iccv2015)|
|20|Li_A_Data-Driven_Metric_ICCV_2015_paper|...Innovation Center, School of EE & CS, Peking University  Abstract  In the past decades, hundreds of saliency models have been proposed for fixation prediction, along with dozens of evaluation metrics....|
|||...However, existing metrics, which are often heuristically designed, may draw conflict conclusions in comparing saliency models....|
|||...To address this problem, we propose a data-driven metric for comprehensive evaluation of saliency models....|
|||...Instead of heuristically designing such a metric, we first conduct extensive subjective tests to find how saliency maps are assessed by the humanbeing....|
|||...presentative evaluation metrics are directly compared by quantizing their performances in assessing saliency maps. Moreover, we propose to learn a data-driven metric by using Convolutional Neural Netw...|
|||...Compared with existing metrics, experimental results show that the data-driven metric performs the most consistently with the human-being in evaluating saliency maps as well as saliency models....|
|||...Introduction  Due to the booming of visual saliency models in the past decades, model benchmarking has become a popular topic in the field of computer vision (e.g., [3, 4, 7])....|
|||...Multiple subjects are asked to determine which estimated saliency map (ESM) is more similar to the ground-truth saliency map (GSM)....|
|||...e, the metric Kullback-Leibler Divergence (KLD) can be computed as the relative entropy between: 1) saliency histograms at recorded fixations and random points [3, 17, 18, 36]; 2) saliency histograms ...|
|||...hout knowing the implementation details of these variants, it becomes difficult to directly compare saliency models, even though their performances on the same dataset have been reported by using the ...|
|||... show that the data-driven metric performs the most consistently with the human-being in evaluating saliency maps as well as saliency models....|
|||...Moreover, we also provide the ranking lists of state-of-the-art saliency models by using the learned CNN-based metric....|
|||...sts, which we promise to release so as to facilitate the design of robust and effective metrics for saliency model evaluation; 2) The performances of nine representative metrics are quantized for dire...|
|||...A Brief Review of Evaluation Metrics  In the literature, there already exist many surveys of visual saliency models and evaluation metrics (e.g., [5, 8, 32])....|
|||...Since fixated locations often distribute around image centers (i.e., the center-bias effect), the classic AUC favors saliency models that emphasize center regions or suppress peripheral regions....|
|||...Metrics such as AUC, sAUC and rAUC only focus on the ordering of saliency [28, 39], while the saliency magnitude is ignored....|
|||...Normalized Scan-path Saliency (NSS, 5)....|
|||...As stated in [14], SIM can be computed by summing up the minimum saliency value at every location of S and G, while S and G are both normalized to sum up to one....|
|||...In this study, we combine the KLD metrics in [3] and [32] to compute a symmetric KLD according to the saliency distributions over S and G. In this case, smaller KLD implies a better performance....|
|||...Most existing saliency models adopted 1  9 for performance evaluation....|
|||...[32] investigated the correlation between metrics and provided several ranking  191  lists of saliency models....|
|||...Subjective Tests for Metric Analysis  In this section, we conduct extensive subjective tests to find how saliency maps are assessed by the human-being....|
|||...For each image, we generate 7 ESMs with 7 saliency models, including M0 (AVG, which simply outputs the average fixation density map from Toronto or MIT, see Fig....|
|||...vestigating these explanations, we find the following key factors that may affect the evaluation of saliency maps:  1) The most salient locations....|
|||...In most cases, both ESMs unveil visual saliency to some extent, and the most salient regions play a critical role in determining which ESM performs better....|
|||...In particular, the overlapping ratio of the most salient regions in ESM and GSM is the most important factor in assessing saliency maps....|
|||...The compactness of salient locations is an important factor for assessing saliency maps as well....|
|||...that Sg  k outperforms Sp k.  In the tests, subjects also report the reasons why they think certain saliency maps are good or poor. By in Given the user data obtained from tests, we first address a co...|
|||...Surprisingly, the average fixation density maps outperform ESMs from saliency models in 35.1% subjective comparisons....|
|||...That is, we generate a ranking list of the seven saliency models with each metric, and compare them with the ranks reported in (2)....|
|||...This implies the performances of saliency models keep on improving in the past decades, even though the evaluation metrics are imperfect....|
|||...The best metric NSS, which successfully ranks all the seven models, achieves only 82.7% agreement with subjects in assessing saliency maps....|
|||...Therefore, it is necessary to develop a metric which can assess saliency maps as the human-being does....|
|||...By using this metric, the comparison between two saliency models can be conducted by measuring the times (and probability) that ESMs from one model outperform those from the other model....|
|||...After that, we benchmark 23 saliency models with the data-driven metric to find the best saliency models....|
|||...Benchmarking State(cid:173)of(cid:173)the(cid:173)arts  Given the metric learned from all user data, we use it to benchmark state-of-the-art saliency models....|
|||...The other 20 saliency models can be categorized into three groups, including: 1) The first group contains 8 bottom-up saliency models, including IT [19], GB [13], CA [11], RARE [33], AWS [10], LG [2],...|
|||...cores over all images, we adopt the one-vs-all comparisons to provide a comprehensive evaluation of saliency models (i.e., the way we adopted in generating the subjective ranking list of models)....|
|||...9, we think this one-vs-all ranking methodology can provide a more comprehensive evaluation of saliency models than directly using the average performance scores....|
|||...In this study, we propose a data-driven metric for comprehensive evaluation of saliency models....|
|||...Experimental results show that this CNN-based metric outperforms nine representative metrics in assessing saliency maps....|
|||...We also provide three ranking lists of 23 models to reveal the best saliency models....|
|||...In the future work, we will incorporate eye-tracking devices so as to discover the latent mechanisms in assessing saliency maps....|
|||...We will also explore the feasibility of designing new saliency models under the guidance of such a CNN-based metric....|
|||...Boosting bottom-up and top-down visual features for saliency estimation....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Mit saliency benchmark....|
|||...This implies that existing saliency models perform far from satisfactory, and there is still a long way to go in the area of visual saliency estimation....|
|||...Discussion and Conclusion  In visual saliency estimation, many researchers have noticed that it is often insufficient to use only one metric in model comparison....|
|||...This makes the selection of evaluation metric a much confusing step in developing new saliency models....|
|||...Therefore, it is necessary to address a long-standing concern: how to measure the performance (in other words, the reliability) of a metric in evaluating saliency maps and saliency models?...|
|||...To compare various metrics, we conduct extensive subjective tests to find how saliency maps are assessed by subjects....|
|||...By assuming that human performs the best in assessing saliency maps, we can thus provide a quantitative performance score for each metric....|
|||...Selection of a best metric and evaluation of bottom-up visual saliency models....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform. In CVPR, pages 18, 2008....|
|||...Visual saliency detection using information divergence....|
|||...Spatiotemporal saliency perception via hypercomplex frequency spectral contrast....|
|||...Visual saliency with statistical  priors....|
|||...Probabilistic multitask learning for visual saliency estimation in video....|
|||...Removing label ambiguity IEEE TIP,  in learning-based visual saliency estimation....|
|||...Robust and efficient saliency modeling from image co-occurrence histograms....|
|||...Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
|||...Sun: A Bayesian framework for saliency using natural statistics....|
|||...Learning visual saliency by combining feature maps in a nonlinear manner using adaboost....|
||68 instances in total. (in iccv2015)|
|21|cvpr18-Revisiting Video Saliency  A Large-Scale Benchmark and a New Model|...Introduction  In this work, we contribute to video saliency research in two ways....|
|||...Existing video saliency datasets lack variety and generality of common dynamic scenes and fall short in covering challenging situations in unconstrained environments....|
|||...In contrast, DHF1K makes a significant leap in terms of scalability, diversity and difficulty, and is expected to boost video saliency modeling....|
|||...Second, we propose a novel video saliency model that augments the CNN-LSTM network architecture with an attention mechanism to enable fast, end-to-end saliency learning....|
|||...The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning more flexible temporal saliency representation across successive frames....|
|||...We thoroughly examine the performance of our model, with respect to state-of-the-art saliency models, on three largescale datasets (i.e., DHF1K, Hollywood2, UCF sports)....|
|||...This task, referred to as dynamic fixation prediction or video saliency detection is very useful for understanding human attentional behaviors and has several practical real-word applications (e.g., v...|
|||...As a consequence, existing datasets often fail to offer a rich set of fixations for learning video saliency and to assess models....|
|||...While saliency benchmarks (e.g., MIT300 [32] and LSUN [68]) have been very instrumental in progressing the static saliency field, such standard widespread benchmarks are missing for video saliency modeling....|
|||...Further, we propose a novel CNN-LSTM architecture [12, 46] based video saliency model with a supervised attention mechanism....|
|||...An attention module, learned from existing large-scale image saliency datasets, is used to enhance spatially informative features of the CNN....|
|||...Such a design helps disentangle underlying spatial and temporal factors of dynamic attention and allows convLSTM to learn temporal saliency representations efficiently....|
|||...for predicting human gaze in dynamic scenes, which explicitly encodes static attention into dynamic saliency representation learning by leveraging both static and dynamic fixation data....|
|||...Video Eye(cid:173)Tracking Datasets  There exist several datasets [43, 44, 25, 17] for dynamic visual saliency prediction, but they are limited and often lack variety, generality and scalability of instances....|
|||...Early static saliency models [36, 69, 15, 7, 18, 22, 33, 63] are mostly based on the contrast assumption that conspicuous visual features pop-out and involuntarily capture attention (see [5, 6] for review)....|
|||...ltiple visual features such as color, edge, and orientation at multiple spatial scales to produce a saliency map: an image distribution predicting the conspicuity of specific locations and their likel...|
|||...Deep learning based static saliency models [54, 35, 24, 39, 47, 29, 56, 57] have achieved astonishing improvements, relying on the powerful end-to-end learning ability of neural network and the availa...|
|||...In contrast to previous models learning attentions implicitly, our attention module encodes strong static saliency information and can be learned from existing static saliency dataset in a supervised manner....|
|||...3 presents the overall architecture of our video saliency model....|
|||...The CNN-LSTM network is extended with a supervised attention mechanism, which explicitly captures static saliency information and allows the LSTM to focus on learning dynamic information....|
|||...Thus our model is able to produce accurate, spatiotemporal saliency with improved generalization ability....|
|||...At time step t, with an input frame It with 224224 resolution, we have Xt  R2828512 and a 2828 dynamic saliency map from the convLSTM....|
|||...Network architecture of the proposed video saliency model....|
|||...(c) ConvLSTM used for learning sequential saliency representations....|
|||...Through the additional attention module, the CNN is enforced to generate a more explicit spatial saliency representation....|
|||...To remedy this, we insert a set of downand up-sampling operations into the attention module, which would enhance the intra-frame saliency information with an enlarged receptive field....|
|||...further enhanced by:  The above attention module may lose useful information for learning a dynamic saliency representation, as the attention module only considers static saliency information in still...|
|||...We use the following loss function [24] that considers three different saliency evaluation metrics instead of one....|
|||...The rationale here is that no single metric can fully capture how satisfactory a saliency map is....|
|||...We denote the predicted saliency map as Y  [0, 1]2828, the map of fixation locations as P  {0, 1}2828 and the continuous saliency map (distribution) as Q  [0, 1]2828....|
|||...The continuous saliency map is obtained via blurring each fixation location with a small Gaussian kernel....|
|||...zed Scanpath Saliency (NSS), respectively, which are derived from commonly used metrics to evaluate saliency prediction models....|
|||...X c = M  X c,  (6)  LKL is widely adopted for training saliency models and  where c  {1, ....|
|||...It is calculated by taking the mean of scores from the normalized saliency map Y (with zero mean and unit standard deviation) at human eye fixations P ....|
|||...More specifically, in a video training batch, we apply a loss defined over the final dynamic saliency prediction from LSTM....|
|||...Let {Y d t=1 denote the dynamic saliency predictions, the dynamic fixation sequence and the continuous ground-truth saliency maps, we minimize the following loss:  t=1, and {Qd  t }T  t=1, {P d  t }T ...|
|||... for our static attention module, the ground-truth static fixation map, and the ground-truth static saliency map....|
|||...OM-CNN, Two-stream, SALICON, DVA, ShallowNet, and Deep-Net are deep learning models, and others are classical saliency models....|
|||...For each training setting, we derive two baselines: Our and Attention module, refer to our final dynamic saliency prediction and the intermediate output of our attention module, respectively....|
|||...Here, we employ five classic metrics, namely Normalized Scanpath Saliency (NSS), Similarity Metric (SIM), Linear Correlation Coefficient (CC), AUC-Judd (AUC-J), and shuffled AUC (s-AUC)....|
|||...Table 5 reports the comparative results with the aforementioned saliency models, on the testing set (300 video sequences) of DHF1K dataset....|
|||...Training settings (5.1) for video saliency datasets: (i) DHF1K, (ii) Hollywood-2, (iii) UCF sports, and (iv) DHF1K+Hollywood-2+UCF sports....|
|||...tion module, which makes our model explicitly learn static and dynamic saliency representations in CNN and LSTM separately....|
|||...Dynamic saliency models: deep vs non-deep learning....|
|||...Qualitative results of our video saliency model on three datasets....|
|||...We attribute this into the inherent difficulties of video saliency prediction and previous models neglect of utilizing existing rich static saliency data....|
|||...e carefully designed and systematically collected benchmark dataset to facilitate research in video saliency modeling....|
|||...To the best of our knowledge, our work is the most comprehensive performance evaluation of video saliency models....|
|||...Further, we proposed a novel deep learning based video saliency model, which encodes a supervised attention mechanism to explicitly capture static saliency information and help LSTM better capture dyn...|
|||...Spatio-temporal IEEE  saliency networks for dynamic saliency prediction....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Where should saliency models look next?...|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Video saliency incorporating spatiotemporal cues and uncertainty weighting....|
|||...Discriminant saliency for visual  recognition from cluttered scenes....|
|||...A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression....|
|||...SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes....|
|||...End-to-end saliency mapping via probability distribution prediction....|
|||...Predicting video saliency with object-to-motion CNN and two-layer convolutional LSTM....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Stereoscopic thumbIEEE  nail creation via efficient stereo saliency detection....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
|||...Actions in the eye: dynamic gaze datasets and learnt saliency models for visual recognition....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Correspondence driven saliency transfer....|
||67 instances in total. (in cvpr2018)|
|22|Yang_Saliency_Detection_via_2013_CVPR_paper|...niversity of California at Merced  Abstract  Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, wher...|
|||...We also create a more difficult benchmark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the sali...|
|||...Introduction  The task of saliency detection is to identify the most important and informative part of a scene....|
|||...We note that saliency models have been developed for eye fixation prediction [6, 14, 15, 17, 19, 25, 33] and salient object detection [1, 2, 7, 9, 23, 24, 32]....|
|||...generate bounding boxes [7, 10], binary foreground and background segmentation [12, 23, 24, 32], or saliency maps which indicate the saliency likelihood of each pixel....|
|||...[23] propose a binary saliency estimation model by training a conditional random field to combine a set of novel features....|
|||...[1] compute the saliency likelihood of each pixel based on its color contrast to the entire image....|
|||...[9] consider the global region contrast with respect to the entire image and spatial relationships across the regions to extract saliency map....|
|||...propose a context-aware saliency algorithm to detect the image regions that represent the scene based on four principles of human visual attention....|
|||...[35] propose a novel model for bottom-up saliency within the Bayesian framework by exploiting low and mid level cues....|
|||...[27] show that the complete contrast and saliency estimation can be formulated in a unified way using high-dimensional Gaussian filters....|
|||...Most above-mentioned methods measure saliency by measuring local center-surround contrast and rarity of features over the entire image....|
|||...In this work, we exploit these cues to compute pixel saliency based on the ranking of superpixels....|
|||...We model saliency detection as a manifold ranking problem and propose a two-stage scheme for graph labelling....|
|||...From each labelled result, we compute the saliency of nodes based on their relevances (i.e, ranking) to those queries as background labels....|
|||...The four labelled maps are then integrated to generate a saliency map....|
|||...In the second stage, we apply binary segmentation on the resulted saliency map from the first stage, and take the labelled foreground nodes as salient queries....|
|||...The saliency of each node is computed based on its relevance to foreground queries for the final map....|
|||...Different from [12], the proposed saliency detection algo 3165 3165 3167  rithm with manifold ranking requires only seeds from one class, which are initialized with either the boundary priors or foreg...|
|||...Experimental results using large benchmark data sets show that the proposed algorithm performs efficiently and favorably against the state-of-the-art saliency detection methods....|
|||...(3)  We compare the saliency results using Eq....|
|||...Saliency Measure  Given an input image represented as a graph and some salient query nodes, the saliency of each node is defined as its ranking score computed by Eq....|
|||...queries for saliency detection are selected by the proposed algorithm, some of them may be incorrect....|
|||...Thus, we need to compute a degree of confidence (i.e., the saliency value) for each query, which is defined as its ranking score ranked by the other queries (except itself)....|
|||...If we compute the saliency of each query without setting the diagonal elements of A to 0, its ranking value in f  will contain the relevance of this query to itself, which is meaningless and often abn...|
|||...Lastly, we measure the saliency of nodes using the normalized ranking score f  when background queries are given....|
|||...As neighboring nodes are likely to share similar appearance and saliency values, we use a k-regular graph to exploit the spatial relationship....|
|||...The weights are computed based on the distance in the color space as it has been shown to be effective in saliency detection [2, 4]....|
|||...Two-Stage Saliency Detection  In this section, we detail the proposed two-stage scheme for bottom-up saliency detection using ranking with background and foreground queries....|
|||...Specifically, we construct four saliency maps using boundary priors and then integrate them for the final map, which is referred as the separation/combination (SC) approach....|
|||...From left to right: input images, saliency maps using all the boundary nodes together as queries, four side-specific maps, integration of four saliency maps, the final saliency map after the second stage....|
|||...the saliency measure....|
|||...We normalize this vector to the range between 0 and 1, and the saliency map using the top boundary prior, St can be written as:  St(i) = 1  f   (i)  i = 1, 2, ....|
|||...We note that the saliency maps are computed with different indicator vector y while the weight matrix W and the degree matrix D are fixed....|
|||...The four saliency maps are integrated by the following process:  Sbq(i) = St(i)  Sb(i)  Sl(i)  Sr(i)....|
|||...(6)  There are two reasons for using the SC approach to generate saliency maps....|
|||...As shown in the second column of Figure 5, the saliency maps generated using all the boundary nodes are poor....|
|||...Due to the imprecise labelling results, the pixels with the salient objects have low saliency values....|
|||...From left to right: input image, saliency map of the first stage, binary segmentation, the final saliency map....|
|||...stuff (such as grass or sky) and therefore they rarely occupy three or all sides of image, the proposed SC approach ensures at least two saliency maps are effective (third column of Figure 5)....|
|||...By integration of four saliency maps, some salient parts of object can be identified (although the whole object is not uniformly highlighted), which provides sufficient cues for the second stage detec...|
|||...To alleviate this problem and improve the results especially when objects appear near the image boundaries, the saliency maps are further improved via ranking with foreground queries....|
|||...Ranking with Foreground Queries  The saliency map of the first stage is binary segmented (i.e., salient foreground and background) using an adaptive threshold, which facilitates selecting the nodes of...|
|||...Thus, the threshold is set as the mean saliency over the entire saliency map....|
|||...That is, background saliency can be suppressed effectively (fourth column of Figure 6)....|
|||...Similarly, in spite of the saliency maps after the first stage of Figure 5 are not precise, salient object can be well detected by the saliency maps after the foreground queries in the second stage....|
|||...Algorithm 1 Bottom-up Saliency based on Manifold Ranking  Input: An image and required parameters 1: Segment the input image into superpixels, construct a graph G with superpixels as nodes, and comput...|
|||...Then, compute the saliency map Sbq by Eq....|
|||...4: Bi-segment Sbq to form salient foreground queries and an indicator vector y. Compute the saliency map Sf q by Eq....|
|||...Output: a saliency map Sf q representing the saliency value of each superpixel....|
|||...We compare our method with fourteen state-of-the-art saliency detection algorithms: the IT [17], GB [14], MZ [25], SR [15], AC [1], Gof [11], FT [2], LC [37], RC [9], SVO [7], SF [27], CB [18], GS SP ...|
|||...Similar as prior works, the precisionrecall curves are obtained by binarizing the saliency map using thresholds in the range of 0 and 255....|
|||...Figure 8 (c) shows that our approach using the integration of saliency maps generated from different boundary priors performs better in the first stage....|
|||...We evaluate the performance of the proposed method against fourteen state-of-the-art bottom-up saliency detection methods....|
|||...We note that the proposed methods outperforms the SVO [7], Gof [11], CB [18], and RC [9] which are top-performance methods for saliency detection in a recent benchmark study [4]....|
|||...We also compute the precision, recall and F-measure with an adaptive threshold proposed in [2], defined as twice the mean saliency of the image....|
|||...Figure 10 shows a few saliency maps of the evaluated methods....|
|||...To compute precision and recall values, we first fit a rectangle to the binary saliency map and then use the output bounding box for  3169 3169 3171     1     Precision Recall Fmeasure  Ours CB SVO S...|
|||...The proposed algorithm consistently generates saliency maps close to the ground truth....|
|||...Similar to the experiments on the MSRA1000 database, we also binarize saliency maps using the threshold of twice the mean saliency to compute precision, recall and F-measure bars....|
|||...Our run time is much faster than that of the other saliency models....|
|||...fically, the superpixel generation by SLIC algorithm [3] spends 0.165 s (about 64%), and the actual saliency computation spends 0.091 s. The MATLAB implementation of the proposed algorithm is availabl...|
|||...ground queries for ranking to generate the saliency maps....|
|||...Geodesic saliency using  tuned salient region detection....|
|||...Bayesian saliency via low and  superpixels....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Top-down visual saliency via joint crf and  dictionary learning....|
||67 instances in total. (in cvpr2013)|
|23|Jetley_End-To-End_Saliency_Mapping_CVPR_2016_paper|..., France  sjetley@robots.ox.ac.uk  naila.murray@xrce.xerox.com  eleonora.vig@dlr.de  Abstract  Most saliency estimation methods aim to explicitly model low-level conspicuity cues such as edges or blob...|
|||...Data-driven methods for training saliency models using eye-fixation data are increasingly popular, particularly with the introduction of large-scale datasets and deep architectures....|
|||...However, current methods in this latter paradigm use loss functions designed for classification or regression tasks whereas saliency estimation is evaluated on topographical maps....|
|||...In this work, we introduce a new saliency map model which formulates a map as a generalized Bernoulli distribution....|
|||...We show in extensive experiments the effectiveness of such loss functions over standard ones on four public benchmark datasets, and demonstrate improved performance over state-of-the-art saliency methods....|
|||...Introduction  This work is concerned with visual attention prediction, specifically, predicting a topographical visual saliency map when given an input image....|
|||...Traditional saliency models, such as the seminal work of Itti et al....|
|||...Sample image (left) with ground-truth saliency map (middle) and map predicted by our PDP approach (right)....|
|||...elling paradigm involves using data-driven approaches to learn patch-level classifiers which give a local image patch a saliency score [19, 18], using eye-fixation data to derive training labels....|
|||...A recent trend has emerged which intersects with both of these paradigms: to use hierarchical models to extract saliency maps, with model weights being learned in a supervised manner....|
|||...owever, while these deep methods have focused on designing appropriate architectures for extracting saliency maps, they continue to use loss functions adapted for semantic tasks, such as classificatio...|
|||...Our contributions are therefore the following:   a novel formulation which represents a saliency map  as a generalized Bernoulli distribution;  15753   a set of novel loss functions which are paired ...|
|||...Our extensive experimental validation on four datasets demonstrates the effectiveness of our approach when compared to other loss functions and other state-of-the-art approaches to saliency map generation....|
|||...For an excellent survey of saliency estimation methods, please refer to [2]....|
|||...gnificantly outperform non-deep and shallower models, even if not trained explicitly on the task of saliency prediction....|
|||...[16] and TurkerGaze/iSUN [43], has enabled training deep architectures specifically for the task of saliency prediction....|
|||...This method reports state-of-the-art results on the LSUN 2015 saliency prediction challenge [47]....|
|||...Finally, one of the most recent works in this paradigm [11] proposes the use of deep neural networks to bridge the semantic gap in saliency prediction via a two-pronged strategy....|
|||...The first is the use of the KL-divergence as a loss function motivated by the fact that it is a standard metric for evaluation of saliency methods....|
|||...5754  In this work, we argue for a well-motivated probabilistic modelling of the saliency maps and hence study the use of KL-divergence, among other probability distance measures, as loss functions....|
|||...As we discuss in section 4, we observe that our Bhattacharyya distance-based loss function consistently outperforms the KL-divergence-based one across 4 standard saliency metrics....|
|||...ing framework in which a fully-convolutional network is trained on pairs of images and ground-truth saliency maps ggg modeled as distributions....|
|||...maps as probability distributions  Saliency estimation methods have typically sought to model local saliency based on conspicuity cues such as local edges or blob-like structures, or on the scores of ...|
|||...At the limit of n   this AUC score is 92%, which can therefore be considered a realistic upper-bound for saliency estimation performance....|
|||...Ground-truth saliency maps are constructed from the aggregated fixations of multiple observers, ignoring any temporal fixation information....|
|||...Our goal is to predict this attentional landscape, or saliency map....|
|||...that the maps are based on aggregated fixations without temporal information, we propose to model a saliency map as a probability distribution over pixels, where each value corresponds to the probabil...|
|||...That is, we represent a saliency map as a generalized Bernoulli distribution ppp = (p1, , pi, , pN ), where ppp is the probability distribution over a set of pixels forming an image, pi is the probabi...|
|||...(1)  exp  i  j  Pj exp  exg  i  j  Pj exg  where xxx = (x1, , xi, , xN ) is the set of unnormalized saliency response values for either the groundtruth map (xgxgxg) or the predicted map (xpxpxp)....|
|||...Training the prediction model  The network architecture and saliency map extraction pipeline is shown in Figure 2....|
|||...As saliency datasets tend to be much too small to train such large networks from random initializations (the largest dataset has 15000 images, compared to 1M for ImageNet), it is essential to initiali...|
|||...We then progressively decrease the number of feature maps using additional convolutional layers, until a final down-sampled saliency map is produced....|
|||...Because the response maps undergo several max-pooling operations, the predicted saliency map ppp is lower-resolution than the input image....|
|||...Note that while several deep saliency models explicitly include a center bias (see e.g....|
|||...We tested this by adding Gaussian blurring and a center-bias to our maps, with optimized parameters, using the post-processing code of the MIT saliency benchmark [5]....|
|||...Our proposed saliency map extraction pipeline: the input image is introduced into a convNet with an identical architecture to the convolutional-layer portion of VGGNet....|
|||...Additional convolutional layers are then applied, resulting in a single response map which is upsampled and softmax-normalized at testing time to produce a final saliency map....|
|||...l evaluation  This section describes the experimental datasets used for training and evaluating the saliency prediction models followed by a discussion on the quantitative and qualitative aspects of t...|
|||...Datasets  SALICON This is one of the largest saliency datasets available in the public domain [16]....|
|||...The attentional focus (foveation) in the human attention mechanism that defines saliency fixations is simulated using mouse-movements over a blurred image....|
|||...in adapting the weights of a deep network originally trained for a distinct task to the new task of saliency prediction....|
|||...Generating ground-truth maps To create ground-truth saliency maps from fixation data, we use the saliency map generation parameters established by the authors of each dataset....|
|||...The authors of MIT-1003 and MIT300 provide ground-truth saliency maps which, according to their technical report [17], are computed with a Gaussian kernel whose size corresponds to a cutoff frequency ...|
|||...For each dataset, we follow the established evaluation protocol and report results on standard saliency metrics, including sAUC, AUC-Judd, AUC-Borji, Correlation Coefficient (CC), Normalized Scanpath ...|
|||...This robustness is particularly important as the ground-truth saliency maps are derived from eye-fixations which have a natural variation due to the subjectivity of visual attention, and which may als...|
|||...Figure 3 shows the evolution of the saliency metrics on the SALICON validation set as the training progresses....|
|||...SALICON challenge: The saliency estimation challenge [47] consists in predicting saliency maps for 5000 images held out from the SALICON dataset....|
|||...Discussion  Our probabilistic perspective to saliency estimation is intuitive in two ways....|
|||...GT refers to the ground-truth saliency maps....|
|||...Figure 5 shows saliency map predictions for SALICON training images which were obtained on the forward pass after a given number of training images had been used to train the model....|
|||...The saliency of text also emerges fairly rapidly....|
|||...Conclusion  We introduce a novel saliency formulation and model for predicting saliency maps given input images....|
|||...Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations....|
|||...Mit saliency benchmark....|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Image saliency by isocen tric curvedness and color....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...SUN: A Bayesian framework for saliency using natural statistics....|
|||...Learning a saliency map using fixated  locations in natural scenes....|
|||...Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet....|
|||...Label consistent quadratic In CVPR,  surrogate model for visual saliency prediction....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Low-level spatiochromatic grouping for saliency estimation....|
|||...End-to-end convolutional netTechnical report, 2015.  work for saliency prediction....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
||67 instances in total. (in cvpr2016)|
|24|cvpr18-Cube Padding for Weakly-Supervised Saliency Prediction in 360 Videos|...}@gmail.com  hao.kai@ailabs.tw liutyng@iis.sinica.edu.tw sunmin@ee.nthu.edu.tw  Abstract  Automatic saliency prediction in 360 videos is critical for viewpoint guidance applications (e.g., Facebook 36...|
|||...Note that most existing methods are less scalable since they rely on annotated saliency map for training....|
|||...To evaluate our method, we propose Wild-360, a new 360 video saliency dataset, containing challenging videos with saliency heatmap annotations....|
|||...As a result, we predict high-quality saliency map on the Cubemap....|
|||...In panel (d), desirable Normal Field of Views (NFoVs) are obtained from high-quality saliency map....|
|||...[64] propose to generate a highlight video according to spatial-temporal saliency in a 360 video....|
|||...All methods above involve predicting or require the existence of spatialtemporal saliency map in a 360 video....|
|||...Existing methods face two challenges in order to predict saliency on 360 videos....|
|||...Hence, existing image [9, 24]  11420  or video [37] saliency datasets are not ideal for training saliency prediction model....|
|||...One way to overcome this challenge is to collect saliency dataset directly on 360 videos....|
|||...Then, we only take the saliency prediction in a center sub-region in order to combine all predictions onto the whole sphere....|
|||...Note that the ConvLSTM module and temporal consistency loss encourage the predicted saliency map to be temporally smooth and motion-aware....|
|||...To evaluate our method, we propose Wild-360, a new 360 video saliency dataset, containing challenging videos with saliency heatmap annotations....|
|||...We propose an weakly-supervised trained spatialtemporal saliency prediction model....|
|||...To the best of our knowledge, it is the first method to tackle the 360 video saliency map prediction in an weakly-supervised manner....|
|||...One-third of our dataset is annotated with perframe saliency heatmap for evaluation....|
|||...d the potential contributions, we discuss the recent developments of relevant techniques, including saliency map prediction, localization via weakly-supervised or unsupervised learning, and 360 vision...|
|||...However, all these approaches demand heavy saliency supervision while our method requires no manual saliency annotations....|
|||...Our method treats the deep features from the last convolutional layer, encoded with objectness clues, as saliency features for further processing....|
|||...By designing a well-formulated loss function and top-down guidance from class labels, the generator is demonstrated to output saliency estimation of good quality....|
|||...Indeed only a few attempts for estimating the saliency information in 360 videos have been made....|
|||...To generate a saliency map for a 360 spherical patch, their method computes the corresponding 2D perspective image, and detect the saliency map using model pre-trained on SALICON dataset....|
|||...Taking account of where the spherical patch is located at, the final result of saliency detection can be obtained by refining the 2D saliency map....|
|||...ct an equirectangular image I to a cubemap image I, (2) the CNN with Cube Padding (CP) to extract a saliency feature Ms, (3) the post-process to convert Ms into an equirectangular saliency map OS ....|
|||...Panel (b) shows our temporal model: (1) the convLSTM with CP to aggregate the saliency feature Ms through time into H, (2) the post-process to convert H into an equirectangular saliency map O, (3) our...|
|||...To generate a static saliency map S, we simply pixel-wisely select the maximum value in MS  1423  Figure 3....|
|||...We generate saliency map from Ht equivalent to Eq....|
|||...y) = max  k  {H j  t (k, x, y)} ; j  {B, D, F, L, R, T } ,  (4)  where Sj t (x, y) is the generated saliency score at location (x, y) of cube face j at time step t. Similar to our static model, we ups...|
|||...formed by an assumption: the same pixel across different short-term time step should have a similar saliency score....|
|||...If a pattern in a video remains steady for several time steps, it is intuitively that the video saliency score of these non-moving pixels should be lower than changing patches....|
|||...Sj(x, y) = max  k  {M j  S(k, x, y)} ; j  {B, D, F, L, R, T } ,  (2)  where Sj(x, y) is the saliency score at location (x, y) of cube face j, and the saliency map in equirectangular projection S can b...|
|||...ing objects and changing scenes rather than static, we design our temporal model to capture dynamic saliency in a video sequence....|
|||...By optimizing our model with these loss functions jointly to Ltotal throughout the sequence, we can get the final saliency result by considering temporal patterns though Z frames....|
|||...Similar to [54], we further apply Gaussian mask to every viewpoint to get aggregated saliency heatmap....|
|||...In order to foster future research related to saliency prediction in 360 videos, we plan to release the dataset, once the paper is published....|
|||...Experiments  We compare our saliency prediction accuracy and speed performance with many baseline methods....|
|||...To generate ground truth saliency map of Wild-360, referring to [54] and heatmap providers [1], the saliency distribution was modeled by aggregating viewpoint-centered Gaussian kernels....|
|||...To avoid the criterion being too loose, only locations on heatmap with value larger than  + 3 were considered salient when creating the binary mask for the saliency evaluation metrics, e.g....|
|||...3.2 mentioned, our static model takes the six faces of the cube as an input to generate the saliency map....|
|||...Hence we directly use the normalized magnitude of [62] as saliency map to see how much motion clue contributes to video saliency....|
|||...Consistent Video Saliency  [61] detects salient regions in spatio-temporal structure based on the gradient flow and energy optimization....|
|||...SalGAN  [42] proposed a Generative Adversarial Network (GAN) to generate saliency map prediction....|
|||...is the current state-of-the-art model on well-known traditional 2D saliency dataset SALICON [24] and MIT300 [9]....|
|||...Note that this work focuses on saliency prediction on single image and needs ground truth annotations to do supervised learning....|
|||...Evaluation metrics  We refer to the MIT Saliency Benchmark [9] and report  three common metrics: AUC-Judd (AUC-J)....|
|||...AUC-Judd [25] measures differences between our saliency prediction and the human labeled ground truth by calculating the true positive and false positive rate for the viewpoints....|
|||...The linear correlation coefficient is a distribution based metric to measure the linear relationship of given saliency maps and the viewpoints....|
|||...Saliency comparison  From saliency comparison shown in Table....|
|||...Our temporal model typical predicts smooth saliency map in time and is more effective to salient regions on image boundaries or in the top/bottom part of the image....|
|||...Panel (b) shows EQUI (top) and Ours Static (bottom) saliency maps....|
|||...by saliency score, we use AUTOCAM [52] to find a feasible path of salient viewpoints....|
|||...We ask  16 viewers to select the saliency map prediction which (1) activates on salient regions more correctly, (2) is smoother across frames....|
|||...On a newly collected Wild-360 dataset with challenging videos and saliency heatmap annotations, our method outperforms state-of-the-art methods in both speed and quality....|
|||...Mit saliency benchmark....|
|||...A deep multi-level network for saliency prediction....|
|||...Temporal spectral residual: fast motion saliency detection....|
|||...End-to-end saliency mapping via probability distribution prediction....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
|||...Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition....|
|||...Weakly supervised saliency detection with a category-driven map generator....|
|||...Shallow and deep convolutional networks for saliency prediction....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Learning a combined model of visual saliency for fixation prediction....|
||64 instances in total. (in cvpr2018)|
|25|Frintrop_Traditional_Saliency_Reloaded_2015_CVPR_paper|...  frintrop@iai.uni-bonn.de  Abstract  In this paper, we show that the seminal, biologicallyinspired saliency model by Itti et al....|
|||...The resulting system, called VOCUS2, is elegant and coherent in structure, fast, and computes saliency at the pixel level....|
|||...Furthermore, we integrate the saliency system into an object proposal generation framework to obtain segment-based saliency maps and boost the results for salient object segmentation....|
|||...The approach is considered to be the origin of computational saliency systems and has been cited more than 6000 times....|
|||...Since then, the interest in the computer vision community in saliency computation has increased strongly, since it is a valuable method for tasks such as thumbnailing, retargeting, or object recognition....|
|||...Several databases for evaluating systems have been created and many new sophisticated saliency systems are presented every year (see [5, 7] for surveys)....|
|||...Many systems show now impressive results with precise saliency maps: obviously there has been  a huge progress in the field....|
|||...But do these new methods compute saliency in a better, or just in a different way?...|
|||...In other words, did the original Itti model miss essential ideas to compute saliency that newer approaches include?...|
|||...In this paper, we show that the traditional structure of saliency models based on multi-scale Difference-ofGaussians is still competitive with current salient object detection methods....|
|||...We present the new saliency system VOCUS2 that follows in its basic structure the Itti-Koch model [21]: feature channels are computed in parallel, pyramids enable a multi-scale computation, contrasts ...|
|||...Especially important is the scale-space structure (we use a new twin pyramid) and the center-surround ratio, which has turned out to be the most crucial parameter of saliency systems....|
|||...Our system produces pixel-precise saliency maps, instead of segment-based ones....|
|||...We believe that this is an important quality of a general saliency method, since pixels actually do have different saliency....|
|||...However, for some tasks, segment-based saliency maps are beneficial, especially if precise object boundaries are required....|
|||...The resulting object proposals are then combined to form a segment-based saliency map, which gives especially good results on several benchmark datasets for  1  salient object segmentation....|
|||...This map is similar to what is nowadays called saliency map, and there is strong evidence that such a map exists in the brain, presumably in V1 [41]....|
|||...The earliest computational attention models have been built based on FIT, that means they compute several feature maps, usually with Difference-of-Gaussian and Gabor filters, and fuse them to a saliency map....|
|||... it has been claimed that other methods are required for such tasks.1  During the last decade, many saliency systems have been presented that are less biologically inspired and that were mainly design...|
|||...When looking more closely at the methods, it reveals that the underlying method that exists in basically all saliency systems is a contrast computation....|
|||...While it has turned out during the last decade that there are numerous ways to compute saliency (since there are numerous ways to compute contrasts), it is less clear which of these methods is favorable....|
|||...Saliency Computation with VOCUS2  In this section, we introduce our new implementation2 of the traditional FIT-based saliency approach by Itti et al....|
|||...Our saliency system is called VOCUS2, indicating that it is a successor of our previous VOCUS system [12]....|
|||...3.1 describes the basic saliency system, producing pixel-precise saliency maps....|
|||...Basic Saliency System  Fig....|
|||...1 shows an overview of our new saliency system VOCUS2....|
|||...3  2  3.1.2 Scale Space with Twin Pyramids  The saliency computation is embedded in a scale-space structure on which multi-scale center-surround contrasts are computed....|
|||...Since the center-surround ratio has turned out to be the most crucial parameter in a saliency system, this ability to fine-tune the system is of special importance and justifies the slightly higher co...|
|||...Overview of our saliency system VOCUS2  surround contrast is computed....|
|||...Finally, the scales and feature channels are fused to one saliency map....|
|||...3.1.1 Feature Channels  As in most other saliency systems, we base our computations mainly on intensity and color features....|
|||...The reason is that it assigns high saliency values to object edges, which makes the segmentation of objects difficult....|
|||...ty maps,  Cf = f (F f  1 , F f  2 ), with f  {I, RG, BY },  and these are finally fused to a single saliency map S:  S = g(C I , C RG, C BY ),  (3)  (4)  where g is a fusion operation....|
|||...The latter approach overemphasizes intensity, since the contribution in the saliency map originates fifty-fifty from intensity respectively color....|
|||...We believe that a general saliency system should not rely on a location prior that prioritizes, for example, the image center, in order to be able to compute saliency for arbitrary input....|
|||...We include therefore an optional location prior that can be applied to the saliency map by  multiplying the saliency values with a Gaussian centered at the image center:  s(x, y) = s(x, y)  exp{    (...|
|||...Segment(cid:173)based Saliency (optional)  While we believe that a general saliency system should be able to compute pixel-precise saliency, it is for some applications necessary to also segment the o...|
|||...Additionally, saliency maps with precise object boundaries usually obtain better quantitative results, since evaluation measures usually evaluate how precisely the objects are segmented rather than ho...|
|||...However, it is not necessary to use different saliency methods to obtain segment-based saliency maps, instead we propose a simple way to obtain a segment-based saliency map from the pixel-precise one....|
|||...The resulting candidates are finally overlaid to obtain a segment-based saliency map....|
|||...Segmentation and saliency are combined by selecting for each salient blob the segments which overlap more than k percent with the blob (we use k = 30%)....|
|||...The proposals are then ranked by average saliency and non-maxima suppression removes duplicates; finally, we threshold to maintain only proposals with at least p percent average saliency of the best p...|
|||...The final segment-based saliency map is obtained by taking for each pixel the maximum value of all proposals covering this pixel, which corresponds to the highest average saliency obtained for this pixel....|
|||...Stepwise improvements of Ittis iNVT saliency system [21], until finally obtaining our VOCUS2 system....|
|||...Second, we compare the performance of our saliency system with several other approaches on a large collection of benchmark datasets....|
|||...Step(cid:173)by(cid:173)Step Comparison with iNVT  In this section, we show step by step which adaptations of the original Itti system have which effects for the performance for saliency object segmentation....|
|||...3.1.2)  (version corresponds to VOCUS2-Basic saliency map, eq....|
|||...4)  8. added location prior  (version corresponds to VOCUS2-LP saliency map, eq....|
|||...3.3  (version corresponds to VOCUS2-Prop saliency map)  Fig....|
|||...Examples saliency maps....|
|||...From left to right: original image, ground truth, saliency map for our VOCUS2-Basic, our VOCUS2Proposals, HSaliency, DRFI, Yang13, HZ08, iNVT  and the corresponding adaptation of the center-surround r...|
|||...Comparison with Other Saliency Methods  We have compared our approach with several other saliency methods with two evaluation measures on a large collection of benchmark images from several datasets....|
|||...2 shows several examples saliency maps....|
|||...We evaluated the systems with two measures: first, with the popular precision-recall method from [1]: saliency maps are thresholded with an increasing k  [0, 255]....|
|||...However, it was recently shown that this measure has several limitations and ranks saliency maps differently than humans would [29]....|
|||...It shows that our segment-based VOCUS2 saliency method (V2-Prop) outperforms all other methods, and the pixelprecise VOCUS2 method (V2-LP) outperforms all methods that compute pixel-precise saliency m...|
|||...2 shows in the last row an example image and saliency maps of this sequence....|
|||...Conclusion  Plenty of saliency systems have been introduced during the last decade and it is amazing and interesting in how many different ways it is possible to compute saliency....|
|||...Exploiting local and global patch  rarities for saliency detection....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Bottom-up saliency is a  discriminant process....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
|||...Neural activities in V1 create a bottom-up saliency map....|
||63 instances in total. (in cvpr2015)|
|26|Volokitin_Predicting_When_Saliency_CVPR_2016_paper|...l attention use image features and machine learning techniques to predict eye fixation locations as saliency maps....|
|||...In this paper, we show that using features from DCNNs for object recognition we can make predictions that enrich the information provided by saliency models....|
|||...Namely, we can estimate the reliability of a saliency model from the raw image, which serves as a meta-saliency measure that may be used to select the best saliency algorithm for an image....|
|||...Many computational models of attention predict the image location of eye fixations, which is represented with the so called saliency map....|
|||...Some authors stressed the need to predict properties of the eye fixations beyond the saliency map to study different phenomena of visual attention and to allow for new applications, e.g....|
|||...In this paper, we analyze two ways to augment the eye fixation location information delivered by saliency models by using features extracted from DCNNs trained for object recognition....|
|||...Firstly, inspired by machine learning techniques that provide an estimate of their own accuracy, we show that the accuracy of a saliency model for a given image can be predicted directly from image features....|
|||...Also, we show that current saliency models and our eye fixation consistency model describe complementary aspects of viewing behaviour, and should be used in conjunction for a more complete characteris...|
|||...Eye Fixation and Saliency Maps  In this section, we introduce the datasets we use and review how to build the eye fixation and saliency maps....|
|||...To show the generality of our conclusions, we also report results on the PASCAL saliency dataset [26]....|
|||...Saliency Maps A saliency map is the prediction of the eye fixation map by an algorithm....|
|||...We use seven stateof-the-art models to predict the saliency maps: Boolean Map based Saliency (BMS) [37], Adaptive Whitening Saliency Model (AWS) [9], Graph-based Visual Saliency [11], Attention based ...|
|||...We use the standard procedure (with code from [16]) to optimise these saliency maps for the MIT dataset....|
|||...Enriching Saliency Maps  In this section we introduce the two estimates we use to add to a saliency map: the estimate of the saliency map accuracy, and the consistency of the eye fixations among subjects....|
|||...Predicting the Saliency Map Accuracy  We explore whether the saliency model accuracy can be predicted by features extracted directly from the image, i.e....|
|||...before computing the saliency map....|
|||...This prediction depends on the algorithm for saliency prediction, and also, it depends on the metric used to evaluate the accuracy of the saliency map....|
|||...545  Metric of the Saliency Map Accuracy Since there is no consensus among researchers about which metric best captures the accuracy of the saliency map (c.f ....|
|||...Below, MF is the map of eye fixation map (ground truth) and MS is the (predicted) saliency map: Similarity (Sim)....|
|||...The saliency map is treated as a binary classifier to separate positive from negative samples at various intensity thresholds....|
|||...It is called shuffled because the points of the saliency map are sampled from fixations on other images to discount the effect of center bias....|
|||...Applications Our goal is to predict the evaluation metrics of a saliency model we just introduced, for a given image....|
|||...Providing such estimate of the saliency model accuracy may be used to select the best saliency algorithm for a specific image....|
|||...We define the consistency score to be the score of MH in predicting MO\H using any of the standard metrics for evaluating saliency prediction algorithms (introduced previously in section 3.1)....|
|||...he prediction of the eye fixation consistency can be used to enrich the information provided by the saliency map, because current saliency models have no measure of the consistency of the eye fixation...|
|||...The reader may object that since the entropy of the eye fixation map is related to the consistency, it could be that the entropy of the saliency map is also related to the eye fixation consistency....|
|||...Then saliency models would already provide an estimate of the consistency through the entropy of the saliency map....|
|||...Applications that make use of saliency maps, such as visual design, could incorporate eye fixation consistency information to create designs with a greater consensus of fixations in the location of th...|
|||...We introduce several image features to test the hypothesis that the saliency accuracy and consistency can be predicted from the spatial distribution and the categories of the objects in the image....|
|||...Experiments  We now report results on the MIT benchmark [17] and  PASCAL saliency dataset [26] (introduced in section 2)....|
|||...Predicting the Saliency Map Accuracy  Performance of the Predictor of the Saliency Map Accuracy Fig....|
|||...2 shows the results for predicting saliency  1This heatmap, when normalised, was also evaluated as a saliency map....|
|||...547  sAUC Score of AWS  CC Score of AWS  Sim Score of AWS  PyrObj GIST whole layer semantic spatial  0.5  0.4  0.3  0.2  0.1  0  n o  i t  l  a e r r o C  PyrObj GIST whole layer semantic spatial  0....|
|||...The correlation between the predicted accuracy and the true accuracy of the saliency map is evaluated using different input features (including each of the 7 layers of the DCNN)....|
|||...The metric used to evaluate the accuracy of the saliency map is (a) sAUC, (b) CC, and (c) Sim....|
|||... spatial feature, which suggests that semantic information has a greater contribution to predicting saliency map accuracy than information about the distribution of the objects....|
|||...The accuracy prediction of the saliency models performs similarly or better on PASCAL dataset, with a maximum correlation of 0.80 vs 0.52 on MIT....|
|||...Also, when this prediction is used to select the best algorithm for saliency prediction per image, we find a (modest) absolute improvement of about 1%....|
|||...57  SALICON BMS GBVS AIM AWS SUN IttiKoch  Table 1: Evaluation of the Prediction of the Saliency Accuracy....|
|||...Spearman correlation between the predicted accuracy of the saliency map using layer 5 of the DCNN and the ground truth accuracy....|
|||...Table 2: Does the Accuracy of the Saliency Map Predict the Eye Fixation Consistency?...|
|||...Spearman correlation between the accuracy of the saliency map and (left) the consistency (K = 7 and S = 15, with the same metric consistency and accuracy evaluation), and (right) with the entropy of t...|
|||...Does the Accuracy of the Saliency Map Predict the Eye Fixation Consistency?...|
|||...Now that we have shown that the accuracy of the saliency map can be predicted from our model, the reader may ask whether we really need a dif 548  Accurate  Inaccurate  gt = 0.86, ps = 0.97  gt = 0.4...|
|||...We show images that have accurate and inaccurate AWS saliency maps (under the crosscorrelation metric)....|
|||...gt is the ground truth which corresponds to the cross-correlation score of the saliency map, ps is the predicted score....|
|||...The images are place in a row: original, fixation map, saliency map....|
|||...We extend this result by directly evaluating the Spearman correlation between the accuracy of the saliency map and eye fixation consistency....|
|||...Eye fixation consistency may vary depending on the number of subjects  2The Sim metric tends to assign higher scores when the eye fixation maps are relatively flat, independently of the saliency map....|
|||...the sum for all pixels of the minimum value between the saliency and eye fixation maps....|
|||...In fact, in many works on saliency prediction, the value of the evaluation metric at K   is used as an upper bound of the achievable prediction score....|
|||...Table 3: Is the Eye Fixation Consistency Predicted by the Entropy of the Saliency Map?...|
|||...Correlation between the entropy of the saliency map and (left) the eye fixation consistency (K = 7 and S = 15, with the same metric consistency and accuracy evaluation), and (right) entropy of the fix...|
|||...Is the Eye Fixation Consistency Predicted by the Entropy of the Saliency Map?...|
|||...y, to make sure that the prediction of the eye fixation consistency enriches the information of the saliency map, we check whether eye fixation consistency information is already encoded in saliency m...|
|||...Thus, if the saliency map predicts eye fixation consistency, this would be encoded in the entropy of the saliency map....|
|||...In Table 3, we report the correlation between the entropy of the saliency map and the consistency of the fixations based on the three metrics....|
|||...Conclusions  We used machine learning techniques and automatic feature extraction to predict the accuracy of saliency maps and the eye fixation consistency among subjects in natural images....|
|||...Our results showed that saliency models can be enriched with the two predictions made from our model, because saliency models themselves do not capture eye fixation consistency among subjects, and the...|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||63 instances in total. (in cvpr2016)|
|27|Li_Saliency_Detection_via_2013_ICCV_paper|...OMRON Corporation 3University of California at Merced  Abstract  In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors....|
|||...Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model....|
|||...We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors....|
|||...Introduction  Visual saliency is concerned with the distinct perceptual quality of biological systems which makes certain regions of a scene stand out from their neighbors and catch immediate attention....|
|||...Efficient saliency detection plays an important preprocessing role in many computer vision tasks, including segmentation, detection, recognition and compression, to name a few....|
|||...[18] propose a saliency detection algorithm by measuring the center-surround contrast of a sliding window over the entire image....|
|||...Recent methods [7, 8] measure global contrast-based saliency based on spatially weighted feature dissimilarities....|
|||...[17] formulate saliency estimation using two Gaussian filters by which color and position are respectively exploited to measure region uniqueness and distribution....|
|||...ly from the scene, which means that the most relevant visual information is not fully extracted for saliency detection....|
|||...Therefore, these methods do not uniformly detect salient objects  1550-5499/13 $31.00  2013 IEEE DOI 10.1109/ICCV.2013.370  2976  2ULJLQDO,PDJH  ,PDJH6HJPHQWV  5HFRQVWUXFWLRQ  3URSDJDWHG  3L[HO/H...|
|||...Main steps of the proposed saliency detection algorithm....|
|||...In this work, the saliency of each image region is measured by the reconstruction errors using background templates....|
|||...The saliency of each pixel is then assigned by an integration of multi-scale reconstruction errors followed by an object-biased Gaussian refinement process....|
|||...In addition, we present a Bayesian integration method to combine saliency maps constructed from dense and sparse reconstruction....|
|||...A context-based propagation mechanism is proposed for region-based saliency detection, which uniformly highlights the salient objects and smooths the region saliency....|
|||...Saliency Measure via Reconstruction Error  We use both dense and sparse reconstruction errors to measure the saliency of each region which is represented by a D-dimensional feature....|
|||...ble (e.g., similar regions may have different sparse coefficients), which may lead to discontinuous saliency detection results....|
|||...The saliency measures via dense and sparse reconstruction errors are computed as shown in Figure 1(b)....|
|||...Third, pixel-level saliency is computed by taking multi-scale reconstruction errors followed by an objectbiased Gaussian refinement process....|
|||...Reconstruction Error  Given the background templates, we intuitively consider image saliency detection as an estimation of reconstruction error on the background, with an assumption that there must be...|
|||...The saliency measure is proportional to the normalized reconstruction error (within the range of [0, 1])....|
|||...Figure 2(b) shows some saliency detection results via dense reconstruction....|
|||...The middle row of Figure 2 shows an example where some background regions have large reconstruction errors (i.e., inaccurate saliency measure)....|
|||...ground segments are collected into the background templates (e.g., when objects appear at the image boundaries), their saliency measures are close to 0 due to low sparse reconstruction errors....|
|||...In addition, the saliency measures for the other regions are less accurate due to inaccurate inclusion of foreground segments as part of sparse basis functions....|
|||...Brighter pixels indicate higher saliency values....|
|||...(b) Saliency maps from dense reconstruction....|
|||...Pixel-Level Saliency  For a full-resolution saliency map, we assign saliency to each pixel by integrating results from multi-scale reconstruction errors, followed by refinement with an objectbiased Ga...|
|||...With the object-biased Gaussian model, the saliency of pixel z is computed by S (z) = Go (z)  E (z)....|
|||...Comparing the two refined maps of the saliency via dense or sparse reconstruction in the bottom row, the proposed object-biased Gaussian model renders more accurate object center, and therefore better...|
|||...Bayesian Integration of Saliency Maps  As mentioned in Section 3.1, the saliency measures by dense and sparse reconstruction errors are complementary to each other....|
|||...To integrate both the saliency measures, we propose an integration method by Bayesian inference....|
|||...In this work, we take one saliency map as the prior and use the other one instead of Lab color information to compute the likelihoods, which integrates more diverse information from different saliency maps....|
|||...Given two saliency maps S1 and S2 (i.e., from dense and sparse reconstruction), we treat one of them as the prior Si(i = {1, 2}) and use the other one Sj(j (cid:5)= i, j = {1, 2}) to compute the likel...|
|||...First, we threshold the map Si by its mean saliency value and obtain its foreground and background regions described by Fi and Bi, respectively....|
|||...Bayesian integration of saliency maps....|
|||...Similarly, the posterior saliency with Sj as the prior is computed....|
|||...We use these two posterior probabilities to compute an integrated saliency map, SB(S1(z), S2(z)), based on Bayesian integration:  SB(S1(z), S2(z)) = p(F1 S2(z)) + p(F2 S1(z))....|
|||...(14)  The proposed Bayesian integration of saliency maps is illustrated in Figure 6....|
|||...Evaluation of saliency via reconstruction error....|
|||...(c) F-measure curves of the proposed Bayesian integrated saliency SB and four other integrated saliency of MDEPG and MSEPG....|
|||...We vary K (K=4, 6, 8, 10, 12) and  ( =0.1, 0.3, 0.5, 0.7, 0.9) in experiments, and observe that the saliency results are insensitive to either parameter....|
|||...We evaluate all saliency detection algorithms in terms of precision-recall curve and F-measure....|
|||...For each method, a binary map is obtained by segmenting each saliency map with a given threshold T  [0, 255] and then compared with the ground truth mask to compute the precision and recall for an image....|
|||...We first use the mean-shift algorithm to segment the original image and extract the mean saliency of each segment. We then obtain the binary map by thresholding the seg 2Precision+Recall  (1+2)Precisi...|
|||...For each image in the MSRA database which is labeled with a bounding box (rather than precise object contour), we fit a rectangle to the thresholded saliency map for evaluation, similar to [5]....|
|||...The reconstruction error of a pixel is assigned by integrating the multiscale reconstruction errors, which helps generate more accurate and uniform saliency maps....|
|||...In Section 4, Bayesian Integrated Saliency Detection....|
|||...we discuss that the posterior probability can be more accurate with likelihood computed by a saliency map rather than the CIELab color space on the condition of the same prior in the Bayes formula....|
|||...We present experimental results in which we treat the saliency map by dense (or sparse) reconstruction as the prior, and use the other saliency map by sparse (or dense) reconstruction and Lab color to...|
|||...Figure 8(a) shows that with the saliency via dense reconstruction as the prior, the result with the likelihood based on sparse reconstruction (DenseSparse) is more accurate than that with the CIELab c...|
|||...While using the saliency map based on sparse reconstruction as the prior, the result with the likelihood based on dense reconstruction (Sparse-Dense) is comparable to that with the CIELab color space ...|
|||...We evaluate the performance of Bayesian integrated saliency map SB by comparing it with the integration strategies formulated in [5]:  Q (Si) or Sc = 1  Q (Si),  (15)  i  Z where Z is the partition function....|
|||...Figure 8(c) shows that the F-measure of the proposed Bayesian integrated saliency map is higher than the other methods at most thresholds, which demonstrates the effectiveness of Bayesian integration....|
|||...Comparisons of saliency maps....|
|||...DSR cut: cut map using the generated saliency map....|
|||...Figure 11 shows that our model generates more accurate saliency maps with uniformly highlighted foreground and well suppressed background....|
|||...Conclusions  In this paper, we present a saliency detection algorithm via dense and sparse reconstruction based on the background templates....|
|||...To combine the two saliency maps via dense and sparse reconstruction, we introduce a Bayesian integration method which performs better than the conventional integration strategy....|
|||...Visual saliency detection by spatially weighted dissimilarity....|
|||...Geodesic saliency using back ground priors....|
|||...Bayesian saliency via low and mid  Slic superpixels....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
||63 instances in total. (in iccv2013)|
|28|Li_Contextual_Hypergraph_Modeling_2013_ICCV_paper|...Introduction  Image saliency detection aims to effectively identify important and informative regions in images....|
|||...estimates the saliency degree of an image region by computing the contrast against its local neighborhood....|
|||...Global salient object detection approaches [4,5,7,11,12] estimate the saliency of a particular image region by measuring its uniqueness in the entire image....|
|||...Therefore, the definition of object saliency depends on the choice of context....|
|||...Global saliency defines the context as the entire image, whereas local saliency requires the definition of a local context....|
|||...In this work, we first show that within a fixed context, a cost-sensitive SVM can accurately measure saliency by capturing centre-surround contrast information....|
|||...We introduce hypergraph modeling into the process of image saliency detection for the first time....|
|||...This additional structural information enables more accurate saliency measurement....|
|||...The problem of saliency detection is naturally cast as  Center  Surroundings  center is the positive data   ... ...   Input image  sample surroundings for neg....|
|||...saliency using  SVM results  Saliency map  Figure 2: Illustration of cost-sensitive SVM for saliency detection....|
|||...Consequently, the saliency degree of an image region is measured by its associated normalized SVM coding length....|
|||...Cost-sensitive SVM saliency detection  As illustrated in [9, 23], saliency detection is typically posed as the problem of center-versus-surround contextual contrast analysis....|
|||...To address this problem, we propose a saliency detection method based on imbalanced maxmargin learning, which is capable of effectively discovering the local salient image regions that significantly d...|
|||...The saliency degree of x1 is determined by its  3322 3329  (cid:2)  inter-class separability from {x(cid:2)}(cid:2)=2...N ....|
|||...In other words, if x1 could be easily separated from {x(cid:2)}(cid:2)=2...N , then it is deemed to be salient; otherwise, its saliency degree is low....|
|||...g the weighted LS-SVM classifier f (x), we define  CN  1  i  (cid:4)  0 y  N(cid:5)  (cid:2)=2  the saliency score as:  1  sgn(f (x(cid:2)))  2  ,  (3)  (cid:2)  SSa(x1) = sgn() is 1sgn(f (x(cid:2))) ...|
|||...Conversely, the larger SSa(x1) indicates the lower similarity between x1 and {x(cid:2)}(cid:2)=2...N , and hence a higher saliency degree....|
|||...Note that, this max-margin learning framework can be easily extended to perform saliency detection on a global scale....|
|||...By running the max-margin learning procedure over such training samples, the saliency degree of each  Figure 3: Illustration of hypergraph modeling for saliency detection using nonparametric clustering....|
|||...The rightmost image shows the final saliency map HSa generated by multi-scale hyperedge saliency fusion....|
|||...As a result, the saliency of each superpixel, as measured by the hyperedges it belongs to, is not only determined by the superpixel itself but also influenced by its associated contexts....|
|||...Due to such contextual constraints on each superpixel, we simply convert the original saliency detection problem to that of detecting salient vertices and hyperedges in the hypergraph G. Mathematicall...|
|||...The saliency value of any vertex vi in G is defined as:  HSa(vi) =  (e)H(vi, e),  (cid:7)  1, 0,  (cid:5)  eE  (4)  (5)  Image  Hypergraph saliency Standard graph saliency Figure 4: Illustration of sa...|
|||...Clearly, our hypergraph saliency measure is able to accurately capture the intrinsic structural properties of the salient object....|
|||...Example saliency maps derived from this measure are shown in Figs....|
|||...Hypergraph modeling for saliency detection To more effectively find salient object regions, we propose a hypergraph modeling based saliency detection method that forms contexts of superpixels to captu...|
|||...our method, an image I is modeled by a hypergraph G = (V, E), where V = {vi}  where (e) encodes the saliency information on the hyperedge e. In essence, our hypergraph saliency measure (5) is a genera...|
|||...where Nvi stands for the neighborhood of vi, d(vi,vj ) measures the saliency degree of the pairwise edge (vi, vj), and Ie = 1 if Ie is the pairwise adjacency indicator (s.t....|
|||...Instead of using simple pairwise edges, our hypergraph saliency measure takes advantage of the higher-order hyperedges (i.e., superpixel cliques) to effectively capture the intrinsic structural proper...|
|||...To implement this approach, we need to address the following two key issues: 1) how to adaptively construct the hyperedge set E; and 2) how to accurately measure the saliency degree (e) of each hyperedge....|
|||...To capture the hierarchial visual saliency information, we construct a set of hyperedges by adaptively grouping the superpixels according to their visual similarities at multiple scales....|
|||...i i=1 g(M 2(pt, pi, ))  the following iterative procedure:  pt+1 =  (8)  Q  Q  ,   Mg for hyperedge saliency evalFigure 6: Illustration of Mg and I uation....|
|||...Hyperedge saliency evaluation By construction, a hyperedge defines a group of pixels that is internally consistent....|
|||...If the hyperedge touches the image boundaries, we decrease its saliency degree by a penalty factor....|
|||...As a result, the saliency value of the hyperedge e is computed as:  (m, n) + I 2 y  I 2 x   g  (e) = e   Mg(e)(cid:2)1  (e)  ....|
|||...  	         Figure 7: PR curves based on three different configurations: 1) using the SVM saliency approach only; and 2) using the hypergraph saliency approach only; 3) combining the SVM and...|
|||...Clearly, the saliency detection performance of using the third configuration outperform that of using the first and second configurations....|
|||...(5), we obtain the hypergraph saliency measure HSa(vi) for any vertex vi in the hypergraph G.  After both SVM and hypergraph saliency detection, we obtain the corresponding saliency maps....|
|||...Each element of these saliency maps is mapped into [0, 255] by linear normalization, leading to the normalized saliency maps....|
|||...Finally, the final saliency map is obtained by linearly combining the SVM and hypergraph saliency detection results....|
|||...Evaluation criterion For a given saliency map, we adopt four criteria to evaluate the quantitative performance of different approaches: precision-recall (PR) curves, Fmeasures, receiver operating char...|
|||...Specifically, the PR curve is obtained by binarizing the saliency map using a number of thresholds ranging from 0 to 255, as in [4, 7, 12, 11]....|
|||...Here, P and R are the precision and recall rates obtained by binarizing the saliency map using an adaptive threshold that is twice the overall mean saliency value [4]....|
|||...defined as   SS  SS  (cid:6) is the object segmentation mask obtained by mask, and S binarizing the saliency map using the same adaptive threshold during the calculation of F-measure....|
|||...Implementation details In the experiments, costsensitive SVM saliency detection on an image is performed at different scales, each of which corresponds to a scalespecific image patch size for center-v...|
|||...The final SVM saliency map is obtained by averaging the multi-scale saliency detection results....|
|||...In the experiments, the final saliency detection results are further refined by graph-based manifold propagation....|
|||...Evaluation of our individual approaches  Here, we evaluate the saliency detection performance of the proposed approaches based on three different configurations: 1) using the SVM saliency approach onl...|
|||...7, it is clearly seen that the saliency detection performance of only using the SVM saliency approach is significantly enhanced after combining the hypergraph saliency approach....|
|||...The reason is that the hypergraph saliency approach captures  3325 3332                                                         	            ...|
|||...By incorporating the SVM saliency approach, the saliency detection results of only using the hypergraph saliency approach are further smoothed, leading to an improved saliency detection accuracy....|
|||...Comparison of saliency detection approaches  In the experiments, we qualitatively and quantitatively compare the proposed approach with twelve state-of-theart approaches, including GS SP [5], LR [12],...|
|||...It is clear that our approach obtains the visually more consistent saliency detection results than the other competing approaches....|
|||...10 that our approach obtain visually more feasible saliency detection results than the other competing approaches....|
|||...From left to right: input images, ground truth, saliency maps, segmentation results....|
|||...or original saliency detection results from the authors....|
|||...8 shows the quantitative saliency detection performance of the proposed approach against the twelve competing approaches in the PR and ROC curves on the four datasets....|
|||...Context-aware saliency detection....|
|||...Discriminative spatial saliency for image classification....|
|||...Visual saliency based on scale-space IEEE Trans....|
|||...pecifically, we have designed a hypergraph modeling approach that captures the intrinsic contextual saliency information on image pixels/superpixels by detecting salient vertices and hyperedges in a h...|
|||...Geodesic saliency using background priors....|
||62 instances in total. (in iccv2013)|
|29|Jia_Category-Independent_Object-Level_Saliency_2013_ICCV_paper|...rkeley.edu  Mei Han  Google Research meihan@google.com  Abstract  It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring h...|
|||...In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models....|
|||...We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient r...|
|||...Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics....|
|||...There is a line of saliency detection work centered around visual attention models [13, 15, 12] that focuses on finding locations of images that capture early-stage human fixations before more complex...|
|||...information (such as the frequency domain image signature [12]) often does not produce object-level saliency maps, and high-level information such as common object detectors [15, 28] and global image ...|
|||...An illustration of our approach from images to the final saliency map: (a) Input Image (b) objectness detections, (c) saliency prior from objectness, (d) diverse density scores for pixels, (e) the fin...|
|||...In this paper, we propose a novel approach that fuses top-down object level information and bottom-up pixel appearances to obtain a final saliency map that identifies the most interesting regions in the image....|
|||...Unlike classical frequency-based saliency maps that often focus on edges of the images and objects, we will show that our method returns a more objectcentric saliency map that is consistent over the s...|
|||...Related Work  Pre-attentive bottom up saliency algorithms have been extensively studied from biological and computational perspectives....|
|||...[5] was the first to adopt a high-level object information as saliency prior....|
|||...However, the prior is combined with pixel-wise scores from another low-level saliency model, which then creates an arbitrary bias towards the specific algorithms behaviors, such as favoring high frequ...|
|||...citly need to identify the mixture components of the foreground, it may not be necessary in finding saliency regions, and the multiple parameters to be tuned in these models may hurt performance....|
|||...We empirically tested a parameterized mixture model for foreground modeling, and found the MRF approach in our paper to better fit the saliency problem....|
|||...ed graph as many previous approaches do [5], as locally connected graph may lead to overly smoothed saliency maps....|
|||...color features could achieve state-of-the-art performance, without the need of additional bottom-up saliency prior or additional handcrafted features....|
|||...Saliency Detection with Object-level Infor mation In this section, we formally describe the algorithm we proposed to perform saliency detection based on high-level object information....|
|||...Multiscale Saliency (MS): this cue utilizes the spectral residual of the Fourier Transform on multiple scales to find regions with unique appearances within the image....|
|||...It could be observed that although the saliency map is still coarse, it provides a reasonable initialization for the final saliency map as it correctly identifies the salient object location....|
|||...More importantly, such saliency prior is not biased towards specific low-level appearances such as highfrequency regions, which often misses the inside region of the salient objects....|
|||...However, due to the fact that objectness bounding boxes are often overcomplete, the saliency map is often very coarse, and one would expect low-level appearance based information to be helpful in refi...|
|||...Such statistics will bias our further inference algorithm towards small highly textured areas, a negative effect for saliency detection....|
|||...The per-superpixel image saliency obtained directly from the objectness detection....|
|||...We note that unlike previous works such as [5], we keep the low-level, frequency based saliency component (MS) in the objectness pipeline....|
|||...ately identify possible objects in the image, and as we will discuss in the next section, low-level saliency may impose a negative impact on the final saliency measure when used alone....|
|||...Pixel-level Objectness Scores  As our goal is to obtain a saliency map for the whole image, we transfer the objectness scores from the bounding boxes to the pixel level....|
|||...ceive high salient scores even if it lies in a region with low contrast, thus ensuring a consistent saliency score assignment in the whole region of the salient object....|
|||...As a simple example, consider a red object in a random background while conventional frequencybased saliency detections will only be able to capture the edges of the object, enforcing consistency betw...|
|||...Since we do not have foreground and background labels in the first place, we could use the saliency prior as an approximation, and evaluate the influence of each pixel with a diverse density measure, ...|
|||...The diverse density models how near other salient regions are to it, and how far other non-salient regions are from it, with the saliency approximated by the prior information sj....|
|||... between the pixels indexed i and j is computed as  Gij =  DDi + DDj  2  Wij  (5)  To introduce the saliency prior obtained from objectness, we then add two abstract nodes: one source node with salien...|
|||...Then, we solve for the improved saliency value for each pixel by viewing the graph as a Gaussian MRF, which leads to an efficient computation of the final saliency values s as (cid:7) diag(G1)  G  (ci...|
|||...Analysis of Performance Contributions  With the multiple stages of many current saliency detection algorithms, it would be interesting to observe how much each component contributes to the overall performance....|
|||...We then use them as the initialization of our graph, and perform GMRF inference to get the final saliency measure....|
|||...r precision increase, suggesting that the general objectness measure serves as a good heuristics in saliency detection....|
|||...gnificant performance gain, suggesting that although spatial relationships are crucial in obtaining saliency priors (as is both the case in objectness detection and works like [28]), they should not p...|
|||...Experiments  We evaluated our method on the MSRA saliency dataset containing 1000 images together with the salient object annotated by human participants as the ground-truth saliency map, and compared...|
|||...Our saliency maps on the MSRA dataset are publicly available at http://www.eecs.berkeley....|
|||...Evaluation Criteria  We mainly adopted the criteria introduced in [2] to evaluate the performance of various saliency algorithms using precision recall (PR) curves....|
|||...The precision-recall curves for our saliency detection algorithm and the baseline algorithms....|
|||...Results on the MSRA dataset with the saliency maps of the best 8 methods, ordered from left to right, where GT is the ground truth and OB is our approach....|
|||...age fashion when an uninformative threshold is used, and the second focuses on checking the correct saliency order for pixels in a single image....|
|||...For binarization, we computed the mean m and the standard deviation  of the saliency map, and then set all pixels whose saliency value is larger than m +  to be foreground and the rest to be background....|
|||...1766 1766  (AC)[1], context-aware saliency (CA)[9], graph-based visual saliency (GB)[10], frequency-tuned saliency (IG)[2], Itti et al....|
|||...(IT)[13], contrast based attention model (MZ)[21], spectral residual approach (SR)[12], and saliency filters (SF)[24]....|
|||...(JD) [15], global contrast based saliency (RC)[6], saliency by low rank recovery (LR)[28], Chang et al....|
|||...(SV)[5], geodesic saliency (GS)[30] and also our method....|
|||...Such methods utilize high-level object or global image information to create an informative prior for the saliency map....|
|||...For all baseline methods, we used either the published implementations with their recommended parameters or the author-provided saliency maps....|
|||...In general, methods that utilize high-level information to obtain more informative saliency priors perform better than purely low-level approaches, and our method achieves the highest average precisio...|
|||...Figure 7 shows exemplar images and their corresponding saliency maps from various algorithms....|
|||...Full results on the dataset can be found in the supplemen ground-truth saliency maps3....|
|||...It is interesting to note that a major contribution is also due to the introduction of objectlevel information, further justifying the use of such approaches in saliency detection....|
|||...Performance on the Weizmann Dataset  The MSRA saliency dataset mainly contains single salient objects of medium sizes per image, which is the assumption made by several saliency detection algorithms, ...|
|||...Conclusion  In this paper we proposed a novel image saliency algorithm that utilizes the object-level information to obtain better discovery of salient objects....|
|||...In the model, we obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among...|
|||...high-frequency low-level saliency predictions....|
|||...with discriminative regional features (DR)[14], in which a pixel-wise saliency prediction model is trained on  1767 1767  (a) Weizmann 1 Object Dataset  (b) Weizmann 2 Object Dataset  Figure 10....|
|||...The precision-recall curves for our saliency detection algorithm and the baseline algorithms on the Weizmann dataset....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Context-aware saliency detection....|
|||...Geodesic saliency using background priors....|
||62 instances in total. (in iccv2013)|
|30|Shi_A_Reverse_Hierarchy_2014_CVPR_paper|...Inspired by the theory, we develop a computational model for saliency detection in images....|
|||...Then, saliency on each layer is obtained by image super-resolution reconstruction from the layer above, which is defined as unpredictability from this coarse-to-fine reconstruction....|
|||...Finally, saliency on each layer of the pyramid is fused into stochastic fixations through a probabilistic model, where attention initiates from the top layer and propagates downward through the pyramid....|
|||...In fact, the theoretical investigation of visual saliency has aroused enduring controversies [38]....|
|||...One possible explanation often adopted in the design of saliency detection ap Figure 1....|
|||...Saliency: saliency emerges from image super-resolution from four pairs of coarse-to-fine scales....|
|||...fore, starting from Itti and Koch [22], eye fixations are commonly predicted by directly conjoining saliency activations from multiple channels, which can be global and local channels [2], multiple fe...|
|||...Most saliency detection models, however, do not seriously take this into account....|
|||...up models for saliency detection in the manner of HMAX, CDBN and the like....|
|||...In this paper, we present an effective model based on RHT for saliency detection, which proves that RHT is helpful at least in this particular computer vision application....|
|||...On each layer, saliency is defined as unpredictability in coarse-to-fine reconstruction through image super-resolution [9, 8, 41]....|
|||...The saliency on each layer is then fused into fixation estimate with a probabilistic model that mimics reverse propagation of attention....|
|||...Random Center Surround Saliency [37] adopted a similar centersurround heuristic but with center size and region randomly sampled....|
|||...Several saliency models adopted a probabilistic approach and modeled the statistics of image features....|
|||...Itti and Baldi [20] defined saliency as surprise that arised from the divergence of prior and posterior belief....|
|||...SUN [43] was a Bayesian framework using natural statistics, in which bottom-up saliency was defined as self-information....|
|||...[13] defined the saliency by computing the Hotellings T-squared statistics of each multi-scale feature channel....|
|||...[11] considered saliency in a discriminative setting by defining the KL-divergence between features and class labels....|
|||...A special class of saliency detection schemes was frequency-domain methods....|
|||...Hou and Zhang [18] proposed a spectral residual method, which defined saliency as irregularities in amplitude information....|
|||...[17] introduced a simple image descriptor, based on which a competitive fast saliency detection algorithm was devised....|
|||...Different from our proposal, the conventional practice in fusing saliency at different image scales and feature channels was through linear combination....|
|||...Borji [2] proposed a model that combined a global saliency model AIM [3] and a local model [22, 21] through linear addition of normalized maps....|
|||...But it differs from most existing saliency detection models that incorporate top-down components such as [39, 34, 43, 28] in two aspects....|
|||...Nevertheless, there were a few preliminary studies trying to make use of the hierarchical structure for saliency detection and attention modeling....|
|||...A recent study [40] used hierarchical structure to combine multi-scale saliency, with a hierarchical inference procedure that enforces the saliency of a region to be consistent across  2  downsamplin...|
|||...Therefore, we define saliency S(xh xl) as the Normalized Mean Square Error (NMSE):  S(xh xl) =  ....|
|||...Saliency from Image Super-Resolution  In this section, a coarse-to-fine saliency model based on image super-resolution is presented....|
|||...In the next section, we discuss how to fuse saliency on each layer of the pyramid into fixation estimate....|
|||...With LR or BI, the saliency computed in (2) is the normalized l2-norm of the Laplacian pyramid....|
|||...In addition, the two techniques can be used to implement the centersurround strategy adopted in some saliency models, e.g....|
|||...The coefficients  are then used to reconstruct(cid:98)xh by Once we have obtained(cid:98)xh, saliency of the image patch  (cid:98)xh = Dh....|
|||...2 indicate that the saliency obtained by compressive sensing can largely differ from that obtained by LR and BI....|
|||...Saliency Map  A saliency map M is obtained by collecting patch salien cy defined in (2) over the entire image....|
|||...Then (cid:102)M is  blurred with a Gaussian filter [17] and normalized to be between [0, 1] to yield the final saliency map M. One should not confuse this Gaussian filter with B in Sections 3.1 and 3.2....|
|||...Generating Fixations  We model attention as random variables A0, A1, ..., An on saliency maps M0, M1, ..., Mn, which are ordered in a coarse-to-fine scale hierarchy....|
|||...To compute the term, we first convert the coordinate Ak1 into the corresponding coordinate (u, v) in the saliency map just below it, i.e....|
|||...If we do not consider any prior on the top layer, Pr[A0]  depends on the saliency map only Pr[A0 = (i, j)]  exp  (cid:16)  (cid:17)  M0[i, j]  ....|
|||...The stochastic points were then blurred with a Gaussian filter to yield the final saliency map....|
|||...Several metrics have been used to evaluate the performance of saliency models....|
|||...Following [24], we first matched the histogram of the saliency map to that of the fixation map to equalize the amount of salient pixels in the map, and then used the matched saliency map for evaluation....|
|||...The proposed model was compared with several state-of-the-art models: Itti & Koch [21], Spectral Residual Methods (SR) [18], Saliency based on Information Maximization (AIM) [3], Graph Based Visual Sa...|
|||...This simple model was also combined with other saliency detection models to account for the center bias, which could boost accuracy of fixation prediction....|
|||...Following [15], this was achieved by multiplying the center model with the saliency maps obtained by these models in a point-wise manner....|
|||...The saliency maps of RHM were obtained from the predicted fixations blurred with a Gaussian filter....|
|||...Then a stochastic fixation model is presented, which propagates saliency from the top layer to the bottom layer to generate fixation estimate....|
|||...Finally, in view of the similar hierarchical structure used in this study for saliency detection and other studies for object recognition, it would be interesting to devise a unified model for both tasks....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...AUC of single saliency maps and their linear combination (Linear) on the TORONTO dataset....|
|||...The saliency maps M0, M1 and M2 correspond to the downsampled images by factors of 27, 9 and 3 respectively....|
|||...sive sensing, we substituted it with other saliency models....|
|||...Specifically, we replaced the saliency maps obtained from coarse-to-fine reconstruction by the saliency maps obtained by existing models....|
|||...Notice that blurring with a Gaussian filter is a necessary step in our model to obtain a smooth saliency map from stochastic fixations....|
|||...Previous results have shown that blurring improved the performance of saliency models [17, 2]....|
|||...To investigate the effect of reverse propagation, we substituted it with linear combination of saliency maps, which is widely adopted in literature [21, 3, 2]....|
|||...The linear combination produced an AUC between the best and worst that a single saliency map could achieve....|
|||...Hierarchical saliency detec tion....|
|||...Top-down visual saliency via joint  CRF and dictionary learning....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
|||...Learning a saliency map using fixated locations in natural scenes....|
|||...Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. In CVPR 2008....|
||61 instances in total. (in cvpr2014)|
|31|Attentional Push_ A Deep Convolutional Network for Augmenting Image Salience With Shared Attention Modeling in Social Scenes|...Our model contains two pathways: an Attentional Push pathway which learns the gaze location of the scene actors and a saliency pathway....|
|||...The Attentional Push CNN is then fine-tuned along with the augmented saliency CNN to minimize the Euclidean distance between the augmented saliency and ground truth fixations using an eye-tracking dat...|
|||...ve often been  Input Image (top left) and ground-truth fixation heat Figure 1: map (top right), eDN saliency [35] (bottom left) and BMS saliency [37] (bottom right)....|
|||...The recent publication of large-scale fixation datasets has motivated many saliency models based on convolutional neural networks (convnets), which have made quite significant improvements over tradit...|
|||...This trending increase in performance improvement of convnetbased saliency models seems to have saturated the prediction performance to the extent that further improvements re 2510  quire new and dee...|
|||...The figure compare the performance of two of the best-performing saliency models (according to the MIT saliency benchmark), eDN [35] (neural network-based) and BMS [37] (non-neural network), with the ...|
|||...We propose a model that learns to follow the gaze location of the scene actors and augments saliency models  with the Attentional Push effect of the actors gaze in social scenes (everyday scenes depic...|
|||...Instead of designing a saliency model from scratch, we purposefully use pre-built saliency models to illustrate that even the state-of-the-art in saliency models, either built from hand-crafted featur...|
|||...We present a deep convolutional neural network which augments saliency models with Attentional Push....|
|||...Our network contains two pathways: a saliency pathway, which embeds saliency methods, and an Attentional Push pathway, containing a deep convnet which learns to estimate the gaze location of the scene actors....|
|||...While the saliency pathway is fed with the whole input image to compute the saliency map, we only provide a 2-D grid location of the head of the scene actors and a cropped image region around them to ...|
|||...The Attentional Push convnet is then fine-tuned along with the augmented saliency convnet to minimize the Euclidean distance between the augmented saliency with ground truth fixations....|
|||...Section 2 presents related work on attention tracking and saliency models that have employed gaze following as a subcomponent and data-driven saliency methods using convnets....|
|||...We explain the structure and the training scheme for the Attentional Push CNN and the augmented saliency CNN in Section 3 and 4, respectively....|
|||...[13] saliency map in predicting eye fixations in social scenes and showed that its performance is near chance levels....|
|||...[7] analyzed the prediction performance of the bestperforming saliency models (according to the MIT saliency benchmark [5] and [16]) in social scenes....|
|||... combination scheme is based on a simple element-wise multiplication of a predicted gaze map with a saliency map, whereas our model employs a convnet to effectively merge the complementary  informatio...|
|||...The recent publication of large-scale fixation datasets has motivated many saliency models based on convolutional neural networks....|
|||...Similarly, the SALICON model [12] benefits from two pre-trained convnets, each on a different image scale, that are concatenated to produce the saliency map....|
|||...ayers of the VGG network and instead of using the feature maps of the final layers, it computes the saliency map by combining feature maps extracted from different levels of the VGG network....|
|||...we obtain information solely coming from Attentional Push effect of the actors gaze and we let the augmented saliency convnet to merge them with the information coming from the saliency pathway....|
|||...Architecture  As illustrated in Figure 2, the network consists of fourteen weight layers, including ten convolutional and four  Figure 3: Architecture of the augmented saliency network....|
|||...Augmented Saliency CNN  We combine the complementary information given by the saliency and the Attentional Push maps using a shallow convnet....|
|||...The augmented saliency convnet takes the saliency map, the Attentional Push map, and the actors head locations as inputs and generates the augmented saliency map....|
|||...The reason that we feed the actors head locations to the augmented saliency convnet is that the augmentation process should vary as a function of both the actors head and gaze location....|
|||...In addition, since we are augmenting pre-trained saliency models, the augmentation should vary depending on the employed saliency model....|
|||...Therefore, we train the network once for each saliency model....|
|||...In the following sections, we provide the architecture and the training procedure for the augmented saliency convnet....|
|||...Architecture  As illustrated in Figure 3, the network takes three inputs: the saliency map, the Attentional Push map, and the location of the head of the scene actors; all resized to the same size as ...|
|||...We use a Euclidean loss layer to minimize the Euclidean distance between the augmented saliency with ground truth fixations during training....|
|||...t  3018 603    200 200 200  To illustrate the effectiveness of Attentional Push in augmenting image saliency, we employ five saliency models and train and evaluate the network in Figure 3 once for eac...|
|||...We used the MIT saliency benchmark [5] in selecting the best-performing saliency models which have available implementations....|
|||...The network is trained by back-propagating the Euclidean distance between the augmented saliency and the fixation heatmaps, using mini-batch stochastic gradient descent with a mini-batch size of 2, a ...|
|||...Evaluation protocol  We employ three neural network-based saliency models and two best-performing non neural network saliency models and train the full network in Figure 3 for each....|
|||...Attention models have commonly been validated against  2515  Table 2: Average evaluation scores for the augmented saliency vs. saliency models on the SALICON, CAT2000 and iSUN test sets....|
|||...Since the performance of a model may change remarkably while using different metrics, we use three popular evaluation metrics: the Area Under the ROC Curve (AUC), the Normalized Scan-path Saliency (NS...|
|||...Table 2 compares the prediction performance of the Attentional Push-based augmented saliency with the standard saliency methods on the SALICON, CAT2000 and the iSUN test sets respectively....|
|||...The results show that the augmented saliency consistently improves upon the standard saliency methods....|
|||...The results indicate that all the employed saliency models, both neural network and nonneural network based models, can benefit from Attentional Push to improve the prediction accuracy....|
|||...We present qualitative results for comparing the augmented saliency and the saliency methods in Figure 4....|
|||...the saliency map, the Attentional Push map, the input head location and the augmented saliency map....|
|||...As seen in the figure, augmented saliency maps clearly benefit from the all of them to provide an improved prediction of the groundtruth fixations....|
|||...The results are based on BMS saliency and the SALICON test set....|
|||...The performance of the model without the saliency input suggests that while viewing social scenes, the viewers tend to focus on social cues instead of irrelevant salient regions....|
|||...gions to direct and manipulate the attention allocation of the viewer, with standard saliency models, which generally concentrate on analyzing image regions for their power to pull attention....|
|||...We presented a deep convolutional convnet which learns to follow the gaze location of the scene actors and augments saliency models with Attentional Push....|
|||...Based on evaluation using three eye-tracking datasets, our methodology significantly outperforms saliency methods in predicting the viewers fixations....|
|||...Our results showed that by employing Attentional Push cues, the augmented saliency maps can improve upon the state of the art in saliency mod 2516  (i)  (ii)  (iii)  (iv)  (v)  (vi)  (a)  (b)  (c)  (...|
|||...(i) Input image, (ii) ground-truth fixation heatmap, (iii) saliency map, (iv) face location input, (v) Attentional Push map, and (vi) augmented saliency map....|
|||...CAT2000: A large scale fixation dataset for boosting saliency research....|
|||...Complementary effects of gaze direction and early saliency in guiding fixations during free viewing....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Mit saliency benchmark....|
|||...Where should saliency models look next?...|
|||...A deep multi-level network for saliency prediction....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Deep gaze I: boosting saliency prediction with feature maps trained on imagenet....|
|||...Augmented saliency model using automatic 3D head pose detection and learned gaze following in natural scenes....|
|||...Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
||61 instances in total. (in cvpr2017)|
|32|Ziheng_Zhang_Saliency_Detection_in_ECCV_2018_paper|...This paper presents a novel spherical convolutional neural network based scheme for saliency detection for 360 videos....|
|||...We further take the temporal coherence of the viewing process into consideration, and propose a sequential saliency detection by leveraging a spherical U-Net....|
|||...To validate our approach, we construct a large-scale 360 videos saliency detection benchmark that consists of 104 360 videos viewed by 20+ human subjects....|
|||...Keywords: Spherical convolution  Video saliency detection  360 VR videos  1  Introduction  Visual attention prediction, commonly known as saliency detection, is the task of inferring the objects or re...|
|||...By far, almost all existing works focus on image or video saliency detection, where the participants are asked to look at images or videos with a limited field-of-view (FoV)....|
|||...In this paper, we propose to mimic this process by exploring the saliency detection problem on 360 videos....|
|||...panoramic saliency detection....|
|||...d project the results using local perspective projection can lead to high computational overhead as saliency detection would need to be applied on each tile....|
|||...Directly applying perspective based saliency detection onto the panorama images is also problematic: panoramic images exhibit geometric distortion where many useful saliency cues are not valid....|
|||...In other words, if we leverage the CNN for 360 video saliency detection, the convolution operation corresponding to different view angle/FOV should maintain the same kernels....|
|||...To better cope the 360 video saliency detection task, we propose a new type of spherical convolutional neural networks....|
|||...Then we propose to leverage such temporal coherence for efficient saliency detection by instantiating the spherical convolutional neural networks with a novel spherical U-Net [9]....|
|||...By far, nearly all saliency detection datasets are based on narrow FoV perspective images while only a few datasets on 360 images....|
|||...To validate our approach, we construct a large-scale 360 videos saliency detection benchmark that consists of 104 360 videos viewed by 20+ human subjects....|
|||...We further extend it to panorama case; ii) We propose a sequential saliency detection scheme and instantiate the spherical convolutional neural networks with a spherical U-net architecture for frame-w...|
|||...The dataset and code have been released to facilitate further research on 360 video saliency detection1....|
|||...In other words, the kernels used for saliency detection should be shared across all views....|
|||...So our solution is more natural and more interpretable for saliency detection in 360 videos....|
|||...2.2 Video Saliency Detection  Many efforts have been done to study the video saliency detection, either hand-crafted features based methods [17][18][19] [20], or deep learning based methods [21][22] [...|
|||...s whose length is less than 20 seconds3, and use the remaining 104 video clips as the data used for saliency detection in 360 videos....|
|||... L =  1 n  n  ,  Xk=1  X=0,=0  w,(S(k)  ,  S(k)  ,)2  (6)  where S(k) and S(k) are the ground truth saliency map and predicted saliency map for the kth image, and w, is weights for each points which i...|
|||...5 Spherical U-Net Based 360 Video Saliency Detection  5.1 Problem Formulation  Given a sequence of frames V = {v1, v2, ....|
|||...So deep learning based 360 video saliency detection aims at learning a mapping G that maps input V to S. However, different from perspective videos whose saliency merely depends on the video contents,...|
|||...We define s0 as the eye fixation map at the starting position, which is the saliency map corresponding to the starting point, then the video saliency detection can be formulated as  G = arg min  F  kS...|
|||...  = arg min  F  T  Xt=1  kst  F (vt, st1)k2  (8)  Here F is the prediction function which takes the saliency map of previous frame and video frame at current moment as input for saliency prediction of...|
|||...Inspired by the success of U-Net [9] we propose to adapted it with a Spherical U-Net as F for the frame-wise saliency detection....|
|||...The input to the network is projected spherical images vt at time t and project spherical saliency map st1 at time t  1....|
|||...dtaset and our video saliency dataset....|
|||...Our video saliency dataset consists of 104 360 videos viewed by 20 observers....|
|||...For image saliency, we regress the saliency maps directly from the RGB 360 images....|
|||...We create the ground truth saliency maps through a way similar to spherical convolution using a crown Gaussian kernel with sigma equaling to 3.34....|
|||...Owing to the distortion during projection, it does not make sense to directly compare two panorama saliency maps like typical 2D saliency maps....|
|||...The performance comparison of state-of-the-art methods with our spherical U-Net on our video saliency dataset....|
|||...We compare our proposed spherical U-Net with the following state-of-theart: image saliency detection methods, including LDS [29], Sal-Net [5] and SALICON [30], video saliency detection methods, includ...|
|||...Compared with our spherical U-Net method, the only difference is that the saliency of previous frame is not considered for the saliency prediction of current frame....|
|||...e by following the strategies in [34]:   Baseline-one human: It measures the difference between the saliency map viewed  by one observer and the average saliency map viewed by the rest observers....|
|||... Baseline-infinite humans: It measures the difference between the average saliency map viewed by a subset of viewers and the average saliency map viewed by the remaining observers....|
|||...Recent work has employed several top-down cues for saliency detection....|
|||...Previous work [35] shows that human face boosts saliency detection....|
|||...Therefore, we also design a baseline Top-down cue (face) to use human face as cue, and post-process saliency map following [35]....|
|||...It shows that our method outperforms all baseline methods on our video saliency dataset, which validates the effectiveness of our scheme for 360 video saliency detection....|
|||...dataset, we have modified our model to directly predict the saliency map for a static 360 image....|
|||...The Second: The performance comparison of different components on our method on our video saliency dataset....|
|||...6.5 Saliency Prediction for a Longer Time  Middle and right figures in Fig....|
|||...We can see that the performance of saliency prediction degenerates for as time elapse....|
|||...One possible reason is that as time goes longer, the prediction of previous frame becomes less accurate, which consequently would affect the saliency detection of current frame....|
|||...It takes about 36 hours to train the model on the our video saliency dataset (the total number of iterations is 4000.)....|
|||...7 Conclusion and Discussions  Our work attempts to exploit the saliency detection in dynamic 360 videos....|
|||...Then we propose a spherical UNet for 360 video saliency detection....|
|||...There still exists some space to improve our method for video saliency prediction....|
|||...Currently, to simplify the problem, we only consider the saliency map of the previous frame for the prediction of current frame....|
|||...Considering the saliency map over a longer time range may boost the performance, for example, we can also combine our spherical U-Net with LSTM....|
|||...: Shallow and deep convo lutional networks for saliency prediction....|
|||...Zhong, S.h., Liu, Y., Ren, F., Zhang, J., Ren, T.: Video saliency detection via dynamic  consistent spatio-temporal attention modelling....|
|||...Ren, Z., Gao, S., Chia, L.T., Rajan, D.: Regularized feature reconstruction for spatiotemporal saliency detection....|
|||...: Deep learning for saliency predic tion in natural video....|
|||...Fang, S., Li, J., Tian, Y., Huang, T., Chen, X.: Learning discriminative subspaces on random contrasts for image saliency analysis....|
|||...Huang, X., Shen, C., Boix, X., Zhao, Q.: Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...: Saltinet: Scan-path prediction  on 360 degree images using saliency volumes....|
|||...Judd, T., Durand, F., Torralba, A.: A benchmark of computational models of saliency to  predict human fixations....|
|||...Goferman, S., Zelnikmanor, L., Tal, A.: Context-aware saliency detection....|
||61 instances in total. (in eccv2018)|
|33|Zhu_Saliency_Pattern_Detection_ICCV_2017_paper|...By learning appearance features in rectangular regions, our structural region representation encodes the local saliency distribution with a matrix of binary labels....|
|||...Representing region saliency with structured labels has two advantages: 1) it connects the label assignment of all enclosed pixels, which produces a smooth saliency prediction; and 2) regularshaped na...|
|||...Finally, we introduce a K-NN enhanced graph representation for saliency propagation, which is more favorable for our task than the widely-used adjacent-graphbased ones....|
|||...An accurate saliency detection method is able to recommend regions that are informative, attentive and above all, it implies the presence of prototype of objects [12]....|
|||...Since the first computational attention model [17] was published, the interest in saliency detection related research has increased rapidly....|
|||...Following this idea, it is natural to compute the saliency of a pixel/region by the center-surround contrast [13, 14, 17, 23]....|
|||...However, features that are extracted from superpixels may result in less informative saliency measures [53]....|
|||...In this paper, we propose a new saliency detection method that takes the structural representation of rigid grids as receptive fields....|
|||...Inspired by the work of edge pattern [9], we learn a Structured Saliency Pattern (SSP) that parses the saliency assignment of a local rectangular region....|
|||...During training, we learn SSP by saliency cues such as regional properties and center-surround contrast....|
|||...Finally, a K-NN enhanced graph-based saliency propagation is developed to refine the saliency map....|
|||...2) Region saliency is computed as a weighted combination of structured labels....|
|||...We propose an adaptive ranking method to decide the most appropriate labels that representing the local saliency distribution....|
|||...3) We propose a K-NN enhanced graph for saliency propagation, which considers neighboring relationships in both spatial and feature spaces....|
|||...Among those bottom-up saliency methods, the Feature Integration Theory (FIT) [51] serves as the basis for many biologically motivated models....|
|||...[17] first propose a saliency model for computing the center-surround contrast and searching the local maximum response in the multiscale DoG space....|
|||...Second, it is very hard to achieve precise saliency assignment around the foreground/background boundary when edge information is absent....|
|||...The small number of superpixels benefits the application of graph-based techniques on saliency detection [4, 38, 43, 57, 58]....|
|||...There are however two problems of segment-based methods: first, features from homologous region may result in less informative saliency measures....|
|||...Second, it is difficult for segment-based saliency models to construct a rigid centersurround structure for contrast computation....|
|||...The proposed saliency model  Fig....|
|||...1b, pixelwise saliency is voted by predicting and ranking SSP (see Section 3.1) in each proposal....|
|||...Finally, a K-NN enhanced graph-based saliency diffusion method is used to refine the saliency map as shown in Fig....|
|||...n of size d  d and x refer to the appearance features defined on R. We determine the probability of saliency assignment P (s x) of all enclosed n = d2 pixels: pi  R, i = 1, 2, ..., n, where s = [s1, s...|
|||...(c) The saliency map is refined by K-NN enhanced graph-based saliency propagation....|
|||...id:0)  [W ]2(i, j)(cid:1),  (3)  Figure 2: Regions with different appearance patterns share similar saliency patterns....|
|||...2 for example, R1 and R2 have very different appearance but their spatial saliency distributions l1 and l2 are similar....|
|||...Therefore, we take lj as a common saliency pattern which is capable of describing saliency assignment for a large number of image regions....|
|||...Since lj encodes the saliency structure of image patches, we call it a Structured Saliency Pattern (SSP)....|
|||...5469  CNN-based Binary ClassifierBinary Proposal GenerationStructured Saliency Pattern Ranking Saliency Propogation(a)(b)(c)Binary Proposal MapPixel-level Saliency MapFinal Saliency MapOriginal ImageG...|
|||...1 implies that the corresponding SSP is in more accordance with the real saliency distribution than others in the window....|
|||...To ensure that saliency mass can be smoothly transferred along graph edges based on the similarities, 2 can be identified by minimizing the following reconstruction error [20]:  otherwise  (7)  0,  lj...|
|||...Saliency propagation  Taking some high confidential salient regions as seed, most saliency propagation methods require an adjacent similarities to distribute saliency mass to similar nearby regions al...|
|||...Introducing NN connection enables the exchange of saliency mass between similar regions regardless their spatial connectivities....|
|||...It helps to obtain a balance saliency assignment on separated objects....|
|||...After yi is obtained, we upsample the saliency map using bilateral filtering [44] to get the finial pixel-level saliency map....|
|||...Experiment results  We evaluate the proposed method on six popular benchmarks that are widely used for saliency object detection: THUR15K [7], DUT-OMRON [58], ECSSD [49], PASCAL-S [33], SOD [18] and TCD [53]....|
|||...Note that, some approaches are not evaluated on certain benchmarks due to two reasons: 1) neither saliency maps nor codes are available, including: evaluations of SSD on THUR15K and TCD; evaluations o...|
|||...The training samples  Precision versus Recall (PR) curve measurement is a straightforward way for evaluating saliency models via testing the segmentation precision over all possible thresholds....|
|||...For every test image, we first normalize each saliency map  5472  Precision and Recall THUR15K  Precision and Recall DUTOMRON  Precision and Recall ECSSD  i  i  n o s c e r P  i  i  n o s c e r P  0....|
|||...Figure 7 shows PR curves measurement for all evaluated methods on six popular saliency datasets....|
|||...Segmentation by adaptive threshold  Quantitative comparison can also be achieved by comparing the segmented saliency map with the corresponding ground truth....|
|||...As introduced in [1], this threshold T for a saliency map s can be obtained as T = 2/(W   H)PiPj s(i, j), where W and H refer to the width and  height of saliency map s, respectively....|
|||...As shown in Figure 8, the  1st to 3rd bins of each bar group respectively show the precision, recall and F-measure of all evaluated methods using adaptive thresholding on six saliency datasets....|
|||...We observe that the precision, recall and F-measure do not consider the true negative saliency predictions....|
|||...As shown in Figure 8, the 4th bin of each bar group shows the the MCC measure of all evaluated methods using adaptive thresholding on six saliency datasets....|
|||... Figure 8, the 5th bin of each bar group shows the the MAE measure2 of all evaluated methods on six saliency datasets....|
|||...The effectiveness of saliency propagation  In this section, we evaluate the performance of our saliency propagation method....|
|||...9 compares the performance of SSP prediction and the saliency propagation with different values of  in Eq....|
|||...Conclusion and future work  In this paper we propose to use Structured Saliency Pattern (SSP) for describing local saliency distributions....|
|||...Finally, a new K-NN enhanced graph model is proposed for saliency propagation....|
|||....7584) no propagation (0.7421)  0.2  0.4  0.6  0.8  1  Recall  Figure 9: Evaluation of the proposed saliency propagation method....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Visual saliency based on multiscale deep features....|
|||...Adaptive metric learning for saliency detection....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Regionbased saliency detection and its application in object recognition....|
|||...Rare: A new bottom-up saliency model....|
|||...Pisa: Pixelwise image saliency by aggregating complementary appearance contrast measures with edge-preserving coherence....|
|||...Deep networks for saliency detection via local estimation and global search....|
||61 instances in total. (in iccv2017)|
|34|Khatoonabadi_How_Many_Bits_2015_CVPR_paper|...isco Systems, Boxborough, MA, USA, yshan@cisco.com  Ivan V. Baji c3  Yufeng Shan4  Abstract  Visual saliency has been shown to depend on the unpredictability of the visual stimulus given its surround....|
|||...Various previous works have advocated the equivalence between stimulus saliency and uncompressibility....|
|||...To account for global saliency effects, these are embedded in a Markov random field model....|
|||...The resulting saliency measure is shown to achieve state-of-the-art accuracy for the prediction of fixations, at a very low computational cost....|
|||...Since most modern cameras incorporate video encoders, this paves the way for in-camera saliency estimation, which could be useful in a variety of computer vision applications....|
|||...Early approaches pursued a circuit driven view of the center-surround operation, modeling saliency as the result of center-surround filters and normalization [26]....|
|||...Under these models, saliency is computed by a network of neurons, where a stimulus similar to its surround suppresses neural responses, resulting in low saliency, while a stimulus that differs from it...|
|||...More recently, several works have tried to identify general computational principles for saliency, also applicable to the development of other classes of saliency mechanisms, such as those responsible...|
|||...A particularly fruitful line of research has been to connect saliency to probabilistic inference....|
|||...A first class of approaches models saliency as a measure of stimulus information....|
|||...For example, [11, 57, 41] advocate an information maximization view of visual attention, where the saliency of the stimulus at an image location is measured by the self-information [12] of that stimul...|
|||...[25] proposes a similar idea, denoted Bayesian surprise,  1  which equates saliency to the divergence between a prior feature distribution, collected from surround, and a posterior distribution, comp...|
|||...In parallel to these conceptual developments, there has also been an emphasis on performance evaluation of different approaches to saliency [10]....|
|||...These efforts have shown that saliency models based on the compression principle tend to make accurate predictions of eye fixation data....|
|||...In fact, several of these models predict saliency with accuracy close to the probability of agreement of human eye fixations....|
|||...Second, while many implementations of the saliency as compression principle have been proposed, much smaller attention has been devoted to implementation complexity....|
|||...For such applications, the saliency operation should ideally be performed in the cameras themselves, which would only consume the power and bandwidth necessary to transmit video when faced with salien...|
|||...This, however, requires highly efficient saliency algorithms....|
|||...ntly to different locations of the visual field, the spatial  distribution of bits can be seen as a saliency measure, which directly implements the compressibility principle....|
|||...This saliency measure addresses the three main limitations of the state of the art....|
|||...We propose an implementation of the OBDL measure, and show that saliency can be encoded with a simple feature derived from it....|
|||...Related work  The overwhelming majority of existing saliency models operate on raw pixels, rather than compressed images or video....|
|||...s), block coding modes, motioncompensated prediction residuals, or their transform coefficients, in saliency modeling [33, 2, 32, 40, 14]....|
|||...The extracted data is a proxy for many of the features frequently used in saliency modeling....|
|||...Our approach is quite different from the majority of these methods, most of which do not even explicitly equate stimulus saliency to compressibility....|
|||...On the contrary, we pursue the compressibility principle to the limit, proposing to measure saliency with a compressibility score that has not been previously used in the literature....|
|||...Hence, a video compressor is a very sophisticated implementation of the saliency principle of [11], which evaluates saliency as  S(x) = log  1  p(x)  :  (2)  While [11] proposes a simple independent c...|
|||...Overall, the OBDL combines the accuracy of the non-compressed domain saliency measures with the computational efficiency of their compressed-domain counterparts....|
|||...Although the spatially smoothed OBDL 2 map is already a solid saliency measure, we observed that an additional improvement in the accuracy of saliency predictions is possible by performing further tem...|
|||...OBDL-MRF saliency estimation model  In this section, we describe a measure of visual saliency based on a Markov random field (MRF) model of OBDL feature responses....|
|||...On the other hand, saliency has both a local and a global component....|
|||...For example, many saliency models implement inhibition of return mechanisms [26], which suppress the saliency of image locations in the neighborhood of a saliency peak....|
|||...More specifically, the saliency detection problem is formulated as one of inferring the maximum a posteriori (MAP) solution of a spatio-temporal Markov random field (ST-MRF) model....|
|||...gy functions E( ; !1t1; o1t), E( ; o1t), and E( ) measure the degree of temporal consistency of the saliency labels, the coherence between labels and feature observations, and the spatial compactness ...|
|||...; y   (x  where Et(n) is a measure of inconsistency within Nn, which penalizes temporally inconsistent label assignments, i.e., !t(x; y) = !t  The saliency label !...|
|||...It is defined as  (11)  (n)1 (n) (1  (n)) (n) ;     where (n) is a measure of saliency in the neighborhood of n. This is defined as  (n) = (cid:11)   (m) + (cid:12)   (m);  (12)  mn+  mn  where n+ and...|
|||...Final saliency map  The procedure above produces the most probable, a posteriori, map of salient block labels....|
|||...In this way, a block n labeled as salient by the MRF inference is assigned a saliency equal to the largest feature value within its neighborhood, weighted by its distance from n. On the other hand, fo...|
|||...The complement of this value is then assigned as the saliency value of n.  5....|
|||...Experimental set(cid:173)up  The proposed algorithm was compared with a number of state-of-the-art algorithms for saliency estimation in video, which are listed in Table 2....|
|||...MRF configurations  We started with a number of experiments that tested the role of the different components of the saliency detector in its performance....|
|||...The first set of experiments tested the impact of the MRF inference in the saliency judgments....|
|||...We compared the performance of the saliency measure of (13) for various MRF settings....|
|||...The global fusion of saliency information, by the MRF provides some additional gains....|
|||...Comparison to the state(cid:173)of(cid:173)the(cid:173)art  A set of experiments was performed to compare the OBDL-MRF to state-of-the-art saliency algorithms....|
|||...We start by comparing the processing times of the various saliency measures in Table 3....|
|||...While this is slower than some of the compressed-domain algorithms, it enables the computation of saliency at close to 30 fps....|
|||...Accuracy of various saliency algorithms over the two datasets according to (top) AUC and (bottom) NSS scores....|
|||...Figure 5 illustrates the differences between the saliency predictions of various algorithms....|
|||...The performances of the different saliency measures were also evaluated with a multiple comparison test [21]....|
|||...The number of appearances of the different saliency measures among the top performer class is shown in Fig....|
|||...b) Impact of average PSNR on saliency predictions....|
|||...mine the sensitivity of the saliency measure to the amount of this loss....|
|||...Obviously, in the limit of zero-bit encoding, the proposed OBDL-MRF will not be a very good saliency predictor....|
|||...Somewhat surprisingly, saliency predictions degrade for both very low and high quality video....|
|||...It is also encouraging that saliency estimation is most accurate in the middle of this range, since this is the preferred operating point for most vision applications....|
|||...Conclusion  We proposed a model of visual saliency based on the compressibility principle....|
|||...While, at a high level, this is similar to well-known saliency models, such as those based on self-information and surprise, it has the distinct advantage of being readily available at the output of a...|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...A video saliency detection model in compressed domain....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
||61 instances in total. (in cvpr2015)|
|35|Wang_A_Stagewise_Refinement_ICCV_2017_paper|...To remedy this problem, here we propose to augment feedforward neural networks with a novel pyramid pooling module and a multi-stage refinement mechanism for saliency detection....|
|||...Then, refinement nets are integrated with local context information to refine the preceding saliency maps generated in the master branch in a stagewise manner....|
|||...Early saliency models, inspired by the human visual attention mechanisms, attempted to predict spatial locations where an observer may fixate during free-viewing of natural scenes (e.g., [21, 20, 39, ...|
|||...A strand of saliency research has focused on object-level segmentation since the pioneering works of Liu et al., [34] and Achanta et al., [1]....|
|||...However, these approaches pose clear limitations when dealing with dense prediction tasks such as semantic segmentation [8, 36], scene parsing [35] and saliency detection....|
|||...The pipeline of our proposed saliency detection algorithm....|
|||...Given an input image (c), an intermediate saliency map (d) is generated in stage 1 with the size 1/32 of the original resolution....|
|||...More refinement nets in stage t (t 2) are gradually connected to refine the saliency map generated at the preceding stage....|
|||... refinement network where the refinement nets help renovate sharp and detailed boundaries in coarse saliency maps for highresolution salient object segmentation....|
|||...Saliency Detection  Early saliency models mainly concentrated on the lowlevel features dating back to the feature integration theory [44]....|
|||...In addition, background prior has been utilized to compute saliency based on the observation that image boundary regions usually tend to belong to the background....|
|||...In [30], dense and sparse reconstruction errors based on background prior are utilized for saliency detection....|
|||...[50] focus on the saliency detection from two background priors including boundary and connectivity....|
|||...In [16, 15, 53, 23, 27, 38], saliency is measured by label propagation where initial labeling is propagated from the labeled elements to the unlabeled ones based on their pairwise affinities....|
|||...The low-level saliency cues are often effective in simple scenarios but they are not always robust in some challenging cases....|
|||...Therefore, it is necessary to consider high-level image information and context for saliency prediction....|
|||...Deep Networks for Saliency Detection  Recently, deep convolutional neural networks (CNNs) have achieved near human-level performance in some computer vision tasks [18]....|
|||...CNNs have also achieved state-of-the-art performance when applied to saliency detection....|
|||...In [28], multiscale features are extracted first and then a fully connected regressor network is trained to infer the saliency score of each image segment....|
|||...These methods measure saliency at the patch level where CNNs are run thousands of times to obtain the saliency score of every patch, which is computationally very expensive....|
|||...infuse prior knowledge into a recurrent fully convolutional network for accurate saliency inference....|
|||...Secondly, a stage-wise hierarchical refinement network is utilized to progressively refine the intermediate saliency maps where multiscale nets are optimized to obtain their individual best result....|
|||...Thirdly, each intermediate saliency mask will be upsampled to the size of groundtruth map for computing losses, but in [33] the groundtruth mask is downsampled to meet the needs, which causes spatial ...|
|||...We begin by describing the generation of the initial coarse saliency map in Section 3.1, followed by a detailed description of our multi-stage refinement strategies equipped with pyramid pooling modul...|
|||...In the subsequent stages described in Section 3.2, we also adopt ResNet-50 as our fundamental building block for saliency detection....|
|||...The baseline network takes an entire image as input, and outputs a saliency map of equal resolution....|
|||...al layer (Conv7) with 2 channels (one foreground mask plus one for background) are added to compute saliency confidence for every pixel....|
|||...context information by combining preceding saliency maps with the features fed in the current stage....|
|||...3.2.1 Stage-wise Refinement  3.2.2 Pyramid Pooling Module  The first stage saliency map S1 generated by the feedforward network is coarse compared to the original resolution ground truth....|
|||...led illustration of the first refinement module R1 adopted in stage 2 (i.e., concatenating a coarse saliency map S1 from the master pass with a feature map F2 from a refinement pass) to generate a fin...|
|||...Then, we combine the upsampled saliency map with the feature maps F2 to generate S2....|
|||...It can be seen that the saliency maps generated from the proposed method with PPM can preserve salient object boundaries and suppress background noise....|
|||...Each stage of the framework is trained to repeatedly produce a saliency map based on the preceding one with more finer details recovered and added....|
|||...Then, the pixel-wise cross entropy loss between St and the ground truth saliency mask G is computed as:  L() =  X  X  1(St  i,j = lg) log Pr(li,j = lg )  i,j  lg{0,1}  (4)  where 1() is the indicator function....|
|||...We just feed the fixed-size input image to the  network to generate a final saliency map without using any preor post-processing....|
|||...All saliency maps are binarized at every integer threshold in the range of [0, 255]....|
|||...In addition, we also compute the average precision, recall, and F-measure values, where every saliency map is binarized with an adaptive threshold proposed by [1]....|
|||...The threshold is determined to be twice the mean saliency value of the saliency map....|
|||...is calculated as the average pixelwise absolute difference between the binary groundtruth G and the saliency map S adopted by [37],  M AE =  1  W  H  W  H  X  X  x=1  y=1   S(x, y)  G(x, y) ,  (7)  wh...|
|||...tate of the Art  We compare our method with eleven state-of-the-art deep learning-based and classic saliency detection methods, including DRFI [24], BL [43], LEGS [46], MDF [28], MCDL [56], DS [31], D...|
|||...For a fair comparison, we utilize either the implementations with recommended parameter settings or the saliency maps provided by the authors2....|
|||...Further, our saliency maps are much closer to the ground truth maps in various challenging scenarios....|
|||...As described in Section 3, a stage-wise refinement mechanism plus a pyramid pooling module are utilized to refine the coarse saliency map from the preceding stage....|
|||...Illustration of stage-wise saliency map generation....|
|||...We find that the stage-wise refinement scheme progressively improves details of saliency maps....|
|||...The multistage refinement mechanism is able to effectively combine high-level object-level semantics with low-level image features to produce high-resolution saliency maps....|
|||...alitative evaluations verify that the above contributions can significantly improve stateof-the-art saliency detection performance over five widely adopted datasets and two evaluation measures....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...Salientshape: Group saliency in image collections....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
|||...Recurrent attentional networks for saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Task-driven visual saliency and attention-based visual question answering....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...An eye fixation database for saliency detection in images....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Kernelized In ECCV, pages  subspace ranking for saliency detection....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||60 instances in total. (in iccv2017)|
|36|Jiang_Salient_Object_Detection_2013_CVPR_paper|...l image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency ...|
|||...nal contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine...|
|||...Introduction  Visual saliency has been a fundamental problem in neuroscience, psychology, neural systems, and computer vision for a long time....|
|||...The study on human visual systems suggests that the saliency is related to uniqueness, rarity and sur prise of a scene, characterized by primitive features like color, texture, shape, etc....|
|||...Recently a lot of efforts have been made to design various heuristic algorithms to compute the saliency [1, 6, 11, 15, 18, 27, 31, 34, 38]....|
|||...In this paper, we regard saliency estimation as a regression problem, and learn a regressor that directly maps the regional feature vector to a saliency score....|
|||...Second, we conduct a region saliency computation step with a random forest regressor that maps the regional features to a saliency score....|
|||...Last, a saliency map is computed by fusing the saliency maps across multiple levels of segmentations....|
|||...Unlike most existing algorithms that compute saliency maps heuristically from various features and combine them to get the saliency map, which we call saliency integration, we learn a random forest re...|
|||...PR.2013.271 DOI 10.1109/CVPR.2013.271 DOI 10.1109/CVPR.2013.271  2081 2081 2083  The basis of most saliency detection algorithms can date back to the feature integration theory [43] which posits that...|
|||...Recently, a lot of research efforts have been made to design various saliency features characterizing salient objects or regions....|
|||...The center-surround difference framework is also investigated to compute the saliency from region-based image representation....|
|||...The global contrast based approach [11], computing the saliency map by comparing each region with others, aims to directly compute the global uniqueness....|
|||...Based on the regional contrast, element color uniqueness and spatial distribution are introduced to evaluate the saliency scores of regions [38]....|
|||...The saliency map is generated by propagating the saliency scores of regions to the pixels....|
|||...Object prior, such as connectivity prior [45], concavity context [34], auto-context cue [48], and the background prior [53] are also studied for saliency computation....|
|||...A graphical model is proposed to fuse generic objectness and visual saliency together to detect objects [10]....|
|||...The stereopsis is leveraged for saliency analysis [37]....|
|||...A random forest regression approach is adopted to directly regress the object rectangle from the saliency map [50]....|
|||...Eye fixation prediction, another visual saliency research direction, also attracts a lot of interests [7, 24]....|
|||...context-aware saliency detection [18] aiming to detect the image regions that represent the scene....|
|||...In term of the saliency features, we compute a contrast vector instead of a contrast value used in the existing algorithms for a region....|
|||...In contrast to existing learning algorithms that perform saliency integration by combining saliency maps computed from different types of features, e.g....|
|||...[2, 10, 31], our approach learns to directly integrate feature vectors to compute the saliency map....|
|||...The closely related approach [26] which also learns to integrate the saliency features is a pixel-based algorithm, while our approach is region-based that performs multi-level estimation and can captu...|
|||...integrates three types of regional features in a discriminative strategy for the saliency regression on multiple segmentations....|
|||...Image saliency computation  The pipeline of our approach consists of three main steps: multi-level segmentation that decomposes an image into regions, region saliency computation that maps the feature...|
|||...Region saliency computation....|
|||...Our algorithm computes the saliency score for each region....|
|||...However, our algorithm essentially takes into consideration of such relations because we conduct the region saliency computation on multi-level segmentation....|
|||...Then the feature x is passed into a random forest regressor f , yielding a saliency score....|
|||...Multi-level saliency fusion....|
|||...After conducting rem gion saliency computation, each region R n  Sm has a m saliency value a n ....|
|||...As a result, we generate M saliency maps {A1, A2,    , AM }, and then fuse them together, A = g(A1,    , AM ), to get the final saliency map A, where g is a combinator function introduced in section 4....|
|||...rences of region features like color and texture, and then combine them together directly forming a saliency score, our approach computes a contrast descriptor, which will be fed into a regressor to a...|
|||...Learning  Learning the regional saliency regressor....|
|||...We aim to learn the regional saliency estimator from a set of training examples....|
|||...he training examples include a set of confident regions R = {R1, R2,    , RQ} and the corresponding saliency scores A = {a1, a2,    , aQ}, which are collected from the multi-level segmentation over a ...|
|||...the salient object or the background exceeds 80% of the number of the pixels in the region, and its saliency score is set as 1 or 0 accordingly....|
|||...Learning a saliency regressor can automatically combine the features and discover the most discriminative ones....|
|||...Learning the multi-level saliency fusor....|
|||...Given the multi-level saliency maps {A1, A2,    , AM } for  2084 2084 2086  Table 2....|
|||...aim is  an image, to learn a combinator g(A1, A2,    , AM ) to fuse them together to form the final saliency map A....|
|||...One can see in Figure 2(a) that the AUC score of the saliency maps increases when more levels of segmentations are adopted....|
|||...The AUC scores of the saliency maps of the validation set of MSRA-B using (a) different number of segmentations and (b) different number of trees in the random forest regressor....|
|||...Quantitative comparison of saliency maps produced by different approaches on different data sets....|
|||...Visual comparison of the saliency maps....|
|||...Our method (DRFI) consistently generates better saliency maps....|
|||...One is that we learn to integrate a lot of regional descriptors to compute the saliency scores, rather than heuristically compute saliency maps from different types of features and combine them to get...|
|||...Boosting bottom-up and top-down visual features  for saliency estimation....|
|||...Exploiting local and global patch rarities for saliency detection....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Bottom-up saliency is a dis criminant process....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Object of interest detection by saliency learning....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Leveraging stereopsis  for saliency analysis....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
|||...Image saliency by isocentric curvedness and color....|
|||...Top-down visual saliency via joint In CVPR, pages 22962303,  crf and dictionary learning....|
||60 instances in total. (in cvpr2013)|
|37|Mauthner_Encoding_Based_Saliency_2015_CVPR_paper|... Technology  {mauthner,possegger,waltner,bischof}@icg.tugraz.at  Abstract  We present a novel video saliency detection method to support human activity recognition and weakly supervised training of ac...|
|||...We introduce an encoding approach that allows for efficient computation of saliency by approximating joint feature distributions....|
|||...Introduction  Estimating saliency maps or predicting human gaze in images or videos recently attracted much research interest....|
|||...Thus, saliency estimation is a valuable preprocessing step for a large domain of applications, including activity recognition, object detection and recognition, image compression, and video summarization....|
|||...Furthermore, human saliency maps are sparse and change if content is analyzed per image or embedded within a video [28]....|
|||...hat a sufficient number of individuals have to observe the same image or video to obtain expressive saliency maps, above mentioned human preferences may even be misleading for general salient object d...|
|||...Within our saliency estimation method we enforce the Gestalt principle of figure-ground segregation, i.e....|
|||...Finally, we propose a saliency quality measurement that allows for dynamically weighting and combining the results of different maps, e.g....|
|||...We evaluate the proposed encoding based saliency estimation (EBS) on challenging activity videos and salient object detection tasks, benchmarking against a variety of state-of-the-art video and image ...|
|||...Estimation of local saliency on several scales by foreground and surrounding patches is formulated in Section 3.3....|
|||...L normalized saliency maps i and weighted combination according to reliability of individual saliency maps as discussed in Section 3.5....|
|||...Related Work  Bottom-up vision based saliency has started with fixation prediction [10] and training models to estimate the eye fixation behavior of humans, either based on local patch or pixel inform...|
|||...Grouping image saliency approaches, we see methods working on local contrast [9, 16] or global statistics [1, 5, 14]....|
|||...In contrast to salient object detection, video saliency or finding salient objects in videos is a rather unexplored field....|
|||...Global motion saliency methods are based on analyzing spectral statistics of frequencies [8], the Fourier spectrum within a video [6] or color and motion statistics [31]....|
|||...They adopted the image saliency method by [9] and aggregated color and motion gradients, followed by 3D MRF smoothing....|
|||...Human eye-gaze or annotations as ground truth information for training video saliency methods are another alternative....|
|||...Later, [25] utilized such human gaze data for weakly supervised training of an object detector and saliency predictor....|
|||...[23] learned the transition between saliency maps of consecutive frames by detecting candidate regions created from analyzing motion magnitude, image saliency by [9], and high level cues like face detectors....|
|||...Summarizing the bottom-up video saliency methods we see adaptions from visual saliency methods, that incorporate motion information by rather simple means like magnitude or gradient values....|
|||...A Bayesian Saliency Formulation  Following the Gestalt principle for figure-ground segregation, we are searching for surrounded regions as they are more likely to be perceived as salient areas [20]....|
|||... of encoding vectors is created as described above, we can efficiently compute the local foreground saliency likelihood (x) for each pixel by applying Eq....|
|||...The local foreground maps of sian with kernel width i individual scales are linearly combined to one local foreground saliency map L, which is L normalized....|
|||...The resulting foreground saliency map G is Gaussian filtered and L normalized....|
|||...Processing Motion Information  Studying related approaches for video saliency we found that optical flow information is incorporated in general with less care than appearance information....|
|||...Applying precomputed color wheel lookup tables, we directly generate a three dimensional pseudo-color image taken as input for our motion saliency pipeline....|
|||...Adaptive Saliency Combination  Given the above described steps, we generate up to four foreground maps for local and global estimation of appearance (i.e....|
|||...In contrast, we approximate the uncertainty within our individual saliency maps, by computing weighted covariance matrices of each map....|
|||...A weighted covariance for saliency map j is given as:  j (x,y)( x x)  j (x,y)( x x)( y y)    (cid:80)  x,yI  (cid:80)  x,yI  (cid:80) (cid:80)  j=  j (x,y)  x,yI  j (x,y)( x x)( y y)  j (x,y)  x,yI  (...|
|||...(6)  j  j = 1  det(j) det(u)   = (cid:80)  Then, the final saliency map can be directly obtained by j jj....|
|||...In the following, we denote our encoding based saliency approach EBS for unweighted linear combination of local saliency maps....|
|||...Experiments  In the following, we perform various experiments for both video saliency and object saliency tasks....|
|||...First, we demonstrate the favorable performance of our approach for challenging video saliency tasks using the Weizmann [7] and UCF Sports [22] activity datasets....|
|||...Second, we compare EBS to related saliency approaches and evaluate the influence of parameter settings on the widely used ASD [1] salient object dataset....|
|||...ere solely bounding box  annotations are available, we add spanning bounding boxes to the binarized saliency map before computing the scores (denoted AUC-box, please see supplementary material for mor...|
|||...For given eye-gaze ground-truth data, we measure the exactness of the saliency maps by computing the normalized cross correlation (NCC)....|
|||...Saliency for Activity Localization  Recent evaluation of video saliency methods by [33] on the Weizmann activity dataset [7] has shown the superior performance of solely color-based methods....|
|||...In addition, [18] captured eye-gaze data from 16 subjects, which allows to compare saliency results with these human attention maps given as probability density functions (see Figure 5)....|
|||...This makes the dataset well suited for benchmarking our EBS with other video saliency methods....|
|||...We follow their parametrization and take the top 100 boxes returned by the objectness detector to create a max-normalized saliency map per frame....|
|||...Please note that all saliency methods, others and the proposed EBS, are fully unsupervised and require no training....|
|||...Overall, all compared methods benefit from the box prior when evaluating recall and precision, as it compensates for coarse annotations and supports sparse saliency maps as generated by [27, 33]....|
|||...As can be seen, our EBS methods perform favorably compared to other video saliency methods and on par with previously proposed supervised methods trained and tested on UCF Sports....|
|||...DJS depicts the results for directly modeling the joint distribution of color and motion channels for saliency estimation, as described in Section 3.1....|
|||...But this loss can be captured by our adaptive weighting of individual saliency cues within EBSL and EBSG....|
|||...Altough the focus of our work is on saliency estimation for activity videos, EBS can easily be applied to standard image saliency tasks by switching off the motion components....|
|||...EBSGR uses this over-segmentation and propagates high EBSG saliency values within these segments, leading to less smooth and more object related saliency map results....|
|||...Conclusion  We proposed a novel saliency detection method inspired by Gestalt theory....|
|||...Our robust reliability measurement allows for dynamically merging individual saliency maps, leading to excellent results on challenging video sequences with cluttered background and camera motion, as ...|
|||... Rgbindependent LabEBSG (rgb,60)EBSG (lab,60)EBSG (rgb,30)EBSG (lab,30)Figure 5: Exemplar of video saliency results on UCF sports....|
|||...From row three to bottom: Our proposed method (EBSG), objectness detector [2], color saliency[11], video saliency methods [21] and [33]....|
|||...for saliency reliability and incorporation of additional topdown saliency maps could further augment our approach....|
|||...Learning Video Saliency from Human Gaze Using Candidate Selection....|
|||...Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images....|
|||...Geodesic Saliency Using  Background Priors....|
|||...Hierarchical Saliency De tection....|
|||...MIT Saliency Benchmark....|
|||...Temporal spectral residual:  fast motion saliency detection....|
|||...Dynamic Eye Movement Dataset and Learnt Saliency Models for Visual Action Recognition....|
||59 instances in total. (in cvpr2015)|
|38|Li_A_Weighted_Sparse_2015_CVPR_paper|...A Weighted Sparse Coding Framework for Saliency Detection  Nianyi Li  Bilin Sun  Jingyi Yu  University of Delaware, Newark, DE, USA....|
|||...{nianyi,sunbilin,yu}@eecis.udel.edu  Abstract  There is an emerging interest on using high-dimensional datasets beyond 2D images in saliency detection....|
|||...In this paper, we present a unified saliency detection framework for handling heterogenous types of input data....|
|||...Specifically, we first select a group of potential foreground superpixels to build a primitive saliency dictionary....|
|||...In computer vision, the similar task of visual saliency aims to detect salient regions from 2D, 3D, and most recently 4D imagery data....|
|||...Robust saliency detection algorithms can benefit numerous vision and graphics tasks, ranging from automatic image cropping[26], to image thumbnailing[27], and to image/video compressing[4] and retargeting[25]....|
|||...Existing 2D saliency algorithms, however, are inherently different from how human visual system detects saliency....|
|||...Human eyes have two unique properties that are largely missing in existing 2D saliency solutions....|
|||...Second, human uses two eyes to infer scene depth, e.g., via stereo, for more reliable saliency detection whereas most existing approaches assume that the depth information is largely unknown....|
|||...For example, light field saliency uses the Lytro camera as the acquisition apparatus and then synthesize a focal stack via light field rendering [15]....|
|||...[14] used the Kinect sensor to acquire scene depth and integrate the results with regular 2D saliency via a Gaussian mixture model....|
|||...Despite their effectiveness, saliency detection algorithms based on 2D, 3D and 4D data have adopted completely different frameworks....|
|||...In particular, the features used for distinguishing saliency candidates and more importantly the procedures for utilizing them differ significantly....|
|||...In [20] work, a disparity map is first inferred from a stereo pair and later used to enhance saliency detection....|
|||...In [16], the focal stack is used to infer focusness and objectness of superpixels for more reliably selecting the background candidates and foreground saliency candidates....|
|||...The ultimate goal is to assignment each superpixel r a saliency value Sal(r)....|
|||...A good feature descriptor should exhibit high contrast between saliency objects and background....|
|||...As shown in[2], coupling RGB and Lab color spaces improves the accuracy of saliency maps....|
|||...Processing pipeline of our dictionary-based saliency detection algorithm....|
|||...In this paper, we present a universal saliency detection framework for handling heterogenous types of input data....|
|||...Specifically, we first select a group of potential foreground superpixels to build the saliency dictionary....|
|||...Related Work  The literature of saliency detection is huge and we only discuss the most relevant ones....|
|||...Most contrast-based methods measure saliency by feature (color, texture,gradient, shape, etc.)...|
|||...Notice that the two schemes are complementary to each other and we can apply our saliency detection scheme (Section 4) on each matrix and combine the results....|
|||...Dictionary Based Saliency Detection  From F A and F H, we develop a sparse coding framework: saliency superpixels correspond to the ones that yield to low/high reconstruction error from the saliency/n...|
|||...We use the error measure to refine the foreground superpixels and to identify foreground saliency ones....|
|||...The coefficients should encode the saliency value, if the template D denotes the set of K potential non-saliency/saliency regions respectively:  i = arg mini(cid:107)fi  Di(cid:107)2  2 + (cid:107)i(c...|
|||...Therefore, the weight (penalty) i for saliency detection should be inversely proportional to the similarity between the feature vector fi and template members D. In  Figure 3....|
|||...4(b) shows that, by adding this penalty weight  into the framework, the performance of saliency detection is significantly improved....|
|||...3 to ri respecri and H ri  = (cid:107)F D  ri   DD  ri  (cid:107)2  2  D ri  (5)  Two saliency value SalA(ri) and SalH (ri) are also  computed for ri:  SalD(ri) = Sal(D  ri  )  SalL(ri)  (6)  ri  w...|
|||...Sal(D ) is the saliency function related to the dictionarys type (saliency or non-saliency)....|
|||...Similarly, for saliency dictionary, Sal(D ) will D assign high value to superpixels with low D ri....|
|||...We define the saliency function for non-saliency dictio ri  nary:  Sal(D  ri  ) = D ri  For saliency dictionary:  Sal(D  ri  ) = eD  ri  (7)  (8)  where we set  = 5 in our implementation....|
|||...Dictionary Construction  We define saliency dictionary as a set of superpixels S = {rs1 , rs2, ...rsk} which are regarded as the potential saliency regions and will be refined through our framework....|
|||...To get the initial saliency dictionary, we use a non-saliency dictionary to reconstruct the reference image, and patches with high reconstruction error are selected saliency dictionary....|
|||...After we obtain the non-saliency dictionary, we use the weighted sparse framework described in Section 4.1 to compute a saliency map....|
|||... as:  rj  g(ri, Dj) = e  (cid:107)F D ri  Dj(cid:107)  + Con  ri  (11)  We choose superpixels whose saliency values are higher than the mean to construct the initial saliency dictionary S0....|
|||...At each iteration, we will refine the saliency dictionary using the estimated saliency map....|
|||...The algorithm terminates when there is no change to the saliency dictionary....|
|||...4 and the saliency function is computed as Eqn....|
|||...We then compute two saliency maps as Eqn....|
|||...Next, we apply a center cue on two maps to make saliency regions more compact....|
|||...Finally, we sum the two saliency maps with respect to A and H. A new saliency dictionary Sk+1 is generated with by using superpixels whose saliency values are higher than the mean....|
|||...Notice that we combine two saliency maps to generate the final saliency map, which will cause the ignorable noises on background becoming significant....|
|||...The SOD database is considered as the most challenge database in saliency detection since the contrast between foreground and background is generally rather small....|
|||...The PSU Stereo Saliency Benchmark (SSB) contain 1000 pairs of stereoscopic images and corresponding salient object masks for the left images....|
|||...Visual Comparisons of different saliency detection algorithms vs. ours on 2D (first two rows: MSRA-1000; last two rows: SOD), 3D and 4D datasets....|
|||...f texture/color scene compositions. We have then built a dictionary based framework that constructs saliency and non-saliency dictionaries from the stacked feature vectors....|
|||...For example, the saliency results can be directly used as inputs to existing tracking or streo matching algorithms, to improve their performance in cluttered scenes....|
|||...We follow the canonical precision-recall curve(PRC) and F-measure methodologies to evaluate the accuracy of the detected saliency on databases of different dimension....|
|||...This is due to the fact that the difference between saliency and non-saliency values assigned by our algorithm is much greater than others....|
|||...In another word, the saliency maps computed by our algorithms is of the best similarity to ground truth, as shown in Fig....|
|||...Our approach can handle highly challenging cases such as the blue bird scene in LFSD and the fish scene in SOD where the deemed saliency regions have a similar color/texture to the non-saliency regions....|
|||...This indicates that our algorithm is capable of locating most saliency regions with a high confidence....|
|||...5 shows that our technique also produces more visually pleasing results, e.g., it generates more complete contours and more accurate saliency maps....|
|||...Conclusions  We have presented a novel saliency detection algorithm that is applicable to 2D image data, 3D stereo/depth data, and 4D light field data without modifying the processing pipeline....|
|||...Exploiting local and global patch rarities for saliency detection....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||59 instances in total. (in cvpr2015)|
|39|Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper|...      DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection                                        Nian Liu   Junwei Han*   School of Automation, Northwestern Polytechnical Universit...|
|||...rst makes  a  coarse  global  prediction  by  automatically  learning  various  global  structured  saliency  cues,  including  global  contrast,  objectness,  compactness,  and  their  optimal  combi...|
|||...ork  (HRCNN)  is  adopted  to  further hierarchically  and  progressively  refine  the  details  of saliency maps step by step via integrating local context  information....|
|||...When  testing,  saliency  maps  can  be  generated by directly and efficiently feedforwarding testing  images through the network, without relying on any other  techniques....|
|||...Traditional  saliency  detection  methods  rely  on  various  saliency  cues....|
|||...Some recent works also utilize various prior knowledge  as  informative  saliency  cues....|
|||...their   joint   incorporate   complimentary   structures,  many   these  works  usually  resort   Various saliency cues are also combined in some works  interactions....|
|||..., 16-19] and object proposals used  in  [14])  either  as  the basic computational units to predict saliency  or  as  the  post-processing  methods  to  smooth  saliency  maps....|
|||...Although  these  methods  can  further  improve  saliency  detection  results,  they  are  usually  very  time-consuming,  becoming the bottleneck of the computational efficiency of  a salient object ...|
|||...uild  real  meaningful  feature  representations,  how  to  simultaneously explore  all  potential  saliency  cues,  how  to  find the optimal integration strategy, and how to efficiently  preserve ob...|
|||...To solve these problems, we propose a novel end-to-end  deep  hierarchical  saliency  detection  framework,  i.e.,  the  DHSNet,  via  convolutional  neural  networks  (CNN)  [20]....|
|||...In details, we first adopt a  CNN over the global view (GV-CNN) to generate a coarse  GSm ) to roughly detect and localize  global saliency map ( salient objects....|
|||...sion of the global structured  loss,  learn  feature  representations and various global structured saliency cues,  such as global contrast, objectness, compactness, and their  optimal  combination....|
|||...opose  to  adopt  a  novel  hierarchical  recurrent  convolutional neural network (HRCNN) to refine saliency  maps  in  details  by  incorporating  local  contexts....|
|||...RCLs  into  each  convolutional  layer,  thus  enhancing  the  capability  of  the  model  to  integrate  context  information,  which  is  very  important  for  saliency  detection  models....|
|||...In  HRCNN,  we  refine the saliency map in several steps hierarchically and  successively....|
|||...In  each  step,  we  adopt  a  RCL  to  generate  a  finer  saliency  map  by  integrating  the  upsampled  coarse  saliency map predicted at the last step and the finer feature  maps from the GV-CNN....|
|||...The  contributions  of  this  paper  can  be  summarized  as   follows:    (1)  We  propose  a  novel  end-to-end  saliency  detection  model, i.e., the DHSNet, to detect salient objects....|
|||...DHSNet  can simultaneously learn powerful feature representations,  informative  saliency  cues  (for  instance,  global  contrast,  objectness,  their  optimal  combination  mechanisms  from  the  gl...|
|||...chical refinement model,  i.e.,  the  HRCNN,  which  can  hierarchically  and  progressively refine saliency maps to recover image details  by  integrating  local  context  information  without  using...|
|||...The  proposed  HRCNN  can  significantly  and  efficiently  improve  saliency  detection  performance....|
|||...The name of each step-wise saliency map is also shown....|
|||... for  Saliency   Detection   Some  researchers  have  already  applied  deep  neural  networks  to  saliency  detection,  which  includes  two  branches,  i.e.,  eye  fixation  prediction  [32,  33]  ...|
|||...[35]  used  a   CNN to predict saliency score for each pixel in local context  first,  then  they  refined  the  saliency  score  for  each  object  proposal over the global view....|
|||...[36] predicted the saliency  score for each superpixel by incorporating local context and  global  context  simultaneously  in  a  multi-context  CNN....|
|||...The  GV-CNN  first  coarsely  detects  salient  objects  in  a  global  perspective,  then  the  HRCNN hierarchically and progressively refines the details  of  the  saliency  map  step  by  step....|
|||...Finally this  layer is reshaped to size 2828 as the coarse global saliency  GSm ....|
|||..., i.e., the  map  GSm and  averaged  pixel-wise  cross  entropy  loss  between  the  ground  truth  saliency  mask,  the  fully  connected  layer  learns  to  detect  and  localize  salient  objects  ...|
|||...Thus the GV-CNN can generate a relatively large saliency  map (2828) even though the size of layer Conv5_3 is small  (1414)....|
|||...The  experiments  in  Section  4.5  show  the  effectiveness of the GV-CNN and its learned saliency cues....|
|||...HRCNN   for  Hierarchical  Saliency  Map   Refinement   To further improve   GSm  in details, we propose a novel  architecture,  i.e.,  the  HRCNN,  to  hierarchically  and  progressively render image details....|
|||...Thus  RCLs  can  help  to    incorporate  local contexts efficiently in HRCNN to refine saliency maps....|
|||...Hierarchical  Saliency  Map  Refinement....|
|||... with layer Conv4_3 of the  Figure 2, we first combine  VGG net and adopt a RCL to generate a finer saliency map  (as this saliency map is obtained by adopting a RCL over the  Sm  and the  local featu...|
|||...the   upsampled   Sm  RCL  2   with   layer  Conv1_2   to  generate   1RCL  Sm , which is the final saliency map....|
|||...In  Figure  3,  we  show  the  detailed  framework  of  a  refinement step, i.e., combining a coarse saliency map with  a convolutional layer from the VGG net to generate a finer  saliency  map....|
|||...Second,  by  using sigmoid activation function, we squash  the  range  of  the activation values of the neurons to be [0,1], which is as  the same as the combined saliency map....|
|||...Without doing this,  the combined saliency map will be overwhelmed since the  activation values in each layer of the VGG net are usually  very large with ReLU activation functions....|
|||...Next, the squashed VGG layer is concatenated with the  GSm  is  upsampled  coarse  saliency  map  (except  that  directly  concatenated  with  layer  Conv4_3  without  upsampling), resulting in 65 fea...|
|||...Then we adopt a  RCL  to  combine  the  coarse  saliency  map  and  the  local  features in the VGG layer....|
|||...At last, the refined saliency map  can be generated by adopting a convolutional layer with 11  kernels and sigmoid activation function....|
|||...1RCL  GSm  to   Table 1 shows the size of  each  step-wise  saliency  map  and  the  sizes  of  the  receptive  fields  from  which  they  are  Sm ,  the  sizes  of  step-wise  induced....|
|||...Thus HRCNN refines the saliency maps in a  coarse  to  fine  and  global  to  local  manner....|
|||...  1RCL Sm      28  156   56  72      112  30   224  13   Table  1:  The  sizes  of  the  step-wise  saliency  maps  and  the  receptive fields (RF) from which they are induced....|
|||...To be specific, as shown in Figure 2, we resize  the ground truth saliency mask to sizes ranging from 224 to  28  to  supervise  the  corresponding  learning  of  each  step-wise saliency map....|
|||...Specifically,  saliency  maps  were  binarized  using  different  thresholds  varying  from  0  to  1....|
|||...(4)   where   2  is set to 0.3 and the adaptive threshold is set to  twice  the  mean  saliency  value  of  each  saliency  map  as  suggested in [8]....|
|||...We  show  the  F-measure  scores  and  the  corresponding precision and recall of the step-wise saliency  maps in Table 3....|
|||...In figure 7, we show  GSm as intuitive examples of the  some saliency maps from  GSm  in  (a)  just  saliency  cues  learned  in  GV-CNN....|
|||...olutional  layer  with 1 33 sized kernel and sigmoid activation function to  generate  the  coarse  saliency  map  with  size  2828  at  last....|
|||... than  those  of   approximates that of   (FCN  1RCL  685  Figure 6: Visualization of the step-wise saliency maps....|
|||...Figure  7:  Intuitive  examples  of  the  saliency  cues  learned  in  GV-CNN....|
|||...Geodesic  saliency   using background priors....|
|||...Fusing  generic  objectness  and  visual  saliency  for  salient  object  detection....|
|||...Hierarchical  saliency   detection....|
|||...Deep  Gaze  I:  Boosting saliency  prediction  with  feature  maps  trained  on  ImageNet....|
|||...Visual Saliency Based on Multiscale Deep   Features....|
|||...Deep Networks  for  Saliency  Detection  via  Local  Estimation  and  Global  Search....|
||58 instances in total. (in cvpr2016)|
|40|Lu_Learning_Optimal_Seeds_2014_CVPR_paper|...VCL Lab, UCSD  sol050@ucsd.edu  vmahadev@yahoo-inc.com  nuno@ucsd.edu  Abstract  In diffusion-based saliency detection, an image is partitioned into superpixels and mapped to a graph, with superpixels...|
|||...Saliency information is then propagated over the graph using a diffusion process, whose equilibrium state yields the object saliency map....|
|||...The optimal solution is the product of a propagation matrix and a saliency seed vector that contains a prior saliency assessment....|
|||...This is obtained from either a bottom-up saliency detector or some heuristics....|
|||...Two types of features are computed per superpixel: the bottom-up saliency of the superpixel region and a set of mid-level vision features informative of how likely the superpixel is to belong to an object....|
|||...The combination of features that best discriminates between object and background saliency is then learned, using a large-margin formulation of the discriminant saliency principle....|
|||...The propagation of the resulting saliency seeds, using a diffusion process, is finally shown to outperform the state of the art on a number of salient object detection datasets....|
|||...Early efforts aimed to predict the locations of human eye fixations and introduced the fundamental principles of saliency detection....|
|||...[24] equated saliency to a local center-surround operation and proposed a biologically inspired saliency network....|
|||...[9] proposed a global definition, which equates saliency to uniqueness of the visual stimulus, by setting the saliency of a location to the inverse probability of its responses under the response dist...|
|||...[20] formulated saliency as the stationary distribu tion of a random walk defined over a graph based representation of the image....|
|||...These use high-level information to guide the saliency computation, equating saliency to the detection of stimuli from certain object classes [14]....|
|||...A compromise is to either consider the responses of object detectors as features for bottom-up saliency [27], or formulate saliency as a mid-level vision task....|
|||...In this area, substantial attention has been devoted to the problem of object saliency [8]....|
|||...Second, to overcome the propensity of bottom-up saliency to respond more to edges than (homogeneous) object interiors, they include some form of spatial propagation of saliency information....|
|||...This method has been used for object saliency detection in [38]....|
|||...In all cases, the vector s is a set of saliency seeds, which are propagated throughout the graph, according to the node similarities defined by the affinity matrix W. High affinities wij 1) encourage ...|
|||...Nodes y i with paths of high pairwise similarity to  salient seed nodes in s cases, the optimal solution has the form  j receive high saliency values....|
|||...However, for saliency, the seeds si represent a preliminary1 assessment of superpixel saliency and can play a significant role in the optimal solution....|
|||...In the saliency context, they are best thought of as the outcome of a pre-attentive, purely stimulus driven, and mostly local perceptual process, which is extended into a spatially coherent saliency p...|
|||...In the object saliency literature, saliency seeds have been mostly equated to bottom-up saliency....|
|||...es of pixel contrast [30], various implementations [30, 11, 25] of the discriminant center-surround saliency principle of [16], or variations [19] on the graphbased saliency model of [20]....|
|||...While some more sophisticated objectness measurement, such as the one proposed by [3], have been considered for object saliency [10], the impact of such mid-level cues is still poorly understood....|
|||...Optimal seeds for object saliency  In this section, we propose an algorithm for learning saliency seeds, by combining pre-attentive saliency maps and mid-level vision cues for object perception....|
|||...Feature based saliency diffusion  An image x is first segmented into N superpixels {xi}, i = (1 ....|
|||...Note that, with these seeds, the saliency map of (9) can be written as  y(x) = A(x)F(x)w  wiA(x)fi(x)  = Xi  (13)  (14)  where fi(x) is the ith column of F(x) and contains the saliency information der...|
|||...Hence, the diffusion process can be interpreted as a linear combination of feature saliency diffusions yi(x) = A(x)fi(x), whose weights wi encode the importance of the different features for saliency ...|
|||..., (x(n), m(n))} of images x(i) and object saliency maps m(i)....|
|||...Optimality is defined in the discriminant saliency sense of [16], i.e....|
|||... k w k2  (15)  + Xk X{ij k  i =1,k  j =0}  max(0, 1  (yi(x(k))  yj(x(k)))  where y(x) is the object saliency map of (14) and k i = mi(x(k)) an indicator of the saliency of the ith superpixel of the kt...|
|||...Note that the optimization maximizes the object-background discrimination of the saliency maps at equilibrium, i.e....|
|||...after the diffusion process of (9), by learning to rank the saliency of all pairs of object-background superpixels within each image....|
|||...The feature transformation F(x) includes both a pre-attentive bottom-up saliency map and mid-level features for object perception....|
|||...Pre(cid:173)attentive saliency map  The bottom-up saliency map is based on a sparse image decomposition....|
|||...Given the affinity matrix W, saliency propagation is implemented with (3)....|
|||...Seed representation  The emphasis of this work is on the seed vector s. Since this contains a saliency value per graph node and nodes are image dependent, it is impossible to learn a universal s. It i...|
|||...  distribution of fitted GGD model distribution of real data     patch location l. Finally, 1) the saliency maps derived from the B feature channels are combined with  distribution of fitted GGD mode...|
|||... prior, is given by [16]  , ..., (n)  i  sf (l) =  B  X1  ais(i(l))  (21)  and 2) the pre-attentive saliency score of superpixel xi set to the mean saliency score of its pixels....|
|||...The following were adopted to simulate mid-level saliency cues....|
|||...Following [9], the saliency of image location l is  s(i(l); i, i) =  log p(i(l); i, i)....|
|||...Experiments  Several experiments were performed to evaluate the proposed saliency detector....|
|||...Although eye fixation prediction is not the primary objective of this work, we first study the effectiveness the bottom-up saliency map of Section 3.4 in predicting eye fixations....|
|||...This is followed by an evaluation of the object saliency detector....|
|||...The proposed saliency detector (denoted sparse-GGD) is compared to the image signature method of [21], the sparse method of [23], the spectral method of [22], and the SUN method [40]....|
|||...The proposed saliency model consistently outperforms all other models on the four datasets....|
|||...Salient object detection  Protocol: All models are evaluated by comparing the predicted object saliency map to the binary ground truth, using two metrics: AUC and AP (average precision) scores....|
|||...The proposed saliency detector is compared to eight of the best performing methods in the literature Gof [17], CB [25], HC [11], RC [11], GBMR [38], PCA [32], FT [1], and SF [34]....|
|||...SalseedProp has no learning, using bottomup saliency alone to determine seeds and (11) for propagation....|
|||...The saliency of salient seed locations is propagated through the graph via a diffusion process....|
|||...Unlike previous heuristic approaches to seed selection, an optimal set of salient seeds is learned using a large margin formulation of the discriminant saliency principle....|
|||...Exploiting local and global patch rarities for saliency detection....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Bottom-up saliency is a dis benchmark....|
|||...Decision-theoretic saliency: Computational principles, biological plausibility, and im Table 3: Object saliency detection performance: AUC/AP  AUC/AP  MSRA5000  SOD  SED1  SED2  VOC2008 1023  CB FT G...|
|||...Random walks on graphs to model saliency in images....|
|||...Improved saliency detection based on superpixel clustering and saliency propagation....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||58 instances in total. (in cvpr2014)|
|41|Xu_Learning_to_Predict_ICCV_2015_paper|...Zulin Wang  MaiXu@buaa.edu.cn  Abstract  This paper proposes a novel method, which learns to detect saliency of face images....|
|||...xture model (GMM) distribution from the fixations of eye tracking data as the top-down features for saliency detection of face images....|
|||...ly combined with the conventional bottom-up features (i.e., color, intensity, and orientation), for saliency detection....|
|||...Finally, experimental results show that our learning-based method is able to advance state-of-the-art saliency prediction for face images....|
|||...As the output of saliency detection, the saliency map of an image or a video frame has been widely applied in object detection [3], object recognition [8], image retargeting [20], image quality assess...|
|||...The existing methods on saliency detection can be classified into two categories: bottom-up and top-down methods....|
|||...The representative bottom-up method on detecting image saliency is Ittis model [13], which combines centersurround features of color, intensity, and orientation together....|
|||...Most recently, there has been extensive advanced work (e.g., [5, 7, 10, 11, 26]) on bottom-up saliency detection....|
|||...In fact, top-down visual features play a crucial role in determining the saliency of a scene....|
|||...-of-the-art methods [12, 15, 27] have been proposed to apply machine learning algorithms in topdown saliency detection of Cerfs work [4]....|
|||...[14] has extended Cerfs work [4] to saliency detection in a scene with multiple faces, i.e., saliency detection in a crowd....|
|||...In their work, multiple kernel learning (MKL) is applied to learn a more robust discrimination between salient and non-salient regions in multi-face scenes, for detecting saliency in a crowd....|
|||...Although the existing work has taken into account one or more faces on saliency detection, it does not explore the distribution of eye fixations within faces....|
|||...Examples for saliency prediction vs fixations in face region, selected from [15]....|
|||...Note that both saliency and fixations belonging to face regions are displayed....|
|||...ure 1, a simple isotropic Gaussian model (GM) assumption for saliency distribution in face [4, 27] has the limitation on modeling visual attention attracted by faces....|
|||...As can be seen in this figure, for images with small faces, non-isotropic GM is more accurate in modeling saliency distribution inside face....|
|||...Accordingly, saliency distribution, in the form of Gaussian mixture model (GMM), need to be learnt from eye fixations on face images....|
|||...Figure 1-(d) shows that the saliency with the learnt GMM distribution is more consistent with the ground truth visual attention....|
|||...This paper thereby proposes a learning-based saliency detection method, which learns various GMMs and the corresponding weights across different face sizes1, for predicting visual attention on free-vi...|
|||...Such results motivate our learning-based method on saliency detection of face images....|
|||...Specifically, we utilize Expectation Maximization (EM) algorithm [18] to learn GMM distribution of saliency in face region from the ground truth fixations....|
|||...Note that two among 24 subjects were experts, who worked on the research field of saliency detection....|
|||...Next, we discuss the statistical analysis on the eye fixations falling into different regions of face, to investigate the visual saliency of facial features, i.e., left eye, right eye, nose, and mouth....|
|||...Therefore, both face and facial features need to be taken into consideration for saliency detection, and the weights corresponding to these two channels should be relevant to face sizes....|
|||...The proposed method  This section mainly works on the proposed method for modeling saliency on face and facial features....|
|||...Then, we present in Section 3.3 the saliency detection method based on the learnt GMM....|
|||...Therefore, we can use the GMM to model the facial feature channel, which has large-valued saliency within facial features....|
|||...However, for saliency detection the mean values k in (3) and (4) are replaced by the central points of  3910  (a) Right eye  (b) Left eye  (c) Nose  (d) Mouth  Figure 5....|
|||...Procedure of our learning-based saliency detection method....|
|||...Figure 7 shows an example of overall procedure on our learning-based saliency detection method....|
|||...Learning optimal weights  Now, the remaining task for saliency detection with (5) is to determine weights w = [wC , wI , wO, wF ,wG]T for each conspicuity map....|
|||...Finally, the saliency map of a face image can be worked out via (5) with the learnt optimal weights....|
|||...Experimental results  In this section, experimental results are presented to evaluate the saliency detection performance of our method....|
|||...In the experiments, the area under ROC curve (AUC), normalized scanpath saliency (NSS) [19], and linear correlation coefficient (CC) [1] on all test images, were compared for evaluating the accuracy o...|
|||...In addition, the saliency maps of several test images are also provided for the comparison....|
|||...In our experiments, we used the method of Section 3.2 to learn the GMMs for both face and facial feature channels of saliency detection, from the ground truth fixations of all 360 training images....|
|||...Accordingly, four-component GMM is utilized in our saliency detection method....|
|||...Finally, the saliency maps of all test images can be worked out by (5), with the aforementioned GMMs and optimal weights....|
|||...In order to quantify the accuracy of saliency detection, we tabulate in Table 2 the AUC results of our and other 8 methods....|
|||...This is because face, as a high-level feature, is crucial for improving saliency detection accuracy....|
|||...[27] is that (1) the GMM distribution of saliency of face region is learnt from training data and then incorporated in our method, and that (2) the weights of top-down channels are learnt regarding face size....|
|||...Moreover, we show in Figure 10 the ROC curves of saliency detection by our and other 8 state-of-the-art methods....|
|||...NSS is computed to imply the relevance between fixation locations and saliency predictions, and CC measures the strength of a linear relationship between human fixation map and saliency map....|
|||...The averaged NSS and CC results (with their standard devi ations) of saliency detection by our and other state-of-theart methods are also tabulated in Table 2....|
|||...For saliency detection with CB modeling, similar NSS can CC improvement can be found in our method....|
|||...Figure 11 shows the saliency maps of 10 randomly selected test images, detected by eye tracking data, our, and other 8 methods....|
|||...From this figure, we can see that compared to all other methods, our method is able to well locate the saliency regions, much closer to the maps of human fixations....|
|||...To be more specific, for images with small face (i.e., from first to fourth rows), the saliency maps by our method are much more similar to those of human fixations than other methods, as the learnt n...|
|||...Also, note that the saliency maps here are without any CB....|
|||...Conclusions  For saliency detection on face images, we have proposed in this paper a learning-based method to take into account the top-down channels of face and facial features....|
|||...To facilitate saliency analysis of face images, we first established an eye tracking database including 510 face images....|
|||...Working on our database, GMMs were learnt from the training fixations to model the top-down saliency distributions within face....|
|||...Finally, experimental results validated that our method significantly advanced saliency detection on face images, as our method drastically outperformed other 8 state-of-the-art methods, in terms of A...|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Learning a saliency map using fixated locations in natural scenes....|
|||...A probabilistic saliency model with memory-guided topdown cues for free-viewing....|
||58 instances in total. (in iccv2015)|
|42|Shi_PISA_Pixelwise_Image_2013_CVPR_paper|...d graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical....|
|||...More often, such fine-grained saliency detection is also desired to have a fast runtime....|
|||...ic and fast computational framework called PISA  Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically....|
|||...homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive o...|
|||... image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps....|
|||...put image, the general objective is to automatically detect salient objects and assign consistently high saliency values to them, while the background part should take on zero values ideally....|
|||...Driven by these recent applications, saliency detection has also evolved to aim at assigning pixel-accurate saliency values to uniformly highlight foreground objects, going far beyond its early goal o...|
|||...More often, such finegrained saliency detection is also desired to have a fast runtime....|
|||...Without any user intervention, inferring (pixel-accurate) saliency assignment for diversified natural images is a highly ill-posed problem, because of the lack of a rigorous definition of saliency itself....|
|||...Focusing on bottom-up, low-level saliency computational models in this paper, we can classify most of the previous methods into two basic classes depending on the way the saliency cues are defined: co...|
|||...1(e) and ambiguous saliency detection for images with rich structures in foreground or/and background e.g....|
|||...(b/c) Raw saliency detection result using the color/structure contrast measure alone in the proposed PISA framework....|
|||...Inspired by the insights and lessons from the significant amount of previous work, we target studying this challenging saliency detection problem in a more holistic manner....|
|||...Though the color contrast is a popular saliency cue used dominantly in many methods [5, 19, 14], other influential factors do exist, which make certain pixels or regions outstanding....|
|||...ptive fields are associated with different kinds of visual stimuli, so local analysis regions where saliency cues are extracted should be adapted to match specific image attributes....|
|||...Previous works have used the spatial variance to further  modulate saliency values computed from a single visual attribute (e.g....|
|||...This spatial prior can also be generalized to consider the spatial distribution of different saliency cues, including also other useful location priors such as the center prior [15]....|
|||...Another observation is that pixel-accurate saliency maps are often spatially coherent with the discontinuities well aligned to image edges....|
|||...ic and fast computational framework called PISA  Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically....|
|||...ead of using homogeneous superpixel-based and color contrast-only treatment, PISA directly performs saliency modeling for each individual pixel on two complementary measures (color and structure contr...|
|||...(iii) Without requiring reliable region segmentation and then post-relaxation for pixelwise saliency assignment, PISA exploits an efficient edge-aware image representation and filtering technique [16]...|
|||...It first performs saliency computation for a feature-driven, subsampled image grid, and then uses an adaptive upsampling scheme with the color image as the guidance signal to recover a fullresolution ...|
|||...Compared to segmentation-based saliency methods [19], our F-PISA method reduces the computational complexity similarly by considering a coarse image grid, while having the advantage of utilizing image...|
|||... propose PISA in this paper as a computational framework for effective and efficient pixel-accurate saliency detection, aggregating complementary saliency cues based on color and structure contrasts w...|
|||...These two measures complement each other in detecting saliency cues from different perspectives, and are combined together to give the initial saliency value....|
|||...More formally, given an image I, we compute the initial saliency value  S(p) for each pixel p by aggregating the two contrast measures {U c(p), U g(p)} with spatial priors {Dc(p), Dg(p)}, giving a gen...|
|||...It is also often useful to integrate the center prior in this saliency reweighting process....|
|||...By fusing the two complementary saliency cues in such a pixelwise adaptive manner, the saliency detection effectiveness is significantly boosted....|
|||...Though the initial saliency estimation map  S is already good for some applications, it is not pixel-accurate and still exhibits many spurious noises or unsmooth saliency values even within a small ne...|
|||...ing [16] to smooth out  S to generate a filtered output S, which is spatially coherent and with the saliency discontinuities aligned to the guidance color image edges (Sect....|
|||...In fact, the aforementioned four terms and their aggregation as in (1) present only one specific implementation of our PISA framework, other kinds of saliency cues or priors can be integrated as well....|
|||...F-PISA generates saliency maps at the detection accuracy close to that achieved by PISA, but it brings an over 18-times speedup over PISA....|
|||...They assume that if the spatial correlation is not accounted for, pixels with the similar appearance should be assigned the same saliency values....|
|||...Moreover, taking segments as the atomic units for saliency evaluation does not lend itself to easy integration of other appearance contrast measures....|
|||...Even in the event that the color uniqueness measurement gives a good saliency value to foreground objects, other complementary contrast measures can still be helpful in reinforcing the saliency assignment e.g....|
|||...As will be shown later, we find that our OM structure descriptor, though simple, is more effective and reliable than other gradient features e.g., Gabor [17] and LBP [9] in the image saliency detection task....|
|||...Second, we adopt a linearly-varying smoothing scheme [5] to refine the quantization-based saliency measurement....|
|||...The saliency value of each cluster is replaced by the weighted average of the saliency values of visually similar clusters....|
|||...Such a refinement smooths the saliency assignment to each pixel....|
|||...3 illustrates these dense maps visually and their respective effects to saliency assignment....|
|||...Saliency Coherence  Based on (1), an initial saliency estimation map  S is generated....|
|||...Though good for certain applications, this initial saliency map does not consider the spatial coherence in its evaluation, resulting in spurious noises and non-uniform saliency assignment even for pix...|
|||... technique [16] here to smooth out  S and produce a spatially coherent yet discontinuity-preserving saliency map S. In fact, the same cross-based data structure already computed when evaluating U c(p)...|
|||...Instead of processing the full image grid, we perform a gradient-driven subsampling of the input image I, so the saliency computation in (1) is only applied to this set of selected pixels....|
|||...3 rectangular patch on the regular image grid to form a sparse image I l. The two proposed contrast saliency measures are then computed for I l, giving a sparse saliency map  S l. To obtain a full-siz...|
|||...Thus given a pixel p  I, its saliency value is obtained as,  i=1  (7) where qi  I l and its cross support region qi contains p, namely p  qi....|
|||...Evaluation on Benchmarks  = exp( (cid:3)xp,xqi  (cid:3)    We evaluate the proposed algorithm for saliency detection on three public datasets which have been used as standard benchmarks in [3]....|
|||...Extensive study for different saliency measures (CC, SC) in our method....|
|||...ll datasets with several state-of-the-art works: Spatial-temporal Cues (LC [25]), Spectral Residual saliency (SR [10]), Frequency-Tuned saliency (FT [1]), Context-Aware saliency (CA [7]), Histogram-ba...|
|||...We can observe that the aggregated saliency detection achieves superior performance, as CC and SC capture saliency from different aspects, verified by the visual results in Fig....|
|||...They serve as good evidences to advocate our choice in fusing complementary saliency cues....|
|||...Does luminance-contrast contribute to a saliency map for overt visual attention?...|
|||...Spatial-temporal saliency detection using phase spectrum of quaternion fourier transform....|
|||...Image saliency by isocen tric curvedness and color....|
|||...Geodesic saliency using  background priors....|
|||...For any background regions that have been assigned high saliency values from either of the contrast cues after the modulation of the spatial priors, they remain salient in the final saliency map (see Fig....|
|||...nowledge, which could be beneficial to handle more challenging cases and investigate other kinds of saliency cues or priors to be embedded into the PISA framework....|
||57 instances in total. (in cvpr2013)|
|43|Zhao_Saliency_Detection_by_2015_CVPR_paper|...cience and Technology of China  {rzhao, wlouyang, hsli, xgwang}@ee.cuhk.edu.hk  Abstract  Low-level saliency cues or priors do not produce good enough saliency detection results especially when the sa...|
|||...We employ deep Convolutional Neural Networks to model saliency of objects in images....|
|||...Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated....|
|||...A large number of approaches [63, 52, 40, 39, 32, 35, 60, 57, 56, 47, 41, 31, 27, 25, 24, 23, 11, 44, 17, 8, 13, 1, 21] are proposed to capture different saliency cues....|
|||...Many conventional saliency detection methods focus on design of low-level saliency cues, or modeling background  Figure 1....|
|||...From top left to bottom right: image, groundtruth mask, our saliency maps, and saliency maps of other five latest approaches, including DRFI [25], HS [56], GBMR [57], PCAS [41], and SF[44]....|
|||...1) Computational saliency models need effective feature representations to estimate saliency, but sometimes the contrast between hand-crafted low-level features cannot help salient objects stand out f...|
|||... cannot be classified as salient objects from the low-contrast background either based on low-level saliency cues or background priors, but they are semantically salient in high-level cognition, i.e....|
|||...Therefore, saliency detection is considered as a high-level task in our work....|
|||...From another perspective, saliency detection is a task to simulate the mechanism of human attention, which is a neurocognitive reaction controlled by human brains....|
|||...As indicated in [49], pre-training can provide  1  GTHSPCASSFImageOursDRFIGBMRlenge are tested, and their effectiveness in saliency detection are investigated....|
|||...[22, 17, 36]) design saliency cues by considering the contrast between each image element (pixel, region, or patch) and its locally surrounding neighborhood....|
|||...Global approaches estimate saliency scores by calculating the holistic statistics on uniqueness of each image element over the whole image....|
|||...In addition, some other interesting priors were also proposed to assist saliency detection, such as flash cues [18], boundary and background priors [55, 57, 25, 63]....|
|||...tes preliminarily likely candidates for object detection regardless of their contrastive relations, saliency detection requires to take the context of full images and the contrast between objects into...|
|||...From left to right: image, groundtruth saliency mask, our saliency map predicted with local context, and our saliency map predicted with global context....|
|||...Based on the above motivations, a new multi-context deep learning framework for saliency detection is proposed....|
|||...The global context is utilized to model saliency in full image, while the local context is used for saliency prediction in meticulous areas....|
|||...The global and local context are integrated into the multi-context deep learning framework for saliency detection, and the globaland local-context modeling are jointly optimized....|
|||...Contrarily, saliency detection aims to produce accurate segmentation over the salient ones....|
|||...Despite the difference, objectness score can be used as high-level prior knowledge [23], which could be further combined with with low-level saliency cues for saliency detection....|
|||...Hierarchical Structure for Saliency Detection Latest works on saliency detection have showed the trend of using deep/hierachical architectures to model visual saliency....|
|||...[34] proposed to unsupervisedly learn a set of mid-level filters to capture local contrast, and to fuse multi-level saliency calculation by convolution....|
|||...Our Approach  In this paper, we propose a multi-context deep learning framework for saliency detection, and focus on modeling saliency with global context and local context simultaneously....|
|||...Global-context Modeling by Deep CNN  As shown in Figure 3, the upper branch (global-context modeling) of our saliency detection pipeline is a deep CNN architecture with global and coarse context....|
|||...y deep models into our framework, and in the experimental section we investigate the performance of saliency detection using some of these contemporary architectures....|
|||...Upper branch: Deep CNN-based global-context modeling for saliency detection with a superpixel-centered window padded with mean pixel value....|
|||...(3) (4)  Specifically,  tries to estimate the saliency probability based on global-context modeling, j )  ewT  (xgc;   gc,jxgc,  (5)  and  is based on both the global context and local context,  (xgc,...|
|||...(6)  Then, the corresponding unnormalized saliency prediction score function is formulated as  gc,1xgc + fu(wT  gc,1xgc + )wT  lc,1xlc, (7)  1 ) = wT  f (xgc, xlc;  where fu() is defined as (cid:26) t...|
|||...Overall, prediction of a superpixel-centered input window is performed by estimating the saliency probability  score(xgc, xlc) = P (y = 1   xgc, xlc; 1),  (1) where xgc and xlc are output of the penul...|
|||...y is the prediction of saliency for the centered superpixel, where y = 1 for salient superpixel and y = 0 for background....|
|||...We train a binary classifier on top of the last network layer to classify background and saliency by minimizing a unified softmax loss between the classification result and the groundtruth label....|
|||...j = { wgc,j, wlc,j, , },  for for  0  t  1 otherwise  (8)  gc,1xgc+) models ambiguity of the saliency predicfu(wT tion of the global context model, and  and  can integrate multiple contexts in modelin...|
|||...Similar strategy in [16] can be directly used to finetune the contemporary CNN models for saliency detection....|
|||...Dataset in ImageNet for Image classification has 1, 000 classes, while saliency detection solves a binary classification problem....|
|||...The loss function in image classification task aims to differentiate 1, 000 classes, while the loss function for saliency detection in our approach is defined as in Eq....|
|||...Therefore, we explore several pre-training  strategies, and propose task-specific initialization for finetuning our deep saliency detection models....|
|||...Based on the object-level annotations, we can easily generate another type of annotations to suit the input format in saliency detection, which we call the superpixellevel annotation....|
|||...n Section 4.3, we quantitatively investigate the influences of different pre-training strategies in saliency detection, and validate our hypothesis that task-specific pre training strategy provides a...|
|||...We use the MSRA10k dataset [10] for finetuning our deep saliency detection models....|
|||...It contains 850 natural images with both saliency segmentation groundtruth and eye fixation groundtruth....|
|||...We follow the evaluation protocol as in [1, 13, 32], where saliency maps are binarized at every threshold within range [0, 255], and all saliency maps are evaluated by the F-measure score [1], which i...|
|||...(a): F-measure scores on the five saliency detection datasets for evaluation of the single-context model and the multi-context model....|
|||...(c) is groundtruth saliency map....|
|||...(d) is our saliency map from the single-context model....|
|||...(e) is our saliency map from the multi-context model....|
|||...Also, we qualitatively compare our saliency maps with those by other methods in Figure 7....|
|||...In this paper, we propose a multi-context deep learning framework for saliency detection....|
|||...Example images from five datasets and the saliency maps by compared methods....|
|||...duce multi-context saliency modeling using deep Convolutional Neural Networks....|
|||...Global context and local context are utilized and integrated into a unified multi-context deep learning framework for saliency detection....|
|||...Secondly, different pre-training strategies are investigated to learn the deep model for saliency detection, and a task-specific pretraining scheme designed for our multi-context deep model is proposed....|
|||...Moreover, recently proposed contemporary deep models in ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated....|
|||...Boosting bottom-up and top-down visual  features for saliency estimation....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Top-down visual saliency via  joint crf and dictionary learning....|
||57 instances in total. (in cvpr2015)|
|44|Zhang_Learning_Uncertain_Convolutional_ICCV_2017_paper|...Learning Uncertain Convolutional Features for Accurate Saliency Detection  Pingping Zhang Dong Wang Huchuan Lu Hongyu Wang Baocai Yin  Dalian University of Technology, China  jssxzhpp@mail.dlut.edu.cn...|
|||...The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection....|
|||...Extensive experiments demonstrate that our proposed saliency model performs favorably against state-ofthe-art approaches....|
|||...As a preprocessing procedure in computer vision, saliency detection has greatly benefited many practical applications such as object retargeting [6, 40], scene classification [37], semantic segmentati...|
|||...In this work we focus on the task of improving robustness of saliency detection models, which has been ignored in the literature....|
|||...Previous saliency detection methods utilize several handcrafted visual features and heuristic priors....|
|||...(a) Feature Visualization [7]  (b) Generative Adversarial Example [35]  (c) Saliency Detection [45]  (d) Semantic Segmentation [31]  Figure 1....|
|||...For instance, the object boundary strongly affects the prediction accuracy of a saliency model, it is desirable that the model can provide meaningful uncertainties  212  on where the boundary of dist...|
|||...As far as we know, there is no work to model and analyze the uncertainty of saliency detection methods based on deep learning....|
|||...All of the issues discussed above motivate us to learn uncertain features (probabilistic learning) through deep networks to achieve accurate saliency detection....|
|||... Different from existing saliency detection methods, our model is extremely simplified....|
|||... Our model can learn deep uncertain convolutional features (UCF) for more accurate saliency detection....|
|||... The uncertain feature extraction and saliency detection are unified in an encoder-decoder network architecture....|
|||... Our methods show good generalization on saliency detection and other pixel-wise vision tasks....|
|||...Without any post-processing steps, our model yields comparable even better performance on public saliency detection, semantic segmentation and eye fixation datasets....|
|||...Related Work  Recently, deep learning has delivered superior performance in saliency detection....|
|||...[44] propose two deep neural networks to integrate local estimation and global search for saliency detection....|
|||...[50] take global and local context into account and model the saliency prediction in a multi-context deep CNN framework....|
|||...[45] design a recurrent FCN to leverage saliency priors and refine the coarse predictions....|
|||...onal features by using multiple reformulated dropouts, which improve the robustness and accuracy of saliency detection....|
|||... corresponding decoder FCN for low-level information reconstruction and a pixel-wise classifier for saliency prediction....|
|||...We use the softmax classifier for the pixel-wise saliency prediction....|
|||...listic, thus it can bring forth robustness in the prediction of dense labeling vision tasks such as saliency detection and semantic segmentation....|
|||...Training the Entire Network  Since there is a lack of enough saliency detection data for training our model from scratch, we utilize the front-end of the VGG-16 model [38] as our encoder FCN (13 convo...|
|||...For saliency detection, we randomly initialize the weights of the decoder FCN and fine-tune the entire network on the MSRA10K dataset [4], which is widely used in salient object detection community (M...|
|||... we use the softmax cross-entropy loss function given by the following equation (17) for separating saliency foreground from general background....|
|||...L =  (cid:3)  m  lm log(qm) + (1  lm) log(1  qm),  (17)  where lm (= 0, 1) is the label of a pixel m in the image and qm is the probability that the pixel is the saliency foreground....|
|||...We use the difference between Mf e and Mbe, and clip the negative values to obtain the resulting saliency map, i.e.,  Sal = max(Mf e  Mbe, 0)....|
|||...Experiments  In this section, we start by describing the experimental setup for saliency detection....|
|||...Then, we thoroughly evaluate and analyze our proposed model on public saliency detection datasets....|
|||...For the detection performance evaluation, we adopt six  widely used saliency detection datasets as follows,  DUT-OMRON [49]....|
|||...Thus, this dataset is difficult and challenging in saliency detection....|
|||...The proposed saliency detection algorithm runs at about 7 fps with 448  448 resolution (23 fps with 224  224 resolution)....|
|||...The precision and recall are computed by thresholding the predicted saliency map, and comparing the binary map with the ground truth....|
|||...We report the performance when each saliency map is adaptively binarized with an image-dependent threshold....|
|||...The threshold is determined to be twice the mean saliency of the image:  T =  2  W  H  W  H  (cid:3)  (cid:3)  x=1  y=1  S(x, y),  (20)  where W and H are width and height of an image, S(x, y) is the ...|
|||...The source codes with recommended parameters or the saliency maps of the competing methods are adopted for fair comparison....|
|||...The F-measure and MAE of different saliency detection methods on five frequently used datasets....|
|||...Our saliency maps can reliably highlight the salient objects in various challenging scenarios, e.g., low contrast between objects and backgrounds (the first two rows), multiple disconnected salient ob...|
|||...In addition, our saliency maps provide more accurate boundaries of salient objects (the 1, 3, 6-8 rows)....|
|||...uncertain learning mechanism can indeed benefit to learn more robust features for accurate saliency inference....|
|||...Results imply that the interpolation strategy performs much better in saliency detection....|
|||...The joint comparison of V-B, V-C and UCF confirms that our hybrid upsampling method is capable of better refining the output saliency maps....|
|||...Comparison of saliency maps....|
|||...In addition, though the segmentation performance gaps are not as large as in saliency detection, our new upsampling method indeed performs better than regular deconvolution (mean IOU: 67.45 vs 65.173,...|
|||...Conclusion  In this paper, we propose a novel fully convolutional network for saliency detection....|
|||...A new upsampling method is also proposed to reduce the artifacts of deconvolution operations, and explicitly enforce the network to learn accurate boundary for saliency detection....|
|||...Extensive evaluations demonstrate that our methods can significantly improve performance of saliency detection and show good generalization on other pixel-wise vision tasks....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Kernelized In ECCV, pages  subspace ranking for saliency detection....|
|||...Hierarchical saliency detec tion....|
||56 instances in total. (in iccv2017)|
|45|Deep Level Sets for Salient Object Detection|... University, Singapore  Alibaba Group, Hangzhou, China  Abstract  Deep learning has been applied to saliency detection in recent years....|
|||...ilar receptive fields around the object boundaries, thus deep networks may output maps with blurred saliency and inaccurate boundaries....|
|||...To tackle such an issue, in this work, we propose a deep Level Set network to produce compact and uniform saliency maps....|
|||...Besides, to propagate saliency information among pixels and recover full resolution saliency map, we extend a superpixel-based guided filter to be a layer in the network....|
|||...During testing, the network can produce saliency maps by efficiently feedforwarding testing images at a speed over 12FPS on GPUs....|
|||...Inspired by this biological capability, visual saliency computation is introduced into the field of computer vision for its potential to enhance tasks like image processing [4, 12], image understandin...|
|||...Over the past years, many saliency detecting methods have been proposed....|
|||...Early works focused on fixation-level saliency detection [19, 14, 17] that aims to predict humans attentional priority when viewing an image....|
|||...Later, it was extended to object-level saliency detection [54, 55, 12, 64, 10, 34, 60] which targets at computing saliency maps to highlight the regions of salient objects ac (a)  (b)  (c)  (d)  (e)  Figure 1....|
|||...Examples of pixel-wise saliency prediction....|
|||...(c) Saliency maps by deep network trained with Binary Cross Entropy (BCE) loss....|
|||...(d) Saliency maps by deep network trained with Level Set method....|
|||...In saliency detection, several recent works using CNNs [62, 29, 24, 33, 26, 39, 30, 51, 46] have significantly outperformed previous methods which only use low-level cues and hand-crafted features....|
|||...It is difficult for a network to learn saliency at boundaries of salient regions....|
|||...With the signed distance, the final saliency label can be generated easily by the Heaviside transformation that projects negative numbers to 0 and positive numbers to 1....|
|||...e energy function, so the network can be aware of the salient object as a whole instead of learning saliency for every pixel independently....|
|||...ith Level Set function can discriminate pixels around object boundaries more precisely and generate saliency maps more compact and accurate than network directly trained with binary groundtruth....|
|||...1(e), the proposed network can produce saliency maps that are compact, uniform and accurate....|
|||...Salient Object Detection  Early approaches treat saliency detection as an unsupervised problem and focus on low-level features and cues....|
|||...Cheng et al [10] compute saliency of objects based on color uniqueness and spatial distribution....|
|||...Assuming image boundaries belong to the background, Zhang et al [60] and Tu et al [48] compute the difference between pixels and image boundaries to be saliency and achieve realtime testing speed....|
|||...Deep Saliency Networks  Recently, deep learning has achieved human-competitive performance in many computer vision tasks [18, 16]....|
|||...With this advantage, deep learning has been applied in saliency detection and achieved state-of-the-art performance....|
|||...Wang et al [51] utilize a recurrent fully convolutional network to incorporate saliency prior knowledge for more accurate saliency....|
|||...Instead of predicting saliency for every single pixel, superpixel and object region proposal are also combined with deep network [46, 26, 29, 62, 31, 50] to achieve accurate segmentation of salient object....|
|||...The CNN is built on the VGG16 net and generates coarse saliency level set maps at a resolution of 56*56....|
|||...At the end of the CNN, an up-sampling layer is added to scale saliency level set map into full resolution....|
|||...A Guided Superpixel Filtering(GSF) layer is followed and takes the scaled saliency level set map and superpixels as inputs....|
|||...At last, the output of GSF layer was transformed by a Heaviside Function(HF) to be the final saliency map....|
|||...To incorporate the deep convolutional network with the level set method, we linearly shift the saliency values output by the CNN into [-0.5,0.5] and treat it as the level set . Pixel space of the inpu...|
|||...To produce saliency maps that are compact and accurate, we train the network to learn a level sets  that minimizes the following energy function,   H((x, y))  gt(x, y) 2dxdy +  Length(C)  In the first...|
|||...Minimizing this term with  > 0 supervises the network to learn what is saliency in images....|
|||...rpixel, we extend the guided filter to effectively and efficiently utilize superpixels to propagate saliency information locally and recover full resolution saliency map....|
|||... filter, so that it can help us utilize the object boundary in the guidance image to further detect saliency within objects and suppress saliency outside the objects....|
|||...(c) Saliency map generated using (b)....|
|||...(e) Saliency map generated using (d)....|
|||...An example of a processed saliency map is shown in Fig....|
|||...This layer transforms the level set map into the final saliency map....|
|||...Let saliency value be in the range [0,255], PR curve is generated by computing P recision =  M G  and  M   Recall =  M G  (G) with threshold value varying from 0 to 255.  on the binary mask (M ) and g...|
|||...The Mean Absolute Error (MAE) is another widely used evaluation metric which is the average per-pixel difference between saliency map(S) and groundtruth(G)....|
|||...Given an input, ELD processes the superpixels one by one and takes about 0.799 seconds to produce the final saliency map (implemented in C ++/caf f e)....|
|||...These are two typical methods to refine the saliency maps with superpixels....|
|||...Our model is able to produce saliency maps that highlight salient regions accurately and uniformly....|
|||...A large D will propagate saliency within a large region, and this may produce noise in complex scenes....|
|||...A small results in a narrow support range and may get stuck in a local minimum, on the other hand, a large  may fail to learn a good level set function and cannot generate compact saliency map....|
|||...Furthermore, the proposed method extends the guided image filter to deal with superpixels so that saliency can be further propagated between pixels and the saliency map can be recovered to full resolution....|
|||...And by adding the GSF layer, the network can further improve saliency maps and accurately segment salient objects from the background....|
|||...Recurrent attentional net works for saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  [8] T. F. Chan and L. A. Vese....|
|||...Salientshape: Group saliency in image collections....|
|||...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||56 instances in total. (in cvpr2017)|
|46|Li_Robust_Saliency_Detection_2015_CVPR_paper|...y of Sydney 2Northwestern Polytechnical University                   Abstract   In  the  field  of  saliency  detection,  many  graph-based  algorithms  heavily  depend  on  the  accuracy  of  the  pr...|
|||...To provide more  accurate saliency estimations, we first optimize the image  boundary  selection  by  the  proposed  erroneous  boundary  removal....|
|||... into account, we then propose the regularized  random  walks  ranking  to  formulate  pixel-wised  saliency  maps  superpixel-based  background  and  foreground  saliency  estimations....|
|||...ccuracy  and  robustness  of  the  proposed  algorithm  in  comparison  with  12  state-of-the-art  saliency  detection  approaches....|
|||...Introduction   Human  saliency  is  usually  referred  as  local  contrast  [17], which typically originates from contrasts between an  item  and  its  surroundings,  such  as  differences  in  color,...|
|||...ent  the  histogram-based  contrast (HC), which exploits the pixel-wise color separation to produce saliency maps, and the region-based contrast (RC), which is an improvement of HC that takes spatial ...|
|||...[8]  illustrate  the  workflow  of  a  combined color contrast and color distribution saliency detection algorithm, together with a refinement process to suppress noise and artifacts....|
|||...[13]  offer  the  graph  based  visual  saliency (GBVS), a  graph-based bottom-up  saliency  model  with dissimilarity measurements to extract saliency information....|
|||...[20] formulates saliency  by pixel-wise aggregation, conditional random field (CRF)  aggregation and image-dependent aggregation....|
|||...[19] introduce the discriminative regional feature integration  (DRFI),  which  integrates  regional  contrast,  property  and  backgroundness  descriptor  together  to  formulate  the  master saliency map....|
|||...[27]  utilizes  the  four boundaries of the input image as background prior to  extract foreground queries for the final saliency map....|
|||...The results in [27] demonstrate that the MR algorithm  outperforms most of the state-of-the-art saliency detection  methods and is more computationally efficient....|
|||...Firstly,  the  four  boundaries  used  as  background  queries  in  MR  may be implausible for the background saliency detection....|
|||...Besides, assigning  the  same  saliency  value  to  all  pixels  in  a  superpixel  node  cannot  exploit  the  full  potential  of  the  detail  information from the original image....|
|||...To improve the overall quality of the saliency map, we  first filter out one of the four boundaries that most unlikely belong to the background before conducting the background  saliency  estimation....|
|||...The  regularized  random  walks ranking is independent of the superpixel segmentation, and can generate pixel-wised saliency maps that reflect full-details of the input images....|
|||...(3)   ,  )  (  =  G V E  The  manifold  ranking  model  is  used  to  estimate  the  rough saliency in the proposed method in Section 3....|
|||...(9)   In Section 3, the random walks model is reformulated  for  the  final  saliency  map  computation....|
|||...The Proposed Algorithm   The  proposed  saliency  detection  algorithm  consists  of   three  major  steps....|
|||...ne  removes  the  boundary  with the lowest probability belonging to the background,  and generates saliency estimation via background queries;  the  step  two  generates  foreground  saliency  estima...|
|||...Background Saliency Estimation   As stated in the introduction, it is possible for a boundary in the input image to be occupied by the foreground  object....|
|||...Using  such  a  problematic  boundary  as  queries  in  the background saliency estimation may lead to undesirable  results,  and  a  typical  example  is  illustrated  in  the  second  column  of  Figure  1....|
|||...We  therefore  optimize  the  boundary influences by locating and eliminating erroneous  boundaries before the background saliency estimation....|
|||...From left to right: input images, background saliency estimations  with  all  boundaries,  background  saliency  estimations  after  erroneous boundary removal, ground truth....|
|||...The  results  are  then put into element-by-element multiplication to calculate the saliency result of Section 3.1,   n     ,  S  step 1  ( ) i  =   l  ( ) S i ....|
|||...Foreground Saliency Estimation   Section 3.1 calculates the foreground saliency by complementary subtraction of the background saliency estimation, which leads to favorable results in images with cons...|
|||...Subsequent  foreground-query-based  saliency  estimation is hence desired....|
|||...ng  function  (cid:108)f   can  be  directly  calculated from Eq.3 and is treated as the foreground saliency  estimation as follows,  =  mean(  step 1  =  =  S  )  t  1,...,  n  ,     (cid:108) f  i (...|
|||...Examples  that  Eq.16  leads  to  more  precise  saliency  outputs....|
|||...From  left  to  right:  input  images,  saliency  estimation  results,  saliency  outputs  with  random  walks,  saliency  outputs  with regularized random walks ranking, ground truth....|
|||...Saliency  Map  Formulation  by  Regularized  Random Walks Ranking   Former manifold-ranking-based saliency detection [27]  completely depends on the SLIC superpixel segmentation,  which  may  generate...|
|||...In addition, assigning the  same  saliency  value  to  all  pixels  within  a  same  node  enormously sacrifices the detail information....|
|||...rcome  these  disadvantages,  we  develop  a  regularized  random  walks ranking model to formulate saliency maps, which is  independent of the superpixel segmentation, and  may reveal pixel-wised sal...|
|||...st  a  fitting  constraint,  which  restricts  the  Dirichlet  integral to be as close to the prior saliency distribution as  possible,   k  Dir p        =  T  )  ( L p  k  )  +  k  p  (  1 2    ( 2  ...|
|||...In other words, different pixels within a    share the same saliency value in  same superpixel in  Y ....|
|||...(19)   k  k  2  Up   and  Mp   are  then  combined  to  form  k =   to  select  the  foreground  possibility     set  reshape  it  to  a  matrix  image as the final foreground saliency output....|
|||...The  fitting  constraint  in  Eq.16  provides  a  prior  saliency  estimation  to  all  pixels  instead  of  the  seed  pixels  alone,  which  offers  a  better  guidance in calculating the final saliency map....|
|||...in  Figure  2,  where  the  regularized  random  walks  ranking  not  only  greatly  improves  the  saliency  map  from  the  previous  saliency  estimation  step,  but  also  remarkably  outperforms ...|
|||...Algorithm 1 Saliency Detection by Regularized Random Walks  Ranking  Input: An image and related parameters  1: Establish the graph structure with superpixels as nodes; calculateW and D with Eq.4 and Eq.1....|
|||...3: Acquire the background saliency estimation 4: Acquire the foreground saliency estimation 5:  Establish  the  pixel-wise  graph  structure  and  obtain L with  Eq.5; then compute the saliency possib...|
|||...Output: a saliency map with the same size as the input image....|
|||...as the final saliency out kp with Eq.19....|
|||...Precision and  recall  are  usually  displayed  together  as  precision-recall  curves,  which  are  constructed  by  binarizing  the  saliency  map with thresholds from 0 to 255....|
|||...After  that,  we  generate  the  saliency  maps  right  after  Section  3.2  without  using  regularized  random  walks  ranking....|
|||...ith State-of-the-art   We  then  evaluate  our  proposed  algorithm  against  12  state-of-the-art  saliency  detection  approaches,  namely  CA[10], CB[18], FT[2], GS[26], IT[17], LR[25], MR[27],  PB...|
|||...n  CA  and  CB,  which  are  two  of  the  top-performance  algorithm  from  a  recent benchmark of saliency detection [5]; the proposed  method also completely excels its predecessor, i.e....|
|||...To  provide  a  visual  comparison  of  the  different  saliency  outputs,  we  choose  five  of  the  state-of-the-art  methods  with  the  closest  performances  to  the  proposed  method,  namely  ...|
|||...We notice that  the proposed method generates saliency maps with clearer  details and finer boundary adherences....|
|||...Conclusion  In  this  paper,  we  propose  a  novel  bottom-up  saliency  detection  method  with  erroneous  boundary  removal  and  regularized  random  walks  ranking....|
|||...y-adjacent  foreground  superpixels,  and  thus  neutralizes  their  negative  influences  in  the  saliency  estimations;  secondly,  the  proposed  regularized  random  walks  ranking provides prior...|
|||...blic  datasets  show  that  the  proposed method  significantly  outperforms  12  state-of-the-art  saliency  detection  algorithms  in  terms  of  both  accuracy  and  robustness....|
|||...Examples of output saliency maps using different algorithms on the MSRA10K dataset....|
|||...[9]  D.  Gao  and  N.  Vasconcelos,  "Discriminant  saliency  for  visual  recognition  from  cluttered  scenes,"  in  Advances  in  neural information processing systems, 2004, pp....|
|||...Sun,  "Geodesic  saliency  using  background  priors,"  in  Computer  VisionECCV  2012, ed: Springer, 2012, pp....|
|||...[28]  J.  Yang  and  M.-H.  Yang,  "Top-down  visual  saliency  via  joint crf and dictionary learning," in Computer Vision and  Pattern  Recognition  (CVPR),  2012  IEEE  Conference  on,  2012, pp....|
||55 instances in total. (in cvpr2015)|
|47|Li_Saliency_Detection_on_2014_CVPR_paper|...hbling@temple.edu  Abstract  Existing saliency detection approaches use images as inputs and are sensitive to foreground/background similarities, complex background textures, and occlusions....|
|||...We further develop a new saliency detection algorithm tailored for light fields....|
|||...Experiments show that our saliency detection scheme can robustly handle challenging scenarios such as similar foreground and background, cluttered background, complex occlusions, etc., and achieve hig...|
|||...Accurate and reliable saliency detection can benefit numerous tasks ranging from tracking and recognition in vision to image manipulation in graphics....|
|||...Light field vs. traditional saliency detection....|
|||...Using light field as inputs, our saliency detection scheme is able to robustly handle these cases....|
|||...By far, nearly all existing saliency detection algorithms utilize images acquired by a regular camera....|
|||...Conceptually, the light field data can benefit saliency detection in a number of ways....|
|||...Second, a light field provides an ap 1   All-focus Image  Groud Truth    Saliency by GBMR  Our Saliency proximation to scene depth and occlusions....|
|||...In saliency detection, even a moderately accurate depth map can greatly help distinguish the foreground from the background....|
|||... we choose regions with a high FLS as candidate salient objects. Finally, we conduct contrast-based saliency detection on the all-focus image and combine its estimation with the detected foreground sa...|
|||...For validation, we acquire a light field database of a range of indoor and outdoor scenes and generate the ground truth saliency map....|
|||...Experiments show that our saliency detection scheme can robustly handle challenging scenarios such as similar foreground and background, cluttered background, and images with multiple depth layers and...|
|||...Related Work  The saliency detection literature is huge and existing solutions can be classified in terms of top-down vs. bottomup, center vs. background prior, with vs. without depth cue, etc....|
|||...Approaches in this category are highly effective on task-specified saliency detection, e.g., identifying human activities [24]....|
|||...Bottom-up methods do not require training and rely on low-level features such as color contrast [14], pixel/patch locations [29], histogram [10], etc., for saliency detection....|
|||...Many saliency detection schemes exploit contrast cues, i.e., salient objects are  Figure 2....|
|||...However, only a handful of works incorporate depth maps into saliency models....|
|||...[26] computed saliency based on the global disparity contrast in a pair of stereo images....|
|||...Specifically, we use coarse depth information embedded in a focal stack to guide saliency detection....|
|||...Computing Light Field Saliency Cues  Fig....|
|||...3 shows our saliency detection approach using the light field....|
|||...We further couple the background prior with contrast-based saliency detection for detecting saliency candidates in the all-focus image....|
|||...Finally, we use the objectness as weights for combining the saliency candidates from the all-focus image and from the focal stack as the final saliency map....|
|||...Therefore, brute-force approaches such as applying saliency detection on each slice and then combine the results are not directly applicable since all slices will produce similar results....|
|||...Processing pipeline of our saliency detection algorithm for light fields....|
|||...Our goal is to compute a saliency map w.r.t....|
|||...In the recent focusness-based saliency detection work, Jiang et al....|
|||...To ensure reliable focusness measurements, we use the harmonic vari Focusness PriorAll-focus imageFinal Saliency MapContrast Saliency MapBLSFLSBackground CueLight Field Focal StackForeground Cuesance...|
|||...ber of pixels in r. We will use this region-based focusness prior F(r) for selecting background and saliency candidates in Section 3.3 and 3.4....|
|||... of the image and  h(cid:88)  Dx =  1   w(cid:88)  1   w(cid:88)  h(cid:88)  A common assumption in saliency detection is that an salient object is more likely to lie at the central area surrounded by...|
|||...Once we obtain the background regions, we apply the color-contrast based saliency detection on the non-background region....|
|||...Rather, our goal is to show that the additional information provided by the light field can greatly improve saliency detection tasks....|
|||...For each data, we ask three individuals to manually segment the saliency regions from the all-focus image....|
|||...Saliency Detection  Finally, we combine the cues obtained from the light field focal stack to detect saliency in the all-focus image I....|
|||...Visual Comparisons of different saliency detection algorithms vs. ours on our light field dataset....|
|||...We show our light field saliency detection results and the results using a range of unsupervised schemes on the all-focus image....|
|||...include algorithms based on spectral residual (SR [12]), spatiotemporal-cues (LC[37]),  graph-based saliency (GB [15]), frequency-tuning (FT [1]), global-contrast (HC and RC [7]), Low Rank Matrix Reco...|
|||..., we use the canonical precision-recall curve (PRC) to evaluate the similarity between the detected saliency maps and the ground truth....|
|||...Our experiment follows the settings in [7], i.e., we binarize the saliency map at each possible threshold within [0, 255]....|
|||...Notice that the PRC are less smooth than they appear in traditional saliency works....|
|||...5, we show the saliency detection results for visual comparisons....|
|||...We further compare the saliency components obtained using different cues, i.e., color contrast, location and focusness cues....|
|||...The plot illustrates that each cue has its unique contribution to saliency detection, although in some cases, an image can be dominated by a specific cue as shown in Fig....|
|||...In the first row, color contrast provides most valuable cues and the estimated saliency from it resembles the final one....|
|||...(a) All-focus images; (b) Detected saliency using focusness cues; (c) Detected saliency using color contrast....|
|||...(d) Saliency results by combining (b) and (c)....|
|||...There are also alternative approaches to use the light field for saliency detection....|
|||...Furthermore, it is also possible to first conduct saliency detection on the all-focus image and then use the results to improve the quality and speed of light field stereo matching....|
|||...Conclusions  We have presented a saliency detection algorithm tailored for light fields....|
|||...In recent works [26, 16], these new cues have shown great success in improving accuracy and robustness in saliency detection....|
|||...Another contribution of our work is the construction of the light field saliency dataset which consists of the raw light field data, the synthesized focal stacks and all-focus images, and the ground t...|
|||...Image saliency by isocentric curvedness and color....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||55 instances in total. (in cvpr2014)|
|48|Jiang_Generic_Promotion_of_ICCV_2015_paper|...We first make a novel analysis of the working mechanism of the diffusion matrix, which reveals the close relationship between saliency diffusion and spectral clustering....|
|||...Introduction  The aim of saliency detection is to identify the most salient pixels or regions in a digital image which attract humans first visual attention....|
|||...Results of saliency detection can be applied to other computer vision tasks such as image resizing, thumbnailing, image segmentation and object detection....|
|||...Due to its importance, saliency detection has received intensive research attention resulting in many recently proposed algorithms....|
|||...In the field of saliency detection, two branches have developed, which are visual saliency detection [4,9,10,1215, 19,29,34,39,41] and salient object detection [1,57,11,16 18,2025,27,32,33,35,37,38,40,42]....|
|||...Salient object detection algorithms usually generate bounding boxes, binary foreground and background segmentation,or saliency maps which indicate the saliency likelihood of each pixel....|
|||...However, these methods usually miss small local salient regions or bring some outliers such that the resultant saliency maps tend to be nonuniform....|
|||...stigate the working mechanism of the diffusion matrix through eigen-analysis to find that the final saliency of a node (called focus node) is equal to a weighted sum of all the non-zero seed saliency ...|
|||...Further, since the diffusion map is formed by the eigenvectors and eigenvalues of the diffusion matrix, the process of saliency diffusion has a close relationship with spectral clustering....|
|||...o the best of our knowledge, we for the first time explicitly reveal the close relationship between saliency diffusion and spectral clustering and, correspondingly, propose a generic and systematic sc...|
|||...d ranking matrix or propagation matrix), s is the seed vector (or query vector), and y is the final saliency vector to be computed....|
|||...Here s usually contains preliminary saliency information of a portion of nodes, that is to say, usually s is not complete and we need to propagate the partial saliency information in s to the whole sa...|
|||...Correspondingly, the formula of saliency diffusion is  y =L1s  (3)  where L = D  W ....|
|||...Work [24] computes s by combining hundreds of saliency features F with learned weight w (s = F w), and uses inverse normalized Laplacian matrix L1 rw as the diffusion matrix....|
|||...Correspondingly, the formula of saliency diffusion is  y =(I  P )1s = L1 rw s  (5)  where P = D1W and P is called transition matrix....|
|||...ke a novel interpretation of the working mechanism of diffusion-based salient object detection: the saliency of a node (called focus node) is determined by all the seed values in the form of weighted ...|
|||...This matches our intuition that similar (distinct) nodes should in general have similar (distinct) saliency values....|
|||...2.2, nodes with similar (distinct) diffusion maps tend to obtain similar (distinct) saliency values according to Eq.s 7 and 8....|
|||...Ground truth saliency of the source images are shown in Fig....|
|||...y = eA1 s, = (eA1)2x,  (10)  Thereafter, we obtain the saliency map S by assigning the value of yi to the corresponding node vi, 1  i  N ....|
|||...Seed Vector Construction  Other diffusion-based saliency object detection methods usually generate the seed vector based on low-level features....|
|||...Besides yielding good accuracy in seed value estimation, this approach is time-efficient since we avoid an extra pass of color-based preliminary saliency search....|
|||...Combination  We use  s as the seed vector and eA1 as the diffusion  matrix, and compute the saliency vector y as  (12)  (13)  (14)  6....|
|||...In order to study the performance of saliency detection algorithms, we adopt prevalently used evaluation protocols including precision-recall (PR) curves [1], F-measure score which is a weighted harmo...|
|||...6: Compute the final saliency vector y by Eq....|
|||...Output: The saliency vector y representing the saliency  value of each superpixel....|
|||...In the experiments, we evaluate different diffusion matrices by visual saliency promotion and constrained optimal seed efficiency, as detailed in Sec....|
|||...6.5, we further use standard image processing techniques to increase the contrast of the final saliency maps....|
|||...Promotion of Visual Saliency  Visual saliency detection predicts human fixation locations in an image, which are often indicative of salient objects around....|
|||...In other words, we promote a visual saliency detection algorithm by diffusion for the task of salient object detection....|
|||...In this experiment, we use the results of nine visual saliency detection methods (i.e., IT [15], AIM [4], GB [12], SR [14], SUN [41], SeR [34], SIM [29], SS [13] and COV [9]) on the MSRA10K dataset as...|
|||...The PR curves of the nine visual saliency detection methods before and after difrw are plotted in Fig....|
|||...3, meaning that, with a good diffusion matrix, we can fill the performance gap between two branches of saliency detection methods....|
|||... more consistent promoted performance than eL1 and eL1 synthesized diffusion matrix, eA1, in visual saliency pro motion....|
|||...Constrained Optimal Seed Efficiency  We prefer a diffusion matrix to use as little query information or, equally, as few non-zero seed values to derive as close saliency to the ground truth as possible....|
|||...Correspondingly, for a diffusion matrix, we measure the constrained optimal saliency detection accuracy it may achieve at each non-zero seed value budget, leading to an constrained optimal seed effici...|
|||...Although the saliency detection performance of these resultant seed vectors provides a good reference for  221  i  i  n o s c e r P  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  IT  AIM  GB  SR  SUN  S...|
|||...PR curves of nine visual saliency detection methods before (dash line) and after (solid line) diffusion by (a) eA1, (b) eL1, and (c) eL1  rw ....|
|||... i-th (0  i  100) iteration, we compute and record the pair of nonnegative seed percentage, ri, and saliency detection accuracy, ai, according to the following formulae:  ri =  ai =  100  ksk0  kGT k0...|
|||...According  to the last observation, it appears that the performance of diffusion-based saliency detection is fundamentally determined by the diffusion matrix, again emphasizing the importance in const...|
|||...Note that GMR, MC, DSR, and RBD have been identified as the top performers on the saliency benchmark study of work [3]1....|
|||...further filtered out and the discriminability weighting is rw ,  2  2  diffusion matrix and run our saliency object detection algorithm on the test image....|
|||...Through analysis, we find that the saliency of each node is formed by a weighted sum of all the seeds saliency values, with the weights determined by the diffusion map similarities between the nodes....|
|||...The proposed scheme is a generic one which can be used to promote any diffusion-based saliency object detection algorithm....|
|||...As a particular instance, we use inverse normalized Laplacian matrix, L1 rw , as the original diffusion matrix and promote the corresponding saliency detection algorithm....|
|||...Experiments show that the promoted diffusion matrix is superior in both visual saliency promotion and constrained optimal seed efficiency, and the promoted salient object detection method advances the...|
|||...These should, in principle, be applicable to the saliency problem....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
|||...Improved saliency detection based on superpixel clustering and saliency propagation....|
|||...Hierarchical saliency detec tion....|
|||...Top-down visual saliency via joint  crf and dictionary learning....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||54 instances in total. (in iccv2015)|
|49|cvpr18-Salient Object Detection Driven by Fixation Prediction|...ail.com, {shenjianbing, dongxingping}@bit.edu.cn, aborji@crcv.ucf.edu  Abstract  Research in visual saliency has been focused on two major types of models namely fixation prediction and salient object...|
|||...We build a novel neural network called Attentive Saliency Network (ASNet)1 that learns to detect salient objects from fixation maps....|
|||...Salient object detection is then viewed as fine-grained object-level saliency segmentation and is progressively optimized with the guidance of the fixation map in a top-down manner....|
|||...Recently, the use of deep neural networks for saliency detection has been trending....|
|||...We propose the Attentive Saliency Network (ASNet) that infers the object saliency (b) from predicted fixation maps (c), which is consistent with human attention mechanisms....|
|||...Additionally, for current computational saliency models, their connection with how humans explicitly choose salient objects or watch natural scenes are less clear (as discussed in [3, 6])....|
|||...The suggested model not only generates high-quality object saliency maps, but also pushes the boundary of SOD research by building a close connection to human fixation prediction (FP)....|
|||...1, our model infers object saliency using the fixation prior, where this prior acts as a selective mechanism to enhance the saliency representation for the purpose of accurate object saliency inference....|
|||...The proposed Attentive Saliency Network (ASNet) is based on convolutional LSTM (convLSTM) [59], which has convolutional structures in both the input-to-state and stateto-state transitions....|
|||...By stacking multiple convLSTMs, the ASNet is trainable for gradually rendering object saliency from fixation map in a top-down manner....|
|||...This goes one step beyond previous deep learning based saliency models and offers a deep insight into the confluence between fixation prediction and salient object detection....|
|||... We present the Attentive Saliency Network (ASNet) which is a hierarchy of convLSTMs for step-wise inference of object saliency....|
|||...ConvLSTM has the advantage of the improved flow of information with recurrent connections, which results in more powerful saliency representation....|
|||...Some other methods try to integrate deep learning models with handcrafted features [30], study saliency prior [48], or exploit various deep learning architectures [27, 35, 65, 49, 54]....|
|||...They have quantitatively confirmed that object saliency judgments agree with human fixations....|
|||...The KL-Div measure, the minimization of which is equivalent to crossentropy minimization, is widely used in visual saliency prediction [18, 51]....|
|||...Detecting Object Saliency with Fixation Prior  The fixation map P gives a coarse but informative prior regarding visually salient regions....|
|||...Then, the fine-grained object saliency is gradually inferred from lower layers and is successively optimized via the recurrent architecture of convLSTM....|
|||...Here, we desire our model to be able to infer precise object saliency from the fixation map predicted in the upper network layers....|
|||...For a certain layer, convLSTM discards less informative features while enhances informative features, thus generating gradually improved saliency maps....|
|||...network (from the last convolutional layers prior to pooling layers) as input, and produces refined saliency features for final saliency estimation....|
|||...Here, we take the advantages of recurrent natures of LSTM for iteratively optimizing the saliency features of static images, instead of using LSTM for modeling the temporal dependency of sequential data....|
|||...We apply a 11 convolution kernel to the final convLSTM output H for obtaining an object saliency map Q  [0, 1]1414....|
|||...Several different metrics have been proposed for evaluating saliency models and no single metric can fully summarize the performance of a model....|
|||...Illustration of our convLSTM based object saliency optimization, where (b) shows detailed architecture of our convLSTM optimization module in (a)....|
|||...(12)  After obtaining the object saliency map Q  [0, 1]1414 inferred from the fixation map P , we upsample (2) Q and feed it to the next convLSTM with the compressed features (282864) from conv4-3 lay...|
|||...In sum, the ASNet is able to effectively infer the object saliency thanks to 1) a learnable fixation prior, 2) iteratively updating saliency features with recurrent architecture and 3) efficiently mer...|
|||...Since there are only few datasets that offer annotations for both SOD and FP tasks, most of the training images are either labeled with human fixation annotation or object saliency mask....|
|||...Let yA k  {0, 1} and yS k  {0, 1} indicate whether we have the attention annotation Gk and object saliency mask Sk for the k-th training image....|
|||...We consider three large-scale saliency datasets: SALICON [22], THUS10K [10], and DUT-OMRON [61]....|
|||...Here, we employ five typical metrics, namely Normalized Scanpath Saliency (NSS), Similarity Metric (SIM), Linear Correlation Coefficient (CC), AUC-Judd, and shuffled AUC....|
|||...From top to bottom: example images, fixation maps, and object saliency results....|
|||...We also study the effect of our hierarchical architecture with a stack of several convLSTMs and top-down saliency inference....|
|||...We find that the saliency results are gradually optimized by adding more details from lower layers....|
|||...Such prior was further utilized for teaching the network where the salient object is and the detailed object saliency was rendered step by step by considering finer and finer features in a top-down manner....|
|||...A study of human explicit saliency judgment....|
|||...Predicting human eye fixations via an lstm-based saliency attentive model....|
|||...Discriminant saliency for visual  recognition from cluttered scenes....|
|||...SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...End-to-end saliency mapping via probability distribution prediction....|
|||...Recurrent attentional net works for saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...DHSNet: Deep hierarchical saliency network for salient object detection....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
|||...Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Correspondence driven saliency transfer....|
|||...Stereoscopic thumbIEEE  nail creation via efficient stereo saliency detection....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
||54 instances in total. (in cvpr2018)|
|50|cvpr18-Active Fixation Control to Predict Saccade Sequences|...Fixation modeling in the past decades has been largely dominated computationally by a number of highly influential bottom-up saliency models, such as the Itti-Koch-Niebur model....|
|||...However, on static images the emphasis of these models has largely been based on non-ordered prediction of fixations through a saliency map....|
|||...FC, a novel multi-saccade generator based on the integration of central high-level and object-based saliency and peripheral lowerlevel feature-based saliency....|
|||...del of human fixation control which includes a number of aspects normally neglected by the  13184  saliency literature, including an explicit transform to account for anisotropic retinal acuity, we a...|
|||...A novel computational fixation model which outperforms traditional saliency map models for explicit sequences prediction....|
|||...Background  Eye movement control and early visual attention have frequently been conflated, particularly within the computational saliency literature....|
|||...The term saliency map was coined in [31] in the context of covert, pre-attentive visual processing....|
|||...Since then, many saliency algorithms have been proposed, ranging from information theoretic principles [10], efficient coding [21], spectral analysis [26], or processing pipelines driven largely by em...|
|||...One schism which has formed within saliency research is whether the focus should be on locations or objects....|
|||...[3] and subsequent extensions to saliency [12]) and those which train and test saliency algorithms explicitly using object masks rather than fixation data (e.g., [40])....|
|||...While our goal differs from the standard manner in which saliency algorithms are applied and evaluated, we compare performance against them in order to emphasize the importance of our novel perspective....|
|||...The connection between explicit eye movement patterns and saliency maps was explored from a different direction by [54], in which a saliency algorithm independent of the visual input was based on stat...|
|||...Several efforts have been made to modulate a saliency map with a stochastic model, including a Lev y Flight [8], a mixture of Gaussians learned from human fixation data [55], and a Markov process [43]....|
|||...Outside of the saliency literature there are a number of eye movement control models....|
|||...These results certainly support the traditional approach to saliency evaluation which predominantly seeks to evaluate algorithms on prediction effectiveness over a static ground-truth fixation cloud, ...|
|||...d by the very short window of consumer attention to most advertisements, commercial applications of saliency analysis already include predicted sequences of the first several fixations [2], despite va...|
|||...Robotic modelling of joint attention has previously been improved through the application of saliency [65], and can likely be further improved with a more complete gaze model....|
|||...Although it has long been pointed out that saliency maps predict fixations with differing efficacy over time [53], static maps predicting a probabilistic distribution of the likelihood of any particul...|
|||...in the peripheral field and applying a CNN-based bottomup saliency algorithm such as SALICON in the central field....|
|||...Conspicuity map: The central and peripheral processing streams are recombined into a single map; this is our closest correlate to the original concept of a saliency map [31]....|
|||...porate, as part of the central field, a deep convolutional neural network (CNN), namely the SALICON saliency detection model [27]....|
|||...3187  Our choice of using a CNN-based saliency algorithm is motivated by the idea that such saliency models can be viewed as processing incoming visual information analogous to a full forward pass th...|
|||...SALICON was chosen due to the availability of an opensource implementation, but our formulation is agnostic to the specific saliency representations used in its construction....|
|||...Despite the fact that BMS significantly outperforms AIM on the CAT2000 dataset using the saliency metrics of the MIT Saliency Benchmark [11], when utilized in the peripheral component of STAR-FC both ...|
|||...Although virtually any saliency algorithm can be used within the proposed architecture, both the choice of saliency algorithms for the central/peripheral fields and strategy for combining them have a ...|
|||...The output of our fixation control model is not directly comparable to that of saliency algorithms designed to predict human fixations, as we output a sparse set of explicitly predicted locations rath...|
|||...In order to compare against the static maps which are the standard output of saliency algorithms, we sampled fixation sequences from the maps by applying an iterative WTA procedure....|
|||...This technique is consistent with previous work which samples loci of attention from saliency maps [28]....|
|||...An early criticism of saliency algorithms was that they fail to account for inherent motor biases in how humans move their eyes [54], and it has been suggested that this motor bias could implicitly co...|
|||...ution of saccade amplitude with our model than is found from the predictions of sampled from static saliency maps....|
|||...Results  We compare the performance of our STAR-FC with a range of established saliency models: AIM [10], BMS [66], GBVS [24], LDS [17], SALICON [27, 56], SSR [51], and VOCUS2 [20]....|
|||...Results for saliency models modulated by motoric distributions [8, 55, 43] were not available for comparison on the CAT2000 at the time of publication....|
|||...r o p o r P  0 <100  200  300  400  500  600  700  800  900  >1000  Amplitude (px)  (b) Traditional saliency algorithms, STAR-FC, and human distributions  Figure 4: Plots of the saccadic amplitude dis...|
|||...d with the human distribution shown with a dashed line); (b) shows the distributions of traditional saliency algorithms contrasted with the MCA variant of STAR-FC and the human distribution....|
|||... seen, STAR-FC is an order of magnitude closer to the human distribution than the closest competing saliency model....|
|||...In contrast to the STAR-FC amplitude distributions, virtually all static saliency maps are skewed in the opposite direction with distributions which are much flatter than those seen with human data....|
|||...T2000 dataset are shown for humans along with the MCA variant of STAR-FC and several representative saliency algorithms in Figure 5....|
|||...Likewise, the saliency algorithms with the closest spatial distribution to the human distribution do tend to have a greater propensity for shorter saccades (Figure 4)....|
|||...Additionally, it has been shown that saliency tends to correlate best with early fixations [53], and both saliency correlation and inter-observer consistency degrade largely after the first five fixations....|
|||...er of fixations  4  5  (c) Mean HD  Figure 7: A comparison of fixation prediction scores for static saliency maps and STAR-FC....|
|||...The best performing saliency algorithms (LDS [17] and GBVS [24]) likewise have correspondingly stronger biases toward predicting fixations near the image center....|
|||...This performance is significantly better than what can be achieved  by sequence sampling from static saliency maps (see Table 1)....|
|||...Cat2000: A large scale fixation dataset for boosting saliency research....|
|||...An information theoretic model of saliency and visual search....|
|||...Mit saliency benchmark....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Deep Gaze I: Boosting saliency prediction with feature maps trained on ImageNet....|
|||...Informationtheoretic model comparison unifies saliency metrics....|
|||...Spatially binned roc: A compre hensive saliency metric....|
|||...Visual saliency improves autonomous visual search....|
|||...RARE: A new bottom-up saliency model....|
|||...Opensalicon: An open source implementation of the salicon saliency model....|
||53 instances in total. (in cvpr2018)|
|51|Top-Down Visual Saliency Guided by Captions|...Top-down Visual Saliency Guided by Captions  Vasili Ramanishka Boston University  Abir Das  Boston University  Jianming Zhang Adobe Research  Kate Saenko  Boston University  vram@bu.edu  dasabir@bu.ed...|
|||...Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task....|
|||...In this paper, we propose Caption-Guided Visual Saliency to expose the regionto-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data...|
|||...It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design....|
|||...Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps....|
|||...Introduction  Neural saliency methods have recently emerged as an effective mechanism for top-down task-driven visual search [4, 31]....|
|||...Classification-based saliency methods are insufficient for such language-driven tasks as they are limited to isolated object labels and cannot handle textual queries....|
|||...wn Caption-Guided Visual Saliency approach that generates, for each word in a sentence, (a) spatial saliency in image and (b) spatiotemporal saliency in videos....|
|||...In this work, we address these questions by proposing a Caption-Guided Visual Saliency method that leverages deep captioning models to generate top-down saliency for both images and videos....|
|||...Thus, ours is the first attempt to analyze whether end-to-end visual captioning models can learn top-down saliency guided by linguistic descriptions without explicitly modeling saliency....|
|||...We estimate the saliency of each temporal frame and/or spatial region by computing the information gain it produces for generating the given word....|
|||...Related Work  Top-down neural saliency: Weak supervision in terms of class labels were used to compute the partial derivatives of CNN response with respect to input image regions to obtain class speci...|
|||...Approach  We propose a top-down saliency approach called Caption-Guided Visual Saliency which produces spatial and/or temporal saliency values (attention) for still images or videos based on captions....|
|||...The saliency map can be generated for a caption predicted by the base model, or for an arbitrary input sentence....|
|||...For each word in the sentence, we propose to compute the saliency value of each item in the input sequence by measuring the decrease in the probability of predicting that word based on observing just ...|
|||...Output saliency map for word t  0  ....|
|||...hm+t-1  normalize  Loss(p(cid:894)yt(cid:895),qi(cid:894)yt(cid:895))  Figure 2: Overview of our proposed top-down Caption-Guided Visual Saliency approach for temporal saliency in video....|
|||...We use an encoder-decoder model to produce temporal saliency values for each frame i and each word t in a given input sentence....|
|||..., vi)  Loss(t, i) = DKL(p(yt)kqi(yt))  (6)  With the above formulation we can easily derive topdown saliency for word w predicted at time t. We assume that the query sentence S has one-hot true distri...|
|||...As the approximate receptive field of each descriptor can be estimated1, we can define a saliency map for each word in the sentence by mapping Loss(t, i, w) to the center of the receptive field and an...|
|||...To obtain a saliency value eti, we negate the loss and linearly scale the resulting values to the [0, 1] interval,  eti = scale(Loss(t, i, w))  (8)  It is important to discriminate between the values ...|
|||...Finally, the saliency value for a group of words from the target sentence (e.g....|
|||...a noun phrase a small boy) is defined as sum of the corresponding saliency values for every word in the subsequence:  Loss({t1, ..., tq}, i) =  q  X  j=1  Loss(tj, i)....|
|||...(9)  Next we describe how this approach is applied to gener ate both temporal and spatial saliency in videos....|
|||...Given the word predicted at time t of the sentence, the relative saliency of the input frame vi can be computed as eti (Eq....|
|||..., tm) as saliency over the input sequence V = (v1, ....|
|||...putations are performed independently, we can create a batch of size r  m + m and calculate all the saliency values efficiently in one pass....|
|||...Image Saliency  With minimal changes the above model can be applied to generate saliency for images....|
|||...Generating a spatial saliency map can now be achieved by the same process as described for temporal saliency in the previous section....|
|||...Figure 3: Saliency maps (red to blue denotes high to low value) in Flickr30kentities generated for an arbitrary query sentence (shown below)....|
|||...Each row shows saliency map for different noun-phrases (shown at top-left corner) extracted from the query....|
|||...Figure 4: Saliency maps generated for a caption (shown below the image) predicted by the model....|
|||...Baseline random samples the point of maximum saliency uniformly from the whole image and Baseline center corresponds to always pointing to the center....|
|||...The best model in terms of the METEOR metric on the validation split of Flickr30k was selected for the evaluation of saliency as presented below....|
|||...Quantitative evaluation of saliency Given a pretrained model for image captioning, we test our method quantitatively using the pointing game strategy [31] and attention correctness metric [14]....|
|||...To generate saliency maps, we feed ground truth captions from the test split of Flickr30k into our model....|
|||...In pointing game evaluation, we obtain the maximum saliency point inside the image for each annotated noun phrase in each GT caption of Flickr30kEntities....|
|||...To get a saliency map for noun phrases which are comprised of multiple tokens from the sentence, we sum loss values before their normalizing them to the [0, 1]....|
|||...We compare to Baseline random, where the maximum saliency point is sampled uniformly from the whole image and to a much  stronger baseline denoted as Baseline center....|
|||...This baseline is designed to mimic the center bias present in consumer photos and assumes that the maximum saliency point is always at the center of the image....|
|||...For each nounphrase, the saliency map is generated by summing the responses for each token in the phrase and then renormalizing them....|
|||...The map is color coded where red shows the highest saliency while blue is the lowest....|
|||...The maximum saliency point is marked with an asterisk, while the ground truth boxes for the noun-phrases are shown in white....|
|||...3a, though the saliency for window is not pointing to the groundtruth window, it focuses its highest attention (asterisk) on the gate which looks very similar to a window....|
|||...4, the saliency map the predicted caption fof an image is shown....|
|||...5 shows examples of spatial and temporal saliency maps for videos from MSRVTT dataset with model-predicted sentences....|
|||...Darker gray indicates higher magnitude of temporal saliency for the word....|
|||...Conclusion  We proposed a top-down saliency approach guided by captions and demonstrated that it can be used to understand the complex decision processes in image and video captioning without making m...|
|||...Acknowledgements   a woman  is  skating  on  the  snow   a man  is  talking  about  a phone Figure 5: Spatial and temporal saliency maps in videos....|
|||...For each word, darker grey indicates higher relative saliency of the frame....|
|||...For better visualization, saliency values are not normalized but linearly mapped to the range [0, 1]....|
|||...Saliency visualizations in images Figures 3 and 4 show example saliency maps on images from Flickr30kEntities  This research was supported in part by NSF IIS1212928, DARPA, Adobe Research and a Google...|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||53 instances in total. (in cvpr2017)|
|52|Jiang_Saliency_Detection_via_2013_ICCV_paper|...University of Technology 2University of California at Merced  Abstract  In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model....|
|||...All bottom-up saliency methods rely on some prior knowledge about salient objects and backgrounds, such as contrast, compactness, etc....|
|||...Different saliency methods characterize the prior knowledge from different perspectives....|
|||...Fourier spectrum analysis has also been used to detect visual saliency [15, 13]....|
|||...[25] unify the contrast and saliency computation into a single high-dimensional Gaussian filtering framework....|
|||...[33] exploit background priors and geodesic distance for saliency detection....|
|||...[35] cast saliency detection into a graph-based ranking problem, which performs label propagation on a sparsely connected graph to characterize the overall differences between salient object and background....|
|||...In this work, we reconsider the properties of Markov random walks and their relationship with saliency detection....|
|||...Typically, saliency measure using the hitting time often highlights some particular small regions in objects or backgrounds....|
|||...In addition, equilibrium distribution based saliency models only highlight the boundaries of salient object while object interior still has low saliency value....|
|||...More importantly, the hitting time based saliency measure prefers to highlight the global rare regions and does not suppress the backgrounds very well, thereby decreasing the overall saliency of objec...|
|||...tion to simulate human attention, we exploit it to weigh the absorbed time, thereby suppressing the saliency of long-range background regions with homogeneous appearance....|
|||...Each kind of time is normalized as a saliency map respectively....|
|||...by these observations, we formulate saliency detection as a random walk problem in the absorbing Markov chain....|
|||...We further explore the effect of the equilibrium probability in saliency detection, and exploit it to regulate the absorbed time, thereby suppressing the saliency of this kind of regions....|
|||...Related Work  Previous works that simulate saliency detection in random walk model include [9, 14, 11, 31]....|
|||...[9] identify the saliency region based on the frequency of visits to each node at the equilibrium of the random walk....|
|||...Saliency Measure  Given an input image represented as a Markov chain and some background absorbing states, the saliency of each transient state is defined as the expected number of times  Figure 2....|
|||...Because we compute the full resolution saliency map, some virtual nodes are added to the graph as absorbing states, which is detailed in the next section....|
|||...However, as absorbing nodes for saliency detection are selected by the proposed algorithm, some of them may be incorrect....|
|||...Most saliency maps generated by the normalized absorbed time y are effective, but some background nodes near the image center may not be adequately suppressed when they are in long-range homogeneous r...|
|||...Consequently, the background regions near the image center possibly present comparative saliency with salient objects, thereby decreasing the contrast of objects and backgrounds in the resulted saliency maps....|
|||...To alleviate this problem, we update the saliency map by using a weighted absorbed time yw, which can be denoted as:  yw = N  u,  (10)  where u is the weighting column vector....|
|||...By the update processing, the saliency of the long-range homogeneous regions near the image center can be suppressed as Figure 3 illustrates....|
|||...However, if the kind of region belongs to salient object, its saliency will be also incorrectly suppressed....|
|||...We compare our method with fifteen state-of-the-art saliency detection algorithms: the IT [16], MZ [20], LC [37], GB [14], SR [15], AC [1], FT [2], SER [31], CA [27], RC [8], CB [17], SVO [7], SF [25]...|
|||...13 to indicate the quality of the saliency map....|
|||...First, we bisegment the saliency map using every threshold in the range [0 : 0.05 : 1], and compute precision and recall at each value of the threshold to plot the precision-recall curve....|
|||...Second, we compute the precision, recall and F-measure with an adaptive threshold proposed in [2], which is defined as twice the mean saliency of the image....|
|||...From top to down: input images, our saliency maps....|
|||...We find that object regions have great global contrast to background regions in good saliency maps, while it is not the case in the defective maps as the examples in Figure 3, which consistently conta...|
|||...Hence, given a saliency map, we first calculate its gray histogram g with ten bins, and then define a metric score to characterize this kind of tendency as follows:  10  score =  g(b)  min(b, (11  b))...|
|||...The larger score means that there are longer-range regions with mid-level saliency in the saliency map....|
|||...Algorithm 1 Saliency detection based on Markov random walk  Input: An image and required parameters....|
|||...12, then compute the saliency map S by Eq....|
|||...10 and 9; Output: the full resolution saliency map....|
|||...cision Recall Fmeasure  Ours  LR  CB  SVO RC  CA  SER GB  FT  IT  LC  SR  suppressed badly, the cut saliency map contains almost the entire image with low precision....|
|||...l the other methods, where the CB [17], SVO [7], RC [8] and CA [27] are top-performance methods for saliency detection in a recent benchmark study [5]....|
|||...Similar as previous works, we first fit a rectangle to the binary saliency map and then use the bounding box to compute precision, recall and F-measure....|
|||...We can see that the post-process step improves the precision and recall significantly over the solely saliency measure by absorbed time....|
|||...Fusing generic objectness  and visual saliency for salient object detection....|
|||...Visual saliency and attention as random walks on  complex networks....|
|||...Bottom-up saliency is a discriminant  process....|
|||...Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Conclusion  In this paper, we propose a bottom-up saliency detection algorithm by using the time property in an absorbing Markov chain....|
|||...Furthermore, we exploit the equilibrium distribution in ergodic Markov chain to weigh the absorbed time, thereby suppressing the saliency in long-range background regions....|
|||...Improved saliency detection based on superpixel clustering and saliency propagation....|
|||...Context-aware saliency detection....|
|||...Geodesic saliency using back ground priors....|
|||...Graph-regularized saliency detection with convex-hull-based center prior....|
|||...Top-down visual saliency via joint crf and  dictionary learning....|
||52 instances in total. (in iccv2013)|
|53|cvpr18-Learning to Promote Saliency Detectors|...  Background  The categories and appearance of salient objects vary from image to image, therefore, saliency detection is an image-specific task....|
|||...To solve this issue, we formulate a zero-shot learning problem to promote existing saliency detectors....|
|||...As a preprocessing step, saliency detection is appealing for many practical applications, such as content-ware video compression [37], image resizing [2], and image retrieval [10]....|
|||...A plethora of saliency models have been proposed in the past two decades to locate conspicuous image regions [4, 6, 5]....|
|||...Although much effort has been devoted and significant progress has been made, saliency detection remains a challenging open problem....|
|||...Conventional saliency detection methods usually utilize low-level features and heuristic priors which are not robust enough to discover salient objects in complex scenes, neither are capable of captur...|
|||...DNNs usually need to be trained on a large dataset, while training data for saliency detection is very limited....|
|||...First, saliency detection is an imagespecific task, and labels should be assigned to pixels depending on the image content....|
|||...The DNN can be trained end-to-end supervised by the loss between this saliency map and the ground truth....|
|||...When testing on an image, the saliency map of each image is obtained as in training, but using approximate salient/background regions detected by an existing method....|
|||...Related works  Generally, saliency detection methods can be categorized into two streams: top-down and bottom-up saliency....|
|||...Bottom-up (BU) saliency is stimuli-driven, where saliency is derived from contrast among visual stimuli....|
|||...Conventional bottom-up saliency detection methods often utilize low-level features and heuristic priors....|
|||...[12] formulate saliency detection via an absorbing Markov chain on an image graph model, where saliency of each region is defined as its absorbed time from boundary nodes....|
|||...[16] train CNNs with fully connected layers to predict saliency value of each superpixel, and to enhance the spatial coherence of their saliency results using a refinement method....|
|||...[18] propose a FCN trained under the multi-task learning framework for saliency detection....|
|||...[34] present a generic framework to aggregate multi-level convolutional features for saliency detection....|
|||...Top-down (TD) saliency aims at finding salient regions specified by a task, and is usually formulated as a supervised learning problem....|
|||...Yang and Yang [33] propose a supervised top-down saliency model that jointly learns a Conditional Random Field (CRF) and a discriminative dictionary....|
|||...[9] introduced a top-down saliency algorithm by selecting discriminant features from a pre-defined filter bank....|
|||...Integration of TD and BU saliency has been exploited by some methods....|
|||...For instance, Borji [3] combines lowlevel features and saliency maps of previous bottom-up models with top-down cognitive visual features to predict fixations....|
|||...samples generated using a bottom-up model to exploit the strengths of both bottom-up contrast-based saliency models and top-down learning methods....|
|||...[26] formulate the problem as top-down saliency detection specified by initial saliency maps, there are certain difference between the two....|
|||...Classification results of all pixels constitute a saliency map (i), of which loss between the ground truth is used to supervise the network....|
|||...During testing, the anchors are firstly produced according to an initial saliency map, here (e) is the initial saliency map....|
|||...Given anchors, the nearest neighbor classifier can produce a new saliency map (i), which is utilized to revise the initial map as in Eqn.3....|
|||...1646  In practice, the ground-truth will not be available during testing, and the anchors are produced according to a prior saliency map, which is inaccurate....|
|||...Therefore, we produce anchors using approximate salient/background regions Cmk selected according to the saliency map Y (0) m of an existing method....|
|||...In the t-th iteration (t > 0), the anchors are generated according to salient/background region Cmk selected by the prior saliency map Y (t) m ....|
|||...saliency value, constructing another saliency map Z (t) m ....|
|||...Then, the prior saliency map is updated with  Y (t+1)  m  =  t  t + 1  Y (t)  m +  1  t + 1  Z (t) m ,  (5)  where Y (t+1) is the prior saliency map which will be used for selecting salient and backgr...|
|||...Input  : The input image X, the initial saliency map  Y (0), the number of iterations T ....|
|||...Output: The promoted saliency map Y (T )....|
|||...Compute saliency value of each pixel according to Eqn.3 to constitute another saliency map Z (t) m ....|
|||...Update the prior saliency map: t+1 Z (t) Y (t+1)  t  t+1 Y (t) + 1  6 end  It is known that DNNs, which typically consist of many parameters, have to be trained on large datasets to obtain good performance....|
|||...For tasks where training data is scarce, such as saliency detection, revising a DNN that has been pre-trained on image classification datasets is the most viable option....|
|||...Given a saliency map whose intensities are in the range of 0 and 1, a series of binary maps can be produced by thresholding the saliency map with different values in [0, 1]....|
|||...Also as suggested in [1], we use twice the mean value of the saliency maps as the threshold to generate binary maps for computing the F-measure....|
|||...As complementary to PR curves, mean absolute error (MAE) is used to quantitatively measure the average difference between the saliency map S and the ground truth map G:  MAE =  1 H  H  X  i=1   Si  Gi ....|
|||...MAE indicates how similar a saliency map is compared to the ground truth....|
|||...We apply our method to promote the performance of each baseline method, by using its predicted saliency maps to generate initial anchors in Eqn.3....|
|||...Figure 7 shows a visual comparison of saliency maps produced by some state-of-the-art methods and the promoted ones by our method....|
|||...It can be seen that the saliency maps produced by our methods highlight salient regions that are missed by the baselines....|
|||...Boosting bottom-up and top-down visual features for saliency estimation....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Visual saliency based on multiscale deep features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Hierarchical saliency detection....|
|||...Top-down visual saliency via joint crf and dictionary learning....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||52 instances in total. (in cvpr2018)|
|54|Cheng_Efficient_Salient_Region_2013_ICCV_paper|...posed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection...|
|||...proving salient region detection, there are  Global cues: which enable the assignment of comparable saliency values across similar image regions and which are preferred to local cues [2, 14, 16, 26, 3...|
|||... Image abstraction: where an image is decomposed into perceptually homogeneous element, a process which abstracts out unnecessary detail and which is important for high quality saliency detection [42]....|
|||...In this paper, we propose a novel soft image abstraction approach that captures large scale perceptually homogeneous elements, thus enabling effective estimation of global saliency cues....|
|||...This allows the subsequent global saliency cues to uniformly highlight entire salient object regions....|
|||...and CSD) significantly outperforms existing 18 alternate approaches, and the final Global Cues (GC) saliency map reduces the mean absolute error by 25.2% compared to the previous best results (see Fig...|
|||...Insights from psycho-visual research have influenced computational saliency detection methods, resulting in significant improvements in performance [5]....|
|||...A large number of methods have been proposed to extend this method, including the fuzzy growing method by Ma and Zhang [37], and graph-based visual saliency detection by Harel et al....|
|||...[47] estimate saliency over the whole image relative to a large dictionary of images....|
|||...However, due to the use of image segments, saliency cues like spatial distribution cannot be easily formulated....|
|||...They used superpixels to abstract the image into perceptually uniform regions and efficient N-D Gaussian filtering to estimate global saliency cues....|
|||...We will discuss how our global homogeneous components representation benefits global saliency cue estimation in 4....|
|||...The 0th layer contains all the image pixels, thus allowing us to generate full resolution saliency maps....|
|||...When estimating global saliency cues in 4, we mainly work at the higher layer when feasible in order to allow large scale perceptually homogenous elements to receive similar saliency values, and to sp...|
|||...Since only a few analysis or assignment steps in our global saliency cue estimation algorithm will go to the bottom layer, finding full resolution saliency maps only requires a computational complexit...|
|||...Efficient Global Saliency Cues Estimation 4.1....|
|||...Global uniqueness (GU)  Visual uniqueness in terms of high contrast to other image regions is believed to be the most important indicator of low-level visual saliency [21, 41]....|
|||...To further incorporate the important spatial overlap correlation, the uniqueness based saliency of GMM components belonging to the  same cluster are finally averaged, to encourage semantically correla...|
|||...Color spatial distribution (CSD)  While saliency implies uniqueness, the opposite might not always be true [33, 42]....|
|||...A spatially compact distribution is another important saliency indicator which is an important complementary cue to contrast [25, 36]....|
|||...In the implementation, we set the saliency value for each histogram color as C p(C Ib)S(C)....|
|||...The saliency of each pixel is efS(Ib) = ficiently assigned using the indexing scheme between pixels and histogram bins as discussed above....|
|||...Saliency cues integration  The global saliency cues estimation efficiently produces two different saliency maps, where each is a complementary  1532 1532  (b) IT[30]  (f) CA[25]  (g) RC[16]  (h) SF[4...|
|||...Visual comparison of previous approaches to our two saliency cues (GU and CSD), final results (GC), and ground truth (GT)....|
|||...Here we compare with visual attention measure (IT), fuzzy growing (MZ), spectral residual saliency (SR), graph based saliency (GB), context aware saliency (CA), region contrast saliency (RC), and sali...|
|||...As also discussed in [26], combining individual saliency maps using weights may not be a good choice, since better individual saliency maps may become worse after they are combined with others....|
|||...We automatically select between the two saliency maps as a whole to integrate the two cues and generate a final saliency map according to the compactness measure in [26], which uses the compact assump...|
|||...This is achieved by considering the saliency maps as a probability distribution function and evaluating their spatial variance using Equ....|
|||...(GB[27], SR[28], CA[25], AIM[8], IM[39], MSS[3], SEG[43], SeR[45], SUN[52], SWD[20]), and iii) from saliency maps provided by [16] (IT[30], MZ[37], LC[50])....|
|||...Statistical comparison with 18 alternative saliency detection methods using all the 1000 images from the largest public available benchmark [2] with pixel accuracy saliency region annotation: (a) aver...|
|||...These measures favors methods which successfully assign saliency to salient pixels but fail to detect non-salient regions over methods that successfully do the opposite....|
|||...The GC saliency map is better at uniformly highlighting the entire salient object region but its precision recall values are worse....|
|||...However, a boosting of saliency values could easily result in the boosting of low saliency values related to background (see the small middle left regions in Fig....|
|||...Example saliency detection results to demonstrate the limitation of precision recall analysis....|
|||...When using precision recall analysis, the saliency map in (c) continually achieves near 100% precision for a wide range of recall values, while the saliency map in (d) performs worse because of the sm...|
|||...continuous saliency map S and the binary ground truth G for all image pixels Ix, defined as:  (cid:5)  x  M AE =  1  I    S(Ix)  G(Ix) ,  (9)  where  I  is the number of image pixels....|
|||...5(c) shows that our individual global saliency cues (GU and CSD) already outperform existing methods in terms of MAE, which provides a better estimate of dissimilarity between the saliency map and gro...|
|||...Our final GC saliency maps successfully reduces the MAE by 25% compared to the previous best reported result (SF[42])....|
|||...Average time taken to compute a saliency map for images in the benchmark [2] (most images have resolution 300  400)....|
|||...Moreover, in some application scenarios the quality of the weighted, continuous saliency maps may be of higher importance than the binary masks [42]....|
|||...The two saliency cues based on our abstract global representation already significantly outperform existing methods, while still maintaining faster running times....|
|||...We believe that investigating more sophisticated methods to integrate these complementary saliency cues would be beneficial....|
|||...It would be also interesting to investigate other saliency cues using this representation, e.g....|
|||...We currently only test our algorithm on the most widely used benchmark [2] for saliency region detection so that comparison with other methods are straight  forward....|
|||...ntains images with non-ambiguous salient objects, we argue that efficiently and effectively finding saliency object region for such images is already very important for many important computer graphic...|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Salientshape: Group saliency in image collections....|
|||...Does luminance-constrast contribute to a saliency map for overt visual attention?...|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Hierarchical saliency detec tion....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
||52 instances in total. (in iccv2013)|
|55|Zhang_Supervision_by_Fusion_ICCV_2017_paper|...Although training such deep saliency models can significantly improve the detection performance, it requires large-scale manual supervision in the form of pixellevel human annotation, which is highly ...|
|||...The key insight is supervision by fusion, i.e., generating useful supervisory signals from the fusion process of weak but fast unsupervised saliency models....|
|||...Previous Works  The salient object detection approaches proposed in early ages mainly explored image saliency by evaluating the  Corresponding author....|
|||...[32] adopted a convolutional neural network (CNN) to predict saliency scores for each pixel in local context firstly and then refined the saliency score for each object proposal over the global view....|
|||...In [25], a coarse global prediction was generated by learning the global structured saliency cues firstly and then a hierarchical recurrent convolutional neural network was adopted to progressively in...|
|||...ted by the existing approaches, where the first row are the original images, the second row are the saliency maps obtained by the traditional salient object detector [12], and the third row are the sa...|
|||...Since collecting manual annotations is highly time-consuming and labor intensive, this paper makes the earliest attempt to train deep convolutional saliency model without using any human annotation....|
|||...Along this direction, one naive strategy is to adopt the saliency maps generated by an existing unsupervised salient object detector to provide the initial pseudo ground-truth, and then train the DNN-...|
|||...Training deep models under the generated pseudo ground-truth maps would inevitably lead the learner to build trivial feature representation and capture less informative saliency patterns....|
|||...rvision by fusion strategy: generating reliable supervisory signals from the fusion process of weak saliency models....|
|||...Moreover, some modern fusion models can not only integrate the weak saliency models to obtain stronger saliency prediction but also automatically infer the reliabilities for each weak saliency model u...|
|||...ntioned problems can be addressed in the following ways:  Firstly, instead of directly adopting the saliency maps obtained from one weak saliency model to provide the pseu 4049  Figure 2....|
|||...do ground-truth for the training of the DNN-salient object detector, we propose to fuse the saliency maps of multiple weak but fast saliency models to obtain stronger pseudo supervision (both in the i...|
|||...As such saliency maps can be extracted in parallel, which is efficient, and the fusion process itself is not time-consuming2, this strategy tends to be a cost-effective way to improve the unsupervised...|
|||...ess will again be used to update the difficulties of the training samples based on the current weak saliency predictions, which essentially forms the learning curriculum dynamically....|
|||...Specifically, we first use some fast unsupervised saliency models to extract the weak saliency maps of each training image (the blue blocks in Fig....|
|||...Finally, the obtained deep salient object detector is used to update the weak saliency map collection for the next learning stage....|
|||...2) We reveal the insight of supervision by fusion, i.e., generating reliable supervisory signals from the fusion process of weak saliency models in iterative learning stages....|
|||... N ], we use three unsupervised salient object detectors [29, 45, 44]3 to generate the initial weak saliency maps {WSMm n }, m  [1, M ], M = 3 due to their efficiency....|
|||...The saliency maps generated by the method that inferred as having the lowest prediction reliability (in inter-image fusion) are then replaced by  3Here we choose to use three unsupervised salient obje...|
|||...4050  the saliency maps generated by the learnt deep salient object detector, which forms the new weak saliency maps for guiding the learning in the next iteration....|
|||...Based on our observation, after 4 to 5 learning iterations, the weak saliency maps tend to converge to a single map, and the complementarity among them becomes weak....|
|||...Intra-Image Fusion  n }M  Given each training image In and the corresponding weak saliency maps {WSMm m=1, the goal of intra-image fusion is to infer the superpixel-level reliability of each weak sali...|
|||...For each weak saliency map, the weak saliency value of each superpixel region sm n,i is the mean value of the pixels residing in the superpixel....|
|||...Denoting the average saliency value for each superpixel region as asvn,i = 1 n,i  asvn,i /asvn,i to reflect the agreement between each weak saliency prediction and the average saliency value....|
|||...m  n,i = zn,i am  n , bn,i) =  1  1 + eam  n bn,i  ,  (2)  where zn,i indicates the underlying true saliency label of the i-th superpixel, bn,i  [0, ) and am n  (, +)....|
|||...Given the observed weak saliency labels {lm n,i}, GLAD infers the  4The intra-image fusion can also perform on pixel-level, while this pa per uses superpixels for pursuing lower computational cost....|
|||...Inter-Image Fusion  Different from intra-image fusion, inter-image fusion integrates the weak saliency maps from the entire training image collection instead of the content of each single image, and t...|
|||...Specifically, given the training image collection {In} and the weak saliency map collection {WSMm n }, we first calculate the average saliency map {ASMn} of each image and the distance {m n } between ...|
|||...ers  0,  , T =  1  M N  N  M  Xn=1  Xm=1  m n ,  (5)  which reflects the agreement between the weak saliency map and the corresponding average saliency map....|
|||...yer with sigmoid activation function and 28  28  1 nodes is deployed, which can generate the coarse saliency prediction....|
|||...The output is the 1-channel refined saliency map with the same size of the input feature maps....|
|||...ge-level fusion map IFMn, respectively, (In W)d  [0, 1] indicates the d-th element of the predicted saliency map (In W), r() is the squared l2-norm, and  is the weight decay factor, d indicates the pi...|
|||...Some visualization examples of the saliency maps obtained by the proposed unsupervised learning framework as well as the other supervised state-of-the-art methods....|
|||...Specifically, the PR curve reflects the mean precision and recall of saliency map 2  P recision+Recall  s at different thresholds, while AP score is obtained by accumulating the area of the PR curve....|
|||...ulated by F = (1+2)P recisionRecall , where P recision and Recall are obtained using twice the mean saliency value of saliency maps as the threshold and 2 is set to 0.3 according to [25, 38]....|
|||...The MAE is the average pixel-wise difference between the predicted saliency map and the corresponding ground truth....|
|||...The SOV [19] is the intersectionover-union overlap between the ground truth mask and the predicted saliency map binarized by using the same adaptive threshold as during the calculation of F-measure....|
|||...Comparison with other state-of-the-art methods on four saliency detection datasets in terms of the PR curve....|
|||...Comparison with other state-of-the-art methods on four saliency detection datasets in terms of AP, F, SOV (higher values indicate better results), and MAE (lower values indicate better results)....|
|||...The methods listed in the bottom block are a number of state-of-the-art supervised deep saliency models, which were trained on the same image data with our approach but with additional human annotation....|
|||...eds around 31 seconds per-image, while our approach only needs around 0.7 second to obtain the weak saliency maps for each training image....|
|||...Evaluation of the supervisory signals on four saliency detection datasets in terms of AP, F, SOV (higher values indicate better results), and MAE (lower values indicate better results)....|
|||...Complex event detection using semantic saliency and nearly-isotonic svm....|
|||...Deep saliency with encoded In ICCV,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...A weighted sparse coding frame work for saliency detection....|
|||...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
|||...Revealing event saliency in unconstrained video collection....|
||52 instances in total. (in iccv2017)|
|56|Zhang_Amulet_Aggregating_Multi-Level_ICCV_2017_paper|...Then it adaptively learns to combine these feature maps at each resolution and predict saliency maps with the combined features....|
|||...As a preprocessing step in computer vision, saliency detection has shown a great success in ranges of visual applications, e.g....|
|||...Representative methods have set the benchmark on several saliency detection datasets....|
|||...Motivated by these achievements, several attempts to utilize high-level features of FCNs, have been performed and delivered superior performance in predicting saliency maps [20, 21, 27, 41, 50]....|
|||...From above discussions, we note that 1) how to simultaneously utilize multi-level potential saliency cues, 2) how to conveniently find the optimal multi-level feature aggregation strategy, and 3) how ...|
|||...ggregation network, dubbed AmuletNet, which utilizes convolutional features from multiple levels as saliency cues for salient object detection....|
|||...AmuletNet integrates multi-level features into multiple resolutions, learns to combine these features at each resolution and predicts saliency maps in a recursive manner....|
|||...[21] predict the saliency degree of each superpixel by taking multi-scale features in multiple generic CNNs....|
|||...[50] also predict the saliency degree of each superpixel by taking global and local context into account, and detect salient objects in a multi-context deep CNN....|
|||...[27] propose a deep hierarchical saliency network to learn enough global structures and progressively refine the details of saliency maps step by step via integrating local context information....|
|||...[22] design a pixel-level fully convolutional stream and a segment-level spatial pooling stream to produce pixel-level saliency predictions....|
|||...[44] develop deep recurrent FCNs to incorporate the coarse predictions as saliency priors and stage-wisely refine the generated predictions....|
|||...-level feature aggregation approach based on deep FCNs, and show that beyond refining the predicted saliency map, the approach can also jointly learn to preserve object boundaries....|
|||...Finally, boundary preserved refinements (BPRs) are used to refine the predicted saliency maps....|
|||...The final saliency map is the fused output of multiple predicted saliency maps....|
|||...In the end, we construct saliency inference based on the multi-level predictions of the proposed Amulet....|
|||...of four components: multi-level feature extraction, resolution-based feature integration, recursive saliency map prediction and boundary preserved refinement....|
|||...The four main components are jointly trained to optimize the output saliency detection quality....|
|||...Recursive saliency map prediction....|
|||...The integrated feature maps already contains various saliency cues, so we can use them to predict the saliency map....|
|||...A direct method is to deconvolute the integrated feature maps at each level into the size of the input image, and add a new convolutional layer to produce the predicted saliency map....|
|||...The proposed DRS includes saliency map prediction modules (SMP) and the deeply supervised learning mechanism [45]....|
|||...This way, the pixel-wise supervised information from ground truth will guide the recursive saliency map prediction at each level, making the SMPs be able to propagate fine details back to the predicti...|
|||...Based on the boundary preserved refinements Pb, a additional convolutional layer is applied and learned to produce the fusion saliency prediction (FSP) as the final output....|
|||...For saliency inference, we can simply use the fused prediction as our final saliency map....|
|||...However, saliency inference emphasize the contrast between foreground and background....|
|||...Thus this dataset is more difficult and challenging, and provides more space of improvement for related research in saliency detection....|
|||...Both the training and test set contain very challenging scenarios for saliency detection....|
|||...The precision and recall are computed by thresholding the predicted saliency map, and comparing the binary map with the ground truth....|
|||...The PR curve of a dataset demonstrates the mean precision and recall of saliency maps at different thresholds....|
|||...We report the performance when each saliency map is binarized with an image-dependent threshold....|
|||...The threshold is determined to be twice the mean saliency of the image:  T =  2  W  H  W  H  (cid:3)x=1  (cid:3)y=1  S(x, y),  (12)  where W and H are width and height of an image, S(x, y) is the sali...|
|||...The above overlapping-based evaluations usually give higher score to methods which assign high saliency score to salient pixel correctly....|
|||...The MAE evaluates the saliency detection accuracy by  M AE =  1  W  H  W  H  (cid:3)x=1  (cid:3)y=1   S(x, y)  G(x, y) ,  (13)  where G is the binary ground truth mask....|
|||...For fair comparison, we use either the implementations with recommended parameter settings or the saliency maps provided by the authors....|
|||...The F-measure and MAE of different saliency detection methods on six large-scale saliency detection datasets....|
|||...(2) Although only trained on the MSRA10K dataset, our model significantly outperforms other algorithms that pre-trained on specific saliency datasets, such as LEGS and RFCN on PASCAL-S, MDF on HKU-IS....|
|||...It can be seen that our method generates more accurate saliency maps in various challenging cases, e.g., low contrast between the objects and backgrounds (the first two rows), objects near the image b...|
|||...Whats more, with our BPR component, our saliency maps provide more accurate boundaries of salient objects (the 1, 3, 4, 6 rows)....|
|||...The results suggest that features of all levels are helpful for saliency detection, and with the increment of resolutions, our approach gradually achieves  208  (a)  (b)  (c)  (d)  (e)  (f)  (g)  (h)...|
|||...Comparison of saliency maps....|
|||...Our framework can integrate multi-level feature maps into multiple resolutions, learn to combine feature maps, and predict saliency maps with the integrated features....|
|||...Experiments demonstrate that our method performs favorably against state-of-the-art approaches in saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Biologically inspired object tracking using center-surround saliency mechanisms. IEEE TPAMI, 35(3):541554, 2013....|
|||...Regionbased saliency detection and its application in object recognition....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierarchical saliency detec tion....|
||51 instances in total. (in iccv2017)|
|57|Kruthiventi_Saliency_Unified_A_CVPR_2016_paper|...In addition, our network captures saliency at multiple scales via inceptionstyle convolution blocks....|
|||...Computational models for visual saliency often aim to solve one of the two problems Predict the locations where an observer will fixate while free-viewing an image; Detect and segment the objects in a...|
|||...Nevertheless, only a handful of the works in visual saliency attempt to solve these two problems together [9]....|
|||...Early approaches for modeling saliency were driven by manually crafting features [10, 11] for over-segmented image regions and estimating their saliency using machine learning or optimization methods....|
|||...Our network has a branched architecture, where the shared layers, common to both the tasks, are designed to extract the crucial factors for saliency such as object level semantics and global context....|
|||...Related Work  In this section, we discuss a few important works in the  areas of visual saliency and deep convolutional networks....|
|||...[2] considers low-level features such as color and edge orientation at multiple scales which are combined using a neural network to predict saliency maps....|
|||...[12] attributed saliency to image patches using the criterion of maximizing the selfinformation derived from color based features....|
|||...[13], obtained pixel saliency values by calculating equilibrium distribution of Markov chains constructed over image maps generated from low level features....|
|||...While early saliency works were primarily aimed towards generating saliency maps for predicting eye fixation locations, the works by Liu et al....|
|||...[10] obtained an abstract image representation having homogeneous regions by removing unnecessary details and assigned saliency scores based on the factors of a regions uniqueness and spatial distribution....|
|||...Segmentation by assigning saliency scores to over-segmented image regions (super-pixels) using various priors (background [18], objectness [19]) has been another popular approach....|
|||...[9] proposed a salient object segmentation model which used the saliency maps from existing fixation prediction algorithms to classify image regions marked by an object proposal algorithm as a salient object....|
|||...The output features from the two networks were concatenated for determining the saliency of the corresponding superpixel....|
|||...The multi-scale aspects of saliency are captured through convolutional kernels of different sizes operating in parallel....|
|||...Capturing Global Context: Saliency is the distinctive quality of an entity which makes it stand out from its neighbors and captures our immediate attention [30]....|
|||...Predicting Eye Fixations  The second task of predicting eye fixation saliency maps requires the model to estimate the saliency score for every pixel in a given test image....|
|||...The ground-truth saliency map for this task is generated by blurring the observer fixation locations on the image with a Gaussian kernel of a constant variance [8]....|
|||...These saliency maps generally tend to be blurry, and do not have sharp boundaries unlike the groundtruth of  Kernel Stride Holes Output Size  1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1  1  ...|
|||...The assignment of saliency scores requires characterizing local regions in an image with semantic features while incorporating the global context of the entire scene [29]....|
|||...The layers in CONV-6 block, owing to their large receptive fields (21  21), can provide contextually rich semantic features necessary for estimating the saliency score of local image regions....|
|||...al size 3  3, which is followed by a 1  1 convolution layer (Fix.Final) for estimating the fixation saliency map....|
|||...Refining Saliency Maps  Our network predicts the saliency maps at a sub-sampled image resolution....|
|||...resolution of 1/8 times the original  5784  Ground-truth saliency maps for eye fixations are usually smooth....|
|||...Hence, we directly interpolate the networks fixation saliency output, using bi-cubic interpolation to obtain the final saliency map....|
|||...The unary costs for a node to take the labels salient and background are defined using the networks object saliency map prediction....|
|||...The object saliency map is bicubic interpolated to the original image resolution and transformed using a sigmoid activation to obtain a pixel-level saliency scores....|
|||...The additive inverse of this object saliency map is taken to be the unary cost for the salient label....|
|||...The authors of [38] show that the mousecontingent saliency annotations strongly correlate with actual eye-tracker annotations....|
|||...The object saliency maps of other state-of-the-art methods and Proposed (CNN) are thresholded such that their F w   values are maximized....|
|||...Earth Movers Distance (EMD), Normalized Scanpath Saliency (NSS) and the shuffled-Area Under Curve (s-AUC) were used for evaluating the performance on eye-fixation prediction....|
|||...5.2.1 Salient Object Segmentation  Mean Absolute Error (MAE) : MAE is computed as the mean of pixel-wise absolute difference between the continuous object saliency map and the binary ground-truth ....|
|||...For binarizing the object saliency map to obtain the salient object segmentation, we follow the procedure described in [45]....|
|||...Initially, the saliency map is binarized by thresholding at various intermediate values in the range of [0 255] and the F w  is computed for each of them....|
|||...redicting Eye Fixations  Earth Movers Distance (EMD) : EMD considers the ground-truth and predicted saliency maps to be two probability distributions and measures the cost of transforming one distribu...|
|||...lized Scanpath Saliency is the average of the response values at eye fixation locations in a models saliency map, normalized to have zero mean and unit standard deviation....|
|||...ives vs. false positives during the binary classification of fixation and non-fixation points using saliency map at various thresholds....|
|||...LSUN Saliency Challenge 2015 SALICON  Method  s-AUC  CC AUC-Borji NSS  Proposed  JuntingNet [49]  0.76 0.67  0.78 0.60  0.88 0.83  2.61    Table 4....|
|||...Our network captures the global context, which is crucial for saliency, through layers with large receptive fields and handles multi-scale aspects of saliency using inception modules....|
|||...[4] D. Gao and N. Vasconcelos, Discriminant saliency for vi sual recognition from cluttered scenes, in NIPS, 2004....|
|||...[5] V. Mahadevan and N. Vasconcelos, Saliency-based discrim [25] R. Zhao, W. Ouyang, H. Li, and X. Wang, Saliency detec inant tracking, in CVPR, 2009.  tion by multi-context deep learning, in CVPR, 2015....|
|||...[7] E. Niebur, Saliency map, Scholarpedia, vol....|
|||...[12] N. Bruce and J. Tsotsos, Saliency based on information  maximization, in NIPS, 2005....|
|||...[15] A. Borji, Boosting bottom-up and top-down visual features  for saliency estimation, in CVPR, 2012....|
|||...[18] C. Sheth and R. V. Babu, Object saliency using a back ground prior, in ICASSP, 2016....|
|||...[33] G. Li and Y. Yu, Visual saliency based on multiscale deep  features, arXiv preprint arXiv:1503.08663, 2015....|
|||...[42] Q. Yan, L. Xu, J. Shi, and J. Jia, Hierarchical saliency de tection, in CVPR, 2013....|
|||...[47] J. Zhang and S. Sclaroff, Saliency detection: A boolean  map approach, in ICCV, 2013....|
|||...[48] E. Vig, M. Dorr, and D. Cox, Large-scale optimization of hierarchical features for saliency prediction in natural images, in CVPR, 2014....|
|||...[49] J. Pan and X. Gir o-i Nieto, End-to-end convolutional net work for saliency prediction, arXiv:1507.01422, 2015....|
|||...[50] J. Pan, K. McGuinness, E. Sayrol, N. OConnor, and X. Giro-i Nieto, Shallow and deep convolutional networks for saliency prediction, in CVPR, 2016....|
||51 instances in total. (in cvpr2016)|
|58|Learning to Detect Salient Objects With Image-Level Supervision|...In the second stage, FIN is fine-tuned with its predicted saliency maps as ground truth....|
|||... recent surge of interests in training DNNs using samples with accurate pixel-level annotations for saliency detection [57, 26, 49]....|
|||...We propose to use image-level tags as weak supervision to learn to predict pixel-level saliency maps (right panel)....|
|||...To alleviate the need of large scale pixel-wise annotations, we explore the weak supervision of image-level tags to train saliency detectors....|
|||...bject categories in the image and is irresponsible of the object locations (Figure 1 left), whereas saliency detection aims to highlight the full extend of foreground objects and neglects their catego...|
|||...On the one hand, saliency detection provides object candidates, enabling more accurate category classification....|
|||...In light of the above observations, we propose a new weakly supervised learning method for saliency detection using image-level supervisions only....|
|||...ect regions, which generalizes well to unseen categories, and provides an initial estimation of the saliency map....|
|||...In the second stage, the self-training alternates between estimating ground truth saliency maps and training the FIN using estimated ground truth....|
|||...To obtain more accurate ground truth estimation, we refine the saliency maps predicted by the FIN with an iterative Conditional Random Field (CRF)....|
|||...Firstly, we provide a new paradigm for learning saliency detectors with weak supervision, which requires less annotation efforts and allows the usage of existing large scale data set with only imagele...|
|||... global smooth pooling layer and foreground inference network, which enable the deep model to infer saliency maps by leveraging image-level tags and better generalize to previously unseen categories a...|
|||...Related Work  Fully Supervised Saliency Detection....|
|||...Many supervised algorithms, like CRFs [32], random forests [17, 19, 30],  SVMs [35], AdaBoost [60], DNNs [31, 25, 24], etc., have been successfully applied to saliency detection....|
|||...Recently, FCN based saliency methods [29, 49] have been proposed, with more competitive performance in terms of both accuracy and speed....|
|||...In comparison, our method predicts saliency maps by only forward passes....|
|||...Weakly Supervised Saliency Detection  A CNN for image-level tags prediction typically consists of a series of convolutional layers followed by several fully connected layers....|
|||...Second and fourth rows: refined saliency maps (Section 3.4) based on the foreground maps....|
|||...For saliency detection, we do not pay special attentions to the object category, and only aim to discover salient object regions of all categories....|
|||...To obtain such a category-agnostic saliency map, one can simply average the category score map across all the channels....|
|||...In consequence, the generated saliency maps either suffer from background noises (Figure 3 (a)) or fail to uniformly highlight object regions (Figure 3 (b-d))....|
|||...It takes the image X as input and predicts a subsampled saliency map F = [Fi,j]nn....|
|||...Before score aggregation, we mask each channel of the score map with the foreground saliency map:  Sk = Sk  F ,  (5)  where Sk denotes the k-th channel of score map S;  represents the element-wise mul...|
|||...In the second stage, the FIN (b,d) is trained for saliency prediction (g)....|
|||...Therefore, the object detection data set is more suitable for solving saliency co-occurrence problem [3]....|
|||...min L(l, s) + kf k1,  (6)  where f denotes the vectorized version of saliency map F ....|
|||...In contrast, we aim to produce accurate saliency maps with less background noise....|
|||...The FIN consists of a convolutional layer followed by a BN and a sigmoid layer, and infers a saliency map F (Figure 4 (d)), which is then used to mask the score map to obtain the masked score map (Fig...|
|||...tropy loss to measure prediction accuracy; The third term is the L1 regularization on the predicted saliency map f ; the last term represents weight decay;  and  are empirically set to be 5e-4 and 1e-...|
|||...The FIN has a stride of 16 pixels, leading to output saliency maps of 16  16....|
|||...Self(cid:173)training with Estimated Pixel(cid:173)level Labels  After pre-training, the coarse saliency maps generated by FIN can already capture the foreground regions....|
|||...In the second training stage, we refine the prediction by iterating between two steps: a) estimating ground truth saliency maps using the trained FIN, and b) finetuning FIN with the estimated ground truth....|
|||...According to the estimated saliency map F by FIN, we label superpixel zi as foreground (i = 1) if the mean saliency value of its pixels is larger than 0.5, or background (i = 0) otherwise....|
|||...To refine the saliency labels  = {1, 2, ....|
|||...After refinement, we assign the label of each superpixel to all its pixels to obtain a refined saliency map R.  141  SED  THUR  HKU-IS  DUTS  Figure 5....|
|||...cid:3) log(1  fi),  where r and f are vectorized version of estimated ground truth R and the output saliency map F of the extended FIN; ai = 1(fi > 0.5); i is pixel index;  is a weight parameter and f...|
|||...At test time, the extended FIN directly generates the saliency maps and no post-processing is required....|
|||...To our knowledge, DUTS is currently the largest saliency detection benchmark with the explicit training/test evaluation protocol....|
|||...We evaluate our Weakly Supervised Saliency (WSS) method on the test set of DUTS and 5 public data sets: SED [2], ECSSD [52], THUR [5], PASCAL-S [30] and HKU-IS [26]....|
|||...Note that most of the saliency detection data sets contain huge amounts of objects not belonging to the 200 training categories....|
|||...This confirms the previously discussed disadvantage (Section 3.2) of using the average score map as saliency estimation, and further proves the contribution of the FIN....|
|||...the first stage, two novel network designs, i.e., GSP and FIN, are proposed to estimate saliency maps through learning to predict image-level category labels....|
|||...Salientshape: group saliency in image collections....|
|||...Recurrent attentional net works for saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierarchical saliency detec tion....|
||50 instances in total. (in cvpr2017)|
|59|cvpr18-Gaze Prediction in Dynamic 360 Immersive Videos|...On the one hand, the saliency is related to both appearance and motion of the objects....|
|||...Considering that the saliency measured at different scales is different, we propose to compute saliency maps at different spatial scales: the sub-image patch centered at current gaze point, the sub-im...|
|||...Then we feed both the saliency maps and the corresponding images into a Convolutional Neural Network (CNN) for feature extraction....|
|||...ii) Previous saliency detection in 360 videos watches STILL scene, so they can directly collect the eye fixation of all participants at the same scene for generating groundtruth....|
|||...So it is extremely difficult to annotate ground-truth for saliency detection....|
|||...Saliency Detection  Tremendous efforts on saliency detection have been focused on predicting saliency map....|
|||...provide a comprehensive study on existing saliency detection schemes....|
|||...Though lots of work has been done for study saliency in image and video, saliency in VR is still in its primitive stage....|
|||...ation and get the saliency map....|
|||...Further, some videos are captured from a fixed camera view and some are shotted with a moving camera that would probably introduce more variance in eye fixation  https://github.com/xuyanyu-shh/VR-EyeT...|
|||...gnitude of optical flow with different scales; (d) and (e) show the coincidence of gaze points with saliency at different scales; (f) show the displacement between the gaze points at neighboring frame...|
|||...So we use the SalNet to calculate the spatial saliency map for each panorama image....|
|||...Then we rank the saliency of all pixels in descending order....|
|||...Based on the highest/lowest saliency value in each frame, we evenly divide these pixels into 10 bins....|
|||...People sometimes are attracted by those moving objects, so temporal saliency is also an important factor for gaze tracking....|
|||...On the one hand, gaze points are largely correlated with spatial saliency which can be inferred from image contents, and temporal saliency which can be inferred from the optical flow between neighbori...|
|||...Two neighboring frames characterize the motion information (optical flow), and the next frame provides contents for saliency characterization....|
|||...Specifically, our network consists of a Trajectory Encoder module and a Saliency Encoder module, and a Displacement Prediction module....|
|||...In other words, saliency provides an important cue for gaze prediction in future frames....|
|||...Thus we propose to incorporate the saliency prior regarding spatial saliency and temporal saliency which is characterized by optical flow features for gaze prediction....|
|||...However, the saliency level of the same object at different spatial scales are different....|
|||..., i) local saliency: the  saliency of local patch centered at current gaze point; ii) FOV saliency: the saliency of sub-image corresponding to current Field of View (FOV); and iii)Global saliency: the...|
|||...Rather than using saliency detection method to calculate the saliency, which is time-consuming, and usually cannot get very satisfactory saliency detection results because such a small image patch doe...|
|||...Further, Gaussian based local saliency is also on par with that of classical saliency detection based solution....|
|||...So it is necessary to take FOV saliency into consideration....|
|||...Specifically, we calculate the saliency map corresponding to the sub-image in FOV with SalNet [33] which is a state-ofthe-art saliency detection method....|
|||...So besides the local saliency and FOV saliency, global saliency is also necessary for understanding the scene....|
|||...Similar to FOV saliency, we also feed the panorama image into SalNet for spatial saliency detection, and use the optical flow calculated with the panorama images as a temporal saliency estimation....|
|||...Previous work [27] has demonstrated the effectiveness of a coarse-to-fine strategy for saliency detection, i.e....|
|||..., feeding an initial saliency result together with the original RGB image into a network for better saliency prediction....|
|||...Motivated by this work, we propose to concatenate the RGB images, all spatial and temporal saliency maps, and feed them into an Inception-ResNet-V2 [38] to extract saliency features for gaze prediction....|
|||...We denote z() represents the Inception V2 network, and denote Sp t+1 as all spatial and temporal saliency maps, and denote the saliency features as gp can be obtained as follows:  t+1, then gp  t+1  g...|
|||...We design the following baselines: i) saliency based method: we use the location corresponding to the highest saliency in FOV as gaze prediction; ii) optical flow based method: we use the location cor...|
|||...iii) saliency encoder: we feed all the saliency maps and RGB images in different scales into saliency encoder for gaze point prediction; iv) trajectory encoder only: we only feed the history gaze path...|
|||...Further, image saliency method and optical flow based method do not consider the interaction between spatial and temporal saliency, which both attract users visual attention....|
|||...Both saliency encoder baseline and trajectory encoder baseline do not take all factors related to trajectory prediction, thus corresponds to poor performance....|
|||...Evaluation of Different Components in Our  Mothed  Evaluation of the necessity of temporal saliency In order to validate the effectiveness of temporal saliency, we train a network without optical flow...|
|||...We compare the performance with/without temporal saliency in Fig....|
|||...We can see that the network with temporal saliency performs better than the one without temporal saliency....|
|||...This is because the network with temporal saliency takes the different importance of different moving objects into consideration....|
|||...Evaluation of the necessity of RGB images and saliency maps in the input of saliency encoder We propose to remove the RGB image and saliency maps in the inputs of saliency encoder in our framework....|
|||...Fig 5 (c) shows that after removing RGB images or saliency maps, the performance drops, this agrees with previous work [27] that the combination of initial saliency map and RGB image leads to better s...|
|||...Deep learning for saliency prediction in natural video....|
|||...A deep multi-level network for saliency prediction....|
|||...Temporal spectral residual:fast motion saliency detection....|
|||...Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. In Ieee,conf....|
|||...A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
||50 instances in total. (in cvpr2018)|
|60|Hongmei_Song_Pseudo_Pyramid_Deeper_ECCV_2018_paper|...Extensive experimental results show that our method outperforms previous video saliency models in a large margin, with a real-time speed of 20 fps on a single GPU....|
|||...1  Introduction  Video saliency detection aims at finding the most interesting parts in each video frame that mostly attract human attention....|
|||...Similar to visual saliency detection in static images, research on video saliency detection can also be divided into two categories, i.e., eye fixation prediction [41, 39] and salient object detection...|
|||...Most existing video saliency methods [8, 42, 43, 11], are built upon shallow, hand-crafted features (e.g., color, edge, etc....|
|||...Currently, only a few works [44, 24] on video saliency detection, based on deep learning, can be found in the literature....|
|||...[44] proposed a fully convolutional network (FCN) based video saliency model, as a very early attempt towards an end-to-end deep learning solution for this problem, achieves a speed of 2 fps....|
|||...To employ deep learning techniques for video saliency detection, two problems should be considered [44]....|
|||...Optical flow offers explicit motion information, but also incurs significant computational cost, which severely limits the applicability of current video saliency models....|
|||...A sufficiently large, densely labeled video saliency training data is desirable, but hard to obtain....|
|||...Second, we train our model with massive staticimage saliency data, in addition to video saliency data....|
|||...d video object segmentation as an example application task, we further show that the proposed video saliency model, equipped with a CRF segmentation module,  Pyramid Dilated Deeper ConvLSTM for Video...|
|||...These models generally achieve far better performance compared with traditional static saliency models [49, 45]....|
|||...Conventional video salient object detection methods [8, 9, 28, 42] extract spatial and temporal features separately and then integrate them together to generate a spatiotemporal saliency map....|
|||...As demonstrated in [42], video saliency models are able to offer valuable information for guiding video object segmentation....|
|||...4  H. Song, W. Wang, S. Zhao, J. Shen, K.-M. Lam  PDC Module  PDB-ConvLSTM Module  DB-ConvLSTM  ResNet  r=2  Concatenate     r=4     r=16  ...  ...  Concatenate     Loss     DB-ConvLSTM  ...  ...  Out...|
|||...he proposed video salient object detection model, which consists of two components, e.g., a spatial saliency learning module based on Pyramid Dilated Convolution (PDC) ( 3.1) and a spatiotemporal sali...|
|||...PDB-ConvLSTM takes the spatial features learnt from the PDC module as inputs, and outputs improved spatiotemporal saliency representations for final video salient object prediction ( 3.2)....|
|||...3.1 Spatial Saliency Learning via PDC Module  A typical CNN model is comprised of a stack of convolution layers, interleaved with non-linear downsampling operation (e.g., max pooling) and point-wise n...|
|||...on), thus the network is able to learn the importance of the scales automatically (such as learning saliency feature from a proper distance)....|
|||...T t=1 with T frames, we adopt PDC Module to produce a corresponding sequence of multi-scale spatial saliency features {Xt}T t=1....|
|||...Second, incorporating pyramid dilated convolutions into LSTM for learning saliency features in multi-scales....|
|||...inputs F  R60602048 of the PDC module are further concatenated for generating a multi-scale spatial saliency feature X  R60604096....|
|||...atures {Xt  R606032}T t=1, which will be further fed into the PDBConvLSTM module for spatiotemporal saliency prediction....|
|||...For each frame, the outputs of the two DB-ConvLSTM branches in the PDB-ConvLSTM module are further concatenated as a multi-scale spatiotemporal saliency feature with 60  60  64 dimensions....|
|||...Then the output features from the PDB-ConvLSTM module are fed into a 1  1 convolution layer with 1 channel and sigmoid activation for producing final saliency map....|
|||...The saliency map is upsampled into the original input frame size, i.e., 473  473, via bilinear interpolation....|
|||...For producing better saliency prediction and training the suggested model more efficiently, we propose here a fused loss function that accounts for multiple evaluation metrics, inspired by [40]....|
|||...d Deeper ConvLSTM for Video Salient Object Detection  9  and S  [0, 1]473473 denote the groundtruth saliency map and predicted saliency respectively, the overall loss L can be formulated as follows:  ...|
|||...This means we can utilize the massive static-image saliency data to let the network capture more appearances of the objects and the scenes....|
|||...rst, we pre-train the spatial-learning part (including PDC module and base network) using two image saliency datasets: MSRA10K [6], and DUTOMRON [49], and one video dataset: the training set of DAVIS ...|
|||...In this way, although the densely labelled video saliency data is scarce, our video saliency detection model still achieves good generalization performance for unseen videos....|
|||...The proposed videos saliency model is implemented using PYTHON, with the Caffe toolbox....|
|||...4.1 Performance on Video Salient Object Detection  We compared our model with 18 famous saliency methods, including 11 image salient object detection models: Amulet [51], SRM [36], UCF [52], DSS [16],...|
|||...Quantitative comparison against 18 saliency methods using PR curve on DAVIS [31], FBMS [2] and ViSal [43] datasets....|
|||...Quantitative comparison results against 18 saliency methods using MAE and maximum F-measure on DAVIS [31], FBMS [2] and ViSal [43]....|
|||...Qualitative comparison against other top-performing saliency methods with groundtruths on three example video sequences....|
|||...For better demonstrating the advantages of the proposed video saliency model, we extend our model for unsupervised video object segmentation and test it on DAVIS and FBMS datasets in segmentation settings....|
|||...Given an input frame It and corresponding saliency estimation St, we formulate the segmentation task as an energy function minimization problem....|
|||...Runtime comparison with 6 existing video saliency methods....|
|||...FCNS needs to calculate static saliency first....|
|||...The proposed model generates high-quality saliency maps with a real-time processing speed of 20 fps....|
|||...Fang, Y., Wang, Z., Lin, W., Fang, Z.: Video saliency incorporating spatiotemporal  cues and uncertainty weighting....|
|||...Lee, G., Tai, Y.W., Kim, J.: Deep saliency with encoded low level distance map  and high level features....|
|||...Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object  detection....|
|||...Liu, Z., Li, J., Ye, L., Sun, G., Shen, L.: Saliency detection for unconstrained videos using superpixel-level graph and spatiotemporal propagation....|
|||...Wang, L., Wang, L., Lu, H., Zhang, P., Xiang, R.: Saliency detection with recurrent  fully convolutional networks....|
|||...Wang, W., Shen, J., Shao, L.: Consistent video saliency using local gradient flow  optimization and global refinement....|
|||...Wang, W., Shen, J., Shao, L., Porikli, F.: Correspondence driven saliency transfer....|
|||...: Saliency detection via graph based manifold ranking....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional  features for accurate saliency detection....|
||50 instances in total. (in eccv2018)|
|61|cvpr18-Revisiting Salient Object Detection  Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects|...The solution presented in this paper solves this more general problem that considers relative rank, and we propose data and metrics suitable to measuring success in a relative object saliency landscape....|
|||...A novel deep learning solution is proposed based on a hierarchical representation of relative saliency and stage-wise refinement....|
|||...Such methods follow a multi-branch architecture where a CNN is used to extract semantic information across different levels of abstraction to generate an initial saliency prediction....|
|||...As an alternative to superpixels and object region proposals, other methods [26, 8, 37] predict saliency per-pixel by aggregating multi-level features....|
|||...[8] implement stage-wise short connections between shallow and deeper feature maps for more precise detection and inferred the final saliency map considering only middle layer features....|
|||...[37] combine multi-level features as cues to generate and recursively fine-tune multi-resolution saliency maps which are refined by boundary preserving refinement blocks and then fused to produce fina...|
|||...Liu and Han [24] propose a network that combines local contextual information step-bystep with a coarse saliency map....|
|||...[31] propose a recurrent fully convolutional network for saliency detection that includes priors to correct initial saliency detection errors....|
|||...This is followed by a detailed description of the stage-wise refinement network, and multi-stage saliency map fusion in sections 3.2 and section 3.3 respectively....|
|||...Then, we append a Stacked Convolutional Module (SCM) to compute the coarse level saliency score for each pixel....|
|||...A fusion layer combines predictions from all stages to generate the final saliency map (S T m) of each refinement stage....|
|||...the convolution C. S t  is the coarse level NRSS for stage t that encapsulates different degrees of saliency for each pixel (akin to a prediction of the proportion of observers that might agree an obj...|
|||...The SCM consists of three convolutional layers for generating the desired saliency map....|
|||...arries out a sequence of operations to generate a refined NRSS that contributes to obtain a refined saliency map....|
|||...As a final stage, refined saliency maps generated by the SCMs are fused to obtain the overall saliency map....|
|||...However, how to effectively combine local  Previous saliency detection networks [32, 24] proposed refinement across different levels by directly integrating representations from earlier features....|
|||...Note that one can interpret S t  as the predicted saliency map in the decoding process, but our model forces the channel dimension to be the same as the number of participants involved in labeling sal...|
|||...The refinement unit takes the gated feature map Gt generated  7144       and f t+1  to generate the refined saliency map S t+1  by the gate unit [12] as a second input....|
|||...Unlike other approaches, we apply supervision for both of the refined NRSS and the refined saliency map....|
|||...The procedure for obtaining the refined NRSS and the refined saliency map for all stages is identical....|
|||...Multi(cid:173)Stage Saliency Map Fusion  Predicted saliency maps at different stages of the refinement units are capable of finding the location of salient regions with increasingly sharper boundaries....|
|||...facilitate interaction, we add a fusion layer at the end of network that concatenates the predicted saliency maps of different stages, f m. Then, we apply a 1  1 resulting in a fused feature map S con...|
|||...To address this problem, we propose to generate a set of stacked ground-truth maps that corresponds to different levels of saliency (defined by inter-observer agreement)....|
|||...Given a ground-truth saliency map Gm, we obtain a stack G of N ground-truth maps (Gi, Gi+1, ....., GN ) where each map Gi includes a binary indication that at least i observers judged an object to be ...|
|||...The stacked groundtruth saliency maps G provides better separation for multiple salient objects (see Eq....|
|||...the Network  Our proposed network produces a sequence of nested relative salience stacks (NRSS) and saliency maps at each stage of refinement; however, we are principally interested in the final fused...|
|||...Each stage of the network is encouraged to repeatedly produce NRSS and a saliency map with increasingly finer details by leveraging preceding NRSS representations....|
|||...training image with ground-truth saliency map Gm  Rhw....|
|||...As described in section 3.4, we generate a stack of groundtruth saliency maps G  Rhw12....|
|||...We can summarize these operations as:  ) and saliency map St  S and t m, Gt   and Gt  , Gt  t  S (W ) =  1  2dN  d  N  X  X  i=1  z=1  (xi(z)  yi(z))2  t  Sm (W ) =  1 2d  d  X  i=1  (xi  yi)2  Lt  au...|
|||... aux(W )  (6)  where Lmas(W ) refers to the euclidean loss function computed on the final predicted saliency map S T m. We set t to 1 for all stages to balance the loss, which remains continuously dif...|
|||...For saliency inference, we can simply feed an image of arbitrary size to the network and use the fused prediction as our final saliency map....|
|||...Virtually all existing approaches for salient object segmentation or detection threshold the ground-truth saliency map to obtain a binary saliency map....|
|||...Since some of these rely on binary decisions, we threshold the groundtruth saliency map based on the number of participants that  7146  deem an object salient, resulting in 12 binary ground truth maps....|
|||...For each binary ground truth map, multiple thresholds of a predicted saliency map allow for calculation of the true positive rate (TPR), false positive rate (FPR), precision and recall, and correspond...|
|||...the average pixel-wise difference between the predicted saliency map and the binary ground-truth map that produces the minimum score....|
|||...ison, we build the evaluation code based on the publicly available code provided in [20] and we use saliency maps provided by authors of models compared against, or by running their pre-trained models...|
|||...Overall, this analysis hints at strengths of the proposed hierarchical stacked refinement strategy to provide a more accurate saliency map....|
|||...Rank order of a salient instance is obtained by averaging the degree of saliency within that instance mask....|
|||...mage  GT  RSDNet-R  AMULET [37]  UCF [38]  where  represents a particular instance of the predicted saliency map (ST m),  denotes total numbers of pixels  contains, and (xi, yi) refers to saliency sco...|
|||...Salient areas in the ground truth are captured in the variability across layers demonstrating the value of our stacking mechanism for saliency ranking....|
|||...Sometimes, the ground truth has multiple objects with the same degree of saliency (ties in participants agreeing) (see 1st row in Fig....|
|||...he second row) or when there is occlusion among two objects which have a relatively close degree of saliency as shown in the last row....|
|||...Central to the success of this approach, is how to represent relative saliency both in terms of ground truth, and in network in a manner that produces stable performance....|
|||...What do saliency models predict?...|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Hierarchical saliency detec tion....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||50 instances in total. (in cvpr2018)|
|62|Riche_Saliency_and_Human_2013_ICCV_paper|...s (UMONS) 20, Place du Parc, 7000, Mons, Belgium  {firstname.surname}@umons.ac.be  Abstract  Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency...|
|||...In this paper, on human eye fixations ,we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics....|
|||...As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance....|
|||...By outputing saliency maps that estimate the probability of each image area to grab our attention, those models allow to automatically predict the most relevant regions from images....|
|||...In 2011, Toets proposed in [27] to compare saliency mod els based on the Spearmans rank correlation coefficient....|
|||...Lemeur in [16] also reported about methods for comparing scanpaths and saliency maps....|
|||...Literature Review of Similarity Metrics  In this section, all the similarity metrics that have been used to assess saliency models are presented....|
|||...cond dimension, among those metrics, some are general metrics which were not specifically built for saliency models and are called common metrics, others are existing similarity measures which were ad...|
|||...Value-based metrics:  focus on saliency map  values at eye gaze positions  This first category of metrics compares the saliency am plitudes with the corresponding eye fixations maps....|
|||...2.1.1 Normalized Scanpath Saliency (NSS)  The Normalized Scanpath Saliency (NSS) metric was introduced in 2005 by Peeters and Itti [23]....|
|||...The idea is to quantify the saliency map values at the eye fixation locations and to normalize it with the saliency map variance:  2.1.3 Percentage of fixations into the salient region (Pf)  This metr...|
|||...First, saliency maps are thresholded at T = 0.8 where the saliency is normalized between 0 and 1....|
|||...Indeed, the NSS score should be decreased if the saliency map variance is important or if all values are globally similar (small difference between fixation values and mean) because it shows that the ...|
|||...Many authors like [24], [26] and [17] already used this metric to compare saliency maps with human eyes fixations....|
|||...The KL-Div is a measure of the information lost when the saliency maps probability distribution (called SM) is used to approximate the human eye fixation map probability distribution (called F M)....|
|||...where X is the set of all pixels of the saliency map SM, p is the location of one eye fixation and  SM  indicates the total number of pixels....|
|||...It computes the minimal cost to transform the probability distribution of the saliency maps SM into the the one of the human eye fixations F M.  EM D = (min fij  (cid:2)  i,j  s.t.fij  0,  fijdij)+ (c...|
|||...larity metric  The similarity metric [14] also uses the normalized probability distributions of the saliency map SM and human eye fixation map F M. The similarity is the sum of the minimum values at e...|
|||...First, fixations pixels were counted once and the same number of random pixels are extracted from the saliency map....|
|||...For one given threshold, saliency pixels can be treated as a classifier, with all points above threshold indicated as fixation and all points below threshold as background For any particular value of ...|
|||...The idea is that no saliency algorithm can perform better (on average) than the area under the ROC curve dictated by inter-subject variability for each image....|
|||...Finally, the AUC of the saliency map is normalized by this ideal AUC....|
|||...2.3.3 Borji implementation (AUC Borji)  In [5], Borji applied to saliency maps validation a suitable AUC metric called shuffled AUC....|
|||...In the shuffled AUC metric, saliency values and fixations from another image (instead of random) of the same dataset are taken into account....|
|||...This point is important because the AUROC scores can dramatically increase if a saliency map is weighted by a centred Gaussian....|
|||...Experimental Setup  The 12 metrics presented in section 2 have two inputs: the saliency map SM and the human fixation maps F M. The output is a similarity (or dissimilarity) score (scalar)....|
|||...The images are quite different with objects of interest of different sizes (small, medium and large) to avoid a size-based bias of the saliency models....|
|||...Saliency maps from 12 models  Twelve state-of-the-art models are used to obtain different saliency maps in this study....|
|||...They represent the updated version of the online available models from Borjis review paper [4] where models are sorted based on their mechanism to obtain saliency map....|
|||...So, we use a wide methodbased range of recently published saliency models....|
|||...SR [9], PFT [8], PQFT [8] and Achanta [1] use a spectral analysis approach to compute their saliency map....|
|||...For this purpose, the 12 models from section 3.2 produce a saliency map for each of the 235 images of the database and these saliency maps are compared with the corresponding human eye fixation map....|
|||...The third uses previous results to provide a fair assessment of the 12 saliency models....|
|||...This threshold means that only the rank of 2 or 3 couples of models can be inverted on the 12 models which means that the differences in terms of classification of the saliency models are really minor....|
|||...5 shows an assessment of each of the 12 saliency models with different metrics....|
|||...One can see the rankings of the different saliency models from the best (1) to the weakest (12) on the 8 remaining metrics after dimensionality reduction....|
|||...5 shows that the best saliency models in predicting human eye-tracking on Lis database are RARE, AWS and PQFT....|
|||...Discussion  Our approach is able to fairly compare saliency models rankings and not values, thus it is not possible here to test if the differences between the saliency models are statistically significant....|
|||...Our study also shows that one metric is not enough to evaluate the saliency model ranking on eye fixation data....|
|||...The 3 bold-marked similarity metrics are enough to provide a fair saliency models rank comparison in terms of similarity with human eye-tracking data....|
|||...Therefore, some saliency model benchmarks existing online as Borji [3] or Judd [13] use partly redundant similarity measures....|
|||...Conclusions  In this paper, we reviewed 12 state-of-the-art similarity metrics for visual saliency models validation and compared them over Jian Lis human eye-tracking fixations database with 12 recen...|
|||...The conclusion of our comparison study is that evaluating a saliency model with human fixations using only one similarity metric is not enough to be fair....|
|||...An implementation of the codes used in this paper to fairly assess his own saliency models on Li database is provided online [2] in the website project section....|
|||...Rare2012: A multi-scale raritybased saliency detection with its comparative statistical analysis....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
|||...Learning a saliency map using fixated  locations in natural scenes....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Methods for comparing scanpaths and saliency maps: strengths and weaknesses....|
||50 instances in total. (in iccv2013)|
|63|Shivanthan_Yohanandan_Saliency_Preservation_in_ECCV_2018_paper|...In contrast, many state-of-the-art computational saliency models are complex and inefficient....|
|||...Most saliency models process high-resolution color images; however, insights into the evolutionary origins of visual salience detection suggest that achromatic low-resolution vision is essential to it...|
|||...Previous studies showed that low-resolution color and high-resolution grayscale images preserve saliency information....|
|||...However, to our knowledge, no one has investigated whether saliency is preserved in low-resolution grayscale (LG) images....|
|||...for LG, and show, through a range of human eyetracking and computational modeling experiments, that saliency information is preserved in LG images....|
|||...Keywords: Saliency detection, Fully convolutional network, Peripheral vision  1 Introduction  Visual scenes often contain more items than can be processed concurrently due to the visual systems limite...|
|||...Recently, deep neural networks have achieved state-of-the-art performance on various saliency benchmarks [6, 7, 8, 9]....|
|||...A deeper understanding of the evolutionary origins of visual salience detection suggests that bottom-up saliency is computed from achromatic low-resolution information [13]....|
|||... color (LC) [14, 15, 16] and highresolution grayscale (HG) [17, 18, 19, 20, 21, 22] images preserve saliency information, yet are significantly more computationally attractive than high-resolution col...|
|||...Nevertheless, to our knowledge, no one has investigated whether saliency information is preserved in LG images....|
|||...In this study, we therefore investigate saliency preservation in LG images, and present the following three contributions: (1) linking low-resolution grayscale information with the bio-inspired evolut...|
|||...Therefore, based on a deeper understanding of the evolutionary origins of visual saliency, together with knowledge gained from studies investigating salience preservation in LC and HG images, we hypot...|
|||...Therefore, multiresolution models are often used to capture saliency at different scales....|
|||...[32] also investigated whether saliency information is preserved in grayscale images using a novel minimization function....|
|||...They showed that saliency is well-preserved in grayscale images of the same resolution, but did not extend their investigation to lower resolutions, which our study aims to do....|
|||...This structure has only recently emerged as a likely candidate for encoding the saliency map  a well-known precursor for bottom-up salience detection [38, 40, 41]....|
|||...This could explain why bottom-up saliency detection is rapid and reflex-like,  Saliency Preservation in Low-Resolution Grayscale Images  5  Fig....|
|||...Color to grayscale conversion is a lossy operation, resulting in luminance degradation, which may affect saliency [17]....|
|||...6 Experiments  This section assesses how well saliency information is preserved after transforming HC images to LG images using methods outlined above....|
|||...Previous studies used fixation maps to compare saliency similarity between images [14, 46]....|
|||...This is sufficient to highlight a few points of interest per image, and offers a reasonable testing ground for saliency models [48]....|
|||...NSS is computed as the average normalized saliency at fixated locations....|
|||...Moreover, this result confirms saliency preservation in LG images in terms of fixation map similarity....|
|||...6.3 HC vs. LG Saliency Detection Models  Model Architecture....|
|||...For tasks requiring spatial labels, like generating pixel-wise saliency heatmaps, we consider fully convolutional neural networks (FCNs) with deconvolutional layers....|
|||...This architecture has been previously used for saliency detection in video with enormous success [56], which is why we used a slightly modified version in our study (Figure 7)....|
|||...It is capable of generating saliency maps the same size as the input image, which was ideal for our experiment since we needed to compare the same model on datasets comprising images of different reso...|
|||...We were only interested in a HC saliency detection model with comparable accuracy and performance to the state-of-the-art so we could show that an LG model can achieve the same performance faster and ...|
|||...The model generates a saliency heatmap from a given input, which can then be compared with the ground-truth density map, just as in the above experiments....|
|||...The 2000 labeled images from the same CAT2000 saliency benchmark dataset used previously was split into training (1800 images) and validation (200 images) sets....|
|||...mpling the original resolution to 512  512 pixels (typical resolution used by many state-of-the-art saliency detection models) using methods described in section 5.2, and low-resolution (64  64 pixels...|
|||...Model accuracy was defined as a function of NSS, JuddAUC, SIM, and CC, described above, and computed using MATLAB code from the MIT saliency benchmark GitHub repository [14]....|
|||...Furthermore, detection time, defined as the average time taken by the model to generate a predicted saliency map based on each of the 200 test images, was also measured for MHC and MLG....|
|||...Therefore, this is further evidence suggesting saliency is well-preserved in LG images....|
|||...Furthermore, MLG is capable of generating a predicted saliency map almost 10 faster than MHC (12 vs. 114 milliseconds)....|
|||...e implications of using LG images over HC are substantial; thus, the motivation to use LG images in saliency detection should now be more obvious and appealing....|
|||...Additionally, we trained fully convolutional neural networks for saliency detection using LG and HC data from a benchmark dataset and found no significant difference between HC, LG and state-ofthe-art...|
|||...Therefore, these results confirm our hypothesis that saliency information is preserved in LG images, and we conclude by proposing LG images for fast and efficient saliency detection....|
|||...Hou, W., Gao, X., Tao, D., Li, X.: Visual saliency detection using information divergence....|
|||...Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: Predicting Human Eye Fixations via an  LSTM-based Saliency Attentive Model....|
|||...Hamel, S., Guyader, N., Pellerin, D., Houzet, D.: Contribution of color in saliency model for  videos....|
|||...Dorr, M., Vig, E., Barth, E.: Colour Saliency on Video....|
|||...Advani, S., Sustersic, J., Irick, K., Narayanan, V.: A multi-resolution saliency framework to drive foveation....|
|||...Tavakoli, H.R., Ahmed, F., Borji, A., Laaksonen, J.: Saliency Revisited: Analysis of Mouse  Movements versus Fixations....|
|||...Borji, A., Itti, L.: CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research....|
|||...Bylinskii, Z., Judd, T., Oliva, A., Torralba, A., Durand, F.: What do different evaluation  metrics tell us about saliency models?...|
|||...Liang, J., Zhang, Y.: Top down saliency detection via Kullback-Leibler divergence for object recognition....|
|||...Borji, A., Tavakoli, H.R., Sihite, D.N., Itti, L.: Analysis of Scores, Datasets, and Models in Visual Saliency Prediction....|
|||...Judd, T., Durand, F., Torralba, A.: A benchmark of computational models of saliency to  predict human fixations...|
||49 instances in total. (in eccv2018)|
|64|Gong_Saliency_Propagation_From_2015_CVPR_paper|...The propagation sequence generated by existing saliency detection methods is governed by the spatial relationships of image regions, i.e., the saliency value is transmitted between two adjacent regions....|
|||...Extensive experimental results on benchmark saliency datasets demonstrate the superiority of the proposed algorithm over twelve representative saliency detectors....|
|||...Recently, propagation methods have gained much popularity in bottom-up saliency detection and achieved state-ofthe-art performance....|
|||...To conduct saliency propagations, an input image is represented by a graph over the segmented superpixels, in which the adjacent superpixels in the image are connected by weighted edges....|
|||...The saliency values are then iteratively diffused along these edges from the labeled superpixels to their unlabeled neighbors....|
|||...Therefore, we assume different superpixels have different difficulties, and measure the saliency values of the simple superpixels prior to the difficult ones....|
|||...By taking advantage of these psychological opinions, we propose a novel approach for saliency propagation by leveraging a teaching-to-learn and learning-to-teach paradigm (displayed in Fig....|
|||...This paradigm plays two key roles: a teacher behaving as a superpixel selection procedure, and a learner working as a saliency propagation procedure....|
|||...ated from simple to difficult with the updated curriculum, resulting in more confident and accurate saliency maps than those of typical methods (see Fig....|
|||...Saliency Detection Algorithm  This section details our saliency detection scheme (see Fig....|
|||...After that, the saliency values are propagated from the background seeds to form a coarse map (Stage 1)....|
|||...Finally, this map is refined by propagating the saliency information from the most confident foreground regions to the remaining superpixels (Stage 2)....|
|||...j  A coarse saliency map is built from the perspective of background, to assess how these superpixels are distinct from the background....|
|||...els  graph  graph   construction  construction (white lines are  (white lines are   edges)  edges)  saliency   map   Stage 1  Stage 1  Stage 2  Stage 2  (cid:313)   coarse saliency  coarse saliency   ...|
|||...(cid:5)   N  f   1  f    (cid:6)T , where f  modified by the subsequent propagation process, and every superpixel will receive a reasonable saliency value as a result....|
|||...2) for saliency propagation....|
|||...ose the propagation result is expressed by an N-dimensional vector i (i = 1, , N) are obf =  tained saliency values corresponding to the superpixels si, then after scaling f to [0, 1] (denoted as f no...|
|||...-th superpixel is inside (SM ask(i) = 1) or outside (SM ask(i) = 0) the convex hull H. Finally, the saliency map of Stage 1 is obtained by integrating SConvexHull, SBoundary, and SM ask as  SStage1 = ...|
|||...Therefore, we need to propagate the saliency information from the potential foreground regions to further improve SStage1....|
|||...Finally, by setting the labels of seeds to 1 and conducting the teaching-to-learn and learningto-teach propagation, we achieve the final saliency map SStage2....|
|||...Teaching-to-learn and Learning-to-teach  For Saliency Propagation Saliency propagation plays an important role in our algorithm....|
|||...Suppose we have l seed nodes s1, , sl on G with saliency values f1 =  = fl = 1, the task of saliency propagation is to reliably and accurately transmit these values from the l labeled nodes to the rem...|
|||...This is because such superpixel is very likely to share the similar saliency value with its neighbors, thus can be easily identified as either foreground or background....|
|||...However, since the correctness of the (t1)-th output saliency is unknown, we define a confidence score to blindly evaluate the previous learning performance....|
|||...Intuitively, the (t1)-th learning is , , f (t1) q(t1) are close confident if the saliency values f to 0 (very dissimilar to seeds) or 1 (very similar to seeds) (t1) q(t1) are close to the after scaling....|
|||...Saliency Propagation After the curriculum T (t) =  (cid:9)  s1, s2, , s  q(t)  is specified, the learner will spread the saliency values from L(t) to T (t) via propagation....|
|||...l corf responds to seed, and 0 otherwise), terminates when U becomes an empty set, and the obtained saliency value vector is denoted by  f....|
|||...As a result, the learner is more confident to assign the correct saliency values to the land after the 4th iteration, and the target (pyramid) is learned in the end during the 7th9th iterations....|
|||...earn and Learning-toTeach approach (abbreviated as TLLT) with twelve popular methods on two popular saliency datasets....|
|||...(a) shows two saliency maps generated by MR [27] and our method....|
|||...In contrast, the weighted F-measure F w  (light blue numbers in (a)) provides more reasonable judgements and gives our saliency maps higher evaluations (marked by the red boxes)....|
|||...This is because the designed teaching-to-learn and learning-to-teach paradigm propagates the saliency value carefully and accurately....|
|||...As a result, our approach has less possibility to generate the blurred saliency map with confused foreground....|
|||...Visual comparisons of saliency maps generated by all the methods on some challenging images....|
|||...Comparison of different methods on two saliency detection datasets....|
|||...To further present the merits of the proposed approach, we provide the resulting saliency maps of evaluated methods on several very challenging images from the two datasets (see Fig....|
|||...Though the backgrounds in these images are highly complicated, or very similar to the foregrounds, TLLT is able to generate fairly confident and clean saliency maps....|
|||...10(a), the color of the target is very close to the background, therefore the generated saliency map (Fig....|
|||...10(c), the convex hull encloses the non-target regions, so the real targets are not precisely detected in the final saliency map (Fig....|
|||...Actually, the two listed situations 1) and 2) are also challenging for the existing saliency algorithms....|
|||...Conclusion  This paper proposed a novel approach for saliency propagation through leveraging a teaching-to-learn and learningto-teach paradigm....|
|||...Different from the existing methods that propagated the saliency information entirely depending on the relationships among adjacent image regions, the proposed approach manipulated the propagation seq...|
|||...Consequently, our approach can render a more confident saliency map with higher background suppression, yielding a better popping out of objects of interest....|
|||...Random walks on graphs to model saliency in images....|
|||...Improved saliency detection based on superpixel clustering and saliency propagation....|
|||...Geodesic saliency using In European Conference on Computer  background priors....|
|||...Bayesian saliency via low and Image Processing, IEEE Transactions on,  mid level cues....|
|||...Hierarchical saliency detection....|
|||...Top-down visual saliency via joint CRF and dictionary learning....|
||49 instances in total. (in cvpr2015)|
|65|Qin_Saliency_Detection_via_2015_CVPR_paper|...The saliency values of all cells will be renovated simultaneously according to the proposed updating rule....|
|||...Introduction  Recently, saliency detection aimed at finding out the most important part of an image has become increasingly popular in computer vision [1, 14]....|
|||...As a pre-processing procedure, saliency detection can be used for many vision tasks, such as visual tracking [26], object retargeting [11, 39], image categorization [37] and image segmentation [35]....|
|||...Generally, methods of saliency detection can be categorized as either top-down or bottom-up approaches....|
|||...To better distinguish salient objects from background, highlevel information and supervised methods are incorporated to improve the accuracy of saliency map....|
|||...In contrast, bottom-up methods [15, 18, 22, 40, 41, 48] usually exploit low-level cues such as features, colors and spatial distances to construct saliency maps....|
|||...Secondly, a novel propagation method based on Cellular Automata [42] is introduced to enforce saliency consistency among similar image patches....|
|||...Through interactions with neighbors, boundary cells misclassified as background seeds will automatically modify their saliency values....|
|||...Many effective methods have been established to deal with saliency detection and each of them has their own superiorities [7, 17, 23, 33, 52]....|
|||...Related works  Recently, more and more bottom-up methods prefer to construct the saliency map by choosing the image boundary as the background seeds....|
|||...[44] define each regions saliency value as the shortest-path distance towards In [19], the contrast against image boundthe boundary....|
|||...first apply Bayesian theory to optimize saliency maps and achieve better results....|
|||...[23] attain saliency maps through dense and sparse reconstruction and propose a Bayesian algorithm to combine saliency maps....|
|||...All of them demonstrate the effectivity of Bayesian theory in the optimization of saliency detection....|
|||...Sbg  i =  Xk=1  As Figure 1 shows, the geodesic constraint enforced on GCD maps greatly facilitates saliency accuracy by strengthening the contrast in local regions....|
|||...However, in this paper, we use the saliency value of each superpixel as its state, which is continuous between 0 and 1....|
|||...The initial St when t = 0 is Sbg in Eqn 3, and the ultimate saliency map after N1 time steps (a time step is defined as one traversal iteration through all cells) is denoted as SN1 ....|
|||...(b) Saliency maps achieved by BSCA....|
|||...Via exploiting the intrinsic relationship in the neighborhood, Single-layer Cellular Automata can enhance saliency consistency among similar regions and form a steady local environment....|
|||...When salient superpixels are selected as the background seeds by mistake, they will automatically increase their saliency values under the influence of local environment....|
|||...Multi-layer Cellular Automata  Many innovative methods by far have been put forward to deal with saliency detection....|
|||...o take advantage of the superiority of each method, we propose an effective method to incorporate M saliency maps generated by M state-of-the-art methods, each of which serves as a layer of Cellular A...|
|||...In Multi-layer Cellular Automata (MCA), each cell represents a pixel and the number of all pixels in an image is denoted as H. The saliency values consist of the set of cells states....|
|||...That is, for any cell on a saliency map, it may have M  1 neighbors on other maps and we assume that all neighbors have the same influential power to determine the cells next state....|
|||...The saliency value of pixel i stands for its probability to be the foreground F , denoted as P (i  F ) = Si, while 1  Si stands for its possibility to be the background B, denoted as P (i  B) = 1  Si....|
|||...The threshold is only related to the initial saliency map and remains the same all the time....|
|||...The threshold of the m-th saliency map is denoted as m....|
|||...(b)-(f) Saliency maps generated respectively by HS [48], DSR [23], MR [49], wCO [52] and our algorithm BSCA....|
|||...If pixel i belongs to the foreground, the probability that one of its neighboring pixel j (with the same coordinates on another saliency map) is measured as foreground is  = P (j = +1 i  F )....|
|||...d as:  (i  F ) =  St i  1  St i  , (i  F  j = +1) =  St+1  i  1  St+1  i  (13) where St i means the saliency value of pixel i at time t. And we define the synchronous updating rule f : SM 1  S as:  M ...|
|||...Intuitively, if a pixel observes that its neighbors are binarized as foreground, it ought to increase its saliency value....|
|||...After N2 time steps, the final integrated saliency map SN2 is calculated as:  SN2 =  1 M  M  Xm=1  SN2 m  (15)  In this paper, we use Multi-layer Cellular Automata to integrate saliency maps generated...|
|||...we also introduce the mean absolute error (MAE) which calculates the average difference between the saliency map and the ground truth GT in pixel level:   S(h)  GT (h)   (17)  M AE =  1 H  H  Xh=1  Th...|
|||...And it has a wider range of high F-measure compared to others. Furthermore, the fairly low MAEs displayed in Table 1 indicate the similarity between our saliency maps and the ground truth....|
|||...Several saliency maps are shown in Figure 8 for visual comparison of our method with other results....|
|||...Even though the original saliency maps are not satisfying, the optimized results are comparable to the state-of-the-arts....|
|||...We can observe from Figure 8 that saliency maps generated by MCA are almost the same as the ground truth....|
|||...Furthermore, Multi-layer Cellular Automata (MCA) only takes on average 0.043s to integrate different methods and can achieve a much better saliency map....|
|||...Comparison of saliency maps on different datasets....|
|||...MCA: The integrated saliency maps via Multi-layer Cellular Automata....|
|||...It can take advantage of the superiorities of different state-of-the-art saliency maps and incorporate them into a more discriminative saliency map with  higher precision and recall....|
|||...context-aware saliency detection....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Top-down visual saliency via joint crf and dictionary learning....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
|||...Geodesic saliency using background priors....|
|||...Visual saliency detection based on In Image Processing (ICIP), 2011 18th bayesian model....|
|||...Bayesian saliency via low and mid level cues....|
|||...Hierarchical saliency detection....|
||49 instances in total. (in cvpr2015)|
|66|Adria_Recasens_Learning_to_Zoom_ECCV_2018_paper|...For such tasks, where I is available but unused, we show that using a saliency sampler to downsample the image (rather than uniform downsampling) can lead to significant improvement in the task networ...|
|||...1, given a target image input size, our saliency sampler learns to allocate pixels in that target to regions in the underlying image which are found to be particularly important for the task at hand....|
|||...Our layer consists of a saliency map estimator connected to a sampler which varies sampling density for image regions depending on their relative saliency values....|
|||...Examples of resampled input images for various tasks using our proposed saliency sampler....|
|||...Our module is able to discover saliency according to the task: for gaze estimation in (a), the sampler learns to zoom in on the subjects eyes to allow for higher precision gaze estimation; for fine-gr...|
|||...eural Networks  5  samples from the same low-resolution input as the original CNN architecture, our saliency sampler is designed to sample from any available resolution, allowing it to take advantage ...|
|||...Second, our approach estimates the sample field through saliency maps which have been shown to emerge naturally when training fully convolutional neural networks [15]....|
|||...Finally, our approach produces human readable outputs in the form of the saliency map and the deformed image which allow for easy visual inspection and debugging....|
|||...We note that our proposed saliency sampler and DCNs are not mutually exclusive: our saliency sampler is designed to sample efficiently across scale space and could potentially make use of deformable c...|
|||...As in [13], they predict directly a parametrization of these transformations instead of using a saliency map....|
|||...Similarly to our concept, this can be driven by saliency [24] and formulated as an energy minimization [25] or Finite Element Method [26] problem....|
|||...3 Saliency Sampler  Let I be a high-resolution image of an arbitrary size and let Il be a low-resolution image bounded by size M  N pixels suitable for a task network ft (Fig....|
|||...The saliency sampler executes this by first analyzing Il before sampling areas of I proportionally to their perceived importance....|
|||...In the first stage, a CNN is used to produce a saliency map....|
|||...In the second stage, the most important image regions are sampled according to the saliency map....|
|||...3.1 Saliency Network  The saliency network fs produces a saliency map S from the low resolution image: S = fs(Il)....|
|||...3.2 Sampling Approach  Next, a sampler g takes as input the saliency map S along with the full resolution image I and computes J = g(I, S)  that is, an image with the same dimensions as Il, that has b...|
|||...The saliency map S (center, top) describes saliency as a mass attracting neighboring pixels (arrows)....|
|||...Each pixel (red square) of the output lowresolution image J samples from a location (cyan square) in the input high-resolution image I which is offset by this attraction (yellow arrow) as defined by t...|
|||...The main goal for the design of u and v is to map pixels proportionally to the normalized weight assigned to them by the saliency map....|
|||...s certain desirable properties for our functions u and  v, notably:  Sampled areas: Areas of higher saliency are sampled more densely, since those pixels with higher saliency mass will attract other p...|
|||...In all our experiments, we use a Gaussian kernel with  set to one third of the width of the saliency map, which we found to work well in various settings....|
|||...3.3 Training with the Saliency Sampler  The saliency sampler can be plugged into any convolutional neural network ft where more informative subsampling of a higher resolution input is desired....|
|||...This image is used by the saliency network fs to compute a saliency map S =  fs(Il), where task-relevant areas of the image are assigned higher weights....|
|||...We use the deterministic grid sampler g to sample the high resolution image I according to the saliency map, obtaining the resampled image J = g(I, S) which has the same resolution as Il....|
|||...It forces the saliency sampler to zoom deeper into the image in order to further magnify small details otherwise destroyed by the consequent blur....|
|||...We show the low-resolution input image Il, the saliency map S estimated by fs, the sampling grid g, and the resampled image J....|
|||...Note that the saliency network naturally discovers the eyes to be the most informative regions in the image to infer subject gaze, but also learns to preserve the approximate position of the head in t...|
|||...4 Experiments  In this section we apply the saliency sampler to two important problems in computer vision: gaze-tracking and fine-grained object recognition....|
|||...As an architecture for the saliency network fs, in all the tasks we use ablations of ResNet-18 [4] pretrained on the ImageNet Dataset [28] and one final 1  1 convolutional layer to reduce the dimensio...|
|||...We benchmark our model against the iTracker dataset [21], and show how their original model can be simplified by using the saliency sampler....|
|||...10  A. Recasens, P. Kellnhofer , S.Stent, W. Matusik and A.Torralba  Model  iPad (cm) iPhone (cm)  iTracker Plain AlexNet (AN) AN + Deformable Convolutions AN + STN TPS AN + STN AN + Grid Estimator AN...|
|||...As saliency network fS we use the initial 10 layers of ResNet-18....|
|||...We aim to prove that our simple saliency sampler can allow a normal network to deal with the complexity of the four-input iTracker model by just magnifying the correct parts of a single input image....|
|||...Third, we modify the network fs to directly estimate the sampling grid functions u and v without the saliency map (Grid Estimator)....|
|||...4, the saliency network naturally discovers and zooms in on the most informative regions in the image, which tend to correspond to object parts....|
|||...We used an input resolution of 227  227 for both the task and saliency networks, ft and fs....|
|||...As saliency network fS we use the initial 14 layers of ResNet-18, although the performance for other saliency networks can be found in Tbl....|
|||...We then cropped this region from the  12  A. Recasens, P. Kellnhofer , S.Stent, W. Matusik and A.Torralba  Model  Top-1(%) [diff] Top-5(%) [diff]  ResNet-101 227 (RN) RN + Deformable Convolutions RN ...|
|||...To justify our claim that the saliency sampler can benefit different task network architectures, we repeat our experiment using a Inception V3 architecture [31]....|
|||...Visualization of sampler behavior for the CUB-200 dataset: We show the sampled images for a ResNet-50 trained with the saliency sampler in the CUB-200 dataset....|
|||...The saliency amplifies relevant image regions such as the birds head....|
|||...3, we retrained ResNet-101 with different depths of saliency network fs....|
|||...The performance of the overall network increases with the complexity of the saliency model but with diminishing returns....|
|||...We used ResNet-50 as our task network and the initial 14 layers of ResNet18 as our saliency network....|
|||...5 Discussion  Adding our saliency sampler is most beneficial for image tasks where the important features are small and sparse, or appear across multiple image scales....|
|||...The non-uniform approach to the magnification introduced by our saliency map also enables variability of zoom over the spatial domain....|
|||...6 Conclusion  We have presented the saliency sampler  a novel layer for CNNs that can adapt the image sampling strategy to improve task performance while preserving memory allocation and computational...|
||48 instances in total. (in eccv2018)|
|67|Luo_Label_Consistent_Quadratic_2015_CVPR_paper|...eleqiz}@nus.edu.sg  Abstract  Recently, an increasing number of works have proposed to learn visual saliency by leveraging human fixations....|
|||...In the learning based saliency prediction literature, most models are trained and evaluated within the same dataset and cross dataset validation is not yet a common practice....|
|||...To address these problems, we proposed a new learning based saliency model, namely Label Consistent Quadratic Surrogate algorithm, which employs an iterative online algorithm to learn a sparse diction...|
|||...As shown in this work, the proposed saliency model achieves better performance than the state-of-the-art saliency models....|
|||...To tackle the information overload problem, visual saliency detection has emerged to be an efficient solution to detect the regions of interest to enhance existing computer vision system....|
|||...Conventional saliency models employ a straightforward bottom-up solution to predict visual saliency [13, 14, 17, 39]....|
|||...Recently, learning based saliency prediction models were proposed to leverage the power of machine learning techniques and human knowledge (from human fixation maps), and decipher the pattern to bette...|
|||...However, the conventional learning based models in saliency prediction assume that the training data is fully observed and there exist sufficient training data....|
|||...To address all the aforementioned problems, we propose a new saliency prediction model, namely Label Consistent Quadratic Surrogate (LCQS) Algorithm, which employs an iterative online dictionary learn...|
|||...Second, we add label consistent constrain in the dictionary learning process  1  to ensure that the learned sparse dictionary can generate discriminative sparse code for saliency prediction....|
|||...Last but not least, the proposed saliency model can adapt a trained dictionary with new training data....|
|||...As shown in Section 5, the proposed saliency prediction model achieves better performance than the state-of-the-art saliency models....|
|||...Sections 3 and 4 elaborate our proposed online saliency framework with LCQS algorithm....|
|||...[13] introduced a GraphBased Visual Saliency (GBVS) model to weigh the dissimilarity between two arbitrary positions to detect the conspicuous regions....|
|||...considered saliency detection as a figure-ground separation problem and employed sparse signal analysis to solve it....|
|||...[38] proposed a Boolean map based saliency model to compute the saliency map by analyzing the topological structure of Boolean maps....|
|||...The aforementioned bottom-up saliency models are straightforward solutions for saliency detection....|
|||...Recently, learning based saliency prediction models were emerged to leverage the power of machine learning techniques and human knowledge (from human fixation maps), and decipher the pattern to better...|
|||...[18] proposed a learning based model based on the Label Consistent K-SVD (LC-KSVD) algorithm [20], where the goal is to fill the semantic gap between computational saliency models and human behavior....|
|||...Sparse Coding Based Saliency Model 3.1....|
|||..., w, Q, H >= Flcqs(v, Z, Q, H)  M = Slcqs(Z, D, w)  Saliency map  Figure 1: An overview of the LCQS saliency model....|
|||...First, a ground truth saliency map, MGT , of an image is derived from visual fixation maps from human eye tracking data....|
|||...Each training sample is represented as a duple < v, z > which consists of: (1) the saliency value v; and (2) feature vector z by extracting r  r neighborhood at corresponding pixel from F and concaten...|
|||...We select  n and  m samples with the highest and lowest saliency values, respectively....|
|||...In this work, sparse coding approach is employed to learn an efficient representation of image features in relation to visual saliency in an online fashion....|
|||...Similar to [18], the saliency prediction problem is casted as a binary classification problem in this work....|
|||...o learn a sparse dictionary D. The objective function in the dictionary learning problem for visual saliency prediction can be formulated as:  < D, L, X, w > = arg min  (cid:107)Z  DX(cid:107)2  F + (...|
|||...v is saliency labels from the human fixation ground truth and w is the classification weights to reconstruct the ground truth saliency labels....|
|||...The sparse code x and saliency value v for each corresponding z can be computed as follows:  x = arg min  v = (wT x) (cid:12)(cid:12)wT x(cid:12)(cid:12)  1 2  x  (cid:107)z  Dx(cid:107)2  2 + (cid:10...|
|||...The predicted v from each pixel location form a saliency response M.  Finally, to represent the conspicuity at every location in the visual field by a scalar quantity and simulate the field of view of...|
|||...In this work, we compared the proposed method with the LCKSVD saliency model [18] and 4 state-of-the-art bottomup saliency models (i.e., Itti [17], GBVS [13], SUN [39] and Image Signature [14])....|
|||...In this configuration, saliency model is trained on dataset B and predicts on dataset A. Thirdly, we leverage the prior information from another dataset to improved the models quality....|
|||...There are several widely used metrics to evaluate the performance of visual saliency models with human fixation data....|
|||...In addition, the Normalized Scanpath Saliency (NSS) [31] and the Correlation Coefficient (CC) [30] are employed to measure the performance....|
|||...NSS is defined as the average saliency value at the fixated locations in the normalized predicted saliency map which has zero mean and unit variance, whereas CC measures the linear correlation between...|
|||...The GBVS model implicitly used center-preference to predict saliency [13]....|
|||...To conduct fair comparison, we use a 200  200 pixels Gaussian blob ( = 60) as center bias and multiplying with saliency maps to compute CC and NSS [3]....|
|||...We first generate the saliency maps from various models without smoothing, followed by blurring them with various kernels....|
|||...The blurred saliency maps are used to generate the respective scores....|
|||...Quite a number of MIT images have a dominant object in the center of the image, this results a saliency model to detect the same object as predicted by other models....|
|||...For the cross dataset validation, LCQS-CrossDB sig Table 2: Qualitative results of the proposed LCQS saliency model and various state-of-the-art models....|
|||...This is partly due to the fact that MIT has a considerable portion of fixations in the center and the saliency map of LCQS-CrossDB has more false detections on the center....|
|||...ins, the proposed model consistently achieves noticeable improvement over existing state-of-the-art saliency models, as well as addressing the problem of insufficient eye fixation datasets by leveragi...|
|||...Input  Fixation  LCQS-A LCQS-B LCQS-C LCKSVD-BLCKSVD-C GBVS  Signature  Itti  SUN  Figure 4: Qualitative results of the proposed LCQS saliency model and various state-of-the-art models....|
|||...Boosting bottom-up and top-down visual features for saliency estimation....|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Leveraging human fixations in sparse coding: Learning a discriminative In IEEE Internadictionary for saliency prediction....|
|||...Learning a saliency map using fixated locations in natural scenes....|
||48 instances in total. (in cvpr2015)|
|68|cvpr18-A Bi-Directional Message Passing Model for Salient Object Detection|...How to integrate multi-level features becomes an open problem in saliency detection....|
|||...We use the features after message passing, which simultaneously encode semantic information and spatial details, to predict saliency maps....|
|||...Finally, the predicted results are efficiently combined to generate the final saliency map....|
|||...From left to right: (a) input image, (b) ground truth, (c) saliency map of the proposed method, (d) saliency map of RFCN [29]....|
|||...Firstly, most previous FCN-based saliency detection models [29, 27, 38, 12] stack single-scale convolutional and max pooling layers sequentially to generate deep features....|
|||...Secondly, early works [29, 18, 15, 12] predict saliency maps by mainly using high-level features from deep convolutional layers....|
|||...The lack of low-level spatial details may make the saliency maps fail to retain fine object boundaries....|
|||...This motivates several efforts [37, 8, 14] to exploit multi-level convolutional features for saliency detection....|
|||...Salient Object Detection  Early methods predict saliency using bottom-up computational models and low-level hand-crafted features....|
|||...A majority of them utilize heuristic saliency priors, such as color contrast [4, 1], boundary background [33, 41] and center prior [10]....|
|||...Some early deep saliency models utilize CNN features to predict the saliency scores of image segments like superpixels [15] or object proposals [27]....|
|||...These methods could achieve state-ofthe-art saliency detection results....|
|||...propose to embed low-level spatial features into the feature maps and then combine them with CNN features to predict saliency maps....|
|||...[29] generate saliency prior map using low-level cues and exploit it to guide the saliency prediction in a recurrent fashion....|
|||...The above-mentioned methods mainly use specific-level features for generating saliency maps....|
|||...Multi(cid:173)level Feature Integration  We compare the proposed approach with 13 state-ofthe-art saliency detection methods on five datasets....|
|||...The integrated features {h3 i } (gray boxes) are used for saliency prediction and the final saliency map is the fused output of multiple predicted saliency maps....|
|||...propose a feature aggregating framework, in which the multi-level CNN features are integrated into different resolutions to predict saliency maps....|
|||...So the multiple saliency cues are incorporated at each level....|
|||...At last, we introduce the saliency inference module in Sec....|
|||...Our network consists of three components: multi-scale contextaware feature extraction module, gated bi-directional message passing module and saliency inference module....|
|||...We generate saliency maps at different resolutions by using these integrated multi-level features....|
|||...And the final saliency map is obtained via fusing the multi-scale predicted results in a coarse-to-fine manner....|
|||...A recent work [30] proposes to use the pyramid pooling before the final prediction layer to extract multi-scale features for saliency detection....|
|||...To integrate them for more accurate saliency prediction, we propose a Gated Bi-directional Message Passing Module....|
|||...Besides, the semantic information at deeper layers helping localize the salient objects and spatial details at shallower ones are both important for saliency detection....|
|||...For the individual input image, the multi-level features may be not all helpful to precisely predict saliency maps....|
|||...With the GBMPM, multi-level features h3 i can adaptively encode various saliency cues and are robust enough to produce accurate saliency prediction....|
|||...The multi-level features are complementary and robust, so we use them together to predict saliency maps....|
|||...Some methods [37, 30] directly upsample and integrate the multi-level features into the size of the input image and exploit a convolutional layer to produce saliency map....|
|||...To avoid this problem, we fuse multi-level features and generate saliency map in a coarse-to-fine manner....|
|||...  i ), i = 5  (7)  where Conv(; f ) is the convolutional layer with kernel size 1  1 for predicting saliency maps....|
|||...And we take S1 as the final saliency map of our model without any post-processing....|
|||...The proposed model is trained end to end using the crossentropy loss between the final saliency map and the ground truth....|
|||...The precision and recall are calculated by thresholding the predicted saliency map and comparing it with the corresponding ground truth....|
|||...Except for PR curve and F-measure, we also calculate the mean absolute error (MAE) to measure the average difference between predicted saliency map and ground truth....|
|||...It is computed as:  M AE =  1  W  H  W  H  Xx=1  Xy=1   S(x, y)  G(x, y)   (10)  where S and G are predicted saliency map and ground truth, respectively....|
|||...Performance Comparison with State(cid:173)of(cid:173)the(cid:173)art  We compare the proposed saliency detection model with 13 state-of-the-art methods, including 11 deep learning based methods (LEGS ...|
|||...We perform comparisons of the proposed algorithm and 13 state-of-the-art saliency detection methods on five datasets....|
|||...The maximum F-measure (larger is better) and MAE (smaller is better) of different saliency detection methods on five released saliency detection datasets....|
|||...4 lists some saliency maps generated by the proposed method as well as other 13 stateof-the-art algorithms....|
|||...The multi-level features containing both high-level semantic concept and low-level spatial details are further utilized to produce the final saliency maps....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Visual saliency based on multiscale deep features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierarchical saliency deIn Proceedings of IEEE Conference on Computer  tection....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||48 instances in total. (in cvpr2018)|
|69|Huang_Predicting_Gaze_in_ECCV_2018_paper|...Specifically, we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottomup saliency prediction....|
|||...Most previous methods have formulated gaze prediction as the problem of saliency detection, and computational models of visual saliency have been studied to the find image regions that are likely to a...|
|||...However, the saliency model-based gaze prediction becomes much more difficult in natural dynamic scenes, e.g....|
|||...In this paper, we propose a hybrid gaze prediction model that combines bottom-up visual saliency with task-dependent attention transition learned from successively attended image regions in training data....|
|||...The first module generates saliency maps directly from video frames....|
|||...It is based on a two-stream Convolutional Neural Network (CNN) which is similar to traditional bottom-up saliency prediction models....|
|||...The last module is based on a fully convolutional network which fuses the saliency map and the attention map from the first two modules and generates a final gaze map, from which the final prediction ...|
|||...Main contributions of this work are summarized as follows:   We propose a new hybrid model for gaze prediction that leverages both  bottom-up visual saliency and task-dependent attention transition....|
|||...2 Related Works  Visual Saliency Prediction....|
|||...Visual saliency is a way to measure image regions that are likely to attract human attention and thus gaze fixation [2]....|
|||...Traditional saliency models are based on the feature integration theory [35] telling  Predicting Gaze in Egocentric Video  3  that an image region with high saliency contains distinct visual features...|
|||...After Itti et al.s primary work [19] on a computational saliency model, various bottom-up computational models of visual saliency have been proposed such as a graph-based model [13] and a spectral clu...|
|||...Recent saliency models [25][16][26] leveraged a deep Convolutional Neural Network (CNN) to improve their performance....|
|||...More recently, high-level context has been considered in deep learning-based saliency models....|
|||...In [31][8], class labels were used to compute the partial derivatives of CNN response with respect to input image regions to obtain a class-specific saliency map....|
|||...In [29], region-to-word mapping in a neural saliency model was learned by using image captions as high-level input....|
|||...ns and gaze fixation locations [27], it has been found that traditional bottom-up models for visual saliency is insufficient to model and predict human gaze in egocentric video [37]....|
|||...In their model, bottom-up saliency map is integrated with an attention map obtained based on camera rotation and translation to infer final egocentric gaze position....|
|||...In this paper, we propose a new hybrid model to predict gaze in egocentric videos, which combines bottom-up visual saliency with task-dependent attention transition....|
|||...To leverage both bottom-up visual saliency and task-dependent attention transition, we propose a hybrid model that 1) predicts a saliency map from each video frame, 2) predicts an attention map by exp...|
|||...The saliency prediction module generates a saliency map based on the extracted latent representation....|
|||...The late fusion module combines the results of saliency prediction and attention transition to generate a final gaze map....|
|||...3.3 Saliency Prediction Module  Biologically, human tends to gaze at an image region with high saliency, i.e., a region containing unique and distinctive visual features [34]....|
|||...We fuse the latent representations F S t as an input to a saliency prediction decoder (denoted as S) to obtain the initial gaze prediction map Gs t (Eq....|
|||...The equation for generating the visual saliency map is:  t and F T  Gs  t = S(F S  t , F T t )  (1)  However, a saliency map alone does not predict accurately where people actually look [37], especial...|
|||...3.5 Late Fusion  We build the late fusion module (LF) on top of the saliency prediction module and the attention transition module, which takes Gs t as input and outputs the predicted gaze map Gt....|
|||...3.6 Training  For training gaze prediction in saliency prediction module and late fusion module, the ground truth gaze map G is given by convolving an isotropic Gaussian over the measured gaze positio...|
|||...Since the input of the saliency prediction module contains latent representations from both S-CNN and T-CNN, we use a 3d convolution layer (with a kernel size of 1  3  3) and a 3d pooling layer (with ...|
|||...We fix the learning rate as 1e-7 and first train the saliency prediction module for 5 epochs for the module to converge....|
|||...We then fix the saliency prediction module and train the LSTM-based weight predictor and the fixation state predictor in the attention transition module....|
|||...After training the attention transition module, we fix the saliency prediction and the attention transition module to train the late fusion module in the end....|
|||...It is a commonly used evaluation metric in saliency prediction....|
|||...We use the following baselines for gaze prediction:   Saliency prediction algorithms: We compare our method with several representative saliency prediction methods....|
|||...More specifically, we used Ittis model [18], Graph Based Visual Saliency (GBVS [13]), and a deep neural network based saliency model as the current state of the art (SALICON [16])....|
|||...Our baselines include: 1) single-stream saliency prediction with binary cross entropy loss (S-CNN bce and T-CNN bce), 2) single-stream saliency prediction with our modified bce loss (S-CNN and T-CNN),...|
|||...The SP module performs better than either of the single-stream saliency prediction (S-CNN and T-CNN), indicating that both spatial and temporal information are needed for accurate gaze prediction....|
|||...del outperforms all separate components by a large margin, which confirms that the bottom-up visual saliency and high-level task-dependent attention are complementary cues to each other and should be ...|
|||...We show the output heatmap from the saliency prediction module (SP) and the attention transition module (AT) as well as our full model....|
|||... SP: The saliency prediction module is treated as a generic component and trained on a separate subset of the dataset....|
|||...The taskdependent attention transition is further integrated with a CNN-based saliency model to leverage the cues from both bottom-up visual saliency and high-level attention transition....|
|||...Borji, A., Tavakoli, H.R., Sihite, D.N., Itti, L.: Analysis of scores, datasets, and  models in visual saliency prediction....|
|||...Kuen, J., Wang, Z., Wang, G.: Recurrent attentional networks for saliency detec tion....|
|||...Lin, Y., Kong, S., Wang, D., Zhuang, Y.: Saliency detection within a deep convo lutional architecture....|
|||...: Shallow and  deep convolutional networks for saliency prediction....|
|||...Ramanishka, V., Das, A., Zhang, J., Saenko, K.: Top-down visual saliency guided  by captions....|
|||...Riche, N., Duvinage, M., Mancas, M., Gosselin, B., Dutoit, T.: Saliency and human  fixations: state-of-the-art and study of comparison metrics....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep  learning....|
||48 instances in total. (in eccv2018)|
|70|Non-Local Deep Features for Salient Object Detection|...We trained our model on the MSRA-B dataset, and tested it on six different saliency benchmark datasets....|
|||...Results show that our method is on par with the state-of-the-art while reducing computation time by a factor of 18 to 100 times, enabling near real-time, high performance saliency detection....|
|||...Several applications benefit from saliency detection including image and video compression [14], context aware image re-targeting [25], scene parsing [50], image resizing [3], object detection [44] an...|
|||...The result of that comparison is called a saliency score which is stored in a saliency map....|
|||...Recently, deep learning has entered the field of saliency detection and quickly established itself as the de facto benchmark....|
|||...While some methods apply a straight forward convolutional neural net (CNN) model [36], others have proposed a model tailored to the saliency detection problem [25, 26, 29, 43, 50]....|
|||...overarching objectives of state-of-the-art CNN models (enforcing spatial coherence of the predicted saliency map and using both the local and global features in the optimization) can be achieved with ...|
|||...Section 2 provides an overview of deep learning based saliency detection techniques....|
|||...Finally, Section 4 discusses the performance of non-local feature model compared to other state-of-the-art saliency detection methods....|
|||...With CNNs, the saliency problem has been redefined as a labeling problem where feature selection between salient and non-salient objects is done automatically through gradient descent....|
|||...CNN methods are a priori unfit to predict a saliency map since their output is a k-D vector (where k is the number of classes), and not an N  M map (where N  M is the size of the input image) as one w...|
|||...Several deep visual saliency detection methods use this same patch trick for predicting a saliency map [29, 50, 25,  43]....|
|||...ntext of an image into a single, multi-context network, where the global context helps to model the saliency in the full image, and the local context helps to estimate the saliency of finegrained, fea...|
|||...Such a complex model was designed to capture the saliency map of objects with various scales, geometry, spatial positions, irregularities, and contrast levels....|
|||...[43] developed a two tier strategy: each pixel is assigned a saliency based upon a local context estimation in parallel to a global search strategy used to identify the salient regions....|
|||...These two saliency maps are then combined using geodesic object proposal techniques [21]....|
|||...Roughly speaking, the idea is to set the saliency score of a superpixel as the mean saliency score of each pixel located inside of it....|
|||...Model Based Saliency Detection  Salient region detection as well as image segmentation often boils down to the optimization of a non-convex energy function which consists of a data term and a regulari...|
|||...unctional with the sum of a cross entropy data fidelity term between the ground truth and estimated saliency and a boundary loss term:  F MS  Xj    jZ  Hj(y(v), y(v))  vj  cross entropy  {z  }  +Xj   ...|
|||...Network Architecture  Here we provide a deep convolutional network architecture whose goal is to learn discriminant saliency features (our model is shown in Figure 1)....|
|||...2, good saliency features must account for both the local and global context of an image and incorporate details from various resolutions....|
|||...The input I to our model (on the left) is an 352  352 image and the output (on the right) is a 176  176 saliency map which we resize back to 352  352 with a bilinear interpolation....|
|||...The SCORE block has 2 convolution layers and a softmax to compute the saliency probability by fusing the local (XL) and global (XG) features....|
|||...Contrast features: Saliency is the distinctive quality of a foreground object which makes it stand out from its surrounding background....|
|||...Capturing global context: Detecting salient objects in an image requires the model to capture the global context of the image before assigning saliency to individual small regions....|
|||...Cross Entropy Loss  The final saliency map is computed as a linear combination of the local features XL and global features XG using two linear operators (WL, bL) and (WG, bG)....|
|||...Given the gradient magnitude of saliency maps Cj and gradient magnitude of true saliency maps C of region j, the Dice or IoU boundary loss can be computed as  IoU Loss = 1   2 Cj  Cj   Cj  +   Cj   , ...|
|||...MSRA-B: contains 5000 images, and is widely used for visual saliency detection....|
|||...Without further optimization, this trained model was used to compute the saliency maps of the other datasets....|
|||...The PR curve is computed by binarizing the saliency maps under different probability thresholds ranging from 0 to 1 and comparing against the ground truth....|
|||...LEGS, MC, MDF and DCL are the latest deep learning based saliency detection methods....|
|||...Also the MDF only provided 200 pre-compute saliency maps on SOD dataset, we use the same subset for evaluation....|
|||...Visual comparison of saliency detection results with and without the boundary loss term in Eq....|
|||...As shown in Figure 4, the saliency maps generated from NLDFare fairly coarse and the boundary of the salient objects are not well preserved....|
|||...A visual comparison of the saliency maps is provided in Figure 3....|
|||...All saliency maps of other methods were either provided by the authors or computed using the authors released code....|
|||...Conclusion  The integration of local and global features has already been shown to be a powerful mechanism for saliency detection....|
|||...The resulting model achieves state of the art performance across multiple saliency detection benchmark datasets, does not use any special preor post-processing steps and computes saliency maps 18 to 1...|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Image saliency detection using gabor texture cues....|
|||...Robust saliency detection  via regularized random walks ranking....|
|||...Visual saliency based on multiscale deep features....|
|||...Shallow and deep convolutional networks for saliency prediction....|
|||...Grab: Visual saliency via novel graph model and background priors....|
|||...Geodesic saliency using back ground priors....|
|||...Bayesian saliency via low and mid  level cues....|
|||...Hierarchical saliency detection....|
||47 instances in total. (in cvpr2017)|
|71|Vig_Large-Scale_Optimization_of_2014_CVPR_paper|...y relies on hand-crafted (multiscale) features that are combined in different ways to form a master saliency map, which encodes local image conspicuity....|
|||...Without additional training, these models generalize well to two other image saliency data sets, Toronto and NUSEF, despite their different image content....|
|||...Finally, our algorithm scores best of all the 23 models evaluated to date on the MIT300 saliency challenge [16], which uses a hidden test set to facilitate an unbiased comparison....|
|||...Early algorithms for saliency prediction typically followed the Feature Integration Theory [31] and fused together hand-crafted image features  such as orientation, contrast, color  extracted on multi...|
|||...Other approaches defined saliency in terms of information theory, e.g....|
|||...by self-information [33], information maximization [5], or discriminant saliency that distinguishes target from null hypotheses [7]....|
|||...Moreover, these representations that were learned on MIT1003 generalize well to two other image saliency data sets, Toronto [5] and NUSEF [26], despite their different image content....|
|||...Additionally, we show that our model outperforms all 22 algorithms evaluated to date on the MIT300 saliency benchmark (see Figure 1 and [16])....|
|||...Our results demonstrate that a richer, automatically-derived base of hierarchical features can challenge the state of the art in saliency prediction....|
|||...Finally, we make publicly available the software to compute our saliency maps at http: //coxlab.org/saliency  2....|
|||...Bio-inspired saliency features  We start by reviewing the broad class of biologicallyinspired visual representations that we use here and present adjustments to this architecture to suit the task at hand....|
|||...Richly-parameterized multilayer visual repre sentations  Typical saliency features are hand-tuned and have a loose connection to the architecture of biological visual systems....|
|||...In the following, we describe our changes to generalize  this architecture to the task of saliency prediction....|
|||...Because of the importance of color in determining the saliency of a region we extend the models to multispectral input....|
|||...YUV has been shown to give better results than RGB for saliency prediction (e.g....|
|||...Guided search for optimal saliency features  The guiding principle in the architectural design of the above model family was configurability....|
|||...[3] to more efficiently search the vast bioinspired model space for optimal saliency features....|
|||...Feature learning pipeline  To evaluate the performance of our biologically-inspired representations, we follow the standard saliency learning pipeline of Judd et al....|
|||...the positive class) and non-salient (negative class) image regions is obtained from ground-truth empirical saliency maps (also called fixation density maps) derived from real eye movement data....|
|||...For each image in the training set, we randomly pick 10 salient samples from the top 20% salient regions and 10 non-salient samples from the bottom 70% salient areas of the empirical saliency map....|
|||...To search for good representations for saliency prediction, we consider a subset of 600 images of the MIT1003 eye movement data set [18], and perform model search on this set....|
|||...By thresholding these continuous saliency maps, image regions above the threshold are classified as salient....|
|||...Our final saliency model is an ensemble of individual models, hence the name Ensemble of Deep Networks (eDN)....|
|||...Left to right, top: input, histogram-matched eDN saliency map (blend of 6 Li models), human saliency map; Middle: raw L1, L2, and L3 output; Bottom: L1, L2, L3 output histogram-matched to the human sa...|
|||...As a final step, saliency maps are often smoothed to bet TrainimagesL3Normalizel1l2l3L2Normalizel1l2FeatureextractionOptimalblendofL1,L2,L3features:L1Normalizel1...LinearSVMFixationmap++--TestimagesLa...|
|||...Evaluation: eye movement prediction  Because of the tight link between saliency, attention, and eye movements, saliency models are typically evaluated in terms of how well they predict human gaze in n...|
|||...e consider three other common evaluation metrics: the Earth Movers Distance (EMD, in the context of saliency see [34]), Normalized Scanpath Saliency (NSS) [23], and a similarity score [17]....|
|||...As reference for comparisons, we consider 11 stateof-the-art saliency algorithms: GBVS [10], the multiscale quaternion DCT signature on YUV input (denoted QDCT) [28], Judd [18], AWS [8], CovSal [6] (w...|
|||...ject-out empirical saliency maps and used these to predict the eye movements of the left-out viewer....|
|||...Performance of the various saliency algorithms (baselines, controls, and our best blend) is summarized as averages over all MIT1003 test images in the top part of Table 2. eDN outperforms all individu...|
|||...Performance of saliency algorithms  with (w/ C) and without (w/o C) center bias  on the MIT1003, Toronto, and NUSEF benchmarks for four metrics: AUC, similarity (sim), Normalized Scanpath Saliency (NS...|
|||...Many image processing steps used in saliency computations introduce border artifacts (see [33] for a detailed analysis of several algorithms)....|
|||...Because of repeated filtering across multiple layers, our saliency maps suffer from some border effects as well....|
|||...Example saliency maps for our algorithm and some ref erence methods are shown in Fig....|
|||...Finally, on the MIT300 saliency benchmark, our model achieves 0.8192 AUC (0.5123 similarity and 3.0129 EMD), slightly better than the second best model of Judd et al....|
|||...Discussion and conclusion  Hierarchical feature learning has become a common approach in computer vision, but has not been adequately explored in the context of saliency prediction....|
|||...Our  results  show that methods employing rich, automatically-derived feedforward representations can challenge the state of the art in the field of saliency prediction....|
|||...Boosting bottom-up and top-down visual features  for saliency estimation....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Bottom-up saliency is a dis criminant process....|
|||...MIT saliency benchhttp://people.csail.mit.edu/tjudd/  mark....|
|||...Sample images from MIT1003 [18] with ground truth (Human), proposed (eDN) and reference saliency maps....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...An eye fixation database for saliency detection in images....|
|||...Quaternion-based spectral saliency detection for eye fixation prediction....|
|||...SUN: A Bayesian framework for saliency using natural statistics....|
|||...Learning a saliency map using fixated locations in natural scenes....|
||47 instances in total. (in cvpr2014)|
|72|Wang_Saliency-Aware_Geodesic_Video_2015_CVPR_paper|...Unlike traditional methods, our method incorporates saliency as prior for object via the computation of robust geodesic measurement....|
|||...We first generate framewise spatiotemporal saliency maps using geodesic distance from these indicators....|
|||...Then, high-quality saliency results are produced via the geodesic distances to background regions in the subsequent frames....|
|||...Through the resulting saliency maps, we build global appearance models for foreground and background....|
|||...Our method is based on the proposed visual saliency detection technique that incorporates several visual cues such as motion boundary, edge and color....|
|||...This topic is less explored, mainly due to only a few methods specifically designed for video saliency till now....|
|||...These saliency methods [14, 20, 28, 26, 13, 21], however, usually build their system as a simple combination of existing image saliency models with motion cues....|
|||...Our method correctly estimates the locations of object and background and gains uniform saliency maps....|
|||...Next an inter-frame graph is constructed for producing spatiotemporal saliency maps by the computation of geodesic distance to the estimated background regions of two adjacent frames....|
|||...Finally, to achieve refined estimation of foreground, global appearance model for foreground and background is established by saliency results....|
|||...Spatiotemporal saliency maps, global appearance model and dynamic location model are combined into an energy function for final segmentation....|
|||...(h) Spatiotemporal saliency result Sk for frame F k with consideration of (e) and (f)....|
|||... (cid:27)k}  Y k n is temporally connected to Bk1}; Uk = Yk  Bk; Based on the graph Gk, we obtain a saliency value Sk n ) of frame F k (F k+1) as  ) of superpixels Y k  (5)  n (Y k+1  n  n  (P k+1 fol...|
|||...After obtaining spatiotemporal saliency map Sk and Sk+1 for frame F k and F k+1 , we keep executing this process for next two adjacent frame F k+1 and F k+2 until the end of the video sequence....|
|||...The purpose of U k is to evaluate how likely a pixel is foreground or background according to saptio-temporal saliency maps computed by prior step....|
|||...The unary appearance term Ak encourages labeling pixels which have similar colors as pixels with high saliency for foreground....|
|||...Saliency term U k. The unary saliency term U k is based on our saliency detection results, which penalizes labelings which assign pixel with low saliency value to the foreground....|
|||...Each pixel is stacked into histograms according to its color values and weighted by its saliency value, where the weight for pixel x is Sk(x) and 1  Sk(x) for Hf and Hb, respectively....|
|||...belonging to two kinds superpixels are sampled for forming Hf and Hb: one that the superpixels with saliency value larger than the adaptive threshold defined as the mean value of spatiotemporal salien...|
|||...(a) Input frame F k. (b) Spatiotemporal saliency map Sk....|
|||...The regions within the red boundaries are the superpixels with the saliency value larger than the adaptive threshold....|
|||...Statistical comparison with 5 alternative saliency detection methods using SegTrack dataset [29] with pixel-level ground truth: (a) average precision recall curve by segmenting saliency maps using fix...|
|||...In this section, we first test our method on video saliency detection....|
|||...osed algorithm, we still evaluate the effectiveness of our approach by comparing our spatiotemporal saliency results against the stateof-art saliency methods [31, 13, 28, 14] on the SegTrack dataset [...|
|||...Comparison of previous methods to our spatiotemporal saliency results using SegTrack dataset [29] with ground truth (GT)....|
|||...In the proposed algorithm, we utilize the spatiotemporal edge information to compute the prior saliency maps for videos....|
|||...As this is an important step of our method, we evaluate the results through other saliency methods....|
|||...Using the codes obtained from the corresponding authors, we compare our spatiotemporal saliency results with five alternate methods [31, 13, 28, 14]....|
|||...The first method aims at image saliency detection while the later three ones are designed for video saliency detection....|
|||...To plot  the precision-recall cures, we generate binary saliency maps from each method using a fixed threshold....|
|||...[24] to evaluate the mean absolute error (MAE) between a continuous saliency map S and the binary ground truth G for all image/frame pixels....|
|||...MAE is defined as: MAE =  S  G =N, where N is the number of image/frame pixels. The MAE estimates the approximation degree between the saliency map and the ground truth, which is normalized to [0, 1]....|
|||...MAE provides a better estimate of dissimilarity between the saliency map and ground truth....|
|||...ious saliency maps highlight salient regions in images....|
|||...The minimum recall value of the proposed method does not drop to zero because the corresponding saliency maps are able to effectively detect the salient region with strong response....|
|||...Moreover, our saliency method achieves the best performance up to a precision rate above 0.8, which indicates our saliency maps are more precise and responsive to the salient regions....|
|||...Our saliency maps successfully reduce the MAE by 75% compared to the best result [29] of other methods....|
|||...5 gives a visual comparison of different methods, where brighter pixels indicate higher saliency probabilities....|
|||...The performance of image saliency method [31] is not well, some saliency maps even cannot correctly detect the foreground object....|
|||...In most cases, saliency methods [13, 28, 14] for video are able to accurately locate the salient objects, which perform better than the method [31] for image saliency detection....|
|||...However, some saliency maps using [13, 28, 14] are generated in low resolution and tend to assign relatively low probabilities to pixels inside the objects....|
|||...Conclusions  We presented an unsupervised method that incorporates geodesic distance into saliency empowered video object segmentation....|
|||...Our approach integrated spatiotemporal edge map and geodesic distance to obtain accurate spatiotemporal saliency results as a prior to object segmentation....|
|||...We produced spatiotemporal saliency maps via the computation of geodesic distance to the estimated background on the inter-frame graph for each pair of adjacent frames....|
|||...Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform. In CVPR, 2008....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
||46 instances in total. (in cvpr2015)|
|73|Kim_Salient_Region_Detection_2014_CVPR_paper|...Our main idea is to represent a saliency map of an image as a linear combination of high-dimensional color space where salient regions and backgrounds can be distinctively separated....|
|||...Its goal is to detect salient regions, in terms of saliency map, from an image where the detected regions would draw the attentions of humans at the first sight of an image....|
|||...(b) Saliency maps  (c) Salient regions  upon distinctive color detection from an image....|
|||...mbination of color values in the high-dimensional color transform space that results in a per-pixel saliency map....|
|||...Figure 1 shows examples of our detected saliency map and salient regions....|
|||...There are many previous methods that utilize low-level features such as color and texture for saliency detection....|
|||...[13] proposed a saliency detection method based on color contrast called center-surround difference. Harel et al....|
|||...[11] suggested a graph-based visual saliency (GBVS) model based on the Markovian approach....|
|||...[10] combined global and local contrast saliency to improve the detection performance....|
|||...[14] performed salient object segmentation with multi-scale superpixel-based saliency and closed boundary prior....|
|||...Recently, statistical learning-based models have also been examined for saliency detection....|
|||...regional saliency regressor by using regional descriptors....|
|||...Borji and Itti [5] used patch-based dictionary learning for a rarity-based saliency model....|
|||...First, we over-segment an image into super pixels and estimate an initial saliency map using existing saliency detection techniques....|
|||...From the initial saliency map, we threshold it to obtain a trimap where pixel colors within the definite foreground and the definite background regions will be used as initial color samples of the sal...|
|||...Initial Salient Regions Detection  Superpixel Saliency Features As demonstrated in recent works [15, 26, 27, 34], features from superpixels are effective and efficient for salient object detection....|
|||...To build feature vectors for saliency detection, we combine multiple information that are commonly used in saliency detection....|
|||...Next, we concatenate the histogram feature since this is one of the most effective measurements for the saliency feature as demonstrated in [15]....|
|||...We divide the initial saliency map into 2  2, 3  3, and 4  4 regions and apply adaptive thresholding algorithm for each region individually....|
|||...After that, we sum up the thresholded saliency map to obtain a new saliency map in (b)....|
|||...This saliency map is then thresholded globally to obtain our trimap in (c)....|
|||...The aforementioned features are concatenated and will be used to estimate our initial saliency map....|
|||...In short, our superpixel feature vectors consist of 75 dimensions which combine multiple evaluation matrixes for saliency detection....|
|||...Initial Saliency Map Estimation via Regression After we calculate the feature vectors for every superpixels, we use a regression algorithm to estimate each regions degree to be salient....|
|||...resent our high-dimensional color transform and describe a step-by-step process to obtain our final saliency map starting from the initial saliency map from the previous section....|
|||...Trimap Construction The initial saliency map usually does not detect salient objects accurately and may contain many ambiguous regions....|
|||...This trimap construction step is to identify very salient pixels from the initial saliency map that definitely belong to salient regions and backgrounds, and use our high-dimensional color transform m...|
|||...After merging the three different scale thresholded saliency maps by summation, we obtain a locally thresholded 21-level map T (cid:48)....|
|||...This new saliency map has better local contrast than the initial saliency map....|
|||...(a) test images, (b) initial saliency map after Section 4, (c) refined saliency map SLS using high-dimensional color transform, and (d) our final saliency map after including spatial refinement....|
|||...(a) Input original images, (b) saliency maps are obtained by using a linear combination of RGB channels, and (c) Ground truth saliency map....|
|||...cid:13)2  2  Saliency Map Construction via Optimal Linear Combination of Coefficients To obtain our saliency map, we utilize the definite foreground and definite background color samples in our trimap...|
|||...After we get the optimal coefficient , we can construct  the saliency map as:  SLS(Xi) =  Kijj,  i = 1, 2, , N  (4)  j=1  which denotes the linear combination of the color coefficient of our high-dime...|
|||...In our experiments, we found that three iterations are sufficient to converge to a stable accurate saliency map....|
|||...The final saliency map is obtained adding the color-based saliency map and the spatial saliency map:  Sf inal(Xi) = SLS(Xi) + Ss(Xi)....|
|||...GB with gamma correctionsOnly RGBInitial Saliency MapVisual examples of our estimated step-by-step saliency maps are presented in Figure 6....|
|||...To speed up our refinement processes,we perform saliency map refinements in superpixel level using the mean color of a superpixel as a pixel color, and the center location of a superpixel as a pixel location....|
|||...Experiments  We evaluate and compare the performances of our algorithm against previous algorithms on three representative benchmark datasets: the MSRA salient object dataset [19], the Extended Comple...|
|||...This dataset contains comparatively obvious salient objects on the simple background and is considered as a less challenging dataset in saliency detection....|
|||...The first and second row of Figure 8 show the precisionrecall curves for comparing our saliency method with the aforementioned state-of-the-art saliency detection methods, including those of Zhai et al....|
|||...We compute the F-measure rates for the binarized saliency map as the threshold changes over the range [0, 255], where the F-measure rate is given by  F =  (1 + 2)  P recision  Recall  2  P recision + Recall  ....|
|||...The trimap-based robust estimation overcomes limitations of inaccurate initial saliency map....|
|||...In future, we plan to use more features to solve these limitations and improve the accuracy of saliency detection....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Hierarchical saliency detec tion....|
||46 instances in total. (in cvpr2014)|
|74|Exploiting Saliency for Object Segmentation From Image Level Labels|...Exploiting saliency for object segmentation from image level labels  Seong Joon Oh  Rodrigo Benenson  Anna Khoreva  joon@mpi-inf.mpg.de  benenson@mpi-inf.mpg.de  khoreva@mpi-inf.mpg.de  Zeynep Akata, ...|
|||...We propose using a saliency model as additional information and hereby exploit prior knowledge on the object extent and image statistics....|
|||...Figure 1: We train a semantic labelling network with (a) image-level labels and (b) saliency masks, to generate (c) a pixel-wise labelling of object classes at test time....|
|||...In this work, we propose to exploit class-agnostic saliency as a new ingredient to train for class-specific pixel labelling; and show new state of the art results on Pascal VOC 2012 semantic labelling...|
|||...Using class-agnostic object saliency we can find the segment corresponding to some of the detected object seeds....|
|||...Our saliency model is itself trained from bounding box annotations only....|
|||... Compare recent seed methods side by side, and ana lyse the importance of saliency towards final quality....|
|||...3 presents our overall architecture, 4 investigates suitable object seeds, and 5 describes how we use saliency to guide the convnet training....|
|||...Second, they use a manually crafted classagnostic saliency method, while we use a deep learning based one (which provides better cues)....|
|||...Fourth, we report significantly better results, showing in better light the potential of saliency as additional information to guide weakly supervised semantic object labelling....|
|||...By using bounding boxes, these maps end up being diffuse; in contrast, our saliency map has sharp object boundaries, giving more precise guidance to the semantic labeller....|
|||...ation about the class scores at each pixel (albeit with low recall for foreground classes), and the saliency output provides a per-pixel (class agnostic) objectness score....|
|||...Image saliency has multiple connotations, it can refer to a spatial probability map of where a person might look first [48], a probability map of which object a person might look first [23], or a bina...|
|||...In this paper we use saliency as an ingredient: improved saliency models would lead to improved results for our method....|
|||...We describe in 6.1 our saliency model design, trained itself in a weakly supervised fashion from bounding boxes....|
|||...Guided Segmentation architecture  While previous work have explored sophisticated training losses or involved pipelines, we focus on saliency as an effective prior knowledge, and keep our architecture simple....|
|||...Given an image and image-level labels, the guide labeller module combines cues from a seeder (4) and saliency (5) sub-modules, producing a rough segmentation mask (the guide)....|
|||...Intuitively, for the foreground region we only need a small discriminative region, as saliency will fill in the extent; we thus care about precision at  20% recall....|
|||...In comparison our saliency approach will provide +17 mIoU and  5.1....|
|||...Saliency  We propose to use object saliency to extract information about the object extent....|
|||...We use a convnet based saliency estimator (detailed in 6.1) which adds the benefit of translation invariance....|
|||...When using saliency to guide semantic labelling at least two difficulties need to be handled....|
|||...For one, saliency per-se does not segment object instances....|
|||...We measure the saliency quality when compared to the ground truth foreground on Pascal VOC 2012 validation set....|
|||...Albeit our convnet saliency model is better than handcrafted methods [14, 52], in the end only about 20% of images have reasonably good (IoU > 0.6) foreground saliency quality....|
|||...Crucially, our saliency system is trained on images containing diverse objects (hundreds of categories), the object categories treated as unknown....|
|||...To ensure clean experiments we handicap the system by removing any instance of Pascal categories in the object saliency training set (figure 5)....|
|||...Our saliency model captures a general notion of plausible foreground objects and background areas (details in 6.1)....|
|||...We want to combine them in such a way that seed signals are well propagated throughout the foreground saliency mask....|
|||...Given a saliency mask, we assign all foreground pixels to a class randomly picked from the ground truth image labels....|
|||...4414  (a) High quality  (b) Medium quality  (c) Low quality  Figure 4: Example of our saliency map results on Pascal VOC 2012 data....|
|||...t c e j b o  t n e i l a S  y c n e i l a S  s e x o b d n a  t l u s e r  l e d o m  Figure 5: Example of saliency results on its training data....|
|||...2) We assume the saliency might trigger on objects that are not part of the classes of interest....|
|||...We train the DeepLab-v2 ResNet [7] over a subset of MSRA [26], a saliency dataset with class agnostic bounding box annotations....|
|||...Thus, the saliency model does not leverage class specific features when Pascal images are fed....|
|||...The image, its labels (bicycle, chair), seeds, and saliency map are their input....|
|||...At test time, the saliency model generates a heatmap of foreground probabilities....|
|||...Adding a classifier on top of the saliency (G0  G1) provides only a negligible improvement (45.8  46.2)....|
|||...Table 2 also reports a saliency oracle case on top of G2....|
|||...Thus, the quality of saliency is an important ingredient, and there is room for further gains....|
|||...This shows that saliency by itself is already a strong cue....|
|||...methods using saliency (STC) or using additional human annotations (MicroAnno, CheckMask)....|
|||...We have shown that saliency is a viable option for feeding the object extent information....|
|||...We expect that a deeper understanding of the seeder methods and improvements on the saliency model can lead to further improvements....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Can saliency map models predict human egocentric visual attention?...|
||46 instances in total. (in cvpr2017)|
|75|cvpr18-Detect Globally, Refine Locally  A Novel Approach to Saliency Detection|...Detect Globally, Refine Locally: A Novel Approach to Saliency Detection  Tiantian Wang1, Lihe Zhang1, Shuo Wang1, Huchuan Lu1, Gang Yang2, Xiang Ruan3, Ali Borji4  1 Dalian University of Technology,2 ...|
|||...Introduction  Visual saliency has gained a lot of interest in recent years....|
|||...overlap partly or entirely with each other, saliency detection still remains challenging in computer vision tasks....|
|||...Existing saliency detection methods [18, 10, 33] attempt to combine hierarchical features to capture distinctive objectness and detailed information simultaneously....|
|||...Moreover, a recurrent structure is proposed in order to gradually refine the predicted saliency map over time....|
|||...The BRN takes both the initial RGB image and the saliency map as input....|
|||...The saliency map serves as the prior map which can assist the learning process to generate more accurate predictions....|
|||...Related Work  Various approaches have been proposed to solve the problem of saliency detection....|
|||...Region(cid:173)based Saliency  Region-based approaches leverage each image patch as the basic processing unit for making saliency prediction....|
|||...A classifier network is employed to infer the saliency score of each image segment....|
|||...A neural network with fully-connected layers is proposed to evaluate the saliency of every region....|
|||...nd-crafted features by a large margin, they ignore important spatial information as they assign one saliency label to each image patch....|
|||...Skip connections aim to add deeper layers to lower ones and integrate saliency prediction at multiple resolutions....|
|||...In [18], a multiscale FCN is proposed to capture effective semantic features and visual contrast information for saliency inference....|
|||...[33] learn to aggregate multi-level feature maps at each resolution and predict saliency maps in a recursive manner....|
|||...propose a stagewise refinement model and a pyramid pooling module to include both local and global context information for saliency prediction....|
|||...Kuen [15] firstly adopt a convolutionaldeconvolutional network to produce a coarse saliency map....|
|||...Then a spatial transformer and recurrent network units are used to iteratively search for the attentive image sub-regions for the saliency refinement....|
|||...A hierarchical recurrent CNN is adopted to progressively recover image details of saliency maps through integrating local context information....|
|||...utilize the predicted saliency map as the feedback signal, which serves as the saliency prior to automatically learn to refine the saliency prediction by correcting its previous errors....|
|||...The Proposed Method  In this section, we will elaborate on the proposed framework for saliency detection....|
|||...Recurrent Localization Network  3.1.1 Base Network  We tackle the saliency detection problem based on the fully convolutional network....|
|||...3.1.2 Network Architecture  Most of the existing saliency detection methods typically involve a combination of multi-scale convolutional features, which is driven by the notion that different layers o...|
|||...Contextual information [22, 36, 29] has been proved effective in saliency detection....|
|||...Larger  context usually captures global spatial relations among objects while smaller context focuses on the local appearance, both contributing to the saliency detection....|
|||...Compared to the initial saliency map, the refined one should not change too much in terms of the visual appearance....|
|||...Following this initialization, saliency prediction of a certain pixel will be primarily influenced by the central coefficient of the propagation map and also be be affected by the other coefficients....|
|||...Given a saliency map with continuous values normalized to the range of 0 and 255, we compute the corresponding binary maps by using every possible fixed integer threshold....|
|||...Also, we utilize the F-measure score to evaluate the quality of a saliency map, which is formulated by a weighted combination of Precision and Recall....|
|||...e refinement map at position i will be generated by a multiplied sum of the propagation map and the saliency map in the neighborhood of i.  s i =  nn  X  d=1  vd i  sd  i , d  1, 2, ..., n  n,  (5)  w...|
|||...The parameters of the BRN, where K = 1 represents the one-channel saliency map....|
|||...Given the saliency map S and ground truth mask G, the MAE score can be calculated by the element-wise difference between S and G,  M AE =  1  W  H  W  H  X  X  i=1  j=1   S(i, j)  G(i, j) ,  (8)  wher...|
|||...To qualitatively evaluate the proposed method, we visualize some example saliency maps of our method with respect to the above-mentioned approaches in Figure 8....|
|||...From this picture, we can see that our method can produce more accurate saliency maps which are much closer to the ground truth masks....|
|||...Salientshape: Group saliency in image collections....|
|||...Environment exploration for object-based visual saliency learning....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
|||...Recurrent attentional networks for saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Task-driven visual saliency and attention-based visual question answering....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Kernelized In ECCV, pages  subspace ranking for saliency detection....|
|||...Hierarchical saliency detec tion....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||46 instances in total. (in cvpr2018)|
|76|Siva_Looking_Beyond_the_2013_CVPR_paper|...ang,lourdes]@eecs.qmul.ac.uk  Abstract  We propose a principled probabilistic formulation of object saliency as a sampling problem....|
|||...We then sample the object saliency map to propose object locations....|
|||...As the word saliency is widespread in the literature and used to refer to whatever a researcher currently considers interesting, it is important to distinguish between dif Author funded by EPSRC under...|
|||...Human saliency was first formulated as a predictor of human fixation in images [16]....|
|||...Recent applications in computer vision have led to an increased interest in object saliency formulations [3, 6, 13, 15, 31] that propose salient bounding boxes in images as potential object locations....|
|||...Most existing approaches for object saliency can be characterised as extensions of expert-driven human saliency methods or supervised learning methods....|
|||...Object saliency methods that build on expert-driven human saliency approaches [6, 13, 15] tend to use cognitive psychological knowledge of the human visual system and finds image patches on edges and ...|
|||...Recently, object saliency approaches based on supervised learning have emerged [3, 20, 25]....|
|||...These annotations can then be used to train a saliency model (based on global and local image features) to predict patches of interest in unseen images....|
|||...Our approach to object saliency and object location proposal in comparison with some existing techniques....|
|||...Furthermore, our object proposals, based on sampling our object saliency map, correctly locate objects in many images on the first proposal....|
|||...Prior work  Early works on human saliency were developed from biological models of the human visual system, and estimated fixation points where a human viewer would initially focus....|
|||...Our interest, motivated by applications in object detection, lies in object saliency approaches that can detect salient regions  3237 3237 3239  as potential object locations....|
|||...Object saliency methods have made use of global frequency-based features [1, 15], which finds regions characterised by rare frequencies in the Fourier domain as salient....|
|||...[12] develops an unsupervised approach that integrates both saliency computation and object location proposal....|
|||...[3] starts by sampling rectangular regions based on the global frequency saliency map of [15] then adds additional cues such as colour contrast and super-pixel straddling....|
|||...In [32] the current image is registered to similar images and the difference between the registered image and the similar images are used as the saliency map....|
|||...In this paper we show that our saliency based object location proposal achieves higher weakly supervised annotation accuracy than other methods that propose object locations, or even those weakly supe...|
|||...Sampling-based Saliency Given an image I and a large corpus of unlabelled images D, we wish to find a saliency map SI for image I....|
|||...Finally, the saliency map is smoothed based on image segmentation (SI)....|
|||...of px indicates that the patch x is common in the image corpus, and the saliency of patch x is obtained as:  Sx = 1  px  (8)  Assuming the noise is uniform and Gaussian2 over the space of image patche...|
|||...To account for scale changes in salient objects, we compute saliency Sx (8) at four different image scales [1, .8, .5, .3] and average the result over the four scales  Sx as the patch saliency....|
|||...First, as in [13], immediate context information is included by weighting the saliency value of each pixel by their distance from the high salient pixel locations....|
|||...2, the resulting saliency map Sc is blurred due to the use of overlapping patches and image boundaries (edges between objects and background) are not preserved....|
|||...For each segment region, the average saliency from Sc is obtained and used as the final saliency value for that segment, producing our saliency map SI....|
|||...Instead we consider B, the set of all boxes that would be blocked by b0, including itself, and seek b  B, the box that best explains the saliency of all bounding boxes in B....|
|||...To find such a box, we describe the region from which the boxes in B are drawn using a saliency weighted average BoW SIFT histogram:  dif SIFT(bi)  (10)  SIFT =  1(cid:10)N  i=0 di  N(cid:5)  i=0  whe...|
|||...b = arg min bi  (cid:5) f SIFT(bi)  SIFT (cid:5)2  The saliency score di for box bi is defined as: (cid:5) pui  S(p)  1 ui r  (cid:5) pbi  S(p)  di =  1 bi r  (11)  (12)       refers to the size of th...|
|||...Sampling from the saliency map without non-maximum suppression (NMS) results in an over sampling of high saliency regions....|
|||...Sampling with NMS means that the lower saliency region will still be sampled from....|
|||...Bounding Box Sampling  Bounding boxes that should contain an object can be selected by sampling from a per-pixel saliency map....|
|||...In the past several options have been explored [19], such as thresholding the saliency map followed by connected region detection [15] or selecting a bounding box containing 95% of the image saliency [19]....|
|||...For proposing multiple bounding boxes per image the saliency map may be randomly sampled from [3], or sampled from the highest score to the lowest score with non-maximum suppression (NMS) [12]....|
|||...Random sampling based on saliency map density results in over-sampling regions of high saliency....|
|||...As with NMS, we select the box with the highest saliency score (b0) that is not near the other T locations....|
|||...Let Di,j  {0, 1} be a vector indicating if the jth box proposed by the saliency algorithm correctly detects an object in the ith image, then:  (cid:10)N (cid:10)N  Rone(j) =  P one(j) =  N  jN  i=1 max(Di,1, ....|
|||...Comparison with Other Saliency Methods: We compare our object proposal based annotation (Our) results against other object proposal methods in table 3....|
|||...Evaluation of Saliency Maps  As in [13] we evaluate the saliency maps ability to predict foreground pixels by reporting the precision recall curve (PRC) and average precision (AP) as a function of the...|
|||...For completeness we also report PRCs for the MSRA saliency dataset [1]....|
|||...Figure 6 shows the PRCs of our method and some other existing saliency approaches; some examples can be seen in fig....|
|||...Our image saliency in comparison to CA [13], SR [15], and FT [1] methods....|
|||...7, we analyse the contribution of using the current image I in addition to other similar images when computing the saliency map....|
|||...We plot the precision-recall curve of the saliency map computed using similar images without current image DI \ {I} (across image saliency A), using just the current image I (within image saliency W),...|
|||...Note that although across-image saliency and within-image saliency have similar performance, combining them provides a boost in performance, particularly in the region of high-precision we are most co...|
|||...Conclusion  We have presented a novel unsupervised approach to the problems of saliency and bounding box annotation5, and shown how it substantially outperforms all other saliency based approaches to ...|
||45 instances in total. (in cvpr2013)|
|77|Pan_Shallow_and_Deep_CVPR_2016_paper|...Shallow and Deep Convolutional Networks for Saliency Prediction  Junting Pan, Elisa Sayrol and Xavier Giro-i-Nieto  Kevin McGuinness and Noel E. OConnor  Image Processing Group  Universitat Politecnic...|
|||...The recent publication of large datasets of saliency prediction has provided enough data to train end-to-end architectures that are both fast and accurate....|
|||...Our objective is to compute saliency maps that represent the probability of visual attention on an image, defined as the eye gaze fixation points....|
|||...In our case we have adopted a completely data-driven approach, using a large amount of annotated data for saliency prediction....|
|||...Figure 1 provides an example of an image together with its ground truth saliency map and the two saliency maps predicted by the proposed convnets: a shallow one and a deep one....|
|||...Input Image (top left) and saliency maps from the ground truth (top right), our shallow convnet (bottom left) and our deep convnet (bottom right)....|
|||...The saliency prediction problem, however, poses two specific challenges that differentiate it from classic image  598  classification....|
|||...The second challenge to address when using convnets for saliency prediction is that a saliency score must be estimated for each pixel in the input image, instead of a global-scale label for the whole image....|
|||...The saliency map at the output must present a spatial coherence and a smooth transition between neighbouring pixels....|
|||...To the authors knowledge, these were the first convnets that formulate saliency prediction as an end-to-end regression problem....|
|||...It also shows, prediction performance in the MIT Saliency Benchmark and LSUN Saliency Prediction Challenge 2015 and they are compared with other models....|
|||... the next natural step to two main trends in deep learning: using convolutional neural networks for saliency prediction and training these networks by formulating saliency prediction as an end-toend r...|
|||...An early attempt of predicting saliency with a convnet was the ensembles of Deep Networks (eDN) [27], which proposed an optimal blend of feature maps from three different convnet layers, that were fin...|
|||...[20] proposed an architecture with three convnets working in parallel where the three final fully connected layers are combined in a single layer to obtain the saliency map....|
|||...The work by Li and Yu [18] proposes three nested windows as inputs to three different convnet at different scales that are fused together to obtain an aggregated saliency map....|
|||...That is, first, to detect local saliency, a deep neural network (DNN-L) learns local patch features to determine the saliency value of each pixel....|
|||...Second, the local saliency map together with global contrast and geometric information are used as global features to obtain object candidate regions....|
|||...In our work we are interested in finding saliency maps rather than salient object detection by training convnets endto-end....|
|||...We also focus on novel databases that are annotated for the purpose of saliency prediction....|
|||...A first model was built using the 10, 000 saliency maps from the SALICON dataset [14], and a second model using the 6, 000 saliency maps from the iSUN dataset....|
|||...Deep Convnet  The second approach explored in this paper is the adaptation of an existing very deep convnet trained for image classification for the task of saliency prediction....|
|||...The network was designed considering the amount of available saliency maps for training it from scratch....|
|||...Notice that the 2,304-dimensional vector at the output is mapped into a 2D array of [48  48], which corresponds to the saliency map....|
|||...Our hypothesis is that these lower layers trained for classification can also be transferred for the task of saliency prediction....|
|||...A deconvolution layer follows the final convolution to produce a saliency map that matches the input width and height....|
|||...We used several standard pre-processing techniques on both the input images and the target saliency maps....|
|||...We similarly preprocessed the saliency maps by subtracting the mean and scaling to [1, 1]....|
|||...It was built from images of the Microsoft CoCo: Common Objects in Context [19] dataset, which inspired the SALICON naming: SALIency in CONtext....|
|||...However, the saliency maps in  SALICON were not collected with eyetrackers as in most popular datasets for saliency prediction, but with mouse clicks captured in a crowdsourcing campaign....|
|||...MIT1003 and MIT300 [15] This dataset is the most well-known among saliency prediction researchers....|
|||...Results  The evaluation of saliency prediction has received the attention of several researchers, resulting in various proposed approaches....|
|||...Our experiments consider several of these, in a similar way to the MIT saliency benchmark [2]....|
|||...Some of these metrics compare the predicted saliency maps with the maps generated from the fixation points of the ground truth, while some other metrics directly compare with the fixation points....|
|||...Figure 5 presents a qualitative comparison of the two networks, showing the predicted saliency maps alongside the ground truth fixation maps....|
|||...Conclusions  We propose a novel end-to-end approach for training convnets in the task of saliency prediction....|
|||...with respect to hand-crafted solutions and highlight the importance of an end-to-end formulation of saliency prediction....|
|||...On the other hand, the shallow network requires less memory at train time and generates saliency maps much faster because it has fewer layers....|
|||...Mit saliency benchmark....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet....|
|||...Visual saliency based on multiscale deep features....|
|||...Rare2012: A multi-scale raritybased saliency detection with its comparative statistical anal 605  ysis....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Deep networks for saliency detection via local estimation and global search....|
||45 instances in total. (in cvpr2016)|
|78|Tavakoli_Paying_Attention_to_ICCV_2017_paper|...Encouraged enough, we lay the foundation of this study on the role of saliency in the construct of image descriptions, where the order of named objects is momentous in a sentence....|
|||...On  the other hand, obeying natural scene statistics, the importance and saliency of an object are equal [68]....|
|||...Furthermore, building on top of the findings of such comparison, we study the contribution of saliency in image captioning models....|
|||...3 summarizes these statistics, signifying that some objects are often more attended in agreement with the findings of [65], providing us some idea about the importance and saliency of objects....|
|||...Example saliency maps generated from the sentences given in Figure 1 with the fixations overlaid....|
|||...Some generated saliency map examples are provided in Fig....|
|||...Having a saliency map and fixation information, we employ the trivial fixation prediction evaluation criteria [8] for assessing a sentence in terms of attention....|
|||...We utilized area under the curve (AUC) [30], correlation coefficient (CC), and normalized scanpath saliency (NSS) [47]....|
|||...Model performance in terms of attention: the evaluation of saliency maps generated from descriptions using fixations....|
|||...1 and its corresponding saliency in Fig....|
|||...Thus, we build a captioning model with visual features boosted by a saliency model in order to investigate potential improvements using a bottom-up saliency model....|
|||...In other  words, we focus on answering: Can saliency benefit image captioning by machine?...|
|||...We compute the saliency using the image features of VGG network in order to be consistent with the image feature pipeline....|
|||...Afterwards, we learn a regression to approximate the human fixations using extreme learning machines [27], following the saliency model of [59]....|
|||...That is, an ensemble of saliency predictors are learnt to perform a regression from image features to the saliency space....|
|||...The final saliency is the mean of the predicted saliencies from the members of the ensemble....|
|||...The VGG features are not fine-tuned for the specific task of saliency prediction, and are treated as generic descriptors [3], preventing the explicit learning of top-down factors that contribute to sa...|
|||...The saliency computation is hence bottom-up and in the category of learning-based saliency models [66]....|
|||...Despite the proposed model of saliency computation is bottom-up, to prevent arguments on the role of implicitly learned top-down factors, we also used the saliency maps from a traditional pure bottom-...|
|||...We boost the CNN features before feeding them to the language model with the saliency map of the image....|
|||...The CNN feature maps and saliency maps are of size 7  7....|
|||...The saliency maps are normalized so that each pixel has a value between 0 and 1....|
|||...Block diagram showing the saliency boosting and feature linearization before feeding the feature to the language model....|
|||...A mean pooling is also performed on the 7  7 saliency map with a 3  3 moving window, denoted as Sal....|
|||...To combine this saliency data with the image features, the local features corresponding to the feature maps are weighted by their corresponding saliency value....|
|||...(i, j) = 1 + SalL1 (i, j),  (2)  (3)  where Fl is the image feature map, SalL1 is the L1 normalized saliency map, and  is an attenuation factor to control compactness of the aggregated saliency....|
|||...Evaluating saliency contribution....|
|||...To understand the contribution of saliency, we define a baseline using a uniform saliency where the saliency map is all ones....|
|||...Comparing the proposed baseline and the saliency-boosted model, there is no significant improvement by boosting the model using a saliency in this dataset....|
|||...As depicted, contrary to the MS COCO results, the proposed model outperforms the baseline, indicating that a better generalization is achieved by using saliency boosting considering both saliency models....|
|||...paring between the results obtained on the MS COCO evaluation and augmented PASCAL50S datasets, the saliency boosting seems not contributing when we are training and testing a model on one database....|
|||...wever, shows that for across database and an unseen data with different visual characteristics, the saliency boosting contributes and improves the performance of the captioning model....|
|||...We quantified the attention agreement between human and machine by generating saliency maps from descriptions....|
|||...(2) Can saliency benefit image captioning by machine?...|
|||...We investigated the role of bottom-up saliency by proposing a simple saliencyboosted captioning model....|
|||...The use of bottom-up attention coincides with the role of saliency in language formation, which followers of Wundts theory mostly back....|
|||...In saliency research, SALICON [29] is the largest crowd-source based database using mouse movements....|
|||...he contextual information on a finegrained manner and is recommended to be avoided for interpreting saliency results [58]....|
|||...Following evaluation purposes for understanding saliency contribution, we, thus, avoid using data that relies on mouse movements....|
|||...ing a system for assistive purposes, we look into the fundamental questions of the relation between saliency and sentence constructs and how saliency may benefit captioning....|
|||...Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations....|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition....|
|||...Top-down  visual saliency guided by captions....|
||44 instances in total. (in iccv2017)|
|79|cvpr18-Emotional Attention  A Study of Image Sentiment and Visual Attention|...ming to model the human emotion prioritization computationally, we design a deep neural network for saliency prediction, which includes a novel subnetwork that learns the spatial and semantic context ...|
|||...Such properties have been incorporated in computational models that predict visual saliency with impressive performance [34, 2, 70]....|
|||...Related work  Predicting human attention: Substantial research has been done on saliency predictionusing computational models to predict human attention [30, 4]....|
|||...Early saliency prediction models use pixel-level image attributes, such as contrast, color, orientation, and intensity [34, 41, 23]....|
|||...An earlier advocate for context-aware saliency is [27], which also focuses on low-level image features....|
|||...Large gain in saliency prediction has resulted from the recent resurgence of deep neural networks [60, 66, 68, 32, 42, 69, 18, 49], such as SALICON [33], DeepGaze [44], and DeepFix [42]....|
|||...igma equal one degree of visual angle and then normalizing the map to maximum 1 (a common method in saliency research [46])....|
|||...The gray dashed arrows illustrate how the relative saliency of different regions within an image are modified through the subnetwork....|
|||...Each dimension represents the saliency weight of the corresponding input channel....|
|||...We then perform a convolutional layer after the new subnetwork with a 11 kernel to reduce the 1024-channel 2D images into a single-channel 2D saliency map of dimension 2518 pixels....|
|||...Finally, we resize the saliency map back to the dimension of the original image....|
|||...The Area Under the ROC Curve (AUC) [28] treats the saliency map as a binary classifier....|
|||...We further use six similarity metrics to measure the similarity between the  7525  Figure 6: Qualitative results generated by our saliency model in comparison with state-of-the-art methods....|
|||...ference between predictions from CASNet and N-CASNet: colors close to orange/red indicate increased saliency after applying the subnetwork for contextual saliency, whereas colors close to blue/green i...|
|||...Two are non-DNN models with top performance in the non-DNN model category: Boolean Map based Saliency (BMS) [72] and Saliency via Sparse Residual & Outlier Detection (SROD) [63]....|
|||...Two are classic bottom-up approaches: Graph-Based Visual Saliency (GBVS) [31] and Itti-Koch model (IttiKoch) [34]....|
|||...h state-of-the-arts models: We report results for our model both with the subnetwork for contextual saliency prediction (i.e., CASNetContext-Adaptive Saliency Network) and without the subnetwork (i.e....|
|||...7 illustrates how CASNet uses contextual information to improve saliency prediction by learning the relative importance of emotional objects, which more closely matches human emotion prioritization th...|
|||...We compute the average predicted saliency scores of negative, neutral, and positive objects in EMOd by CASNet....|
|||...An ANOVA (object saliency scores as the dependent variable, object emotion types as the independent variable) for each model further shows that CASNet has the largest F value (CASNet: 92.17, N-CASNet:...|
|||...By correlating the differences of each model with the human ground truth across images, we evaluate the degree to which models predict the relative saliency of the co-occurring objects....|
|||...A larger correlation indicates that the model does a better job at predicting the relative saliency of co-occurring objects in the ground truth data....|
|||...Relative saliency of co-occurring objects: The capability of the proposed channel weighting subnetwork is not limited to emotion prioritization, but more broadly, it is able to predict the relative im...|
|||...Conclusion  In this paper, we present EMOda new emotional attention dataset for research on visual saliency and emotioneliciting stimuli....|
|||...Cat2000: A large scale fixation dataset for boosting saliency research....|
|||...Mit saliency benchmark....|
|||...Where should saliency models look next?...|
|||...A deep multi-level network for saliency prediction....|
|||...Predicting human eye fixations via an lstm-based saliency attentive model....|
|||...Attention identification via relative saliency of localized crowd faces....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Informationtheoretic model comparison unifies saliency metrics....|
|||...Methods for comparing scanpaths and saliency maps: strengths and weaknesses....|
|||...A deep spatial contextual long-term recurrent convolutional network for saliency detection....|
|||...Salgan: Visual saliency pre [54] H. Pashler....|
|||...An eye fixation database for saliency detection in images....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Visual saliency detection via sparse residual and outlier detection....|
|||...Opensalicon: An open source implementation of the salicon saliency model....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Turkergaze: Crowdsourcing saliency with webcam based eye tracking....|
||44 instances in total. (in cvpr2018)|
|80|Kummerer_Understanding_Low-_and_ICCV_2017_paper|...gh-level features in predicting fixation locations, while simultaneously achieving state-of-the-art saliency prediction....|
|||...In computer vision this problem is framed as saliency prediction1: predicting human fixation locations for a given image [21, 26, 25]....|
|||...The success of these saliency prediction models suggests that the high-level image features encoded by deep networks (e.g....|
|||...Here, we instead suggest that saliency prediction models may be neglecting low-level image features (local contrast) and overweighting  1 Note that the term saliency prediction is sometimes also used ...|
|||...This results in a saliency map, which is then blurred, combined with a center bias and converted into a probability distribution by means of a softmax....|
|||...We come to this conclusion via three novel contributions:   A new state-of-the-art model for saliency prediction (the DeepGaze II model) that is based on deep neural network features pre-trained on ob...|
|||... A strong low-level baseline model for saliency prediction (Intensity Contrast Feature or ICF) that is based on local intensity and contrast....|
|||...tics [51] or combinations of lowand high-level features [26] (see [3] for a comprehensive review of saliency models before the advent of pre-trained deep features)....|
|||...The state-of-the-art  in saliency prediction improved markedly since 2014 with the advent of models using deep neural networks....|
|||...Since the initial success of transfer learning for saliency  4790  prediction, a variety of new models followed this example to further improve saliency prediction performance....|
|||...The SALICON model [19] fine tunes a mixture of deep features from AlexNet [29], VGG-16 [39] and GoogLeNet [41] for saliency prediction using the SALICON and OSIE [50] datasets....|
|||...DeepFix [30] and PDP [22] fine-tune features from the VGG-19 network [39] for saliency prediction using the SALICON and the MIT1003 dataset....|
|||...ew state-of-the art model we introduce here is that rather than fine-tuning the VGG-19 features for saliency prediction, we train a read-out network that uses a point-wise nonlinear combination of dee...|
|||...Therefore we train and evaluate our models using the framework of log-likelihood (specifically reported as information gain explained, see [31]) and additionally report key metrics (AUC, sAUC and NSS)...|
|||...Deep Object Features (DeepGaze II) model  Here we describe the architecture of our saliency prediction model that is based on deep features that are trained on object recognition (Fig....|
|||...Model Training  Our models are trained using maximum likelihood learning (see [31] for an extensive discussion of why loglikelihoods are a meaningful metric for saliency modelling)....|
|||...This dataset consists of 10000 images with pseudofixations from a mouse-contingent task and has proven to be very useful for pretraining saliency models [19, 22, 30]....|
|||...We determine this point by comparing the performance from the last three epochs to the performance five  Model DeepGaze I [32] DSCLRCN [34] DeepFix [30] SALICON [19] DeepGaze II  AUC 84% 87% 87% 87% 8...|
|||... we do not optimize a blur kernel for the models because all state-of-the-art models produce smooth saliency maps)....|
|||...Additionally, we evaluate the traditional area under the ROC curve metrics AUC and sAUC and the more recent Normalized Scanpath Saliency (NSS, [35])....|
|||...For AUC and NSS the models density prediction is the right saliency map to use for evaluation....|
|||...MIT300 Saliency Benchmark  Here we report the performance of our Deep Object Feature model DeepGaze II on the MIT saliency benchmark (the held-out MIT 300 set) (Table 1)....|
|||...Because the MIT Benchmark requires submission of model predictions as JPEG images, one must decide how to store the saliency maps as JPEG images....|
|||...The different metrics interpret the saliency maps differently and we translated the predic 4793  BaselineIttiKochAIMBMSeDNICFDeepGaze IOpenSALICONDeepGaze IIGoldStandard020406080100information gain ex...|
|||...MIT1003 dataset  The MIT300 hold-out set determines the state-of-theart in saliency prediction....|
|||...We evaluate a number of important saliency models using information gain explained (Fig....|
|||...Here we compare our low-level ICF and high-level DeepGaze II saliency models to improve our understanding of the features that can explain human fixation locations....|
|||...First we look at the images for which each model performs best and worst compared to the center bias and show the respective saliency predictions of the models (Fig....|
|||...Even though the best images for ICF and DeepGaze II are partly the same, the predicted saliency maps clearly separate the models....|
|||...Discussion  In this paper we compare the predictive performance of lowand high-level features for saliency prediction by introducing two new saliency models that use the same readout architecture on t...|
|||...Our results suggest that explicitly modelling low-level contributions to saliency could be used to improve the robustness of saliency models....|
|||...Mit saliency benchmark....|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes....|
|||...End-to-end saliency mapping via probability distribution prediction....|
|||...A Benchmark of Computational Models of Saliency to Predict Human Fixations A Benchmark of Computational Models of Saliency to Predict Human Fixations....|
|||...Opensalicon: An open source implementation of the salicon saliency model....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
|||...Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet....|
|||...A deep spatial contextual long-term recurrent convolutional network for saliency detection....|
|||...RARE2012: A multi-scale raritybased saliency detection with its comparative statistical analysis....|
||44 instances in total. (in iccv2017)|
|81|cvpr18-Progressive Attention Guided Recurrent Network for Salient Object Detection|...ba-inc.com  Abstract  Spatial Attention  Effective convolutional features play an important role in saliency estimation but how to learn powerful features for saliency is still a challenging task....|
|||...Conventional saliency methods usually utilize hand-crafted low-level features such as color, intensity, contrast to predict saliency....|
|||...Due to the semantic information obtained from high-level features, CNN based saliency detection approaches have successfully broken the bottleneck of hand-crafted features....|
|||...Many state-of-the-art methods design saliency models by integrating multi-level convolutional features together....|
|||...To the best of my knowledge, there are not many works that utilize attention mechanisms to process features for saliency estimation....|
|||...In an image, not all spatial positions are contributing to saliency prediction in the same way and there sometimes exist background regions that generate distractions....|
|||...Moreover, in most of the state-of-the-art CNN based methods, saliency values are estimated by dealing with multi-scale side-output convolutional features....|
|||...But effects of saliency prior is greatly weakened due to the concatenation with raw images....|
|||...Most of traditional saliency methods are based on low-level manually designed features, such as color, region contrast, etc....|
|||...Recently, deep convolutional neural networks have set new state-of-the-art on saliency detection....|
|||...In [13], multi-scale features are extracted from CNNs to estimate saliency of all super-pixels in the image....|
|||...[37] take both local and global context into account and integrate them into a multi-context deep CNNs for saliency detection....|
|||...Following the success of end-to-end deep networks in semantic segmentation [19], more works are trained end-to-end to predict pixel-wise saliency maps....|
|||...propose a refinement model by adding low-level detailed features to the saliency map generated in a stagewise manner....|
|||...Motivated by the above mentioned methods, we find that effective features are of great importance to saliency detection....|
|||...At each time step, both the input RGB image and a saliency prior map feed forward through the RFCN to obtain the predicted saliency map and this saliency map will be treated as a saliency prior for th...|
|||...This kind of recurrent architecture can refine saliency map to a certain extent, however, it can not produce enough effective features for saliency detection because the effects of saliency prior are ...|
|||...Then the attended sub-region will be the input of the next time step and a local saliency map of this sub-region is generated to refine the corresponding region of the whole prediction map....|
|||...Although attention can drive the model to focus on a specific sub-region at each time step, this will cause redundancy in saliency prediction because of the overlap between these sub-regions....|
|||...Powerful attentive features are generated to conduct saliency prediction....|
|||...Therefore, directly exploiting convolutional features to predict saliency can lead to sub-optimal results because of the distraction of non-salient regions....|
|||...Instead of considering all spatial positions equally, spatial attention is able to focus more on the foreground regions, which helps to generate effective features for saliency prediction....|
|||...Dotted line represents the saliency map generated by the feature is under the supervision of ground truth....|
|||...To address the problem, we propose a novel attention driven network, which progressively encodes multi-level contextual information to produce more effective features for saliency estimation....|
|||...Then we use the attentive feature to predict saliency and the saliency map generated by each layer-wise attentive feature is supervised by the ground truth....|
|||...Attentive features of the final stage (i.e.,  Scsa  ) is exploited to predict the final saliency map....|
|||...3  In Figure 7, we compare saliency maps of our progressive attention guidance mechanism (CA and CSA) with FCN-based method (FCN)....|
|||...Inspired by their work, we apply multi-path recurrent connections to our saliency model where highlevel information is adaptively transferred back to different shallower layers....|
|||...  Ht  + N (Ut fl(Wl  Ht  l1 + bWl ) l  Ht1 L + bt l1 + bWl ),  Ul )),  l  R otherwise  (9)  Most of saliency works predict saliency through sideoutput features of certain convolutional blocks....|
|||...The output saliency maps at the final time step achieve state-of-the-art performance....|
|||...AE is defined as the average pixel-wise absolute difference between the binary ground truth and the saliency map:  M AE =  1  W  H  W  H  X  x=1  X  y=1   S(x, y)  G(x, y) ,  (11)  where W and H denot...|
|||...For fair comparison, we use the recommended parameter settings to implement these methods or utilize the saliency maps provided by the authors....|
|||...Figure 7 shows the output saliency maps of our attention guided models (i.e., CA and CSA) and FCN-based method....|
|||...To make attentions at each stage meaningful, we use the three attentive features to estimate saliency respectively and the three output saliency maps are all supervised by ground truth....|
|||...Every time step can generate saliency maps....|
|||...Salientshape: group saliency in image collections....|
|||...Recurrent attentional net works for saliency detection....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Kernelized In ECCV, pages  subspace ranking for saliency detection....|
|||...Hierarchical saliency detec tion....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||44 instances in total. (in cvpr2018)|
|82|Jiang_Submodular_Salient_Region_2013_CVPR_paper|...The saliency of a selected region is modeled in terms of appearance and spatial location....|
|||...Experimental results demonstrate that our approach outperforms several recently proposed saliency detection approaches....|
|||...Introduction  Visual saliency modeling is relevant to a variety of computer vision problems including object detection and recognition [29, 26], image editing [13, 4, 6] and image segmentation [16]....|
|||...Most saliency models [2, 20, 4, 6, 8] are based on a contrast prior between salient objects and backgrounds....|
|||...Saliency models map natural images into saliency maps, in which each image element (pixel, superpixel, region) is assigned a saliency strength or probability....|
|||...For example, Figure 1 illustrates saliency detection results using four state-of-art algorithms [2, 6, 4, 26]....|
|||...For example, the poor saliency detection results in Figure 1(f) using the low-rank prior are due to the cluttered background....|
|||...We present a submodular objective function for efficiently creating saliency maps from natural images; these maps can then be used to detect multiple salient regions within a single image....|
|||...rmation only [29, 5], or heuristic integration approaches [13, 6] based on weighted averages on the saliency maps from low level features and high level priors....|
|||...This provides a new perspective using submodularity for salient region detection, and it achieves state-of-art performance on two public saliency detection benchmarks....|
|||...[27] proposes to use the boundary prior, which assumes the image boundary is mostly background for saliency detection....|
|||...[29] learns interesting region features by dictionary learning and then generates the saliency map by modeling spatial consistency via a CRF model....|
|||...[5] proposes a topdown saliency algorithm by selecting discriminant features from a pre-defined filter bank....|
|||...The saliency maps are combined by weighted averaging, where the weights are predefined [6, 8], learned by a SVM [13] or estimated by a CRF [20]....|
|||...Five regions generated; (k) Ground truth salient region; (l) Saliency map without prior; (m) Saliency map with prior; (n) Salient region mask based on the saliency map in (m)....|
|||...A region which has a high color contrast with respect to other regions should have a high saliency [2, 4]....|
|||...The spatial saliency of ri is computed as fs(ri) = 1  V (ri) k D(i)(k), where D(i)(k) is the average of all the distances of superpixels from ri to the spatial mean k of region rk....|
|||...After fc and fs are maximum normalized to [0, 1], the saliency of ri is computed as: f (ri) = fc(ri)fs(ri)....|
|||...We generate the final saliency map S by weighted averaging over superpixels, where the weights are computed by pair-wise feature distances between superpixels using a Gaussian kernel to enforce that s...|
|||...Figures 3(l) and 3(m) present the saliency maps using our approach without and with high-level priors, respectively....|
|||...Compared to the ground truth region in Figure 3(k), the saliency maps with priors are better than the result without priors....|
|||...ages; (b) Ground truth salient regions; (c) High-level prior map; (d) Saliency map without high level prior; (e) Saliency map with high level prior; (f) Salient Region extraction based on (e) by simpl...|
|||... end if },  A  A  {a for i  V \A do  6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end loop 16: Construct the saliency map S for I;  i < cia then if  i = cia , x  end if  end for  cur  cur  cur i = a    in Figu...|
|||...For the second evaluation, we follow [2, 4, 26] to segment a saliency map by adaptive thresholding....|
|||...If the saliency of a superpixel is larger than two times the saliency mean, the superpixel is considered as foreground....|
|||...Computation time per image for saliency detection, measured on an Intel 2.40GHZ CPU machine with 4GB RAM....|
|||...By combining the high-level priors, the saliency detection performance is improved....|
|||...Because our approach needs superpixel segmentation, it is slower than IT [12] and GB [10] but produces superior quality saliency maps as shown in Figure 7....|
|||...Figure 7 shows some examples of saliency map construction using our approach and IT, FT, GB, LC, CA, RC and LR....|
|||...Figure 9 shows some saliency maps using different approaches....|
|||...Examples of saliency map construction using different approaches on the MSRA-1000 database....|
|||...The saliency maps in (j) are used to segment the salient regions by simple thresholding....|
|||...Examples of saliency map construction using different approaches on the Berkeley-300 database....|
|||...The saliency maps in (j) are used to segment the salient regions by simple thresholding....|
|||...31]  (i) LR [26]  (c) IT [12]  (g) CA [6]  (h) RC [4]  (d) FT [2]  into the objective function, the saliency of detected regions is improved and more consistent with human visual perception....|
|||...Experimental results indicate that the algorithm outperforms recently proposed saliency detection techniques including FT [2], CA [6], RC [4] and LR [26] and is comparable to GS [27]....|
|||...Discriminant saliency for visual recognition from  cluttered scenes, 2004....|
|||...Context-aware saliency detection,  2010....|
|||...Sun: Top-down saliency using  natural statistics....|
|||...Geodesic saliency using background priors,  2012....|
|||...Visual saliency detection via sparsity  pursuit....|
|||...Top-down visual saliency via joint crf and dictionary  learning, 2012....|
||42 instances in total. (in cvpr2013)|
|83|Saliency Revisited_ Analysis of Mouse Movements Versus Fixations|...of Computer Science, University of Central Florida, Orlando  Abstract  This paper revisits visual saliency prediction by evaluating the recent advancements in this field such as crowdsourced mouse tra...|
|||...Further, this study will be addressing some of the questions that have surfaced by introduction of mouse tracking based databases and help better understand saliency models performance....|
|||...[14] analyzed some properties of mouse data such as center-bias, evaluated mouse maps against fixation maps, and compared saliency models using mouse tracking data....|
|||...ked in their work such as congruency among participants, the effect of mouse tracking on training a saliency model, model evaluation and comparison using mouse versus eye tracking....|
|||...They chose several top-performing saliency models based on deep learning architectures over the MIT300 dataset [6]....|
|||...[4] analyzed different parameters affecting saliency evaluation (e.g., center-bias, class categories, etc) in order to benchmark models....|
|||...[26] employed statistical analysis in order to do metric selection for saliency evaluation....|
|||...Saliency databases & contextual annotation  There exists numerous databases for saliency evaluation....|
|||...The EFC database is collected for analyzing saliency in crowds....|
|||...Input: Sal : a saliency map of size W  H, a tensor of contextual masks Cm of size W  H  O, where O is the number of regions, the contextual attribute matrix Ca of size O  N , which reports existence o...|
|||...Metrics  Metrics of saliency evaluation....|
|||...core (SIM) [17], Earth movers distance (EMD) [17], Information Gain (IG) [20]), Normalized scanpath saliency (NSS) [24], and metrics based on fixation sequence (e.g., Scanpath evaluation score [4])....|
|||...a technique to measure the similarity between the distribution of fixated and random locations in a saliency map, while in [6, 26], the KL is measured in terms of the similarity between fixation densi...|
|||...Many of the saliency metrics convey the same information making model performance interpretation difficult....|
|||...Contextual saliency evaluation....|
|||...Algorithm 1 presents how to compute the agreement between human eye fixations and saliency maps inside annotated regions associated with attributes such as gaze, face, and text....|
|||...In principle, all existing saliency evaluation metrics can be employed for the purpose of contextual evaluation by the proposed algorithm....|
|||...To achieve this, given the score vector of a saliency map, Score, we define a weighted average score as CScore = PN n=1 wnScoren where N is the number of contextual attributes and w is the weight vect...|
|||...On the other hand, the result is alarming as the influence of the existing large gap on saliency models is not well-investigated....|
|||...This indicates that while training using mouse data is similar to training using eye data in terms of sAUC, the generated saliency maps are not necessarily similar to ground truth....|
|||...Visual comparison of saliency maps from OpenSalicon, trained by mouse and eye data....|
|||...More importantly, the proposed scheme allows one to identify the weak points of a saliency model more easily and efficiently....|
|||...Nonetheless, mouse tracking captures an acceptable level of visual saliency as a low cost alternative to eye tracking....|
|||...Our results suggest that high performance achieved by recent saliency models, based on deep learning, might be merely due to high volume of training data....|
|||...We believe that future research should focus on finegrained analysis of ground truth data and models in order to understand attentional mechanism better and improve existing saliency models....|
|||...Cat2000: A large scale fixation dataset  for boosting saliency research....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Mit saliency benchmark....|
|||...Where should saliency models look next?...|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Informationtheoretic model comparison unifies saliency metrics....|
|||...Methods for comparing scanpaths and saliency maps: strengths and weaknesses....|
|||...Shallow and deep convolutional networks for saliency prediction....|
|||...An eye fixation database for saliency detection in images....|
|||...Opensalicon: An open source implementation  of the salicon saliency model....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Spatially binned roc: A compre hensive saliency metric....|
|||...Turkergaze: Crowdsourcing saliency with webcam based eye tracking....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||42 instances in total. (in cvpr2017)|
|84|Liu_Predicting_Eye_Fixations_2015_CVPR_paper|...Meanwhile  bottom-up  visual  saliency  can  also  be  inferred  via  combining information over multiple resolutions....|
|||...In  saliency  models,  contrast  computation  over  early  features is another key procedure....|
|||...information   The  last  step  for  saliency  modeling  is  to  integrate  various contrast features to yield saliency maps....|
|||...Although  most  previous  works  mainly  concentrate  on  contrast-based  bottom-up  saliency,  it  is  believed  that  at  early  stage  of  free  viewing,  eye  movements  are  mainly  directed  by ...|
|||...In computer science field, researchers  normally develop computational visual saliency models to  quantitatively predict human eye attended locations using  computer vision techniques....|
|||...o attract  our  attention,  most  traditional  approaches  typically  cope  with  the  problem  of  saliency  modeling  by  three  steps  in  sequence:  early  feature  contrast  inference,  and  cont...|
|||...e motivate us to design a  new  unified  learning  model  to  enhance  the  hand-crafted  bottom-up saliency features and top-down factors for eye  fixation prediction....|
|||...Meanwhile,  it  can  also  learn  bottom-up  saliency  via  combining  information  over  multiple resolutions....|
|||...Finally, the last logistic regression layer  learns to integrate bottom-up saliency with top-down cues  to predict eye fixations....|
|||...In contrast to these previous  works, our proposed work builds a unified framework to  learn  both  the  bottom-up  saliency  and  top-down  factors  simultaneously....|
|||...It demonstrates that the proposed Mr-CNN can  learn both low-level features related to bottom-up saliency  and  high-level  top-down  factors  to  improve  eye  fixation  prediction....|
|||...We  notice  that  some  researchers  have  applied  deep  learning algorithms to model visual saliency lately....|
|||...Then,  a  linear  SVM  was  utilized  to  detect saliency from those learned concepts....|
|||...When testing, we just evenly sample 5050 locations per image  to estimate their saliency values to reduce computation cost....|
|||...The obtained down-sampled saliency map is rescaled to the original size to  achieve the final saliency map....|
|||...inferring   the  bottom-up  saliency  among   As for the network architecture, as shown in Figure 2,  our Mr-CNN starts from three streams in lower layers....|
|||...In the training stage, we randomly sample fixation and  non-fixation locations based on the saliency values in the  ground truth density maps which are generated by applying  Gaussian blur on the raw ...|
|||...Then the activation value of the last layer in the Mr-CNN is  obtained as the saliency value of each location to form the  down-sampled  saliency  map....|
|||...Ultimately,  the  obtained  down-sampled saliency map is rescaled to the original size  of the testing image to achieve the final saliency map....|
|||...X-axis indicates the Gaussian blur STD  (in image  width) by which saliency maps are smoothed and Y-axis indicates the average shuffled-AUC score on one dataset....|
|||...blurring applied on saliency maps, we follow many recent  works  [4,  6]  to  smooth  the  saliency  maps  using  small  Gaussian filters with various standard deviation (STD) ....|
|||...Evaluation metrics   One of the most widely used metrics to evaluate saliency  models is the Area Under the ROC Curve (AUC) [11]....|
|||...Then, the computed saliency map is binarily  classified  into  salient  region  and  non-salient  region  by  a  threshold....|
|||...When testing, for an original  testing image, we averaged its saliency map and the one of  its horizontally  flipped version as the final saliency  map....|
|||...We compare some saliency maps of our Mr-CNN model with other 6 models, i.e....|
|||...The  saliency  maps  were smoothed by Gaussian kernels with various blur STD   first, then average shuffled-AUC scores of each model  on  different  datasets  over  varying   were  presented  in  Figure  3....|
|||...1 https://github.com/nitishsrivastava/deepnet  2 The authors of the eDN model only published their saliency maps on  three datasets, i.e....|
|||...As we can see, our  Mr-CNN  model  can  detect  not  only  bottom-up  saliency  patterns  (e.g.,  Column  3,  5,  6,  7,  9),  but  also  diverse  top-down factors, such as faces (e.g., Col 1, 8, 9), ...|
|||...This  indicates  that  our  Mr-CNN  can  learn  both  bottom-up saliency cues and high-level top-down factors....|
|||...Our  model has achieved the best performance with significant  improvement to 11 state-of-the-art saliency models on four  publically  available  benchmark  datasets....|
|||...The  learned hierarchical features were visualized to show that  our  Mr-CNN  learns  both  low-level  saliency  cues  and  high-level factors....|
|||...Boosting bottom-up and top-down visual features   for saliency estimation....|
|||...Exploiting local and global patch rarities   for saliency detection....|
|||...Bottom-up saliency based on  weighted sparse coding residual....|
|||...SUN:  A  Bayesian  framework  for  saliency  using  natural statistics....|
|||...Visual saliency  based on scale-space analysis in the frequency domain....|
|||...An  object-oriented  visual  saliency  detection  framework  based  on sparse coding representations....|
|||...A  novel  multiresolution  spatiotemporal saliency detection model and its applications  in  image  and  video  compression....|
|||...Learning a saliency map using fixated   locations in natural scenes....|
|||...An  eye  fixation  database  for  saliency  detection  in  images....|
|||...Predicting  human  gaze  using  low-level  saliency  combined  with  face  detection....|
|||...Large-scale  optimization  of  hierarchical  features  for  saliency  prediction  in  natural  images....|
||42 instances in total. (in cvpr2015)|
|85|Wang_GraB_Visual_Saliency_CVPR_2016_paper|... Piramuthu2  1University of Delaware 2eBay Research  Abstract  We propose an unsupervised bottom-up saliency detection approach by exploiting novel graph structure and background priors....|
|||...A novel graph model is proposed to effectively capture local and global saliency cues....|
|||...To obtain more accurate saliency estimations, we optimize the saliency map by using a robust background measure....|
|||...Therefore, a fast and robust saliency detection algorithm can benefit various other vision tasks....|
|||...The literature of saliency map estimation is vast....|
|||...A fast saliency technique can be an essential preprocessing step for background removal or object/product detection and recognition in large ecommerce applications....|
|||...Groups of background seeds are selected for initial saliency based on their influence via the graph structure....|
|||...This estimated saliency map is refined by passing it again through the system....|
|||...The resulting saliency map is further used to generate seeds to perform another query to obtain the final saliency map....|
|||...Our claim is that the proposed graph model provides more desirable characteristics for saliency detection and achieves unprecedented balance between computational complexity and accuracy....|
|||...[10] proposed the graph based visual saliency (GBVS), a graph-based saliency model with multiple features to extract saliency information....|
|||...to generate pixel-wise saliency maps via regularized random walks ranking [19]....|
|||...This boundary prior is more general than previously used center prior, which assumes that the saliency object tend to appear near the image center [17, 24]....|
|||...[32] define the saliency of a region to be the length of its shortest path to the virtual background node....|
|||...This process is crucial to the estimation of the final saliency map as the edge weights are calculated by comparing the feature descriptors of two nodes....|
|||...The edge weights can be obtained by the  2(h1(i)  h2(i))2 h1(i) + h2(i)  is the chi-squared  i=1  537  Algorithm 1: Visual Saliency via Novel Graph Model and Background Priors  Data: Input image I an...|
|||...Acquire initial saliency estimation S using Eq....|
|||...Result: A saliency map S with the same size as the  input image  Gaussian similarity function:  two parts: the boundary prior and the connectivity prior....|
|||...The final saliency estimation is conducted on each level independently and the output saliency map is combined using results from all levels (see Sec....|
|||...Our algorithm is based on background priors, which consists of  To provide more accurate saliency estimations, we first compare the four borders of the image and remove one with the most distinctive c...|
|||...Malik filter bank [18], LBP = local binary patterns [26], AVE = simple averaging, HS = hierarchical saliency [35], GMR = graph based manifold ranking [36], BC = boundary connection, GEO = geodesic dis...|
|||....327 0.538  node, we still need to calculate their complement values to obtain the foreground-based saliency:  Si(b) = 1  f   i (b), i = 1, 2, ..., n.  (8)  The results are then put into element-wise ...|
|||...(7), we obtain the final saliency map  S = (D   W   + 1  )1f ....|
|||...Sampling Our method estimates saliency by using boundary superpixels as queries....|
|||...(7) to estimate saliency values for all superpixels....|
|||...In our experiment, we set n = 5 and perform the query 20 times for each image to get the initial saliency map S....|
|||...Scaling In the saliency detection literature, hierarchical models are often adopted for robustness purpose [35, 14]....|
|||...Combination We have presented 5 different strategies to facilitate more accurate saliency estimation....|
|||...The Extended Complex Scene Saliency Dataset (ECSSD) [35] contains 1000 semantically meaningful but structurally complex images from the BSD dataset [3], PASCAL VOC [9] and the Internet....|
|||...The JuddDB dataset [4] is created from the MIT saliency benchmark [16], mainly for checking generality of salient object detection models over real-world scenes with multiple objects and complex background....|
|||...Additionally, we compared with all saliency object segmentation methods mentioned in [23] and [37] on the PASCALS dataset, including CPMC+GBVS [23], CPMC+PatchCut [37], GBVS+PatchCut [37], RC [7], SF ...|
|||...The PASCAL-S is proposed to avoid the dataset design bias, where the image selection process deliberately emphasizes the concept of saliency [23]....|
|||...Specifically, the PR curve is obtained by binarizing the saliency map using varying thresholds from 0 to 255, as mentioned in [1, 7, 27, 29]....|
|||...Also, our method performs favorably against the more recent PatchCut method [37] and clearly above all other saliency algorithms....|
|||...6 shows a few saliency maps for qualitative evaluation....|
|||...Conclusion  We present a novel unsupervised saliency estimation method based on a novel graph model and background priors....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Geodesic saliency using background priors....|
|||...Bayesian saliency via low  and mid level cues....|
|||...Hierarchical saliency detec tion....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Visual saliency based on multiscale deep  features....|
||42 instances in total. (in cvpr2016)|
|86|cvpr18-Flow Guided Recurrent Neural Encoder for Video Salient Object Detection|...Image saliency detection has recently witnessed significant progress due to deep convolutional neural networks....|
|||...However, extending state-of-the-art saliency detectors from image to video is challenging....|
|||...It can be considered as a universal framework to extend any FCN based static saliency detector to video salient object detection....|
|||...The challenges of still-image saliency detector and the effectiveness of temporal coherence modeling in video based salient object detection....|
|||...Graphics model based methods generally employ generative framework which first infers an initial saliency map from either intra-frame appearance contrast  3243  information [3] or inter-frame gradien...|
|||...In this work, we present flow guided recurrent neural encoder (FGRNE), an end-to-end learning framework to extend any FCN based still-image saliency detectors to video salient object detection....|
|||...DSS [10]) as our host network for feature extraction and ultimate saliency inference, and a pre-trained FlowNet [7] for motion estimation between a frame pair....|
|||...The output feature map at the last time-step is considered as our encoded feature and is fed to the upper part of the host network for saliency inference....|
|||...To overcome this deficiency, deep FCN based models have been developed to directly map a raw input image to its corresponding saliency map in an end-to-end trainable way....|
|||...They can produce superior saliency maps and have become the fundamental component in state-of-the-art methods of this field....|
|||...ich incorporates both temporal and motion information to improve the feature map representation for saliency inference....|
|||...Video Salient Object Detection  Compared with saliency detection in still images, detecting video salient objects is much more challenging due to the high complexity in effective spatio-temporal model...|
|||...Earlier methods to this problem can be considered as simple extensions of some static saliency models with extra crafted temporal features [24, 9]....|
|||...More recent and noteworthy works generally formulate video saliency detection as a spatio-temporal context modeling problem over consecutive frames, and incorporates energy functions with handcrafted ...|
|||...relevant work with us is [33], which exploits a second FCN to improve the temporal coherence of the saliency map generated from an initial static FCN based saliency network, by taking as input the con...|
|||...DSS [10] model), it can be considered as a feature extraction module Nfea followed by a pixel-wise saliency regression module Nreg....|
|||...Directly applying this model to each individual frame usually generates unstable and temporally inconsistent saliency maps due to the lack of temporal coherence modeling in feature representation....|
|||...The output saliency map is thus computed as Si = Nreg (Fi)....|
|||...The continuous saliency map is rescaled to [0, 255] and is binarized using all integer thresholds in the interval....|
|||...At each threshold value, a pair of precision and recall value can be obtained by comparing the binary saliency map against the groundtruth....|
|||...The PR curve is obtained from the average precision and recall over saliency maps of all images in the dataset....|
|||...E is defined as the average pixelwise absolute difference between the binary ground truth G and the saliency map S [26],  4.1.3 Implementation Details  Our proposed FRGNE has been implemented on the M...|
|||...Comparison of precision-recall curves of 10 saliency detection methods on DAVIS and FBMS....|
|||...The first six are the latest stateof-the-art salient object detection methods for static images while the last three are video-based saliency models....|
|||...We also fine-tune all the public static saliency models using the training set as same as we train our FGRNE, and use the refined model for comparison....|
|||...As can be seen, deep learning based static saliency models can generate seemly promising saliency maps when watched independently, they are unsurprisingly inconsistent when putting in a whole sequence....|
|||...In general, our method generates much more accurate and consistent saliency maps in various challenging cases....|
|||...Sa refers to the saliency maps generated from the singleframe baseline model....|
|||...Visual comparison of saliency maps generated from state-of-the-art methods, including our FGRNE....|
|||...Our model consistently produces saliency maps closest to the ground truth....|
|||...Noted that the feature extraction are shared during the saliency inference of all the frames in a given window and our algorithm runs in a sliding window mode....|
|||...4.3.2 Sensitivities to Feature Extractor Selection  As described in Section 3, our FGRNE is dependent on a pre-trained static saliency detector as our host network....|
|||...y similar to that of unsupervised video object segmentation, except that its goal is to calculate a saliency probability value for each pixel instead of a binary classification....|
|||...It can be considered as a universal framework to extend any FCN based static saliency detector to video salient object detection, and can easily benefit from the future improvement of image based sali...|
|||...Video saliency incorporating spatiotemporal cues and uncertainty weighting....|
|||...Bottom-up saliency is a dis criminant process....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Visual saliency detection based on multi scale deep cnn features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Top-down visual saliency via joint In CVPR, pages 22962303....|
||42 instances in total. (in cvpr2018)|
|87|Zhang_Saliency_Detection_A_2013_ICCV_paper|...2013 IEEE International Conference on Computer Vision  Saliency Detection: A Boolean Map Approach  Jianming Zhang  Stan Sclaroff  Department of Computer Science, Boston University  {jmzhang,sclaroff}@...|
|||...Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps....|
|||...Computing such saliency maps has recently raised a great amount of research interest (see [4] for a review) and has been shown to be beneficial in many applications, e.g....|
|||...1 shows an example that global cues for figureground segregation can help in saliency detection....|
|||...(b) Saliency maps estimated by (from left to right) AIM [6], LG [3] and our method....|
|||...AIM and LG measure an image patchs saliency based on its rarity....|
|||...ver, without the awareness of this global structure, rarity based models [6, 3] falsely assign high saliency values to the edge area between the trees and the sky in the background, because of the rar...|
|||...mgSal eye tracking dataset [27]; (b) is the ground truth eye fixation heat map; (c) and (d) are the saliency maps generated by BMS for eye fixation prediction and salient object detection respectively...|
|||...Then saliency is modeled as the expected attention level given the set of randomly sampled Boolean maps....|
|||...2 shows two types of saliency maps of BMS for eye fixation prediction and salient object detection....|
|||...Related Works  A majority of the previous saliency models use centersurround filters or image statistics to identify salient patches that are complex (local complexity/contrast) or rare in their appea...|
|||...The negative logarithm of the probability, known as Shannons selfinformation, is used to measure the improbability of a local patch as a bottom-up saliency cue in [6] and [39]....|
|||...Recently, [10] uses a hierarchically whitened feature space, where the square of the vector norms serves as a saliency metric to measure how far a pixel feature vector deviates from the center of the data....|
|||...Unlike models based on properties like contrast, rarity and symmetry, another family of saliency models are based on spectral domain analysis [15, 14, 33, 27]....|
|||...[20] train a SVM using a combination of low, middle and high level features, and the saliency classification is done in a pixel-by-pixel manner....|
|||...Instead,  Only a few attempts have been made to leverage the topological structure of a scene for saliency detection....|
|||...In [36], a local patchs saliency is measured on a graphical model, by its shortest distance to the image borders....|
|||...In contrast, BMS computes saliency entirely based on the set of randomly thresholded Boolean maps....|
|||...Boolean Map based Saliency  To derive a bottom-up saliency model, we borrow the Boolean Map concept that was put forward in the Boolean Map Theory of visual attention [17], where an observers momentar...|
|||...Then the saliency is modeled by the mean attention map  A over randomly generated Boolean maps: A(B)p(B I)dB   A =  (cid:2)  (1)  where I is the input image....|
|||... A can be further post-processed to form a final saliency map S for some specific task....|
|||...Finally, some post-processing is applied on the mean attention map to output a saliency map S. Each step will be described in the following sections....|
|||...We post-process  A to produce the saliency map S by Gaussian blurring with standard deviation (STD) ....|
|||...Experimental Setup  We have quantitatively evaluated our algorithm in comparison with ten state-of-the-art saliency methods shown in Table 2....|
|||...One of the most widely used metrics for saliency method evaluation is the ROC Area Under the Curve (AUC) metric....|
|||...Under the shuffled-AUC metric, a perfect prediction will give an AUC of 1.0, while any static saliency map will give a score of approximately 0.5....|
|||...Results  AUC scores are sensitive to the level of blurring applied on the saliency maps....|
|||...Applying an opening operation over Boolean maps does not significantly change the average AUC scores on most of the datasets, but the score on the ImgSal dataset improves by more than 0.006 when o = 9...|
|||...The rest columns show the saliency maps from BMS and the compared methods....|
|||...Because eye fixations are sparsely distributed and possess some level of uncertainty, the corresponding saliency maps are usually highly blurred and very selective....|
|||...However, salient object detection requires object level segmentation, which means the corresponding saliency map should be highresolution with uniformly highlighted salient regions and 3Note that some...|
|||...ollowed by a closing-by-reconstruction operation [35] with kernel radius 15, in order to smooth the saliency maps but keep the boundary details....|
|||...Similar to previous works [1, 36], we binarize the saliency maps at a fixed threshold and compute the average precision and recall (PR) for each method....|
|||...Conclusion and Future Work  In this work, a novel Boolean Map based Saliency model is proposed to leverage the surroundedness cue that helps in figure-ground segregation....|
|||...This representation leads to an efficient algorithm for saliency detection....|
|||...Figure 8: Saliency maps on the ASD dataset....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Geodesic saliency using  background priors....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
|||...Figure 9: Saliency maps on the ImgSal dataset....|
||41 instances in total. (in iccv2013)|
|88|Wloka_Spatially_Binned_ROC_CVPR_2016_paper|...t  York University, Toronto  calden@cse.yorku.ca, tsotsos@cse.yorku.ca  Abstract  A recent trend in saliency algorithm development is large-scale benchmarking and algorithm ranking with ground truth p...|
|||...To capture and quantify these known sources of bias, we propose a novel metric for measuring saliency algorithm performance: the spatially binned ROC (spROC)....|
|||...This metric provides direct insight into the spatial biases of a saliency algorithm without sacrificing the intuitive raw performance evaluation of traditional ROC measurements....|
|||...By quantitatively measuring the bias in saliency algorithms, researchers will be better equipped to select and optimize the most appropriate algorithm for a given task....|
|||...We use a baseline measure of inherent algorithm bias to show that Adaptive Whitening Saliency (AWS) [14], Attention by Information Maximization (AIM) [8], and Dynamic Visual Attention (DVA) [20] provi...|
|||...One of the earliest and most popular saliency map models, referred to here as IKN, was developed by Itti et al....|
|||...Since then an enormous variety of saliency map models have been developed and refined; the unifying feature of these disparate algorithms is the assignment of a conspicuity value to every location wit...|
|||...In addition to a rapidly expanding set of approaches, the concept of saliency has grown beyond just attentional gating and has been applied to a number of additional areas....|
|||...Additionally, a third avenue of saliency research seeks to develop a system useful for prioritizing attentional resources (irrespective of human performance) for tasks such as mobile robot navigation ...|
|||...om-up attentional gating based on Koch and Ullmans architecture for atten A continuing challenge in saliency modeling is the formulation of fair and informative metrics with which to evaluate and comp...|
|||...Over the  1525  years a number of metrics have been adapted from signal analysis or developed for measuring saliency performance....|
|||...Several recent benchmarking studies have provided summaries of these metrics and their role in evaluating saliency algorithms [4, 38]....|
|||...Center Bias and Shuffled ROC  Early in the study of saliency it was noticed that stimulus location had a strong effect on the likelihood of fixation, with regions closer to the image center being more...|
|||...[52], who discussed in detail the confounding effect center bias can have on saliency algorithm evaluation....|
|||...As they pointed out, a saliency map consisting solely of a centered Gaussian (the cG model) outperforms many of the leading saliency models in predicting human fixations despite being independent of t...|
|||...nvolution had a tendency to reward models with a greater undefined border due to a concentration of saliency values toward the image center....|
|||...e been accomplished in an alternative fashion by simply enlarging the undefined border (zeroing all saliency values) of all models to an equivalent size (as was done in [29]), such an approach would n...|
|||...[23], whose saliency model is based on a machine learning classifier trained to label pixels in saliency space regardless of biological plausibility in the calculation [25]....|
|||...In both the shuffled and classical ROC, the true positive rate is the percentage of human fixation points which are above a saliency threshold....|
|||...(if the most interesting visual stimuli consistently appear near the image center a high performing saliency algorithm should likewise consistently detect the image center as most salient)....|
|||...s of movement (independent of the image itself) was more predictive of human gaze data than the IKN saliency model....|
|||...Although some saliency work has attempted to incorporate task bias [12, 26], the majority of saliency modeling is nevertheless done under the assumption of free-viewing....|
|||...Thus, we see that the Gaussian central prior, which is prevalent in improving saliency model scores with traditional ROC metrics, can be derived by a simple translating saccadic model [50]....|
|||...rraros [1] work showing that saccadic movements can be well captured as stochastic sequences over a saliency field which correspond well to Levy flight random walks....|
|||...As a result, a model of saliency should inherently account for these effects rather than view their manifestation as a nuisance which must be separately corrected for....|
|||...We demonstrate the spROC metric using the following  algorithms:   Attention by Information Maximization (AIM) [8]  Adaptive Whitening Saliency (AWS) [14]  Context Aware Saliency (CAS) [15]  A centere...|
|||... ability to identify salient visual stimuli, which is important for future scientific pursuits into saliency and saliency algorithm design....|
|||...Thus, we are able to begin to quantify the complex interactions smoothing has on the saliency signal, which opens the doors to further research into generally optimized post-processing techniques....|
|||...Although we have concentrated here on one particular form of spatial binning, it should be straightforward to extend this methodology to explore other interesting aspects of saliency model performance....|
|||...overwhelming spatial component of the ground-truth set, discounting the role of spatial location in saliency can likewise lead to misleading conclusions regarding relative algorithm performance....|
|||...This provides us with the ability to begin quantifying how rather than simply how much smoothing modulates the saliency signal, which opens up a novel avenue of research into saliency algorithm optimization....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Sun: Top-down saliency using natural statistics....|
|||...Visual saliency estimation by nonlinearly integrating features using region covariances....|
|||...The effects of image padding in saliency algorithms....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
|||...A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression....|
|||...Visual saliency improves In Computer and Robot Vision  autonomous visual search....|
|||...Space-variant descriptor sampling for action recognition based on saliency and eye movements....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||41 instances in total. (in cvpr2016)|
|89|Deeply Supervised Salient Object Detection With Short Connections|...ity  2CRCV, UCF  3UCSD 4University of Oxford  http://mmcheng.net/dss/  Abstract  Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional...|
|||...Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs)....|
|||...Holisitcally-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious....|
|||...In this paper, we propose a new saliency method by introducing short connections to the skip-layer structures within the HED architecture....|
|||...Inspired by cognitive studies of visual attention [21, 41, 11], computational saliency detection has received great research attention in the past two decades [22, 4, 1, 5]....|
|||...However, the skip-layer structure with deep supervision in the HED model does not lead to obvious performance gain for saliency detection....|
|||... 1  (d) s-out 2  (e) s-out 3  (f) s-out 4  (g) s-out 5  (h) s-out 6  Figure 1: Visual comparison of saliency maps produced by the HED-based method [49] and ours....|
|||...Though saliency maps produced by deeper (4-6) side output (s-out) look similar, because of the introduced short connections, each shallower (1-3) side output can generate satisfactory saliency maps an...|
|||...Related Works  Over the past two decades [22], an extremely rich set of saliency detection methods have been developed....|
|||...[29] proposed to use multi-scale features extracted from a deep CNN to derive a saliency map....|
|||...[46] predicted saliency maps by integrating both local estimation and global search....|
|||...To combine them together, a unified fully connected neural network was designed to estimate saliency maps....|
|||...On one hand, saliency detection is a more difficult vision task than edge detection that demands special treatment....|
|||...A good saliency detection algorithm should be capable of extracting the most visually distinctive objects/regions from an image instead of simple edge information....|
|||...On the other hand, the features generated from lower stages are too messy while the saliency maps obtained from the deeper side-output layers are short of regularity....|
|||...To overcome the aforementioned problem, we propose a top-down method to reasonably combine both low-level and high-level features for accurate saliency detection....|
|||...HED(cid:173)based saliency detection  ground truth map and the fused predictions, which is set to be image-level class-balanced cross-entropy loss [49] here....|
|||...,  Xn }, z(n)  [0, 1] denotes the corresponding continuous ground truth saliency map for Xn....|
|||...,  X } and saliency map Z = {zj, j = 1, ....|
|||...The main focus of saliency locating stage is on looking for the most salient regions for a given image....|
|||...nt objects/regions and refine the results from deeper side outputs, resulting in dense and accurate saliency maps....|
|||...To improve spatial coherence and quality of our saliency maps, we adopt the fully connected conditional random field (CRF) method [26] as a selective layer during the inference phase....|
|||...we leverage the following unary term  i(xi) =   log Si  h(xi)  ,  (14)  where Si denotes normalized saliency value of pixel xi, h() is the sigmoid function, and  is a scale parameter....|
|||...The pairwise potential is defined as  two classes in our case, we use the inferred posterior probability of each pixel being salient as the final saliency map directly....|
|||...Therefore, our approach uses less than 0.5s to produce the final saliency map, which is much faster than most present CNN-based methods....|
|||...A good saliency detection model should perform well over almost all datasets [1]....|
|||...As can be seen, our proposed method produces more coherent and accurate saliency maps than all other methods, which are the closest to the ground truth....|
|||...For a given continuous saliency map S, we can convert it to a binary mask B using a threshold....|
|||...Let S and Z denote the continuous saliency map and the ground truth that are normalized to [0, 1]....|
|||...Because of the refinement effect of lowlevel features, our saliency maps look much closer to the ground truth....|
|||...Our experiments demonstrate that these mechanisms result in more accurate saliency maps over a variety of images....|
|||...Exploiting local and global patch rarities for saliency detection....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Does luminance-contrast contribute to a saliency map for overt visual attention?...|
|||...A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression....|
|||...Visual saliency based on multiscale deep features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Bayesian saliency via low  and mid level cues....|
|||...Hierarchical saliency detec tion....|
||40 instances in total. (in cvpr2017)|
|90|cvpr18-Excitation Backprop for RNNs|...Moreover, estimates of the models attention (e.g., saliency maps) can be used directly in localizing a given action within a video or in localizing the portions of a video that correspond to a particu...|
|||...[15] map words to regions in the video captioning task by dropping out (exhaustively or by sampling) video frames and/or parts of video frames to obtain saliency maps....|
|||...Our approach models the top-down attention mechanism of deep models to produce interpretable and useful task-relevant saliency maps....|
|||...In addition, we show how the spatiotemporal saliency maps produced for these two models can be utilized for localization of segments within a video that correspond to specified activity classes or nou...|
|||...In summary, our contributions are:   We are the first to formulate top-down saliency in deep  recurrent models for space-time grounding of videos....|
|||...Such approaches are mainly devised for image understanding and can identify the importance of class-specific image regions by means of saliency maps in a weakly-supervised way....|
|||...[15] explored visual saliency guided by captions in an encoder-decoder model....|
|||...ention mechanism of CNN-RNN models to produce interpretable and useful task-relevant spatiotemporal saliency maps that can be used for action/caption localization in videos....|
|||...Each P (ai) in the saliency map is computed over the complete parent set Pi....|
|||...ing the sum of backpropagated probabilities layer by layer, it is possible to compute task-specific saliency maps from any intermediate layer in a single backward pass....|
|||...To improve the discriminativeness of the saliency maps, [34] introduced contrastive EB (cEB) which cancels out common winner neurons and amplifies the class discriminative neurons....|
|||...Recurrent models such as LSTMs are well-suited for top-down temporal saliency as they explicitly propagate information over time....|
|||...The task takes as input a video sequence and the action (A) to be localized, and outputs spatiotemporal saliency maps for this action in the video....|
|||...Performing cEB-R results in a sequence of saliency maps M apt for t = 1, ..., T at conv5 (various layers perform similarly [34])....|
|||...Third, we extend a window around the anchor map in both directions in a greedy manner until a saliency map with a negative sum is found....|
|||...The task takes as input a video and word(s) to be localized, and outputs spatiotemporal saliency maps corresponding to the query word(s)....|
|||...8)through the VGG until the conv5 layer where we obtain the corresponding saliency map....|
|||...Performing cEB-R results in a sequence of saliency maps M apt for t = 1, ..., T grounding the words in the video frames....|
|||...The saliency map for each time-step is used for the spatial localization....|
|||...The sum of each saliency map, over time, is then used for temporal localization of the action within the video, as described in Sec....|
|||...Output: Spatial saliency maps of A : M apt for  t = 1, ..., T ....|
|||...;  2 Backprop the indicator vector through time and down to the fc CNN layer using EB-R obtaining a saliency map M apt at every time step t;  3 Normalize the resulting frame-wise saliency maps  over t...|
|||...We compare our formulation against spatial top-down saliency using a CNN (treating every video frame as an independent image)....|
|||...We locate the point having maximum value on each top-down saliency map....|
|||...Extending top-down saliency in time (-R) consistently improves the accuracy for all three methods, compared to performing top-down saliency separately on every frame of the video using a CNN....|
|||...The saliency map for each time step is used for spatial localization....|
|||...The sum of each saliency map, over time, is then used for temporal localization of the word within the clip....|
|||...The results show that extending top-down saliency in time (-R) improves the accuracy compared to performing top-down saliency separately on every frame of the video using a CNN....|
|||...llDunk using EB-R (L) and cEB-R (R)  (b) Grounding of the action Skiing in the video  Figure 5: The saliency maps produced by EB-R (left) and cEB-R (right) together with the THUMOS14 groundtruth bound...|
|||...It can be seen that the sum of saliency maps is: positive and increasing as more of the GT action is observed, negative and decreasing as more of the rand action is observed....|
|||...We perform the pointing game by pointing [34] in time to the peak sum of saliency maps....|
|||...The models have comparable METEOR scores to the Caption-Guided Saliency work of [15], to which we compare our results: 26.5 (vs. 25.9) for video captioning and 18.0 (vs. 18.3) for image captioning....|
|||...We quantitatively evaluate our results of spatial grounding using the pointing game on the Flickr30kEntities and compare our method to the Caption-Guided Saliency work of [15], following their evaluat...|
|||...g box annotations for each noun phrase in the ground truth captions and check  Figure 7: Sum of the saliency maps at fc7 over time, in frames, for synthetic videos that (blue) have a rand action follo...|
|||...whether the maximum point in a saliency map is inside the annotated bounding box....|
|||...e center Caption-Guided Saliency [15] Ours  0.268 0.492 0.501 0.512  Table 5: Evaluation of spatial saliency on Flickr30kEntities using cEB-R. Baseline random samples the maximum point uniformly and B...|
|||...In order to obtain spatial saliency maps for a word in a video, c-EB-R requires one forward pass and one backward pass through the CNN-LSTM-LSTM, while [15] requires one forward pass through the CNN p...|
|||...These datasets provide annotations for detection and/or localization, to which we have compared the evidence in our generated saliency maps....|
|||...Top-down visual saliency guided by captions....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||40 instances in total. (in cvpr2018)|
|91|Scharfenberger_Statistical_Textural_Distinctiveness_2013_CVPR_paper|...Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on ...|
|||...how that the proposed approach provides interesting and promising results when compared to existing saliency detection methods....|
|||...Introduction  The underlying goal of saliency detection in natural images is to identify and localize objects of interest that attract the visual attention of a human observer compared to the rest of ...|
|||...The research area of saliency detection from natural images has gained tremendous interest in the field of computer vision given its wide applicability for many computer vision tasks such as image seg...|
|||...To achieve saliency detection in an automatic manner, one must define what constitutes as a salient object based on some quantifiable visual attributes such as intensity, color, structure, texture, si...|
|||...icularly interesting visual attribute that deserves deeper exploration for the purpose of automatic saliency detection in natural images is texture, which reveals significant information about not onl...|
|||...In the context of saliency in natural images, one can then view salient objects of interest as objects that possess textural characteristics that are highly distinctive from a human observer perspecti...|
|||...As such, we are interested in explicitly taking advantage of textural characteristics in a quantitative manner to detect saliency objects of interest within a scene....|
|||...lgorithms such as image segmentation means that the computational complexity and performance of the saliency detection method depends heavily on the properties of the segmentation method used, even if...|
|||...Therefore, an efficient method for performing saliency detection based explicitly on descriptive textural characteristics that does not rely on additional pre-processing would be much desired....|
|||...The main contribution of this paper is the introduction of a novel approach to saliency detection based on the concept of statistical textural distinctiveness....|
|||...Related Work  Existing saliency are either biologically motivated, computational oriented, or perform local or global analysis of contrast using intensity only, and/or different colorspaces....|
|||...Biologically inspired techniques [13, 10] for saliency detection are commonly based on the approach of Koch et al....|
|||...All these approaches are designed to identify salient regions with high visual stimuli, but tend to blur saliency maps and to highlight local features such as small objects....|
|||...Local saliency detection methods usually evaluate saliency of input image with respect to small neighborhoods....|
|||...[5] generates a color histogram of the entire image, and compute the saliency based on the dissimilarity between the histogram bins, and also use image segmentation for improving saliency estimation....|
|||...The overall good performance of LR is the result of three strong priors applied to saliency computation....|
|||...erent heterogenous textural characteristics in the image in an efficient manner for quantifying the saliency of regions within the image....|
|||...global texture modeling is highly computationaland memoryintensive for the purpose of texture-based saliency detection given the use of pair-wise textural representation analysis to establish a quanti...|
|||...representative texture atoms, i.e., var((cid:107)tr i(cid:107)p), as it was found to provide strong saliency detection performance....|
|||...f a texture atom relative to the other texture atoms in the sparse texture model for the purpose of saliency detection, a more meaningful metric for quantifying textural distinctiveness is the probabi...|
|||...Given the saliency  computed for each of the m representative texture atoms in the sparse texture model, one can easily compute the saliency for each pixel x in the image I(x) (denoted here by (x)) ba...|
|||...ural distinctiveness graphical model, and complimented by general visual attentive constraints, the saliency map for an image I(x) can now be computed based on the following extended assumptions:  1....|
|||...Given these two key assumptions, the saliency of a representative texture atom tr i (which we will denote as i) can be computed as the product of:  1. the expected statistical textural distinctiveness...|
|||...Only m saliency computations are needed, one for each representative texture atom in the sparse texture model....|
|||...The atoms i I(x)) used to compute the saliency of each P (tr representative texture atom only needs to be computed once per image....|
|||...Following this scheme, we performed binary segmentation of saliency maps using each possible fixed threshold  to compute precision-recall curves in a first experiment....|
|||...In a first experiment, we segmented the saliency maps using a fixed threshold f ix  [0, 255] to obtain binary images, with highlighting regions with saliency values larger than f ix as foreground....|
|||...It also has to be noticed that the low-rank saliency approach (LR) requires a variety of additional constraints (such as color, spatial and semantic priors) to achieve excellent precision-recall curve...|
|||...In the second experiment, we applied an image dependent threshold on the saliency maps to segment salient regions....|
|||...[1] defined this threshold as twice the mean of saliency maps S(x), i.e., ada = 2 E(S(x))....|
|||...However, a closer analysis of the saliency maps obtained showed that the distribution of saliency values follows a Gaussian mixture model, with non-salient values having larger probabilities than sali...|
|||...c) Precision, recall and F-measure for cut-based (GrabCut [23]) segmentation of salient objects, initialized with saliency maps from all tested saliency approaches....|
|||...Conclusions  In this paper, a novel saliency detection approach for natural images based on the concept of statistical texture distinctiveness was presented....|
|||...Future work involves investigating alternative sparse textural representation and textural models to evaluate whether improvements in saliency detection can be achieved....|
|||...precision such as the region contrast (RC) saliency approach [5] and low-rank (LR) saliency approach [25]....|
|||...[5] suggested to perform GrabCut [23] as a post processing step on thresholded saliency maps....|
|||...However, this depends on the chosen saliency appraoch, and requires prior knowlegde which is difficult to extract from unknown images....|
|||...e best precision and F-measure due to the consideration of texture and the sparse texture model for saliency computation, which can help to reduce the influence of cluttered background on saliency com...|
|||... [5]  RC [5]  LR [25]  SF [21]  TD  GT  Figure 8: Visual comparison of our approach (TD) with other saliency approaches and ground truth (GT)....|
||40 instances in total. (in cvpr2013)|
|92|Rudoy_Learning_Video_Saliency_2013_CVPR_paper|...Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Learning video saliency from human gaze using candidate selection  Dmitry Rudoy  Technion  Haifa, Israel  Dan B Gol...|
|||...Therefore, video saliency estimation methods should differ substantially from image saliency methods....|
|||...In this paper we propose a novel method for video saliency estimation, which is inspired by the way people watch videos....|
|||...Image saliency is well explored in the computer vision community....|
|||...The saliency maps overlayed on the images show that video saliency is tighter and more concentrated on a single object, while image saliency covers several interesting locations....|
|||...To this end we learn a model that predicts a saliency map for a frame given the fixation map from a recent preceding moment and test it on a large set of realistic videos....|
|||...Instead, we select a set of candidate gaze locations, and compute saliency only at these locations....|
|||...Later, Koch and Ullman [20] proposed a feedforward model for the integration, along with the concept of a saliency map  a measure of visual attraction of every point in the scene....|
|||...Since then much progress in image saliency has been made....|
|||...[8] take a different approach: they concentrate on motion saliency only and detect it by using temporal spectral analysis....|
|||...Our work differs from previous video saliency methods by narrowing the focus to a small number of candidate gaze locations, and learning conditional gaze transitions over time....|
|||...Motivation and overview  Most previous saliency modeling methods calculate a saliency value for every pixel....|
|||...In our work we propose to calculate saliency at a small set of candidate locations, instead of at every pixel....|
|||...First, we observe that image saliency studies concentrate on a single image stimulus, without any prior....|
|||...For a given frame of interest we calculate the graph-based visual saliency (GBVS), proposed by Harel et al....|
|||...Given the image saliency map we wish to find the most attractive candidate regions within it....|
|||...We treat the normalized saliency map as a distribution and use it to sample a large number of random points....|
|||...Motion candidates  Modeling the saliency in independent frames is insufficient for videos since it ignores the dynamics....|
|||...The motion candidates are created from the DoG map in the same way as the static candidates are created from the image saliency map (i.e., mean-shift clustering and Gaussian fitting)....|
|||...The original frame is shown in gray (for visualization) It is overlaid with: (a) the GBVS saliency map and (b) optical flow magnitude....|
|||...Since our method computes the probability to shift from a location in a source frame to a location in a destination frame, we calculate the video saliency in a sequential order....|
|||...si)  (cid:2)  iS  where  P (si) =  (cid:3)  Sal(si) iS Sal(si)  and Sal(si) is the source candidate saliency and S is the set of all the sources....|
|||...Finally, we produce the saliency map in a similar fashion to how Gaussian mixture models are used to create a continuous distribution: we replace each candidate with a Gaussian of corresponding covari...|
|||...Experimental validation  In this section we experimentally validate the proposed video saliency detection method....|
|||...The first metric is the area-under-curve (AUC), which utilizes the receiver-operator curve to compute the similarity between human fixations and the predicted saliency map....|
|||...Since the AUC considers the saliency results only at the locations of the ground truth fixation points, it cannot distinguish well between a peaky saliency map and a smooth one....|
|||...The 2 distance will prefer a peaky saliency map over a broad one, when comparing them to the tight distribution of the ground truth....|
|||...We compare the proposed saliency prediction approach with five different methods....|
|||...The first, referred to as humans, serves as an upper bound for the saliency prediction and measures how much the fixation map explains itself....|
|||...We further compare our results to the image saliency approach of GBVS [12], and two video saliency methods PQFT [11] and the method of Hou and Zhang [14] (annotated in figures and tables as Hou for brevity)....|
|||...Both methods are among the highest rated video saliency algorithms according to the recent benchmark of Borji et al.[3]....|
|||...We further visually compare our saliency maps to those of other methods....|
|||...As can be seen, the saliency maps produced  1150 1150 1150 1152 1152  Table 1....|
|||...The method is substantially different from existing methods and uses a sparse candidate set to model the saliency map....|
|||...It is shown experimentally that using candidates boosts the accuracy of the saliency prediction and speeds up the algorithm....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Temporal spectral residual: fast motion saliency detection....|
|||...Contextaware saliency detection....|
|||...Our saliency maps resemble the ground truth....|
|||...Examples of saliency detection results using different methods show that the saliency predicted by the proposed method better approximates the human gaze map....|
||40 instances in total. (in cvpr2013)|
|93|Yan_Hierarchical_Saliency_Detection_2013_CVPR_paper|...of Hong Kong  {qyan,xuli,jpshi,leojia}@cse.cuhk.edu.hk  http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/  Abstract  When dealing with objects with complex structures, saliency detection confronts...|
|||...We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues....|
|||...The final saliency map is produced in a hierarchical model....|
|||...Stemming from psychological science [28, 22], the commonly adopted saliency definition is based on how pixels/regions stand out and is dependent of what kind of visual stimuli human respond to most....|
|||...These examples are not special, and exhibit one common problem  that is, when objects contain salient smallscale patterns, saliency could generally be misled by their complexity....|
|||...Aiming to solve this notorious and universal problem, we propose a hierarchical model, to analyze saliency cues from multiple levels of structure, and then integrate them to infer the final saliency map....|
|||...t scales, and 2) construction of a new scene dataset, which contains challenging natural images for saliency detection....|
|||...Related Work  Bottom-up saliency analysis generally follows locationand object-based attention formation [22]....|
|||...A survey of human attention and saliency detection is provided in [27]....|
|||...These difficulties boil down to the same type of problems and indicate that saliency is ambiguous in one single scale....|
|||...put with different degrees of details, balanc 1154 1154 1154 1156 1156  (a) Input image  (b) Final saliency map  (c) Image layers  (d) Cue maps  (e) Inference  j  E s ( i  S  l  l s + 1  j  )  ,  l  ...|
|||...We extract three image layers from the input, and then compute saliency cues from each of these layers....|
|||...We use it for saliency inference described in Section 3.3....|
|||...Single-Layer Saliency Cues  For each layer we extract, saliency cues are applied to find important pixels from the perspectives of color, position and size....|
|||...We define the local contrast saliency cue for Ri in an image with a total of n regions as a weighed sum of color difference from other regions:  Ci =  n(cid:2)  j=1  w(Rj )(i, j)  ci  cj  2,  (2)  whe...|
|||...After computing  si for all layers, we obtain initial saliency maps separately, as demonstrated in Fig....|
|||...We propose a hierarchical inference procedure to fuse them for multi-scale saliency detection....|
|||...Hierarchical Inference  Cue maps reveal saliency in different scales and could be quite different....|
|||...8  (a) Input  (b) Cue map at Layer L1  (c) Cue map at Layer L2  (d) Cue map at Layer L3  (e) Final saliency map  Figure 6....|
|||...Saliency cue maps in three layers and our final saliency map....|
|||...We  fine a saliency variable sl minimize the following energy function (cid:2)  (cid:2)  (cid:2)  (cid:2)  E(S) =  ED(sl  i) +  ES(sl  i, sl+1  j  )  (5)  l  i  l  i,Rl i  Rl+1  j  The energy consists...|
|||...Data term ED(sl i) is to gather separate saliency confidence, and hence is defined, for every node, as  ED(sl  i) = l  sl  i   sl  i  2 2,  (6)  controls the layer confidence and  sl  where l i is the...|
|||...The hierarchical term makes saliency assignment for corresponding regions in different levels similar, beneficial to effectively correcting initial saliency errors....|
|||...(5), resulting in new saliency sl i representation with regard  1157 1157 1157 1159 1159  to the initial values  sl ....|
|||...In each layer, since there is already a minimum energy representation obtained in the previous step, we optimize it to get new saliency values....|
|||...After all variables sl j are updated in a top-down fashion, we obtain the final saliency map in L1....|
|||...Our result in (e) contains information from all scales, making the saliency map better than any of the single-layer ones....|
|||...In fact, solving the three layer hierarchical model via belief propagation is equivalent to applying a weighted average to all single-layer saliency cue maps....|
|||...MSRA-1000 [2] and 5000 Datasets [18]  We first test our method on the saliency datasets MSRA1000 [2] and MSRA-5000 [18] where MSRA-1000 is a subset of MSRA-5000 and contains 1000 natural images with t...|
|||...For LC, MZ, SF and LR, we directly use author-provided saliency results....|
|||...Our experiment follows the setting in [2, 4], where saliency maps are binarized at each possible threshold within range [0, 255]....|
|||...It is because combining saliency information from three scales makes background generally have low saliency values....|
|||...Evaluation on Complex Scene Saliency Dataset  Although images from MSRA-1000 [2] have a large variety in their content, background structures are primarily simple and smooth....|
|||...On these difficult examples, our method can still produce reasonable saliency maps....|
|||...The difference between our method and others is clear, manifesting the importance to capture hierarchical saliency in a computationally feasible framework....|
|||...Single-layer saliency computation does not work similarly well....|
|||...(4) in different layers, as well as the average of them, as the saliency values and evaluate how they work respectively when applied to our CSSD image data....|
|||...Our proposed method achieves high performance and broadens the feasibility to apply saliency detection to more  1159 1159 1159 1161 1161  1  0.8  0.6  n o i s i c e r P  0.4  0  Precision-Recall  [12...|
|||...Discriminative spatial saliency for image classification....|
|||...Geodesic saliency using  cy....|
||40 instances in total. (in cvpr2013)|
|94|Feng_Local_Background_Enclosure_CVPR_2016_paper|...Here, we propose a novel RGB-D saliency feature....|
|||...Early work on computing saliency aimed to model and predict human gaze on images [12]....|
|||...The saliency of a region is usually obtained by measuring contrast at a local [12] and/or global scale [7]....|
|||...RGB-D saliency methods typically incorporate depth directly, or use depth in a contrast measurement framework [11,15,2224], where contrast is computed as the difference between the means or distributi...|
|||...Thus we pro 2343  pose a depth saliency feature that incorporates two components....|
|||...Further, we validate the proposed depth feature in a saliency system....|
|||...Related Work  RGB-D saliency computation is a rapidly growing field, offering object detection and attention prediction in a manner that is robust to appearance....|
|||...Early works use depth as a prior to reweight 2D saliency maps [4, 18, 27]....|
|||...Many existing methods measure global depth contrast, usually combined with colour and other modalities, to compute saliency [11, 15, 2124] ....|
|||...While depth contrast measurement forms the foundation of many approaches, it is common practice to enhance the resulting saliency maps by applying various priors and other refinement steps....|
|||...[23] explore orientation and background priors for detecting salient objects, and use PageRank and MRFs to optimize their saliency map....|
|||...[22] incorporate object bias, and optimize their saliency map using a region growing approach....|
|||...[15] apply Grabcut segmentation to refine the boundaries of the generated saliency map....|
|||...As shown in Figure 3, even though P2 and P3 have similar angular densities, we would expect P2 to have a significantly higher saliency since the background directions are more spread out....|
|||...(7)  Figure 8 shows the generated saliency map on some example images....|
|||...Specifically, we reweight the Local Background Enclosure feature saliency using depth and spatial priors, and then refine the result using Grabcut segmentation....|
|||...Accordingly, scaling saliency by depth is a common refinement step in previous work [4,5,9,11,15,15,2123,25,  2345  Figure 3....|
|||...(b) The distribution functions F , G, and final LBE saliency S = F  G at each point....|
|||...Overview of our saliency detection system....|
|||...We perform absolute depth reweighting using a depth prior D(x, y) to modulate the saliency of pixels with depth greater than the median depth of the image [15]....|
|||...Existing saliency methods commonly incorporate a center bias term to model this effect [5, 11, 15, 22, 24]....|
|||...The low-level saliency map with priors applied is thus  given by:  Sb = S  D  G  B  (8)  4.2....|
|||...Grabcut Segmentation  The saliency map Sb may contain inaccurate foreground boundaries for parts of the object that do not exhibit strong pop-out structure....|
|||...The refined saliency map is thus given by  Sg = A  Sb....|
|||...Experiments  The performance of our saliency system is evaluated on two datasets for RGB-D salient object detection....|
|||...PR curves showing the effect of each component of the saliency system on (c) RGBD1000 and (d) NJUDS2000....|
|||...PR curve of our saliency system against state-of-theart RGB-D saliency systems on (a) RGBD1000 and (b) NJUDS2000....|
|||...the state-of-the-art 2D saliency algorithms DRFI [14] and DSR [17], which were found to be top ranking methods by a recent study [3]....|
|||...The F-score is computed from the saliency output using an adaptive threshold equal to twice the mean of the image [1]....|
|||...Our saliency method has one parameter the threshold 0 used to generate the foreground mask for Grabcut initialisation....|
|||...Figure 6 shows that our saliency system outperforms all  2347  Recall00.20.40.60.81Precision00.10.20.30.40.50.60.70.80.91Precision vs Recall on RGBD1000Ours[F]Ours[f]ACSDLMH-SDCLMH-DCGP-SDCGP-DCRecall...|
|||...Our saliency system achieves the highest F-score on both datasets, with GP obtaining the second best performance....|
|||...This is a rare occurrence, and the other depth saliency methods with  2348  (a) RGB  (b) Depth  (c) G. T.  (d) Ours  (e) GP [23]  (f) ACSD [15]  (g) LMH [22]  Figure 8....|
|||...Comparison of output saliency maps produced by our salient object detection system against the output of GP [23], ACSD [15], and LMH [22]....|
|||...Our LBE depth feature allows for a more accurate final saliency map compared to methods using contrast-based depth features....|
|||...Exploiting global priors for RGB-D saliency detection....|
|||...Depth enhanced saliency detection method....|
|||...A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression....|
|||...Salient object detection in RGB-D image based on saliency fusion and propagation....|
||39 instances in total. (in cvpr2016)|
|95|Kuang-Jui_Hsu_Unsupervised_CNN-based_co-saliency_ECCV_2018_paper|...We decompose co-saliency detection into two sub-tasks, single-image saliency detection and cross-image co-occurrence region discovery corresponding to two novel unsupervised losses, the single-image s...|
|||...(a) Our method optimizes an objective function defined on a graph where single-image saliency (SIS) detection (red edges) and crossimage co-occurrence (COOC) discovery (blue edges) are considered jointly....|
|||...The following three rows show the detected saliency maps by using COOC, SIS, and both of them, respectively....|
|||...Such heavy annotation cost makes these methods less practical as pointed out in other applications, such as semantic segmentation [15] and saliency detection [16]....|
|||...The former detects the saliency object in a single image, which may not repetitively appear across images....|
|||...To this end, we design two novel losses, the singleimage saliency (SIS) loss and the co-occurrence (COOC) loss, to capture the two different but complementary sources of information....|
|||...These two losses measure the quality of the saliency maps by referring to individual images and the co-occurrence regions for each image pair, respectively....|
|||...The results show that our approach remarkably outperforms the state-of-the-art unsupervised methods and even surpasses many supervised DL-based saliency detection methods....|
|||...2 Related work  2.1 Single-image saliency detection  Single-image saliency detection is to distinguish salient objects from the background by either unsupervised [20,21,22,23,24,25] or supervised [26,...|
|||...We accomplish the task by decomposing it into two sub-tasks, single-image saliency detection and crossimage co-occurrence discovery....|
|||...It optimizes an objective function defined on a graph by learning two collaborative FCN models gs and gc which respectively generates single-image saliency maps and cross-image co-occurrence maps....|
|||...  n=1  s(In; w) +  N  X  n=1  X  m6=n  c(In, Im; w),  (1)  where the unary term s(In; w) focuses on saliency detection for the image In, the pairwise term c(In, Im; w) accounts for co-occurrence disco...|
|||...For image In, FCN gs investigates intra-image clues and generates its saliency map Ss n. In contrast, FCN gc discovers cross-image evidence and produces its co-occurrence map Sc n, where the repetitiv...|
|||...It guides the training of FCN gs, which produces saliency map Ss n =  n for image In, i.e., Ss  6  Hsu et al....|
|||...[42], we apply an existing unsupervised method for saliency detection to image In, and use its output saliency map  Sn as the desired target for learning FCN gs....|
|||...Given the co-saliency map Sn, the three groups are yielded by  qk n      On, Bn, Tn,  n > n + n, n < n  0.25  n,  if k if k otherwise,  for k = 1, 2, ..., K,  (9)  n is the mean saliency value of supe...|
|||...AP is computed from the area under the Precision-Recall (PR) curve, which is produced by binarizing saliency maps with every integer threshold in the range of [0, 255]....|
|||...The structure measure (S) [57] is adopted to evaluate the spatial structure similarities of saliency maps based on both regionaware structural similarity Sr and object-aware structural similarity So, ...|
|||...h comparison with state-of-the-art methods, we divide them into four groups, i.e., the unsupervised saliency [20,22,23,24,25,42] and co Table 1....|
|||...SI and CS denote the single-image saliency and co-saliency methods, respectively....|
|||...Please note that all compared supervised single-image saliency detection methods are CNN-based....|
|||...Among unsupervised single-image saliency methods, SVFSal [42] is CNN-based....|
|||...Example saliency maps generated by our method and some state-of-the-art methods....|
|||...Although both with the unsupervised setting, by taking advantage of additional information within an image set, our method clearly outperforms the group of unsupervised single-image saliency detection methods....|
|||...Its worth mentioning that our method also outperforms the unsupervised CNN-based single-saliency method, SVFSal [42] that requires saliency proposal fusion for generating high-quality pseudo ground-tr...|
|||...In general, the supervised CNN-based single-image saliency methods perform the best among four groups of methods as they better utilize the object annotations....|
|||...4 shows example saliency maps produced by our method and some state-of-the-art methods, including unsupervised co-saliency detection methods (CSSCF [3], CoDW [12]), unsupervised single-image saliency ...|
|||...For example, the cattle have lower saliency values in gc+gs (the fourth row of Fig....|
|||...Our method decomposes the problem into two sub-tasks, single-image saliency detection and cross-image co-occurrence region discovery, by modeling the corresponding novel losses: single-image saliency ...|
|||...Jerripothula, K., Cai, J., Yuan, J.: Image co-segmentation via saliency co-fusion....|
|||...: Weakly supervised saliency detection with a  category-driven map generator....|
|||...: Saliency detection via graph based manifold ranking....|
|||...: Video saliency map detection by  dominant camera motion removal....|
|||...Wang, L., Lu, H., Ruan, X., M.-M.-Yang: Deep networks for saliency detection via  local estimation and global search....|
|||...Liu, N., Han, J.: DHSNet: Deep hierarchical saliency network for salient object  detection....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional  features for accurate saliency detection....|
|||...: Saliency detection via absorb ing markov chain....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep  learning....|
||38 instances in total. (in eccv2018)|
|96|Zhu_Saliency_Optimization_from_2014_CVPR_paper|...ient object detection have exploited the boundary prior, or background information, to assist other saliency cues such as contrast, achieving stateof-the-art results....|
|||...It has an intuitive geometrical interpretation and presents unique benefits that are absent in previous saliency measures....|
|||...Second, we propose a principled optimization framework to integrate multiple low level cues, including our background measure, to obtain clean and uniform saliency maps....|
|||...It is motivated by the importance of saliency detection in applications such as object aware image retargeting [5, 11], image cropping [13] and object segmentation [20]....|
|||...Besides contrast prior, several recent approaches [26, 8, 29] exploit boundary prior [26], i.e., image boundary regions are mostly backgrounds, to enhance saliency computation....|
|||...This property provides unique benefits that are absent in previously used saliency measures....|
|||...Our second contribution is a principled framework that regards saliency estimation as a global optimization problem....|
|||... directly achieve the goal of salient object detection: object regions are constrained to take high saliency using foreground cues; background regions are constrained to take low saliency using the pr...|
|||...All constraints are in linear form and the optimal saliency map is solved by efficient least-square optimization....|
|||...Related Work  Another research direction for visual saliency analysis [12, 7, 4, 27, 24, 21] aims to predict human visual attention areas....|
|||...In [26], an image patchs saliency is defined as the shortest-path distance to the image boundary, observing that background regions can easily be connected to the image boundary while foreground regio...|
|||...Similarly, in [28], saliency maps computed on multiple scales of image segmentation are combined....|
|||...These methods adapt viewpoints and optimization techniques from other problems for saliency estimation....|
|||...By contrast, previous saliency measures are incapable of achieving such good uniformity, since they are usually more sensitive to image appearance variations and vary significantly across images....|
|||...The absolute value of previous saliency measures is therefore much less meaningful....|
|||...For comparison, we treat geodesic saliency [26] as a background measure and show its recall at the same precision....|
|||...Many works use the region contrast against its surroundings as a saliency cue, which is computed as the summation of its appearance distance to all other regions, weighted by their spatial distances [...|
|||...In the next section, we present a principled framework to integrate these measures and generate the final clean saliency map, as in Figure 5(e)....|
|||...Saliency Optimization  To combine multiple saliency cues or measures, previous works simply use weighted summation or multiplication....|
|||...(e) optimized saliency maps by minimizing Eq.(9)....|
|||...cost function is thus defined as  Let the saliency values of N superpixels be {si}N N(cid:88) (cid:124)  i (si  1)2 (cid:123)(cid:122) (cid:125) wf g  wij(si  sj)2 (cid:123)(cid:122) (cid:125)  (cid:8...|
|||...We model the salient object detection problem as the optimization of the saliency values of all image superpixels....|
|||...The optimal saliency map is then obtained by minimizing the cost function....|
|||...Note that for wf g i we can essentially use any meaningful saliency measure or a combination of them....|
|||...A curve is obtained by normalizing the saliency map to [0, 255], generating binary masks with a threshold sliding from 0 to 255, and comparing the binary masks against the ground truth....|
|||...Although commonly used, PR curves are limited in that they only consider whether the object saliency is higher than the background saliency....|
|||...It is the average per-pixel difference between the binary ground truth and the saliency map, normalized to [0, 1]....|
|||...It directly measures how close a saliency map is to the ground truth and is more meaningful for applications such as object segmentation or cropping....|
|||...including saliency filter(SF)  We compare with the most  recent state-of-the-art methods, [17], geodesic saliency(GS-SP, short for GS) [26], soft image abstraction(SIA) [3], hierarchical saliency(HS) ...|
|||...the proposed approach To verify the effectiveness of the proposed boundary connectivity measure and saliency optimization, we use the standard dataset ASD. Results in Figure 6 show that 1) boundary co...|
|||...Integration and comparison with state-of-the-art As mentioned in Section 4, our optimization framework can integrate any saliency measure as the foreground term....|
|||...(8)) can lead to performances comparable to using other sophisticated saliency measures, such as [28, 29]....|
|||...This is very mean 2We normalize and inverse the boundary connectivity map and use it as  a saliency map....|
|||...Its robustness makes it especially useful for high accuracy background detection and saliency estimation....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Image saliency by isocen tric curvedness and color....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||38 instances in total. (in cvpr2014)|
|97|Park_Predicting_Primary_Gaze_2013_ICCV_paper|...2013 IEEE International Conference on Computer Vision 2013 IEEE International Conference on Computer Vision  Predicting Primary Gaze Behavior using Social Saliency Fields   Hyun Soo Park  Carnegie Mel...|
|||...We characterize how information of the time-varying location and charge of multiple moving social charges is combined to induce a social saliency field analogous to an electric field....|
|||...Koch and Ullman [27] proposed a computational foundation for visual saliency detection....|
|||...[20] via their Winner-Take-All networks and showed that their detected visual saliency is matched with eye tracking results....|
|||...Social saliency is another stimulus that drives attention....|
|||...This social saliency states that you are likely to look at what others look at [38]....|
|||...3497 3504  Social charge, Q 1 Social charge, Q 2 Social saliency field, S  90  120   p(v Q  )1 p(v Q  )2 VMF modes  60  180  150  210  30  0    0.2    0.4  330    0.6  300  240  270  Figure 1....|
|||...We model the relationship between a primary gaze direction and a social charge via a social saliency field inspired by Coulombs law....|
|||...The two social charges (the blue and green points) generate the social saliency saliency field on the left figure....|
|||...Social Saliency Field  In this section, we present a computational model that captures the relationship between time-varying social charges and primary gaze behavior....|
|||...The charges induce a social saliency field that enables us to define a probability of the primary gaze direction given a location and time in Equation (3)....|
|||...Comparison between the social saliency field and electric field can be found in Table 1....|
|||...The interacting Social charge, Q = {q(t)  R0, r(t)  R3}  Electric charge Electric field, E Social saliency field, S(x, t)  R3 Enet =  Snet = max Si  (cid:2)  Ei  i  Table 1....|
|||...Electric field vs. social saliency field  force between two charges, Q = (q, r) and Qx = (qx, x), from Coulombs law is: F = K  qqx (r  x) (cid:4)r  x(cid:4)3  (4)  ,  where K is a normalizing constant....|
|||...Analogous to the electric field, a social saliency field is  defined by the limiting process,  S (x) = lim qx0  F qx  = K  q (r  x) (cid:4)r  x(cid:4)3  ,  (5)  where S (x) is the social saliency fiel...|
|||...Unlike the electric field, the net social saliency field selectively takes one of the social saliency fields1, i.e.,  (cid:4)  I  S(x) = argmax {Si(x)}I  i=1  (cid:4)Si(x)(cid:4),  (6)  where Si(x) is...|
|||...Each von-Mises Fisher distribution measures the distance between the primary gaze direction, v, and a unit vector from each social saliency field, Si/(cid:4)Si(cid:4)....|
|||...Given the saliency field from each charge at each time instant, the net time-varying saliency field can be written as  S(x, t) = argmax {Si(x,t)}I  i=1  (cid:4)Si(x, t)(cid:4)....|
|||...Social Saliency Field Estimation  In this section, we present a method to estimate the time-varying location and magnitude of the social charges {Qi(t)}I i=1, given the primary gaze directions of memb...|
|||...For a time-varying social saliency field, we can modify the expectation and maximization steps in Equation (11) and (12) as follows:  (cid:4) (cid:4)  J(cid:5)  j=1  1  0.5  0  1  0.5  0  1  0.5  0  ...|
|||...Results  We validate our social saliency field model and evaluate the prediction accuracy, quantitatively and qualitatively via four real world sequences capturing various human interactions from thir...|
|||...3500 3507  RBF regression Social saliency field  120  100  80  60  40  20  ) e e r g e d (   e l g n a   n i   r o r r e   n o i t c i d e r P  0  1  2  3  4  5  6  7  Number of unobserved members, n...|
|||...Our social saliency model produces lower error with less standard deviation....|
|||...n towards Q 2 Primary gaze direction towards Q 1 RBF regression  Outlier  Unobserved member  Social saliency field  Figure 3....|
|||...The social saliency model shows superior predictive precision....|
|||...The orange vector field and dark gray vector field in Figure 3 are the RBF regression model and a social saliency field, respectively....|
|||...The social saliency field outperforms over the RBF regression in three aspects: (1) The social saliency field is insensitive to outliers while the RBF regression is often biased by the outliers....|
|||...Two sequences (Party and Meeting) are used to estimate the social saliency field as shown in Figure 5(c) and 5(d)....|
|||...Based on the tracked face poses, i.e., primary gaze directions, we generated a social saliency field as shown in Figure 5(a)....|
|||...We estimate a social saliency field from both third person cameras and first person cameras....|
|||...The social saliency field reflects the selective gaze behavior....|
|||...We also apply our method to estimate a social saliency field on a public dataset provided by Park et al....|
|||...The social saliency field model we present in this paper is a step towards this vision....|
|||...We present the social saliency field induced by the motion of social charges as a model to predict primary gaze behavior of people in a social scene....|
|||...We evaluate the predictive validity of spatial and temporal forecasting on real sequences and demonstrate that the social saliency field model is supported empirically....|
|||...An exciting future direction is investigating the predictive validity of the sociodynamic saliency field, obtained by differentiating the potential field over space and time....|
|||...3D social saliency from head-mounted  cameras....|
|||...Gaze prediction improvement by adding a face feature to a saliency model....|
||38 instances in total. (in iccv2013)|
|98|Shtrom_Saliency_Detection_in_2013_ICCV_paper|...Our saliency map is utilized for finding the most informative viewpoint (b), displaying the most interesting buildings of the city  St. Peters Cathedral and Bremens town hall....|
|||...Introduction  While saliency in images has been extensively studied in recent years, there is very little work on saliency of point sets....|
|||...For general data sets, we show that our results are competitive with those of saliency detection of surfaces, although we do not have any connectivity information....|
|||...Less work addresses saliency of 3D surfaces [5, 7, 17] and only a few papers handle point sets [1, 16]....|
|||...Extending the existing techniques of saliency detection for 3D surfaces to operate directly on large point sets is not trivial....|
|||...Low-level distinctness (Dlow)  Low-level association (Alow)  High-level distinctness (Dhigh)  Final saliency map (S)  Figure 2....|
|||...Finally, the maps are integrated to produce the final saliency map....|
|||...Similarly to previous saliency detection algorithms, which operate on other types of data, our saliency detection algorithm is based on distinctness....|
|||...Our algorithm is general and competes favorably with state-of-the-art techniques for saliency detection of general objects, which typically consist of less than a million points....|
|||...We demonstrate the utility of our saliency maps in two applications....|
|||...Taking into account the fact that object recognition is performed hierarchically, from local representations to abstract ones, our saliency detection algorithm analyzes a scene hierarchically....|
|||...Then, we apply association, Alow, which increases the saliency in the neighborhood of the most distinct points....|
|||...Finally, the above three components are integrated into the final saliency map, S, defined for a point pi as follows: Dhigh(pi)....|
|||...Hierarchical Saliency Computation  Our goal is to compute saliency based on the dissimilarity between the descriptors, discussed in the previous section....|
|||...We are not aware of any related work that handles saliency of such huge data....|
|||...Moreover, to assess the quality of our results, we compare them to those produced by surface saliency detection algorithms....|
|||...(a) Our saliency for the campus (zoom-in)  (b) An image of the campus from Google maps  Figure 5....|
|||...The saliency for the Jacobs University campus (15M points)....|
|||...Figure 1 shows our saliency map for the city center of Bremen....|
|||...Figure 5 shows our saliency for the Jacobs University campus....|
|||...produces better saliency maps, detecting fine features, such as the delicate relief features on the bowl and fins of the fish....|
|||...Our saliency is computed based on the vertices, ignoring connectivity information, which is used by [18]....|
|||...However, for larger models, our method produces better saliency maps, detecting fine features, such as delicate relief features on the bowl, and the fins of the fish....|
|||...Applications  We demonstrate the utility of our saliency in two applications....|
|||...Second, given urban data, we construct an informative tour of the city, which maximizes the saliency viewed along the path....|
|||...For each candidate viewpoint and its associated set Vi,  we calculate the amount of saliency it views by:  S(pj)  wi(pj),   S(Vi) =  (10) where S(pj) is the saliency of pj, computed by Equation (1)....|
|||...The first viewpoint selected is the one having the maximal saliency (Equation 10)....|
|||...We define the added visible saliency contributed by the viewpoint Vi as:  (cid:2) wi(pj)  wmax(pj), 0  (cid:3)  ,  (12)  (Vi) =  S(pj) max  (cid:4) pjVi  where wmax(pj) is the maximal weight assigned ...|
|||...the accumulated viewed saliency is at least 30% of the saliency viewed by all the viewpoints....|
|||...Producing the most informative tour: Given a point set of an urban scene and its saliency map, our aim is to suggest  li+1 and two rotated by 30from this direction....|
|||...This can be explained by the fact that we weigh our saliency according to the viewing angle....|
|||...Conclusion  This paper has studied saliency detection for 3D point sets....|
|||...Our saliency detection algorithm is based on finding the distinct points, using a multi-level approach....|
|||...Finally, we demonstrate the utility of our saliency in two applications: selecting a set of informative viewpoints and producing an informative tour in an urban environment....|
|||...We stop when at least 75% of the total saliency is viewed by the candidates....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Computing saliency map from spatial information in point cloud data....|
|||...Sparse points matching by combining 3d mesh saliency with statistical descriptors....|
||38 instances in total. (in iccv2013)|
|99|Jiang_Salient_Region_Detection_2013_ICCV_paper|...  jump@mail.sdu.edu.cn, hbling@temple.edu, yu@cis.udel.edu, jpeng@sdu.edu.cn  Abstract  The goal of saliency detection is to locate important pixels or regions in an image which attract humans visual ...|
|||...While uniqueness has been used for saliency detection for long, it is new to integrate focusness and objectness for this purpose....|
|||...Further, results of saliency detection can be used to facilitate other computer vision tasks such as image resizing, thumbnailing, image segmentation and object detection....|
|||...Due to its importance, saliency detection has received intensive research attention resulting in many recently proposed algorithms....|
|||...One basic idea is to derive the saliency value from the local contrast of various channels, such as in terms of uniqueness defined in [29]....|
|||...While uniqueness often helps generate good saliency detection results, it sometimes produces high values for non-salient regions, especially for regions with complex structures....|
|||...Saliency Detection  According to [26, 35], saliency can be computed either in a bottom-up fashion using low level features or in a topdown fashion driven by specific tasks....|
|||...Many early works approach the problem of saliency detection with bottom-up methods....|
|||...[19] suggest that saliency is determined by center-surround contrast of low-level features....|
|||...Also some methods turn to the frequency domain to search for saliency cues [10, 13, 21]....|
|||...The above methods strive to highlight the object boundaries without propagating saliency to the areas inside, limiting their applicability for some vision tasks like segmentation....|
|||...Later on, many works were proposed which utilize various types of features in a global scope for saliency detection....|
|||...Zhai and Shah [40] compute pixel-level saliency using the luminance information....|
|||...Depth cues are also introduced to saliency analysis by Niu et al....|
|||...[37] turn to  1http://www.dabi.temple.edu/ hbling/code/UFO-saliency.zip  background priors to guide the saliency detection....|
|||...As a fast evolving topic, there are many other emerging saliency detection approaches worth notice....|
|||...sed salient object detection is introduced in [16], and manifold ranking approach is introduced for saliency detection in [39], submodular optimization-based solution is presented in [17], hierarchica...|
|||..., their experiment also shows that simple feature combination does not guarantee the improvement of saliency detection accuracy, suggesting that the widely used features may not be complementary and s...|
|||... results turn out to be very different, suggesting the important role of segmentation algorithms in saliency region detection....|
|||...[30] also compute saliency by taking blur effects into account, but their method identifies blur by wavelet analysis while our solution by scale space analysis....|
|||...The goal is to compute a saliency map denoted as S :   R, such that S(x) indicates the saliency value of pixel x....|
|||...Given the input image I, the proposed UFO saliency first calculates the three components separately, denoted as U :   R for uniqueness, F :   R for focusness, and O :   R for objectness....|
|||...The three components are then combined into the final saliency S....|
|||...Although the saliency map is defined for per pixel, we observe that region-level estimation provides more stable results....|
|||...However, accurate object segmentation by itself is a hard problem and we hence make saliency computation in the sub-object level instead....|
|||...Specifically, we conduct saliency computation for each separate region i, i = 1, ....|
|||...These properties match well our perception of saliency in general....|
|||...Uniqueness Estimation     	                         Uniqueness, i.e., the color contrast feature, has been effectively used for saliency detection in previous works [7, 27, 29]....|
|||...In the first protocol, we binarize each saliency map with a fixed threshold t  [0, 255]....|
|||...We also follow [2,7,29, 34] to use an adaptive binarization threshold ta to binarize the saliency map before calculate the F....|
|||...set as proportional to the mean saliency of the image:  ta =  k  W  H  S(x, y),  (10)  W  H  x=1  y=1  in which we empirically choose k = 1.5....|
|||...In the third protocol, we use the overlap rate to evaluate  the saliency detection algorithms, which is defined as  Ro =  ,  (11)  Fg  Gt Fg  Gt  where Fg and Gt are the areas of the detected foregrou...|
|||...Our methods generate saliency maps closest to the ground truth....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform....|
|||...Geodesic saliency using back ground priors....|
|||...Hierarchical Saliency Detection....|
||37 instances in total. (in iccv2013)|
|100|cvpr18-PiCANet  Learning Pixel-Wise Contextual Attention for Saliency Detection|...28, junweihan2010}@gmail.com  mhyang@ucmerced.edu  Abstract  Contexts play an important role in the saliency detection task....|
|||...As a result, our saliency model can detect salient objects more accurately and uniformly, thus performing favorably against the state-of-the-art methods....|
|||...Recently, convolutional neural networks (CNNs) have been introduced into saliency detection to learn effective contextual representation....|
|||...As a result, the proposed PiCANets can facilitate the saliency detection task significantly....|
|||... from coarse scale to fine scales, and use them to enhance the convolutional features to facilitate saliency inference at each pixel....|
|||...While the learned local attention shown in Figure 1(c) can attend to regions that have the similar appearance with the referred pixel in its local context to make the saliency map more homogeneous....|
|||...We propose a novel saliency detection model by embedding PiCANets into a U-Net architecture....|
|||...PiCANets are used to hierarchically incorporate the attended global context and multiscale local contexts, which can effectively improve saliency detection performance....|
|||...Extensive experimental results on six benchmark datasets demonstrate the effectiveness of the proposed PiCANets and the saliency model when compared with other state-of-the-art models....|
|||...Traditional saliency models mainly rely on various saliency cues to detect salient objects, including local contrast [15], global contrast [4], and background prior [43]....|
|||...Lately, with the utilization of CNNs, many work have achieved promising results on saliency detection....|
|||...[24] and Li and Yu [18] adopt CNNs to extract multiscale contextual features on multiscale image regions to infer saliency for each pixel and each superpixel, respectively....|
|||...In [19], an FCN based saliency model and a multiscale image region based saliency model are combined....|
|||...[37] recurrently adopt an FCN to refine saliency maps progressively....|
|||...[38] also use several stages to progressively refine saliency maps by combining local and global context information....|
|||...In [9], short connections are introduced into the multi-scale side outputs within the HED network [39] to improve saliency detection performance....|
|||...[10] propose to adopt a level sets based loss to train their saliency detection network and use guided super-pixel filtering to refine saliency maps....|
|||...Although existing DNN based models incorporate varthese methods alious contexts for saliency detection, l use context regions holistically....|
|||...In [17], authors use a recurrent attention model to select local regions to refine their saliency maps....|
|||...own in Figure 3(a), in order to preserve relative large spatial sizes in higher layers for accurate saliency detection, we modify the pooling strides of the pool4 and pool5 layers to be 1 and adopt th...|
|||...(a) Architecture of our saliency network with the VGG 16-layer backbone....|
|||...Specifically, in each Di, we use a Conv layer with sigmoid activation on Deci to generate a saliency map with size W  H, then the resized ground truth saliency map is used to supervise the network tra...|
|||...Datasets  We use six widely used saliency benchmark datasets to evaluate our method....|
|||...Most of the images have challenging scenarios for saliency detection....|
|||...It computes the average absolute per-pixel difference between predicted saliency maps and corresponding ground truth saliency maps....|
|||...27 0.724  0.738 0.744 0.747  0.057 0.054  0.057 0.056  0.055 0.054 0.053  the network to obtain its saliency map....|
|||...In Figure 5(a) we show an image and its ground truth saliency map while (b) shows the predicted saliency maps of the baseline U-Net (top) and our model (bottom)....|
|||...(b) Saliency maps of the baseline U-Net (top) and our model (bottom)....|
|||...(a) shows an image and its predicted saliency map of our model....|
|||...Comparison with State(cid:173)of(cid:173)the(cid:173)arts  We compare our saliency model against other 9 state-ofthe-art models, namely, SRM [38], DSS [9], NLDF [26], Amulet [45], UCF [46], DHS [23], ...|
|||...Bottom-up saliency based on weighted sparse coding residual....|
|||...Recurrent attentional net works for saliency detection....|
|||...Visual saliency based on multiscale deep  features....|
|||...A deep spatial contextual long-term recurrent convolutional network for saliency detection....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Hierarchical saliency detec tion....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||37 instances in total. (in cvpr2018)|
|101|Yeong_Jun_Koh_Sequential_Clique_Optimization_ECCV_2018_paper|...Also, saliency detection techniques are widely employed to estimate initial regions of a primary object [2, 4, 24, 25]....|
|||...[25] use saliency maps to build an appearance model....|
|||...Faktor and Irani [2] employ saliency maps as the initial distribution of random walk simulation....|
|||...Also, an instance-level segmentation algorithm for salient objects was proposed in [14], which uses both saliency maps and object proposals....|
|||...[7] produce a spatiotemporal saliency map via the pixel-wise multiplication of spatial and temporal saliency maps....|
|||...[17] generate six kinds of saliency maps and superpose them adaptively....|
|||...[9] combine color and motion saliency maps based on the salient foreground model and the non-salient background model....|
|||...[10] design two networks for static saliency and dynamic saliency, respectively....|
|||...They feed the output of the static network into the dynamic network to obtain a saliency map....|
|||...The th object instance ot, in frame It has two attributes: saliency score st, and feature vector f t,....|
|||...To determine the saliency score st,, we simply estimate a foreground distribution based on the boundary prior....|
|||...Also, object instances in a clique, representing a salient object track, should have  high saliency scores....|
|||...We hence define the saliency Esaliency() of clique  as  Esaliency() =  T  Xt=1  st,t ....|
|||...(3)  We attempt to find the maximal weight clique  that maximizes the similarity  energy:   = arg max    Esimilarity()  (4)  subject to the constraint that the saliency Esaliency() is also high....|
|||...the saliency Esaliency in this work)....|
|||...In SCO, we first initialize the clique (0) to maximize the saliency Esaliency in (3)....|
|||...To summarize, SCO initializes the clique to maximize the saliency Esaliency and then refines it iteratively to  t }T  t  Sequential clique optimization for video object segmentation  7  Initialize th...|
|||...In the initialization, N  1 comparisons are made to find the maximum saliency in each frame, requiring O(N T ) comparisons in total....|
|||...The conventional algorithms use thresholds to binarize continuous saliency maps to compute precision and recall rates....|
|||...Time (SPF)  FCIS 0.24  Optical flow Saliency estimation Feature extraction MRF 0.96  0.15  0.93  1.16  Total 3.44  T , when N is limited to 10....|
|||...The proposed algorithm performs FCIS [11] for generating object instances and also the optical flow estimation, saliency estimation, feature extraction at each frame....|
|||...For this purpose, we developed the SCO technique to consider both the saliency and similarity energies....|
|||...Kim, W., Jung, C., Kim, C.: Spatiotemporal saliency detection and its applications in static  and dynamic scenes....|
|||...: Spatiotemporal saliency detection for video sequences based on random walk with restart....|
|||...Chen, C., Li, S., Wang, Y., Qin, H., Hao, A.: Video saliency detection via spatial-temporal fusion and low-rank coherency diffusion....|
|||...Yang, J., Zhao, G., Yuan, J., Shen, X., Lin, Z., Price, B., Brandt, J.: Discovering primary objects in videos by saliency fusion and iterative appearance estimation....|
|||...Yan, Q., Xu, L., Shi, J., Jia, J.: Hierarchical saliency detection....|
|||...Goferman, S., Zelnik-Manor, L., Tal, A.: Context-aware saliency detection....|
|||...Perazzi, F., Kr ahenb uhl, P., Pritch, Y., Hornung, A.: Saliency filters: Contrast based filtering  for salient region detection....|
|||...Wei, Y., Wen, F., Zhu, W., Sun, J.: Geodesic saliency using background priors....|
|||...Zhu, W., Liang, S., Wei, Y., Sun, J.: Saliency optimization from robust background detection....|
|||...: Saliency detection via graph-based  manifold ranking....|
|||...Fang, Y., Wang, Z., Lin, W., Fang, Z.: Video saliency incorporating spatiotemporal cues and  uncertainty weighting....|
|||...Liu, Z., Zhang, X., Luo, S., Le Meur, O.: Superpixel-based spatiotemporal saliency detection....|
|||...Wang, W., Shen, J., Shao, L.: Consistent video saliency using local gradient flow optimiza tion and global refinement....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...: Saliency detection via absorbing Markov  chain....|
||37 instances in total. (in eccv2018)|
|102|Zou_HARF_Hierarchy-Associated_Rich_ICCV_2015_paper|...ent objects completely from background, largely due to the lack of sufficiently robust features for saliency prediction....|
|||...The original task of saliency detection aims to predict fixation points in an image, where the earliest study on this topic is motivated by the observation from cognitive scientists that the inherent ...|
|||...Following the fixation prediction, saliency detection has  This work was supported by the NSFC projects (No....|
|||...e-of-the-art salient object detection models typically compute visual features based on regions for saliency evaluation, where it is important to note that the robustness and richness of these feature...|
|||...Saliency maps generated by seven state-of-the-art saliency models , and by the proposed model with HARF1 and HARF2 description respectively....|
|||...As a result, even the state-of-the-art saliency models still have significant difficulties in completely highlighting salient objects in complex scenes (see Figure 1)....|
|||...With such a rich feature representation, we are able to cast saliency detection as a regression problem for which a boosted predictor is trained to estimate regional saliency scores....|
|||...Last, we show that the proposed approach outperforms the state-of-the-art saliency models by a large margin, both quantitatively and qualitatively....|
|||...[13] proposed a well-known saliency model which was implemented based on the biological attention mechanisms and feature integration theories....|
|||...nce computed from different scales, are integrated using a center-surround operator to generate the saliency map, in which visually salient points are highlighted, as the prediction of fixations....|
|||...Since then, plenty of saliency models have been proposed for detecting salient objects in images based on various theories and principles, such as information theory[37], graph theory [15, 23, 41], st...|
|||...Thus most of the previous saliency models define regional saliency based on the regional contrasts of traditional low-level features, such as color and texture....|
|||...mage classification, object recognition and semantic segmentation, yet they are rarely exploited in saliency detection domain....|
|||...Therefore, for a compact representation and reasonable saliency evaluation, we compute local regional contrasts and border regional contrasts for each elementary feature type as follows....|
|||...inary segmentation tree, we cast salient object detection as a regression problem that predicts the saliency of a region....|
|||...For reasonable saliency precision, we compute the saliency score sk of Rk by further fitting the output of the boosted regressor into the range of [0, 1] with a sigmoid function, i.e.,  sk =  1  1 + e...|
|||...As in [17] we train our saliency model using the training set that contains 2500 images in MSRA-B dataset, split by [17]....|
|||...Evaluation criteria  We adopt the most widely used precision-recall (PR) criterion, where saliency maps, normalized into the range of [0, 255], are binarized at each integer in the range of [0,255] fo...|
|||...We also validate the saliency performance for the combination of CNN and traditional features....|
|||...The saliency maps generated for the example image by using different configurations of the proposed approach are shown in Figure 5....|
|||...Clearly, the quality of saliency maps is gradually improved as more components are integrated....|
|||...emantic information than the lower ones) contribute to salient object detection or not, we generate saliency results by using only the lower CNN layers (layer 1 to 5) as well as all CNN layers (layer ...|
|||...From Figure 4 and Table 1, we can directly see that using all CNN layers achieves higher performance than using the lower ones only, which validates the proposed CNN feature description for saliency detection....|
|||...To analyze the sensitivity of HARF construction with respect to the number of super-regions in the segmentation tree (parameter ), we compute the saliency performance for increasing values of ....|
|||...5.4.1 Quantitative comparison  The saliency performance of different models on the three datasets is show in Figure 7 for PR curves and in Table 2 for F scores and MAEs....|
|||...5.4.2 Qualitative comparison  Figure 8 shows some saliency maps generated by different models....|
|||...Multiple salient objects: The proposed model generates better looking saliency maps for those images containing multiple salient objects (e.g., the last two rows)....|
|||...Based on the HARF representation, regional saliency scores are estimated through a boosted predictor....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...A visual-attention model using earth movers distance-based saliency measurement and nonlinear feature combination....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
|||...Geodesic saliency using  background priors....|
|||...Bayesian saliency via low  and mid level cues....|
|||...Hierarchical saliency detec tion....|
|||...Top-down visual saliency via joint  crf and dictionary learning....|
|||...Segmentation In  driven low-rank matrix recovery for saliency detection....|
||36 instances in total. (in iccv2015)|
|103|He_Exemplar-Driven_Top-Down_Saliency_CVPR_2016_paper|...Exemplar-Driven Top-Down Saliency Detection via Deep Association  Shengfeng He1, Rynson W.H....|
|||...e and Technology, University of Science and Technology of China  http://www.shengfenghe.com/exemplarsaliency.html  Abstract  Top-down saliency detection is a knowledge-driven search task....|
|||...Our attention is mainly drawn by factors relevant to either bottom-up or top-down saliency detection....|
|||...Bottomup visual saliency is stimulus-driven, and thus sensitive to the most interesting and conspicuous regions in the scene....|
|||...In computer vision, bottom-up saliency detection [19, 37, 36, 16, 17, 41, 40] receives much research attention, due to its task-free nature....|
|||...On the other hand, top-down saliency [21, 22, 12, 38, 23, 7] aims to locate all the intended objects in the scene, which can help  Corresponding author....|
|||...We design a two-stage deep model (Figure 2 left) to detect top-down saliency by associating multiple exemplars with the query object, and explore the performance of different network structures....|
|||...In particular, we explore how different numbers of exemplars as well as the exemplar quality affect saliency detection performance....|
|||...We explore the proposed deep model  in different tasks, including same-class identification, object location predication, and top-down saliency detection (Figure 2 right)....|
|||...We then describe object localization methods, as they share a similar objective to topdown saliency detection....|
|||...Top-down saliency includes two main processes, dictionary learning for each category (i.e., learning category knowledge) and saliency computation (i.e., knowledgedriven searching)....|
|||...In [38, 23], top-down saliency is modeled by jointly learning category-specific dictionaries and CRF parameters....|
|||...The trained network can then be applied to top-down saliency detection, same-class identification, and object location prediction....|
|||...While stage 2 may be more important to top-down saliency performance (reducing background errors), stage 1 is the key for learning association and a universal model....|
|||...Top-down saliency detection: To detect saliency, we apply same-class identification to the entire image....|
|||...(2)  The final saliency map is normalized to [0, 1]....|
|||...Based on the top-down saliency map of a target category, the object location can be easily obtained by applying a global max-pooling operation on the entire map....|
|||...An example of the saliency map and its corresponding predicted location are shown in Figure 2 right....|
|||...td Mean 15.6 40.4 48.1 50.5 52.0 54.3 56.2  7.6 10.3 11.5 12.5 24.1 36.7 2.2      Table 2: Top-down saliency precision rates (%) at EER on the Pascal VOC 2012 validation set....|
|||...(a) Inputs  (b) Using 1 exemplar  (c) Using 2 exemplars  (d) Using 3 exemplars  (e) Using 4 exemplars  Figure 3: Saliency maps generated by the proposed method using different numbers of exemplars....|
|||...Top(cid:173)down Saliency Detection  We then examine the performance of top-down saliency detection and delve into the learnt association....|
|||...We compare our method with two latest top-down saliency detection methods [38, 23], and one latest object localization method [29]....|
|||...We use the sliding window sampling strategy in Section 3.3 to extract patches for saliency detection....|
|||...Unlike the evaluation setting in [38], we evaluate the saliency map in pixel-level rather than patch-level for higher accuracy....|
|||...We first binarize the saliency map for every threshold in the range of [0, 255] to generate the precision-recall curves (PR curves), and the performance of each category is summarized by the precision...|
|||...The two state-of-theart top-down saliency detection methods (rows 1 2 in Table 2) encode object information using dictionary learning, but the large appearance differences among the objects of the sam...|
|||...Figure 3 shows some topdown saliency detection examples....|
|||...The saliency maps are produced using the same sets but differnt numbers of examplars....|
|||...In the second example, the second exemplar of the human feet causes the second saliency map to focus on the human face....|
|||...As mentioned above, a simple max-pooling operator applied on the saliency map is able to predict an object location for a target category....|
|||...Since most of the top-down saliency detection methods (including Yang et al....|
|||...Conclusion  In this paper, we have proposed a novel locate-byexemplar top-down saliency detection framework....|
|||...Boosting bottom-up and top-down visual features for saliency estimation....|
|||...Sun: Topdown saliency using natural statistics....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Top-down visual saliency via joint crf and dictionary learning....|
||36 instances in total. (in cvpr2016)|
|104|cvpr18-Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection|...Traditional  saliency  detection  models  [3,  7-10]  are  performed merely on RGB images and can be categorized  as bottom-up and top-down pipelines....|
|||...Based on these two  frameworks,  various  hand-crafted  saliency  features  have  been proposed....|
|||...However,  when  the  salient  object  and  background  these  RGB-induced saliency detection models may be powerless  to discriminate the salient object from background....|
|||...In this  case,  the  paired  depth  data,  which  contain  affluent  spatial  structure and 3D layout information, can contribute a lot of  additional  saliency  cues....|
|||...Also,  the  robustness  of  depth  sensors  (e.g.,  Microsoft  Kinect  or  Intel  RealSense)  to  lighting  changes  will  benefit  a  lot  in  extending  the  application scenarios of saliency detection....|
|||...For the RGB-D saliency detection task,  how to fuse the RGB and depth information sufficiently is  the key issue....|
|||...tions, a number of CNNs have been proposed for  various  RGB-D  computer  vision  tasks,  such  as  saliency  detection  [18,  19],  semantic  segmentation  [20-23]  and  object  recognition  [1,  24-...|
|||...CNN  features  (i.e.,  late  fusion),  while  we  believe  that  the  cross-modal  complement  for  saliency  detection  exists  across  multiple  levels, which are not well-explored by previous works...|
|||... as  undifferentiated  4-channel  input  (input  fusion),  combining  handcrafted  RGB  and  depth  saliency  features  (feature  fusion),  or  performing unimodal predictions separately and then make...|
|||...[39] serialize a RGB-D pair as  4-channel  input  and  feed  it  to  a  multiple-stage  saliency  inference model....|
|||...[38] use the 4-channel data to  compute  multi-scale  saliency  values....|
|||...[34]  measure  depth-induced  saliency  by  evaluating  the  anisotropic  center-surround  difference....|
|||...Recently,  CNNs  are  adopted  in  RGB-D  saliency  detection to learn more discriminative RGB-D features....|
|||...[18]  combine  the  hand-designed  low-level  saliency  features from RGB and depth modalities as the joint input  and  train  a  CNN  from  scratch  to  generate  RGB-D  hyper-features....|
|||...Then  we  add   supervision  on  the  RGB  branch  to  encourage   m  RF   to  be   discriminative for saliency inference....|
|||...  each  =  all   m 1  m 2  =  1  ,  m  P x y ( ,  cross-entropy  loss  between  the  predicted  2D  saliency  map P ( the ground-truth mask (Y):       and  (x,y)  is  the  pixel  location)  and   [0,1...|
|||...Concretely,  the  saliency  map  will  be  binaried  by  using  a  series  of  the  ground-truth....|
|||...The saliency maps shown in the first row  in  Fig....|
|||...As  a  result,  the  cross-level complement is better captured and incorporated  and  the  saliency  maps  are  enhanced  from  coarse  to  fine  increasingly....|
|||...operated information and boost  better  cross-modal  combination,  thus  generating  more  precise  saliency  maps  (see  the  columns  indexed  as  Fig....|
|||...5, the saliency inference scales decrease from global to local  with the levels goes from deep to shallow....|
|||...LBE  [37],  MDSF  [38],  EGP  [36],  NLPR  [39],  ACSD  [34],  SRDS  [35]  and  two  recent  RGB-D  saliency  detection  networks  DF  [18]  and  CTMF  [19]....|
|||...We also report saliency maps detected on various  challenging scenes to show the advantages of the proposed  method visually....|
|||...Especially in the 4th row,  the  depth  distribution  introduces  misleading  saliency  cues  (i.e., the depth of the rail is more distinctive than the train)....|
|||...Although the CTMF method is able to obtain more correct  and uniform saliency maps  than  others,  the  fine  details  of  the salient objects are lost severely due to the deficiency of  cross-level fusion....|
|||...By contrast, the proposed network is able  to utilize both cross-modal and cross-level complementary  information  to  learn  cooperatively  discriminative  saliency  cues and infer precise saliency values....|
|||...Region-based saliency detection and its application in object  recognition....|
|||...SUN:  A  Bayesian  framework  for  saliency  using  natural statistics....|
|||...Top-down visual saliency via joint   CRF and dictionary learning....|
|||...DHSNet:  Deep  Hierarchical  Saliency   Network for Salient Object Detection....|
|||...CNNs-Based  RGB-D  Saliency  Detection  via  Cross-View  Transfer  and  Multiview Fusion....|
|||...Depth   enhanced saliency detection method....|
|||...Depth saliency  based  on  anisotropic  center-surround  difference....|
|||...Exploiting  global  priors  for  RGB-D  saliency  detection....|
|||...Depth-Aware Salient Object Detection and Segmentation via  Multiscale  Discriminative  Saliency  Fusion  and  Bootstrap  Learning....|
||35 instances in total. (in cvpr2018)|
|105|Shuhan_Chen_Reverse_Attention_for_ECCV_2018_paper|...More specifically, given a coarse saliency prediction in the deepest layer, we first employ residual learning to learn side-output residual features for saliency refinement, which can be achieved with...|
|||... Attention  Side-output Residual Learning  1  Introduction  Salient object detection, also known as saliency detection, aims to localize and segment the most conspicuous and eye-attracting objects or ...|
|||...One is the low resolution of the saliency maps produced by FCNs based saliency models....|
|||...Maximum F-Measure of recent deep CNN-based saliency detection modincluding DS [6], ELD [7], DCL+ [8], DHS [8], RFCN [9], els on ECSSD, NLDF [10], DSS+ [11], MSRNet [12], Amulet [13], UCF [14], and our...|
|||...However, to the best of our knowledge, there are no saliency detection networks explored considering both lightweight model and high accuracy....|
|||...Visual comparison of saliency maps produced by DSS [11] (top row), our method without (middle row) and with reverse attention (bottom row) in different side-outputs, respectively....|
|||...As can be seen clearly that the resolutions of the saliency maps are improved gradually from deep to shallow side-outputs, and our reverse attention based side-output residual learning performs much b...|
|||...Since most of the existing deep saliency models are fine-tuned from image classification network, the fine-tuned network will unconsciously focus on the regions with high response values during residu...|
|||...With the help of the learned side-output residual features, the resolution of the saliency map can be improved gradually with much fewer parameters compared to the existing deep saliency networks....|
|||...2 Related Work  There are plenty of saliency detection methods proposed in the past two deceads....|
|||...Recently, dilated convolution [23] and dense connections [17] are further incorporated to obtain high resolution saliency map....|
|||...Since the resolution of the global saliency map is only 1/32 of the input image, we further learn residual feature in each side-output to improve its resolution gradually....|
|||...In this paper, we implement it in a different yet more efficient way by employing residual learning to remedy the errors between the predicted saliency maps and the ground truth....|
|||...Formally, given the upsampled input saliency map Sup i+1 by a factor 2 in side-output stage i + 1, and the residual feature Ri learned in side-output stage i, then the deep supervision can be formulat...|
|||...Since most of the existing saliency detection networks are fine-tuned from image classification networks which are only responsive to small and sparse discriminative object parts,  Reverse Attention ...|
|||...it obviously deviates from the requirement of the saliency detection task that needs to explore dense and integral regions for pixel-wise prediction....|
|||...While without reverse attention, it learned some redundant features inside object which is helpless for saliency refinement....|
|||...From left to right are saliency map, the last convolutional feature from side output 1 to 4, respectively....|
|||...After appling our reverse attention, the proposed network well captured spatial details near object boundaries which is beneficial for saliency refinement, especially in shallow layers....|
|||... to its low resolution, while in this paper, we apply it to focus on all the undetected regions for saliency refinement, which not only refines object boundaries well but also highlights object region...|
|||...Pairs of precision and recall values are calculated by comparing the binary saliency maps with the ground truth to plot the PR curve, where the thresholds are in the range of [0, 255]....|
|||...Given the normalized saliency map S and ground truth G, the MAE score is calculated by their average per-pixel difference:  MAE =  1  H  W  H  W   S(x, y)  G(x, y)  ,  (8)  Xx=1  Xy=1  where W and H a...|
|||...Note that all the saliency maps of the above methods are produced by running source codes or pre-computed by the authors, and ResNet based methods are not included for fair comparison....|
|||...Instead of directly learning multi-scale saliency features in different side-output stages, we employ residual learning to learn side-output residual features for saliency refinement....|
|||...Nevertheless, the global saliency branch and backbone (VGG-16) network still contain large redundancy, which will be further explored by introducing handcrafted saliency prior and learning from scratc...|
|||...Lee, G., Tai, Y.W., Kim, J.: Deep saliency with encoded low level distance map  and high level features....|
|||...Wang, L., Wang, L., Lu, H., Zhang, P., Ruan, X.: Saliency detection with recurrent  fully convolutional networks....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional  features for accurate saliency detection....|
|||...Chen, T., Lin, L., Liu, L., Luo, X., Li, X.: Disc: Deep image saliency computing via progressive representation learning....|
|||...Tang, Y., Wu, X.: Saliency detection via combining region-level and pixel-level  predictions with cnns....|
|||...Kuen, J., Wang, Z., Wang, G.: Recurrent attentional networks for saliency detec tion....|
|||...Li, G., Yu, Y.: Visual saliency detection based on multiscale deep cnn features....|
|||...Shi, J., Yan, Q., Xu, L., Jia, J.: Hierarchical image saliency detection on extended  cssd....|
|||...: Saliency detection via graph based manifold ranking....|
|||...Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object  detection....|
||35 instances in total. (in eccv2018)|
|106|Chen_Look_Perceive_and_ICCV_2017_paper|...[19] adopted an end-to-end convolution-deconvolution framework to obtain an initial saliency map and then iteratively refined it by recurrent attentional networks....|
|||...Related Work  Hundreds of image-based SOD models have been proposed in the past decade that explore saliency cues such as local/global contrast [17, 4], sparsity and low-rank properties [35, 31, 32] a...|
|||...Finally, multi-scale saliency maps are inferred from such features, which are then fused together to form the final saliency map....|
|||...After that, the descriptor is reshaped to form an initial saliency map, and hierarchical recurrent CNNs are adopted to progressively refine the details in saliency maps so as to highlight the boundari...|
|||...RACDNN [19] is a recurrent model which initializes a coarse saliency map via a convolution-deconvolution network....|
|||...In the first stream, VGG16 is revised by incorporating the idea of dilation convolution [45] to generate a coarse saliency map....|
|||...o superpixels, which are then linked with the features extracted in the first stream so that a fine saliency map can be generated by simultaneously measuring the saliency of all superpixels....|
|||...LEGS [24] first estimates pixel-wise saliency for an image by using CNNs....|
|||...Meanwhile, various object proposals are extracted and incorporated to obtain a local saliency map....|
|||...After that, another deep networks with only FC layers are adopted to predict the saliency of each candidate object from the global perspective so that salient objects can be detected as a whole....|
|||...first stream handles a superpixel-centered window padded with mean pixel value and outputs a global saliency map, which are then fed into the last layer of the second stream that focuses on a closer s...|
|||...Finally, saliency map is generated by fusing local and global features....|
|||...Heuristic saliency maps are first computed on superpixels by using a contrast-based framework....|
|||...Such heuristic maps are then used as prior knowledge that enter the recurrent CNNs along with the original image to obtain the refined saliency map....|
|||...In this process, the foreground map generated by the network is iteratively delivered back to replace the heuristic saliency map so that the quality of a saliency map can be progressively improved....|
|||...The networks consist of a two-stream module for feature extraction and an inception-segmentation module for feature fusion and saliency estimation....|
|||...the end of the inception-segmentation module to output a probability map that represents pixel-wise saliency distribution....|
|||...The training process of the networks are conducted end-to-end with the cross-entropy loss between estimated and ground-truth saliency maps....|
|||...After the training process, the end-to-end networks can be directly used to output a saliency map via the last sigmoid layer....|
|||...Such small responses may make the saliency map somehow noisy....|
|||..., denoted as the Butterworth layer, first normalizes the map to the range of [0,1] and delivers the saliency values into a Butterworth high-pass filter that perfectly rejects small saliency values and...|
|||...M = 3 to prune saliency values smaller than 0.1 (see Fig....|
|||...We can see that saliency values smaller than 0.1 are almost pruned, while the ordering of the rest saliency values stay unchanged since B(x) is monotonically increasing in [0, 1]....|
|||...MAE reflects the average pixelwise absolute difference between the estimated and groundtruth saliency maps that are both normalized to [0, 1]....|
|||...In computing F, we normalize the estimated saliency maps into [0, 255] and simultaneously binarize all saliency maps from the same dataset by enumerating all probable thresholds in {0, ....|
|||...curves can be drawn to show the performance of a model in using different thresholds for binarizing saliency maps, and the maximal F over the curve, denoted as Fmax , is used to represent the overall ...|
|||...Different from the adaptive F that binarizes saliency maps with adaptive thresholds (e.g., twice the mean saliency value as in [1]), Fmax is less sensitive to reparameterization operations that are fr...|
|||... this manner,  three parallel branches with different CONV layers, making it capable to explore the saliency cues from multiple scales....|
|||...In contrast, the MAE metric focuses on the magnitude of saliency and thus become very sensitive to such reparameterization operations....|
|||...exploration for object-based visual saliency learning....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Recurrent attentional  networks for saliency detection....|
|||...Visual saliency based on multiscale deep  features....|
|||...Deep networks for saliency detection via local estimation and global search....|
||34 instances in total. (in iccv2017)|
|107|Zhou_Time-Mapping_Using_Space-Time_2014_CVPR_paper|...Our approach makes these contributions: (1) a robust space-time saliency method for evaluating visual importance, (2) a re-timing technique to temporally resample based on frame importance, and (3) te...|
|||...Results of our space-time saliency method on a benchmark dataset show it is state-of-the-art....|
|||...Methods for retiming and filtering make use of the results of the proposed space-time saliency measure....|
|||...We validate our space-time saliency approach using a benchmark dataset, and subsequently our overall approach through a user study....|
|||...While many models have been proposed in image domain (see [2] for a review), computing spatiotemporal saliency for videos is a relatively unexplored problem....|
|||...Most existing motion saliency methods built upon image attention models by taking into account simple motion cues....|
|||...Compared to previous work, our motion saliency method combines various low-level features with region-based contrast analysis....|
|||...bust bottom-up saliency model to rate spatial-temporal regions in a video....|
|||...Feature contrast  Our space-time saliency measure is defined based on contrast in both appearance and motion features....|
|||...This design decision is inspired by sensitivity of the human visual system to contrast in visual signal, and shared by the recent advance in image saliency [9, 15]....|
|||...The final saliency score si,t for each pixel i at frame t is then computed by combining these responses in a linear fusion scheme, i.e.,  si,t =  lXj=1  uj c(i),tvj  c(i),t,  c(i),t and vj  where uj c...|
|||...5a), our model generates saliency maps that have sharper object and motion boundaries and with less background artifacts....|
|||...It provides a more robust and stable saliency measure for re-timing and filtering high-speed videos....|
|||...In this section, we describe how we make use of saliency information for re-timing, i.e., mapping input frames to output frames....|
|||...More specifically, we apply an optimal dynamic programming approach to sample representative frames based on overall saliency per frame....|
|||...Re-timing a synthetic 1000-frame saliency curve to 50 frames....|
|||...Saliency-based sampling  The importance of frame i is computed to be the average saliency score s 2 Rn over all its pixels....|
|||...The user can set the weight of saliency on re-timing: si    si + (1    ), with parameter   2 [0, 1]....|
|||...3a), while   = 1 results in total reliance on saliency for sampling....|
|||...3a, where we need to sample 50 frames on a 1000frame saliency curve....|
|||...In principle, the cumulative saliency between successive sj   s, should be a constant equal to  s = 1 In practice, this strict criterion can be relaxed to minimize the following sum of least-square er...|
|||...Smoothing  The proposed DP sampler optimally subsamples frames from a high-speed input given its saliency curve....|
|||...In practice, however, the saliency curve can change dramatically at motion boundaries or contain random variations due to image noise....|
|||...input sampling function p, we optimize a to minimize the following reconstruction error weighted by saliency score sp on the sample position:  a  k(p   Qa)   spk2 + kLQak2,  min s. t. FQa   > 0, aT 1 ...|
|||...Given the saliency map, SalBlur renders the video frame in three steps as shown in Fig....|
|||...A direct filtering of the saliency map might blur the objects boundary....|
|||...Second, we compute a binary motion blur mask for each frame based on the refined saliency map and optical flow....|
|||... this intuition, we calculate for each pixel i the difference, r(i, t) = sf (i,t)   si, between its saliency value si at current frame t0 and the ones sf (i,t) at the position f (i, t)  8We ignore iss...|
|||...Comparison of different saliency algorithms on the Weizmann dataset....|
|||...(a) Saliency maps....|
|||...We propose a new space-time saliency technique, shown to be state-of-the-art in performance on a benchmark dataset....|
|||...Spatio-temporal saliency detection using phase  spectrum of quaternion Fourier transform....|
|||...Visual saliency based on scalespace analysis in the frequency domain....|
|||...Static and space-time visual saliency detection by self resemblance....|
||34 instances in total. (in cvpr2014)|
|108|Feichtenhofer_Dynamically_Encoded_Actions_2015_CVPR_paper|...By using the resulting definition of saliency during feature pooling we show that action recognition performance achieves state-of-the-art levels on three widely considered action recognition datasets....|
|||...Our saliency weighted pooling can be applied to essentially any locally defined features and encodings thereof....|
|||... locally aggregated spatiotemporal energy features, which efficiently result as a by-product of the saliency computation, further boosts performance over reliance on standard action recognition featur...|
|||...Our proposed measure of spacetime saliency to enhance feature pooling for action recognition....|
|||...(f) Our spacetime saliency measure, ST, for weighting the contribution of local spatiotemporal features for action recognition....|
|||...Note the large similarities between the user-annotated puppet flow and our saliency measure which is computed efficiently from motion statistics without any form of supervision....|
|||...The most closely related work to ours is previous research that has made explicit use of notions of saliency to capture foreground regions where actions are most likely to occur....|
|||...One such approach [3] made use of three types of saliency measures to pool dense trajectory features [42]....|
|||...In distinction from our work, they employ three very simple saliency measures (detected corners, image brightness and motion magnitude) and focus their work on the learning component of the system, wh...|
|||...Also related is work that combines colour and motion gradients via a graph-based saliency measure to select foreground actions [39]....|
|||...Our approach dynamically encodes and pools primitive feature measurements via a new definition of spacetime saliency weights based on directional motion energy contrast and spatial variance to capture actions....|
|||...Directional motion energy  There are three steps to the proposed approach to capturing directional motion energy as the measurements over which saliency is defined....|
|||...While our notions of saliency being defined in terms of    00.050.10.150.20.250.30.350.4  00.050.10.150.20.250.30.350.4  00.050.10.150.20.250.30.350.4  00.050.10.150.20.250.30.350.4  00.050.10.150.20....|
|||...Spatial distance plays into the saliency calculation via multiplicative weighting with an exponential on the Euclidean distance between the centre of mass coordinates of the elements, i and j, x(i) an...|
|||...2.2.2 Spatial motion variance  The second measure of saliency is based on the common observation that foreground motion typically occurs in a spatially localized region of a video, whereas background ...|
|||...Our second saliency measure therefore ranks local regions as highly salient if the spatial variance of their motion characteristics is low....|
|||...f motion, (11), increases as the the motion of i becomes more spread out across an image, while for saliency we seek the opposite....|
|||...T,VAR, for the ball catch (12)  Finally, the overall spacetime saliency is given by com bining the two saliency measures so far defined, to yield  S (i) T =  S (i) T,CTR + S (i)  T,VAR  2  ....|
|||...(13)  Example overall saliency measurements, S (i) T , for the ball catching sequence are shown in Figure 1(f); additional examples are provided on the project webpage....|
|||...Dynamic feature encoding via saliency  An FV models mean and covariance gradients between features {fl(x)}L  l=1 and the GMM modelled distribution  Our spacetime saliency measure, S (i)  T , can be us...|
|||...Here, ST(x) is derived directly from S (i) T by having the saliency of x be defined as that of the superpixel element i to which it belongs, i.e....|
|||..., c()  (16) To apply our saliency measure, ST(x), to FVs, we employ it as a local weighting function during aggregation of the firstand second-order gradients:  K , c() K ]....|
|||...) Thus, for each feature type, we calculate a FV that is explicitly weighted in favour of spacetime saliency where actions are most likely to occur....|
|||...Primitive features  To emphasize the generality of the proposed approach, the previous section cast saliency weighted encoding and pooling in terms of of arbitrary features, f. This section briefly do...|
|||...It is seen that our algorithmically derived saliency weight result at 64.1% mean accuracy actually surpasses that of groundtruth foreground masks, 60.4%....|
|||...Comparison to alternative saliency approaches  Weighting [39] ST ST ST ST ST ST ST  Features STIP STIP Grid (Sec....|
|||...We now explicitly evaluate our spacetime saliency measure, ST, in comparison to two alternative approaches that previously have employed algorithmically derived saliency for action recognition....|
|||...The first alternative [39] is based on a graph-based notion of saliency defined over colour and optical flow gradients, as noted in Sec....|
|||...In that case, three types of saliency measures are used to pool LLC [45] encoded DT [42] features in a spacetime pyramid and perform classification with a weighted SVM model, as discussed in Sec 1....|
|||...Our approach is compared to those originally presented for DT [42] and IDT [43], which also used Fisher vector encoding (FV), but without any notion of saliency weighting....|
|||...h those for the alternatives documents the performance change provided by inclusion of the proposed saliency weighting....|
|||...Space-variant descriptor sampling for action recognition based on saliency and eyemovements....|
|||...Hierarchical saliency detec tion....|
||33 instances in total. (in cvpr2015)|
|109|Yi_Initialization-Insensitive_Visual_Tracking_2013_ICCV_paper|...To track objects accurately in such situation, the proposed method uses motion saliency and descriptor saliency of local features and performs tracking based on generalized Hough transform (GHT)....|
|||...The proposed motion saliency of a local feature emphasizes features having distinctive motions, compared to the motions which are not from the target object....|
|||...The descriptor saliency emphasizes features which are likely to be of the object in terms of its feature descriptors....|
|||...obustly with inaccurate initializations and severe occlusions, we propose a method employing motion saliency and descriptor saliency of local features to learn and track the target object based on GHT...|
|||...Method in [16] also uses the concept of saliency, but their definition of saliency is a criterion for selection of features (features such as colors or or edges, not to be confused with feature points...|
|||...Also, the bottom-up saliency they use for initialization is a center-surround saliency (un like ours which considers target and non-target rather than center and surround), and does not always guarant...|
|||...initializations, each solution is weighted according to the two proposed saliencies (the descriptor saliency and the motion saliency)....|
|||...The proposed motion saliency is obtained using the learned descriptor saliency of features and the optical flow of the local feature....|
|||...The motion saliency is designed so that the features showing distinctive motion characteristics of the target object have higher values....|
|||...c e S (  Descriptor Saliency   (Sec....|
|||...2.2)   Motion Saliency   (Sec....|
|||...The weight wj is designed to account both the motion saliency j and the descriptor saliency  (t) ....|
|||...i We use the multiplication of the two saliencies to emphasize features which have both saliency values high....|
|||...Descriptor Saliency and Feature DB Update  x = (x, y) = arg max  x,y  A (x, y) ....|
|||...the shape of the target object, we define the descriptor saliency  2914  (a)  (b)  (c)  Figure 4: Illustration of the descriptor saliency in action....|
|||...Detected local features are depicted with circles, having their sizes as their descriptor saliency values (larger means high)....|
|||...For each item in F(t), the descriptor saliency  (t) is learned to hold how good the partial (voting) results were when using the item, i.e....|
|||... matched with the features pointing to the center of the cup in Figure 3 would be updated with high saliency values, whereas items matched with the feature pointing at the wrong direction (e.g....|
|||...ng box in the first frame, feature points inside the bounding box are added to F(1) with descriptor saliency  = 1, and feature points outside the bounding box are added to F(1) with descriptor salienc...|
|||...there exists Mi number of j such that scriptor saliency of this item  (t) of the vote map value for all matches....|
|||...Figure 4a and Figure 4b is an example of some local features (features on leaves) having high descriptor saliency at initialization, but becoming low after learning them correctly as tracking is performed....|
|||...The L best considering the motion saliency j are added to F(t+1) with its motions saliency as initial descriptor saliency, i.e....|
|||...iency  To capture the characteristics of the target object in terms of motion, we define the motion saliency of a feature point with its descriptor saliency and optical flow (Figure 5b), and emphasize...|
|||...We obtain the motion saliency of feature point by simply using the inverse of the motion vote map value....|
|||...Note that for the motion vote map, we weight the votes with , which is the inverse of descriptor saliency so that the vote map is about background  (cid:8) 1   (t) i  (cid:9)  j  motions....|
|||...https://sites.google.com/site/homekmyi  2916  (a)  (b)  (c)  (d)  (e)  Figure 5: Example of motion saliency obtained for the woman sequence....|
|||...(a) Detected local feature points, (b) descriptor saliency of matched local features and their optical flows (denoted yellow if high saliency and red if low, optical flows displayed 3 times their orig...|
|||...Note that in (b), upper left motion on the cars result in low motion saliency values in (d) whereas down right motion on the person result in high values....|
|||...1 (PROPm and PROPd are the results of our method using only motion saliency and descriptor saliency, respectively)....|
|||...The proposed method uses motion saliency and descriptor saliency of local features and obtains the target position through GHT....|
|||...The motion saliency of a local feature emphasizes features having distinctive motions, compared to background motions....|
|||...The descriptor saliency emphasizes features which are likely to be of the object in terms of its feature descriptors....|
||32 instances in total. (in iccv2013)|
|110|Ballas_Space-Time_Robust_Representation_2013_ICCV_paper|...Our pooling identifies regions of interest using video structural cues estimated by different saliency functions....|
|||...troduce an iterative structure learning algorithm, WSVM (weighted SVM), that determines the optimal saliency layout of an action model through a sparse regularizer....|
|||...Beyond standard spatial pooling which uses fixed segmentation grids, we segment a video according to its content through saliency maps....|
|||...Our algorithm relies on the idea that the discriminative information has a non-uniform distribution in saliency spaces....|
|||...We first extract video structural cues using various saliency measures....|
|||...We then aggregate the local feature statistics over fixed saliency subregions, each sub-region defining a structural primitive....|
|||...We do not use saliency information to sample features but to pool them....|
|||...We identify prominent regions in a video through saliency to model the space-time context while preserving the space-time robustness....|
|||...As shown in Figure 4a, we (i) extract saliency information from a video, then, (ii) order local features in rank lists according to each saliency and (iii) capture local feature statistics in various ...|
|||...Since our pooling uses ranks to group features instead of absolute values, it remains invariant to global translation in the saliency space....|
|||...Let P = {p1, ..., pM} be the saliency values for each lo[1, M ]  [1, M ] is a ranking funccal feature....|
|||..., (M )}, we minimize the functional min (1) is the local features having the highest saliency while d (M ) correspond to the lowest one....|
|||...Our content-based pooling becomes:  (i), d  (cid:2)  M  i=1 ip  Xi,k = max j[1,M ]  Gi,k  j   code(d  (j))  (2)  With (2), the pooling is performed in the saliency instead of the space-time domain....|
|||...When to obtain the signature X = [X1,1, ..., X using several saliency functions, we repeat this pooling operation for each measure and concatenate all the resulting structural primitives....|
|||...We take advantage of the video visual data through saliency measures to identify prominent or salient areas: pi = s(di)....|
|||...We focus on 3 different saliency functions : cornerness, light and motion....|
|||...The cornerness saliency highlights visually distinctive features, which are repeatable under geometric transformation....|
|||...Motion saliency considers the video optical flow computed for each video frame through the Farneback algorithm [3]....|
|||...The motion saliency is then computed with the same sliding windows approach as the light saliency [17]....|
|||...Weighting Structural Primitives  As shown in Figure 5, the saliency measures emphasize different areas of the video space-time volume....|
|||...the saliency measures are not equally discriminative for the different actions....|
|||...For instance, motion saliency emphasizes foreground as well  2706  Reference Cornerness Light  Motion  Reference Cornerness Light  Motion  Figure 5: Illustration of prominent areas detected with the ...|
|||...as background area for an action subject to strong camera movement while light saliency remains robust to this phenomena....|
|||...By focusing on only a few structural primitives at classification, we could take advantage of saliency functions which fit best the action of interest while discarding area containing irrelevant or no...|
|||...Since a trajectory spans on several video frames, the average saliency value of its points defines the saliency value associated to the feature....|
|||...To combine saliency and spatial pooling, we concatenate their respective spatial and structural primitives prior to the classification....|
|||...(cid:2)2 norm in W. The most relevant structural primitive can be associated with cornerness, motion or light saliency depending on the action....|
|||...This confirms the non uniform distribution of discriminative information in the saliency spaces....|
|||...By capturing the feature distribution at different saliency levels, we preserve that information in our final representation....|
|||...Light saliency performs a coarse segmentation which groups together the features associated to the human body in those actions (Figure 7)....|
|||...While most of the performance gain comes from the saliency pooling (see Table 1), WSVM has a positive contribution of 2.1% compared to a standard SVM....|
|||...Determining patch saliency using  low-level context....|
||32 instances in total. (in iccv2013)|
|111|Yuan-Ting_Hu_Unsupervised_Video_Object_ECCV_2018_paper|...Our approach computes motion saliency in a given video based on boundary similarity of motion cues....|
|||...2: Motion saliency estimation....|
|||...We detect the saliency score based on the flow vector by calculating a boundary dissimilarity map u(0) and a distance map u(1) indicating the distance of each pixel to the boundaries....|
|||...The motion saliency estimation is computed by averaging the boundary dissimilarity map and the distance map....|
|||...tion  The two most important ingredients for unsupervised video object segmentation are the initial saliency estimate as well as a good assessment of the neighborhood relation of pixels or superpixels...|
|||...For initial saliency prediction in unsupervised video object segmentation we describe a novel method comparing the motion at a pixel to the boundary motion....|
|||...Hence, the approach distributes an initial foreground saliency estimate over the F frames xi, i  {1, ....|
|||...Diffusion of the initial foreground saliency estimates v0  RN for each node is performed by repeated matrix multiplication of the current node estimate with the adjacency matrix G, i.e., for the t-th ...|
|||...We focus on both points in the following and develop first a new saliency estimation of v0 before discussing construction of the neighborhood graph G.  3.1 Saliency estimation  For unsupervised video ...|
|||...While the latter assumption is commonly employed for image saliency detection, it has not been exploited for motion saliency estimation....|
|||...To obtain the initial saliency estimate v0 defined over superpixels, we average the pixelwise motion saliency results u over the spatial support of each superpixel....|
|||...Conventional motion saliency estimation techniques for video object segmentation are based on either background subtraction [6], trajectory clustering [5], or motion separation [13]....|
|||...In contrast, we propose to use the boundary condition that is commonly used for image saliency detection [56, 53] to support motion saliency estimation for unsupervised video segmentation....|
|||...We use u to denote the foreground motion saliency of the video....|
|||...Moreover, ui and ui(pi) denote the foreground saliency for frame i and for pixel pi in frame i respectively....|
|||...To compute the motion saliency estimate, we treat every frame xi, i  {1, ....|
|||...(0) We compute the foreground motion saliency ui of frame i based on two terms u i  (1) and u i  , each of which measures a distance between any pixel pi of the i-th frame and (0) the boundary Bi....|
|||...Examples  after having () i  (1) and u i  (0) for u i We found the proposed changes to result in significant improvements for saliency  and the combined motion saliency are visualized in Figure 2....|
|||...In our method, we construct a graph for diffusing the initial motion saliency estimation....|
|||...We show the initial motion saliency and the diffused saliency map using the constructed graph....|
|||...We found these three types of connections to help propagate the initial saliency estimation effectively....|
|||...4.1  Implementation details  For the proposed saliency estimation algorithm, we set the number of clusters K = 3 for modeling the background....|
|||...The performance in IoU of the motion saliency estimation in our approach (with all the connections disabled) is 57.52%....|
|||...The improvements reported for saliency estimation and neighborhood construction motivate their use for unsupervised video segmentation....|
|||...We also evaluate our method in the semi-supervised setting by simply replacing the saliency initialization of the first frame with the ground truth....|
|||...Comparisons of the saliency estimation: To illustrate the benefits of the proposed motion saliency estimation, we compare the performance of the proposed initialization with other approaches in Table ...|
|||...Note that the saliency estimation in our approach is unsupervised as opposed to FSG and LMP which are trained on more than 10,000 images and 2,250 videos, respectively....|
|||...5 Conclusion  We proposed a saliency estimation and a graph neighborhood for effective unsupervised foreground-background video segmentation....|
|||...Our key novelty is a motion saliency estimation and an informative neighborhood structure....|
|||...Wei, Y., Wen, F., Zhu, W., Sun, J.: Geodesic saliency using background priors....|
||30 instances in total. (in eccv2018)|
|112|Park_Social_Saliency_Prediction_2015_CVPR_paper|... Pennsylvania {hypar,jshi}@seas.upenn.edu  Abstract  This paper presents a method to predict social saliency, the likelihood of joint attention, given an input image or video by leveraging the social ...|
|||...We present a method to estimate the likelihood of joint attention called social saliency from a spatial distribution of social members....|
|||...The heat map shows the predicted social saliency and we overlay this map by projecting onto the ground plane in the image....|
|||...ocial interaction data reconstructed in difference scenes, which allow us to learn and infer social saliency in a unified coordinate system....|
|||...Our method can predict social saliency from a third person video or image of social interactions....|
|||...[11] combined visual saliency and gaze directions to detect joint attention and Mar n-Jim enez et al....|
|||...Social Saliency Prediction  We predict social saliency, the likelihood of joint attention, using a social formation feature that is designed to capture the geometric relationship between joint attenti...|
|||...We leverage this representation to predict social saliency based on their locations....|
|||...n 5, we learn the geometric relationship between joint attention and its members and predict social saliency of a target scene....|
|||...We generate a continuous social saliency map by convolving with a Gaussian kernel....|
|||...The resulting social saliency map and the ground truth locations of joint attention are shown in Figure 2(a)....|
|||...High social saliency forms near the ground truth locations of joint attention....|
|||...To predict social saliency using a social formation feature presented in Section 3, the group detection must be carried out to isolate each social group....|
|||...We represent a gaze direction using a cone shaped gaze model and superimpose all gaze models to produce a social saliency field....|
|||...The modes of the social saliency field that corresponds to joint attention are estimated using a meanshift algorithm [23] that automatically determines the number of joint attention....|
|||...Result  We apply our method to predict social saliency in realworld social scenes by leveraging the social interaction data captured by first person cameras....|
|||... In this section, we quantitatively evaluate our method in two criteria: group detection and social saliency prediction....|
|||...Social saliency prediction We compare our method (SFF+Boosting) with four baseline methods: Random forests predictor with our social formation feature (SFF+RF), predictor using the center of circumcir...|
|||...We measure the area in a scene that corresponds to higher social saliency than a certain threshold....|
|||...The inset images illustrate the configuration of social members, joint attention, the center of mass, the center of circumcircle, and predicted social saliency from the top view of the scene....|
|||...The inset image shows the configuration of social members, joint attention, the center of mass, the center of circumcircle, and predicted social saliency from the top view of the scene....|
|||...Also we predict social saliency that forms around the Mona Lisa painting in the Louvre scene....|
|||...The modified social saliency feature (Section 6.1) is extracted by the locations of the feet of the players detected by [33]....|
|||...The detected players and predicted social saliency are shown in the inset image and overlaid on the image....|
|||...Our method correctly localizes social saliency in the presence of missing data....|
|||...We leverage the social interaction data captured by first person cameras that precisely measures joint attention to predict social saliency in real-world videos and images captured by third person views....|
|||...We apply our method to predict social saliency on third person videos....|
|||...(b) We use the modified social formation feature to predict social saliency in a basketball game....|
|||...3D social saliency from  head-mounted cameras....|
||29 instances in total. (in cvpr2015)|
|113|Fan_Structure-Measure_A_New_ICCV_2017_paper|... and the recently proposed F   (Fbw) have been used to evaluate the similarity between a non-binary saliency map (SM) and a ground-truth (GT) map....|
|||...We compare the ranking of saliency maps generated by 3 state-of-the-art salient object detection algorithms: DISC [11], MDF [27], and MC [48]....|
|||...We employed 10 state-of-the-art saliency detection models to obtain 10 saliency maps (Fig....|
|||...Using the same threshold (0.95), we found that the proportions of destroyed images in four popular saliency datasets (i.e., ECSSD [47], HKU-IS [27], PASCAL-S [31], and SOD [37]) are 66.80%, 67.30%, 81...|
|||...From an application standpoint (3th row; the output of the SalCut algorithm fed with saliency maps  (a)  (b)  Figure 3....|
|||...Object(cid:173)aware structural similarity measure  Dividing the saliency map into blocks helps evaluate the object-part structural similarity....|
|||...Uniform saliency distribution....|
|||...So, it is important to assign a higher value to a SM with salient object being uniformly detected (i.e., similar saliency values across the entire object)....|
|||...The non-binary foreground maps (5000 maps in total) were generated using five saliency detection models including CA [19], CB [23], RC [13], PCA [35], and SVO [9]....|
|||...(a) ground-truth map, (b) morphologically changed version of a, (c) difference map between a and b, (d) saliency map1, (e) saliency map2....|
|||...While the two GT maps in (a) & (b) are almost identical, measures should not switch the ranking between the two saliency maps when using (a) or (b)....|
|||...While ground truth maps (GT and Morphologic GT) differ slightly, both Fbw and our measure keep the ranking order of the two saliency maps, depending on the GT used....|
|||...Source saliency maps collection....|
|||...As mentioned above, we use 10 state-of-the-art saliency models to generate the saliency maps in each dataset....|
|||...Ranking of 10 saliency models using our new measure....|
|||...ffers a better way to evaluate salient object detection models, here we compare 10 state-of-the-art saliency models on 4 datasets (PASCAL-S, ECSSD, HKU-IS, and SOD)....|
|||...Discussion and Conclusion  In this paper, we analyzed the current saliency evaluation measures based on pixel-wise errors and showed that they ignore the structural similarities....|
|||...asure which simultaneously evaluates region-aware and objectaware structural similarities between a saliency map and a ground-truth map....|
|||...Our measure is based on two important characteristics: 1) sharp foreground-background contrast, and 2) uniform saliency distribution....|
|||...Finally, we conducted a behavioral judgment study over a database of 100 saliency maps and 50 GT maps....|
|||...Data from 45 subjects shows that on average they preferred the saliency maps chosen by our measure over the saliency maps chosen by the AP, AUC and Fbw....|
|||...We encourage the saliency community to consider this measure in future model evaluations and comparisons....|
|||...Mit saliency benchmark (2015)....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Salientshape: group saliency in image collections....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Bayesian saliency via low  and mid level cues....|
||29 instances in total. (in iccv2017)|
|114|Zhao_Fixation_Bank_Learning_2015_CVPR_paper|...Biologically-inspired saliency models decompose visual stimuli into feature maps across multiple scales, and then integrate different feature channels, e.g., in a linear, MAX, or MAP....|
|||...Our final saliency map is the weighted sum of all blobs....|
|||...nd 15,793 + 4,505 test frames), our model slightly but significantly outperforms 7 state-of-the-art saliency models....|
|||...Typical computational saliency models employ the following paradigm: (1) compute individual activation maps in several feature channels, (2) combine activation maps into a master saliency map [14, 8, 33]....|
|||...racted, and they are then integrated linearly [14], in a MAX [17], or MAP [29] manner into a master saliency map....|
|||...Several computational saliency models learn from biological or behavioral data....|
|||...Zhao and Koch [33] used adaboost to learn visual saliency by taking into account feature selection, weight assignments, and integration in a unified framework....|
|||...All these works learn saliency in a pixel-wise fashion and ignore neighboring-pixel dependencies....|
|||...A more recent work [24] studied saliency in dynamic scenes and proposed to use Gaussian blobs on the GBVS [8] saliency map as fixation candidates....|
|||...Widely used image saliency data sets include Bruce and Tsotsos[3], Kootstra et al[18] and Judd et al[15], which contain 120, 101 and 1003 images respectively....|
|||...(3) we model saliency in units of blobs instead  of pixels, which is more semantically sensible....|
|||...ch Gaussian blob on feature maps of a test frame, to calculate its contributing weight to the final saliency map, we first solve a group lasso problem and then define its weight as a function of recon...|
|||...Data sets  We use two public available dynamic saliency data sets: DIEM [20] and CRCNS [11]....|
|||...Evaluation Metrics  We utilized three universally used saliency metrics: Area Under ROC Curve (AUC), Normalized scanpath salience (NSS) [22] and 2 distances....|
|||...AUC measures the reliability that a saliency model can predict locations of interest....|
|||...Comparison with other Saliency Models  We compared our algorithm against three static image saliency models, which rank at the top on previous benchmarks [2], including: Adaptive Whitening Saliency mo...|
|||...ed on the raw frames with resolution 640  480 under their default parameter settings, and estimated saliency maps are down-sampled into resolution 20  15 for convenience of comparison....|
|||...In Fig.7, we visually show saliency results by different algorithms on DIEM data set....|
|||...Our algorithm automatically reweights each blob, making one or two stand out while suppressing others, and finally generating a peaky saliency map....|
|||...Other static saliency algorithms or dynamic ones (Hou) tend to produce a spreaded saliency map, making them less suitable for fixation prediction in dynamic videos....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Visual saliency does not account for eye movements during visual search in real-world scenes....|
|||...Learning visual saliency by combining feature maps in a nonlinear manner using adaboost....|
|||...Feature-specific interactions in salience from combined feature contrasts: Evidence for a bottomup saliency map in v1....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||26 instances in total. (in cvpr2015)|
|115|Instance-Level Salient Object Segmentation|...e1  Liang Lin1  Yizhou Yu2   1Sun Yat-sen University  2The University of Hong Kong  Abstract  Image saliency detection has recently witnessed rapid progress due to deep convolutional neural networks....|
|||...In this paper, we present a salient instance segmentation method that produces a saliency mask with distinct object instance labels for an input image....|
|||...Our method consists of three steps, estimating saliency map, detecting salient object contours and identifying salient object instances....|
|||...For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours....|
|||...a dense saliency map, but are unaware of individual instances of salient objects....|
|||...1) Estimating binary saliency map....|
|||...A number of recent papers have explored the use of fully convolutional neural networks for saliency mask generation [30, 33, 45]....|
|||...Most of these methods infer saliency by learning contrast from the internal multi-layer structure of a single VGG network [45, 33]....|
|||...Given the aforementioned sub-tasks of salient instance segmentation, we propose a deep multiscale saliency refinement network, which can generate very accurate results for both salient region detectio...|
|||...MSRNet can not only integrate bottom-up and top-down information for saliency inference but also attentionally determine the pixel-level weight of each salient map by looking at different scaled versi...|
|||...Salient Region Detection  Traditional saliency detection can be divided into bottom-up methods based on low-level features [34, 39, 10] and top-down methods incorporating high-level knowledge [18, 31, 22]....|
|||...Deep CNN based methods can be divided into two categories, segmentation or patch based methods [29, 45, 52] and endto-end saliency inference methods [30, 33, 45]....|
|||...To overcome this deficiency, deep end-to-end networks [30, 33, 45] have been developed for saliency inference....|
|||...We simultaneously learn attentional weights along with saliency maps by adding an attention module to our MSRNet....|
|||...However, as MSRNet is a fully convolutional network, it can take an image of any size as the input and produce a saliency map with the same resolution as the input during testing....|
|||...It is a more meaningful measure in evaluating the applicability of a saliency model in salient instance segmentation....|
|||...In the supplemental materials, we also report the average precision, recall and F-measure using an adaptive threshold which is set to twice the mean saliency value of each saliency map as suggested in [1]....|
|||...Visual comparison of saliency maps from state-of-the-art methods, including our MSRNet....|
|||...MSRNet consistently produces saliency maps closest to the ground truth....|
|||...It is worth noting that MSRNet outperforms all the other six deep learning based saliency detection methods without resorting to any post-processing techniques such as CRF....|
|||...The most important component of our framework is a multiscale saliency refinement network, which generates highquality salient region masks and salient object contours....|
|||...Boosting bottom-up and top-down visual features  for saliency estimation....|
|||...Visual saliency based on multiscale deep  features....|
|||...Dhsnet: Deep hierarchical saliency network for salient object detection....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierarchical saliency detec tion....|
||26 instances in total. (in cvpr2017)|
|116|Recasens_Following_Gaze_in_ICCV_2017_paper|...like in (a), and detecting what object the character is looking at can not be addressed by previous saliency and gaze following models....|
|||...Related Work  We describe the related works in the areas of gazefollowing in both videos and images, deep learning for geometry prediction and saliency below....|
|||...The saliency pathway (top left) finds salient spots on the target view....|
|||...Saliency: Although related, gaze-following and freeviewing saliency refer to different problems....|
|||...In gazefollowing, we predict the location of the gaze of an observer in the scene, while in saliency we predict the fixations of an external observer free-viewing the image....|
|||...Some authors have used gaze to improve saliency prediction, such as in [25]....|
|||...Furthermore, [2] showed how gaze prediction can improve state-of-the-art saliency models....|
|||...Pathways  We estimate the parameters of the saliency map S, the  cone C, and the transformation T using CNNs....|
|||...Saliency Pathway: The saliency pathway uses the target frame xt to generate a spatial map S(xt)....|
|||...The saliency pathway only has access to the target frame xt, which is insufficient to solve the full problem....|
|||...Saliency: The output heatmap is the saliency prediction for xt....|
|||...[23] is used to compute the saliency map....|
|||...The output point is computed as the mode of the saliency output distribution....|
|||...Image only: The saliency pathway is used to generate the output....|
|||...The saliency map shows the output of the saliency pathway....|
|||...This value is higher if the saliency map is more concentrated, which could indicate the presence of a salient object....|
|||...Our model outperforms saliency and static gaze-following in all the similarity range for all the metrics....|
|||...our method, a static gaze-following method [26], a state-ofthe-art saliency method [23] and humans....|
|||...We outperform both static gaze-following and saliency in all the similarity ranges, showing that our model is doing more than just performing this two tasks combined....|
|||...Similarity analysis  How different is our method to a saliency model and to the gaze model on a single image?...|
|||...Where should saliency models look next?...|
|||...A dataset and evaluation methodology for visual saliency in video....|
|||...Predicting primary gaze  behavior using social saliency fields....|
|||...Augmented saliency model using automatic 3d head pose detection and learned gaze following in natural scenes....|
|||...Social saliency prediction....|
||25 instances in total. (in iccv2017)|
|117|Li_The_Secrets_of_2014_CVPR_paper|...Introduction  Bottom-up visual saliency refers to the ability to select important visual information for further processing....|
|||...Unlike other topics such as object detection/recognition, saliency is not a well-defined term....|
|||...In a fixation experiment, saliency is expressed as eye gaze....|
|||...By combining existing fixation-based saliency models with segmentation techniques, our model bridges the gap between fixation prediction and salient object segmentation....|
|||...Fixation prediction  The problem of fixation based bottom-up saliency is first introduced to computer vision community by [17]....|
|||...The goal of this type of models is to compute a saliency map that simulates the eye movement behaviors of human....|
|||...To quantitatively evaluate the performance of different fixation algorithms, ROC Area Under the Curve (AUC) is often used to compare a saliency map against human eye fixations....|
|||...Therefore, there is no need to generate a pixel-accurate saliency map to match human data....|
|||...In fact, as pointed out in [15], blurring a saliency map can often increase its AUC score....|
|||...In fact, [2] used saliency maps as a main feature for predicting objectness....|
|||...eground segmentations generated by CPMC. One fundamental difference between these methods to visual saliency is that an object detector is often exhaustive  it looks for all objects in the image irres...|
|||...The final saliency value of each segment is the total number of click it receives, divided by the number of subjects....|
|||...For salient object segmentation task, the test/ground-truth saliency maps are binary maps obtained by first averaging the individual segmentations from the test/ground-truth subset, and then threshold...|
|||...of a saliency map [14, 16, 26]....|
|||...Global color contrast: The term saliency is also related to the global contrast of the foreground and background....|
|||...To discount the influence of centerbias, we add a fixed Gaussian ( = 0.4 of the image width) to the saliency maps generated by all fixation algorithms, and then benchmark these algorithms on all 3 sal...|
|||...To estimate the saliency of a candidate segment, we utilize the spatial distribution of fixations within the object....|
|||...It is well known that the density of fixation directly reveals the saliency of the segment....|
|||...The non-uniform spatial distribution of fixations on the object also offers useful cues to determine the saliency of an object....|
|||...We use random regression forest to predict the saliency score of an object mask....|
|||...As our saliency scores are defined over image segments, this simple strategy leads to fairly good object boundaries....|
|||...With a perfect segmenter, this model can accurately estimate the saliency of segments using fixation and shape information....|
|||...a study of human explicit saliency judgment....|
|||...Predicting human gaze using low-level saliency combined with face detection....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||25 instances in total. (in cvpr2014)|
|118|Deng-Ping_Fan_Salient_Objects_in_ECCV_2018_paper|...Then, we propose a new high quality dataset and update the previous saliency benchmark....|
|||...Keywords: Salient object detection  Saliency benchmark  Dataset  Attribute  1  Introduction  This paper considers the task of salient object detection (SOD)....|
|||...[23] (MDF) proposed to use multi-scale features extracted from a deep CNNs to derive a saliency map....|
|||...The DISC [8] framework was proposed for fine-grained image saliency computing....|
|||...IMC [48] integrated saliency cues at different levels through FCN....|
|||...Then it adaptively learned to combine these feature maps at each resolution and predicted saliency maps with the combined features....|
|||... DS [25] model set up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation, which shared the information in FCN layers to ge...|
|||...Then, they used FIN fine-tuned with iterative CRF to enforce spatial label consistency to predict the saliency map....|
|||...Center bias has been identified as one of the most significant biases of saliency detection datasets [3, 20, 26]....|
|||...The region similarity evaluation measure does not consider the true negative saliency assignments....|
|||...MDF [23] and AMU [49] use edge cues to promote the saliency map but fail to achieve the ideal goal....|
|||...It creates a multi-scale saliency refinement network that results in the highest performance (Sall)....|
|||...25] & AMU [49] made use of the multi-scale features in the down-sample progress to generate a fused saliency map; UCF [50] proposed an uncertain learning mechanism to learn uncertain convolutional fea...|
|||...All these methods try to get saliency maps containing both global and local features....|
|||...Chen, T., Lin, L., Liu, L., Luo, X., Li, X.: DISC: Deep image saliency computing via progressive representation learning....|
|||...: Salientshape: group saliency in  image collections....|
|||...Gayoung, L., Yu-Wing, T., Junmo, K.: Deep Saliency with Encoded Low level  Distance Map and High Level Features....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...Liu, N., Han, J.: DHSNet: Deep Hierarchical Saliency Network for Salient Object  Detection....|
|||...: Deep networks for saliency detection via  local estimation and global search....|
|||...Wang, L., Wang, L., Lu, H., Zhang, P., Ruan, X.: Saliency detection with recurrent  fully convolutional networks....|
|||...Yan, Q., Xu, L., Shi, J., Jia, J.: Hierarchical saliency detection....|
|||...: Saliency detection via graph based manifold ranking....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning Uncertain Convolutional  Features for Accurate Saliency Detection....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep  learning....|
||25 instances in total. (in eccv2018)|
|119|Lee_Deep_Saliency_With_CVPR_2016_paper|...  KAIST  gylee1103@gmail.com  yuwing@gmail.com  junmo.kim@kaist.ac.kr  Abstract  Recent advances in saliency detection have utilized deep learning to obtain high level features to detect salient regio...|
|||...These advances have demonstrated superior results over previous works that utilize hand-crafted low level features for saliency detection....|
|||...We concatenate the encoded low level distance map and the high level features, and connect them to a fully connected neural network classifier to evaluate the saliency of a query region....|
|||...Our experiments show that our method can further improve the performance of stateof-the-art deep learning-based saliency detection methods....|
|||...by the  (a)  (b)  (c)  (d)  (e)  (f)  Figure 1: (a) Input images, (b) Ground truth masks, (c) Fuzzy saliency masks from VGG16 features (HF setting, described in Section 3.3), (d-f) Results of (d) MDF ...|
|||...level features, several recent works [27, 16, 21] have demonstrated state-of-the-art performance in saliency detection that significantly outperform previous works that utilized only low level feature...|
|||...ndaries, and high level features from the output of the last layer are too coarse spatially for the saliency detection task....|
|||...To generate a precise saliency mask, previous studies utilized various methods including object proposal [27] and superpixel classification [16, 21]....|
|||...Using our new feature vector, we can precisely estimate saliency of superpixels....|
|||...Without any post-processing, this method generates an accurate saliency map with precise boundaries....|
|||...Deep learning has emerged in the field of saliency detection last year....|
|||...Several methods that utilize deep learnings for saliency detection were simultaneously proposed....|
|||...MDF and MCDL utilize superpixel algorithms, and query each region individually to assign saliency to superpixels....|
|||...LEGS first generates an initial rough saliency mask from deep CNN and refines the saliency map using an object proposal algorithm....|
|||...ural network classifier, that seamlessly considers both high level and low level features to assign saliency to query superpixels....|
|||...At the end of this section, we report the results of our self evaluations to analyze the effects of the ELD-map and the high level features in our saliency detection framework....|
|||...Afterwards, two fully-connected layers with 1024 nodes generate a saliency score for the queried region using the concatenated features....|
|||...ELD-map has two main advantages: (1) it can easily generate the fine-grained dense saliency mask, and (2) it provides additional low level feature distances, which can be hard to learn for CNN, such a...|
|||...The overlapping-based evaluations give higher score to methods which assign high saliency score to salient pixel correctly....|
|||...the estimated saliency map and G is the ground truth binary mask....|
|||...Salientshape: group saliency in image collections....|
|||...Visual saliency based on multiscale deep features....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierarchical saliency detection....|
||24 instances in total. (in cvpr2016)|
|120|Bruce_A_Deeper_Look_CVPR_2016_paper|...The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models....|
|||...Many domains of computer vision have benefitted from deep learning, and it is natural to consider that value of deep learning models in saliency prediction....|
|||...One contribution of this paper is a deep learning model for visual saliency prediction based on fully convolutional networks [23]....|
|||...ion that is included reveals a number of issues important to any work involving the intersection of saliency with deep learning, and remaining challenges and possible paths forward are summarized in s...|
|||...This is inspired in part by the apparent benefits of leveraging semantics in the context of saliency prediction....|
|||...Evaluation  The problem of evaluating saliency models is challenging in itself which has contributed to fragmentation among benchmarks that are used....|
|||...We have compared our output with several saliency and  517  ITTI [18], AIM [5], segmentation algorithms including: GBVS [15], DVA [17], SUN [37], SIG [16], AWS [13], FT [1], GC [10], SF [27], and PCAS [24]....|
|||... points [5], and blurring of the fixation map is often used to generate human fixation data derived saliency map that can be treated as a classifier....|
|||...The problem of visu alizing saliency map output appropriately can be a challenge (e.g....|
|||...Pixels covered by the superimposed heatmap correspond to the top 20% of output values in each saliency map, with colormap values mapped linearly to the equalized heatmap values....|
|||...Evaluation corresponds to the shuffled AUC score that is common in saliency evaluation....|
|||...Much of the emphasis of this paper is on saliency as it relates to gaze prediction....|
|||...There has been much debate about the relative importance of objects and saliency defined by feature contrast....|
|||...Some studies have claimed that objects better predict gaze patterns than low level saliency models [11], and others have sought to rebut this claim [3]....|
|||...A whole-object centric model (SAL-FCN in particular) performs comparable to some of the better classic contrast based saliency algorithms....|
|||...Examples for which contrast is important to determining saliency, or contrast based saliency is in conflict with semantics....|
|||...Shown are the source image, output of SALICON, FUCOS (80N), and a recent contrast driven saliency algorithm....|
|||...It is likely that highly flexible models of visual saliency computation are possible with a much simpler network in implementing similar computation alongside semantically driven selection....|
|||...It is clear that deep learning models may be highly capable in visual saliency prediction....|
|||...Feature contrast as a general phenomenon is one central component to saliency driven fixation behavior that may be poorly captured by deep learning models....|
|||...This is a consideration that may also have important implications for all deep learning models, including those that target problems outside of saliency or gaze modeling....|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Salicon: Saliency in context....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||24 instances in total. (in cvpr2016)|
|121|Deep Future Gaze_ Gaze Anticipation on Egocentric Videos Using Adversarial Networks|...Saliency Prediction  Computational saliency models are based on featureintegration theory [35] where low-level features, such as color, contrast and intensity, are combined....|
|||...Subsequent works [13, 41, 7, 14] further improve saliency map predictions via various methods such as graph-based saliency model [13] and boolean map based saliency [40]....|
|||...The most recent saliency models leverage rich pools of semantic regions or objects in the scene from deep convolutional neural network [17, 26], whereas they focus on saliency prediction on static ima...|
|||...In [33], the contextual information from the scene was integrated with low-level features for saliency prediction....|
|||...N) [27, 36] based model, to generate future frames and then to predict their corresponding temporal saliency maps, i.e., spatial probabilistic maps of gaze locations across time where the spatial coor...|
|||...In GN, there are two modules: Future Frame Generation Module (G) and Temporal Saliency Prediction Module (GP)....|
|||...uent frames It+1,t+N from a latent representation h(It) of the current frame It in G and N temporal saliency maps St+1,t+N from It+1,t+N in GP....|
|||...Thus, G is followed by GP to generate temporal saliency maps of dimension N  1  W  H.  3.3....|
|||...4374  Generator  Future Frame Generation Module (G)    2D ConvNet  h  3D ConvNet  3D ConvNet  (cid:1832)    (cid:1833)(cid:4666)h,(cid:4667)  Temporal Saliency  Prediction Module (GP)  3D ConvNet  Di...|
|||...Based on the generated frames, Temporal Saliency Prediction Module predicts the anticipated gaze location (red dots)....|
|||...Meanwhile, GP takes It+1,t+N as input to generate temporal saliency maps....|
|||...; wD), 1)  +Lce(D(G(h; wG)), 0),  (2)  where Pi is the temporal fixation map and Qi is the temporal saliency map for the (t + i)th frame....|
|||...It measures the area under a curve of true positive versus false positive rates under various threshold values on saliency maps....|
|||..., we use G to generate future frames after the training phase and compare DFG with state-of-the-art saliency prediction algorithms on these frames including Graph-based Visual Saliency (GBVS) [13], Na...|
|||...Second, SALICON [17] is a deep learning architecture for saliency prediction on static images....|
|||...Row 6 shows the corresponding predicted temporal saliency maps....|
|||...Results on Current Frame Gaze Prediction  We compare DFG with state-of-the-art saliency prediction algorithms in Section 4.3 on real frames in the testsets of three datasets and report both AAE and AU...|
|||...In DFG, GP is attached after G for temporal saliency map prediction using end-to-end training....|
|||...Secondly, we develop a new model (SalFusion) which averages the temporal saliency maps from both SalDirect and DFG to generate the final temporal saliency maps....|
|||...Visualization  As GP estimates temporal saliency maps based on the generated frames, we analyze the learnt convolution filters in GP and align the observations with human bottom-up visual attention me...|
|||...Analysis of scores, datasets, and models in visual saliency prediction....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep  neural networks....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||23 instances in total. (in cvpr2017)|
|122|What Is and What Is Not a Salient Object_ Learning Salient Object Detector by Ensembling Linear Exemplar Regressors|...In other words, several label ambiguity may inevitably arise when different subjects manually annotate the saliency objects in such images....|
|||...Furthermore, we derive a groundtruth saliency of a proposal O  OI as  G(O) =  1   O  XpO  G(p),  (6)  I and O  where p is a pixel in O....|
|||...Ensembling for Salient Object Detection  Given all  linear exemplar regressors, a proposal O in a testing image gains  I  saliency scores, denoted as {I(vO) I  I}....|
|||...However, the scores of each linear exemplar regressor may fall in different dynamic ranges so that their direct fusion will lead to inaccurate saliency maps (see Fig....|
|||...In computing F-measure curves, the precision and recall are first computed by binarizing the saliency maps with a threshold sliding from 0 to 255 and compare the binary maps with groundtruth maps....|
|||...Besides, we report adaptive F-measure (FM) using an adaptive threshold for generating a binary saliency map....|
|||...The adaptive threshold is computed as twice the mean of a saliency map....|
|||...In addition, MAE is calculated as the average absolute per-pixel difference between the grayscale saliency maps and the ground-truth saliency maps....|
|||...(a) Image, (b) groundtruth, (c) direct fusion by computing the maximum saliency value, (d) direct fusion by computing the mean saliency value, (e) enhanced fusion by computing the mean saliency value ...|
|||...Finally, we sum up the enhanced saliency scores to derive the saliency at a pixel p:  Sal(p) =  1   O  XOO  (p  O)  XII  f (I(vO)),  (9)  where (p  O) is an indicator function which equals to 1 if p  ...|
|||...After that, we normalize the pixelwise saliency into the dynamic range of [0, 1] and adopt the exponential operations proposed in [42] to enhance the contrast of saliency maps, followed by a morpholog...|
|||...ind that the weighted F-measure of using the max and mean value of raw exemplar scores as the final saliency value of a proposal is 0.540 and 0.588, while the weighted F-measure of using enhancement-b...|
|||...Actually, saliency is a relative concept and with the training data from only one image we can simply infer how to separate specific salient objects from specific non-salient ones....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Salientshape: Group saliency in image collections....|
|||...Deep saliency with encoded low level distance map and high level features....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...DHSNet: Deep hierarchical saliency net work for salient object detection....|
|||...Image saliency by isocen [14] C. Gong, D. Tao, W. Liu, S. J. Maybank, M. Fang, K. Fu, and J. Yang....|
|||...Recurrent attentional net works for saliency detection....|
|||...Visual saliency based on multiscale deep  features....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||23 instances in total. (in cvpr2017)|
|123|Ruohan_Zhang_AGIL_Learning_Attention_ECCV_2018_paper|...ttom-up vs. top-down Previous work in computer vision has formalized visual attention modeling as a saliency prediction problem where saliency is derived from image statistics, such as intensity, colo...|
|||...Many saliency datasets collect human eye tracking data in a free-viewing manner due to their task-free nature [19]....|
|||...tion and control, and show that a learned attention model can predict visual attention much better than bottom-up saliency models....|
|||...In contrast, saliency approaches in general prefer a differentiable (or soft) attention model that could be trained more efficiently....|
|||...4 Gaze Network  Computer vision research has formalized visual attention modeling as an endto-end saliency prediction problem, whereby a deep network can be used to predict a probability distribution ...|
|||...The bottom channel includes bottom-up saliency map computed by the classic Itti-Koch model [17]....|
|||...The output of the network is a gaze saliency map trained with Kullback-Leibler divergence as the loss function:  KL(P, Q) = X  i  Qi log (cid:16)o +  Qi  o + Pi(cid:17)  (1)  where P denotes the predi...|
|||...The final output is a gaze saliency map that indicates the predicted probability distribution of the gaze....|
|||...For a performance comparison we use the classic bottom-up saliency model [17] as the first baseline (Saliency(S) in Table 1)....|
|||...The performance of the algorithms are evaluated using four standard metrics in the visual saliency literature [34]: Normalized Scanpath Saliency (NSS), Area Under the Curve (AUC), Kullback-Leibler div...|
|||...The heatmap shows the models prediction as a saliency map, computed using the Image+Motion gaze network....|
|||...Including bottom-up saliency into the model does not improve the performance overall....|
|||...3 where the predicted gaze saliency map and ground truth human gaze positions are overlayed on top of the game frames....|
|||...We treat the predicted gaze heatmap as a saliency mask and multiply the mask with image frame element-wise....|
|||...The top channel takes in the current image frame and the bottom channel takes in the masked image which is an element-wise product of the original image and predicted gaze saliency map by the gaze network....|
|||...This suggests that popular deep saliency models could be used to learn visual attention, given task-driven data....|
|||...Bylinskii, Z., Judd, T., Oliva, A., Torralba, A., Durand, F.: What do different evaluation metrics tell us about saliency models?...|
|||...Bylinskii, Z., Recasens, A., Borji, A., Oliva, A., Torralba, A., Durand, F.: Where should saliency models look next?...|
|||...Jiang, M., Huang, S., Duan, J., Zhao, Q.: Salicon: Saliency in context....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...Marat, S., Phuoc, T.H., Granjon, L., Guyader, N., Pellerin, D., Gu erin-Dugu e, A.: Modelling spatio-temporal saliency to predict gaze direction for short videos....|
|||...Riche, N., Duvinage, M., Mancas, M., Gosselin, B., Dutoit, T.: Saliency and human fixations: State-of-the-art and study of comparison metrics....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep learning....|
||23 instances in total. (in eccv2018)|
|124|Instance-Aware Image and Sentence Matching With Selective Multimodal LSTM|...For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity....|
|||...In particular, the attention scheme first predicts pairwise instance-aware saliency maps for the image and sentence, and then combines saliencyweighted representations of candidates to represent the a...|
|||...ching, which uses a multimodal context-modulated attention scheme to jointly predict instance-aware saliency maps for both image and sentence....|
|||...e  the  road  and  grass  ...  ...  CNN  MLP  (a) Instance candidate extraction  (b) Instance-aware saliency map prediction  (c) Similarity measurement and aggregation  Figure 2....|
|||...Details of the proposed sm-LSTM, including (a) instance candidate extraction, (b) instance-aware saliency map prediction, and (c) similarity measurement and aggregation (best viewed in colors)....|
|||...ing three aspects: (a) instance candidate extraction for both image and sentence, (b) instanceaware saliency map prediction with a multimodal contextmodulated attention scheme, and (c) local similarit...|
|||...Therefore, we have to evaluate the instance-aware saliency of each instance candidate, with the aim to highlight those important and ignore those irrelevant....|
|||...Instance(cid:173)aware Saliency Map Prediction  Apparently, neither the split words nor evenly divided regions can precisely describe the desired sentence or image instances....|
|||...s goal, we propose a multimodal contextmodulated attention scheme to predict pairwise instanceaware saliency maps for image and sentence....|
|||...Based on these variables, we can perform instance-aware saliency map prediction at the  2312  t-th timestep as follows:  pt,i = e pt,i /XI qt,j = eqt,j /XJ  i=1  j=1  e pt,i , pt,i = fp(m, ai, ht1), ...|
|||...The local representations describe all the divided regions independently and are used to compute the initial saliency map....|
|||...Otherwise (e.g., selecting the dog), regions in the initial saliency map corresponding to the instance will be highlighted....|
|||...Note that in this equation, the information in initial saliency map is additively modulated by the global context and subtractively modulated by the previous context, to finally produce the instance-a...|
|||...lts from the reason that without global context, the attention scheme can only refer to the initial saliency map to select which instance to attend to next, but the initial saliency map is computed fr...|
|||...s of element-wise multiplication between each local representation (e.g., ai) and its corresponding saliency value (e.g., pt,i):  t = XI a  i=1  pt,iai, w  t = XJ  j=1  qt,j wj  (3)  where instance ca...|
|||... of image and sentence at different timesteps, we visualize the predicted sequential instance-aware saliency maps by sm-LSTM, as shown in Figure 4....|
|||...In particular for image, we resize the predicted saliency values at the t-th timestep {pt,i} to the same  2316  (a) Input image  (b) Without global context (by sm-LSTM-att)  (c) With global context (...|
|||...We then perform element-wise multiplication between the resized saliency map and the original image to obtain the final saliency map, where lighter areas indicate attended instances....|
|||...While for sentence, since different sentences have various lengths, we simply present two selected words at each timestep corresponding to the top-2 highest saliency values {qt,j}....|
|||...Without the aid of global context, sm-LSTMatt cannot produce accurate dynamical saliency maps as those of sm-LSTM....|
|||...In Figure 6, we also compute the averaged saliency maps (rescaled to the same size of 500500) for all the test images at three different timesteps by sm-LSTM....|
|||...Averaged saliency maps at three different timesteps....|
|||...An effective regional saliency model based on extended site entropy rate....|
||23 instances in total. (in cvpr2017)|
|125|Lu_Human_Action_Segmentation_2015_CVPR_paper|...Overview of our Method  We first propose a human motion saliency representation that is able to account for camera motion and balance human motion and human appearance cues automatically....|
|||...We compare our human motion saliency with an optical flow based camera motion estimation method [25] and actionness [5],  and find a +16% relative improvement in actionness ranking....|
|||...On this hierarchy, we define an MRF model, using our novel human motion saliency as the unary term....|
|||...Motion saliency and human saliency feature....|
|||...(b) Visualization of human saliency response....|
|||...(d) Visualization of our motion saliency response (Note that misclassified trajectories from (c) have low response)....|
|||...Human Motion Saliency for Human Action  Segmentation Our approach inputs a video clip containing human action and outputs a space-time segmentation that labels all the human-action pixels as foregroun...|
|||...Our human motion saliency incorporate two parts: foreground motion and human appearance information....|
|||...We hence combine these two schemes and propose a new motion saliency feature, we use the long term trajectories to build a camera motion model, and then measure the motion  saliency via the deviation ...|
|||... is the displaced frame difference at trajectory point p on frame t. We further reweight the motion saliency of all the trajectories by calculating the mean deviation through the clip....|
|||...appearance information, we use a DPM [7] person detector trained on PASCAL VOC 2007 and construct a saliency map by averaging the normalized detection score of all the scale and all components, as sho...|
|||...We further encode our foreground motion and human saliency in supervoxel as presented in Sec....|
|||...Unary potential: We encode the motion saliency and human saliency feature into supervoxels to get the unary potential components:  i(yi) = M Mi(yi) + P Pi(yi) + SSi(yi)  (4)  where M , P and S are wei...|
|||...The human saliency Pi(yi) can be formed as:  Pi(yi) =  1  exp( p xi   (6)  wP (pxj))  pxjxi  1  (cid:80)  where wP (pxj) is the human saliency weight for pixel j....|
|||...So we also compute the curvature at all boundary points of each supervoxel as the shape saliency feature....|
|||...(d) and (e) Supervoxel pairwise connections of motion saliency map and segmentation respectively, bold line represent strong connections....|
|||...ments  To fully evaluate our method, we report results on three tasks: first, with our human motion saliency feature, we  (a)(b)(c)(d)(e)evaluate actionness ranking and compare with state-of-theart m...|
|||...Evaluate Actionness  Following [5], we measure mean average precision (mAP) of actionness ranking of our motion saliency map and joint human motion saliency map, described in Sec....|
|||...which generate foreground motion saliency map with optical flow and RANSAC....|
|||...1, we can see our human motion saliency map shows a significant improvement of more than 10% mAP gain over state-of-the-art method [5]....|
|||...We make several contributions, including a strong human motion saliency feature and a novel higherorder potentials that connect different granularities of video segments, to empower accurate action se...|
||21 instances in total. (in cvpr2015)|
|126|Fangneng_Zhan_Verisimilar_Image_Synthesis_ECCV_2018_paper|...Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for ...|
|||...It exploits visual saliency to determine the embedding locations within each semantic coherent region as illustrated in Fig....|
|||...texts to be embedded into the background images as shown in the left-side box, a semantic map and a saliency map are first determined which are then combined to identify semantically sensible and apt ...|
|||...Given background images, the regions for text embedding can be determined by combining their Semantic Maps and Saliency Maps as illustrated in columns 3-4 in Fig....|
|||... in the semantic image segmentation research and the Saliency Maps can be determined using existing saliency models....|
|||...3.2 Saliency Guidance  Not every location within the semantically coherent objects or image regions are suitable for scene text embedding....|
|||...With such observations, we make use of visual saliency as a guidance to determine the exact scene text embedding locations....|
|||...In particular, homogeneous regions usually have lower saliency as compared with those highly contrasted and cluttered....|
|||...Scene texts can thus be place at locations that have low saliency within the semantically coherent objects or image regions as described in the last subsection....|
|||...3: Without saliency guidance (SG) as illustrated in (b), texts may be embedded across the object boundary as illustrated in (c) which are rarely spotted in scenes....|
|||...SG thus helps to embed texts at right locations within the semantically sensible regions as illustrated in (d)  Quite a number of saliency models have been reported in the literature [41]....|
|||...We adopt the saliency model in [29] due to its good capture of local and global contrast....|
|||...Given an image, the saliency model computes a saliency map as illustrated in Fig....|
|||...The locations that are suitable for text embedding can thus be determined by  Verisimilar Image Synthesis for Detection and Recognition of Texts  7  thresholding the computed saliency map....|
|||...3 shows, the saliency guidance helps to embed texts at right locations within the semantically sensible regions....|
|||...The use of saliency guidance further helps to improve the verisimilitude of the synthesized images as well as the learned visual representation of detection and recognition models....|
|||...used, Random means embedding texts at random locations, SC, SG and ATA refer to semantic coherence, saliency guidance, and adaptive text appearance....|
|||...We also perform ablation study of the three proposed image synthesis designs including semantic coherence (SC), saliency guidance (SG) and adaptive text appearance (ATA)....|
|||...5: Several sample images from our synthesis dataset that show how the proposed semantic coherence, saliency guidance and adaptive text appearance work together for verisimilar text embedding in scene ...|
|||...: Robust and efficient saliency modeling from image co-occurrence histograms....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||21 instances in total. (in eccv2018)|
|127|He_Delving_Into_Salient_ICCV_2017_paper|...ides strong guidance to salient object detection by reducing false positives and producing coherent saliency maps....|
|||...Meanwhile, it can also constrain the number of object-like regions to be detected in the saliency map....|
|||...In particular, we explore how different levels of shared information affect saliency detection performance....|
|||...[30] first apply a CNN to extract local patch features to obtain intermediate saliency results, and another CNN to globally integrate the initial saliency map with object proposals....|
|||...To address the high computational cost, the fully convolutional network (FCN) [21] and deconvolution network [23] are used to generate a saliency map in an endto-end framework....|
|||...As they obtain the final result through upsampling from a very coarse prediction, they cannot guarantee accurate segmentation of the saliency map....|
|||...Some latest methods [15, 31] aim to refine the resulting saliency map using recurrent networks....|
|||...Deep Neural Network with Weight Prediction  Given an input image I, the salient object detection network produces a saliency map m from a set of weights ....|
|||...The salient object detection is posed as a regression problem, and the saliency value of each pixel (x, y) in m can be described as:  used to detect salient objects for any input images....|
|||...The hierarchical supervision guides all the deconvolution layers with the ground truth, and these layers produce the side-output saliency maps....|
|||...The PR curve is computed by thresholding the predicted saliency map into a set of binary masks, and these masks are compared against the ground truth....|
|||...MAE measures the average pixel-wise error, reflecting the negative saliency assignments....|
|||...The proposed method produces saliency maps closest to the ground truth....|
|||...As the subitizing network produces different numbers of salient objects from those of the ground truth, the salient object network outputs different saliency maps....|
|||...The subitizing guidance may sometimes disagree with the ground truth saliency maps on the number of salient objects....|
|||...This shows that while subitizing can help improve the performance of saliency object detection, saliency object detection can also help improve the performance of subitizing....|
|||...Recurrent attentional net works for saliency detection....|
|||...Visual saliency based on multiscale deep  features....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierachical saliency detec tion....|
||20 instances in total. (in iccv2017)|
|128|Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper|...A purely saliency based approach would also fail: notice that there are salient objects in (b), an American flag, and (c), a mug, which can confound such an approach....|
|||...We tackle this problem by developing a novel generalized visual attention estimation method which jointly learns a subjectdependent saliency map and a 3D gaze vector represented by yaw and pitch....|
|||...le representation, a modified version of the GazeFollow dataset [23] to learn a gaze-relevant scene saliency representation, and the SynHead dataset [13] to complement the first two datasets as it inc...|
|||...Visual Saliency: The objective of visual saliency prediction is to estimate locations in an image which attract the attention of humans looking at the image....|
|||...[16] visual saliency prediction has been extensively studied....|
|||...Our work in generalized visual attention prediction is influenced by the task of visual saliency since people tend to look at salient objects inside a scene, yet it is distinct because we consider cas...|
|||...A method driven primarily by saliency detection would not succeed in the latter case....|
|||...Gorji and Clark [12] study a problem at the intersection of visual saliency and gaze following, which consists of incorporating signals from regions of an image which guide attention to a certain part...|
|||...For example, when subjects in an image look at an object, this amplifies the apparent saliency of the object....|
|||...Again, our problem differs in that we do not predict visual saliency but we predict the subjects gaze fixation and gaze direction....|
|||...e in terms of yaw and pitch degrees (where component of visual attention), 2. the subject-dependent saliency in terms of a heatmap (what component of visual attention), and 3. how likely the subject i...|
|||...3.2 Loss  As our model predicts gaze angle, saliency map and the fixation likelihood, we need to apply appropriate loss functions for each task....|
|||...Specifically, when learning gaze angle estimation, we only update the angle pathway (b) and (d) in Figure 2, when learning saliency we update the scene pathway (a), (b) and (c) while freezing all other layers....|
|||...4.1 Person-Dependent Saliency Prediction  We evaluate the performance of saliency map estimation using the suggested test split of the GazeFollow dataset....|
|||...As shown in the last three rows of Table 5, joint training of gaze and saliency is critical in solving the general attention estimation task because without the gaze angle estimate it is ineffective t...|
|||...a study of human  explicit saliency judgment....|
|||...Li, G., Yu, Y.: Visual saliency based on multiscale deep features....|
|||...Soo Park, H., Shi, J.: Social saliency prediction....|
|||...: Deep networks for saliency detection via local estimation and global search....|
|||...Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context deep learning....|
||20 instances in total. (in eccv2018)|
|129|cvpr18-Salience Guided Depth Calibration for Perceptually Optimized Compressive Light Field 3D Display|...ation for perceptually optimized compressive light field 3D display, including light field capture, saliency detection, depth initialization, layered decompression and perceptually optimized light fie...|
|||...Recently, several studies have developed learning methods in saliency analysis [53, 27, 21]....|
|||...[27] propose a saliency detection framework using dense and sparse coding representations as features, and integrate this framework via the Bayes formula....|
|||...Many saliency detection schemes exploit contrast cues, i.e., salience objects are expected to exhibit high foreground contrast within certain context [39]....|
|||...Preprocessing  The contrast-enhanced saliency is based on the light field color, depth and focus cues on the super-pixel [3]....|
|||...Benefiting from the focused foreground, it will provide a better performance for the final saliency map than an all-focus image....|
|||... we integrate focusness background cues, color contrast and depth contrast for generating the final saliency result....|
|||...To enhance the saliency contrast, a background probability P b on the focusness map F  is calculated through:  P b(i) = 1  exp(  Aval(pi)2  2   kC  Apos(pi)k2),  (7)  where Aval(pi) is the average val...|
|||...Here, we use the similar method to calculate the color contrast saliency MC and depth contrast saliency MD with different inputs, but the same processing unit:  MC(i, j) = kAcol(Pi)  Acol(Pj)kD(i, j),...|
|||...Combined Saliency Metric....|
|||...Finally, we incorporate background probability into the contrast enhanced saliency as follows:  M = MD + (1  )MC,  Mcom = M  Pb....|
|||...(11)  (12)  Here, M is from the focussness background cues and S is the weighted saliency based on color contrast saliency and depth contrast saliency....|
|||...Like the state-of-the-art work [52], we also applied saliency optimization algorithm [54] onto the contrasted enhanced saliency map as post-optimization:  Sopt = OP T (Mcom)....|
|||...Visual comparisons of different saliency detection algorithms vs. ours on a light field dataset....|
|||... [13], (c) configuration C [28], (d) optimized with proposed salience detection, (e) optimized with saliency ground truth, (f) the original captured image....|
|||...Visual saliency with staInternational journal of computer vision,  tistical priors....|
|||...A weighted sparse coding framework for saliency detection....|
|||...Image saliency by isocentric curvedness and color....|
|||...Geodesic saliency using background priors....|
|||...Hierarchical saliency detection....|
||20 instances in total. (in cvpr2018)|
|130|Liu_Semantically-Based_Human_Scanpath_2013_ICCV_paper|...Low-level feature saliency is formulated as transition probabilities between different image regions based on feature differences....|
|||...Low-level feature saliency has been the most widely adopted and investigated cue for visual attention, and is often modeled based on feature contrast....|
|||...Spatial position as well as low-level feature saliency are stimulus-driven rather  than interpretation-driven factors, and as such they both lie in the domain of bottom-up attention....|
|||...In [5], it was experimentally shown that discrete objects attract more attention and predict visual fixation much better than early saliency cues....|
|||...Related Work  There exist numerous works on visual attention that estimate saliency or the gaze distribution over an image....|
|||...In this section, we first review existing saliency calculation methods since saliency  3226 3233  reflects gaze allocation and represents part of the basis for gaze shifts....|
|||...Saliency Calculation  The family of contrast based methods occupies a major position in the field of saliency calculation, and is motivated by the biological aspect of attention....|
|||...The second important family of saliency methods is based on information pursuit and explores the psychological aspect of attention....|
|||...In contrast to these two methods, the Saliency Using Natural statistics method (SUN) [31] estimates information by obtaining the distributions of various features from natural image datasets and then ...|
|||...fed a saliency map into a neural network and employed the Winner Take All (WTA) and Inhibition of Return strategies [10], while Walthera and Koch identified proto-objects in the image and ranked them ...|
|||...These two methods generate scanpaths according to saliency but in fact what motivates gaze shifts is far more than that....|
|||..., color, orientation, and texture have been widely adopted and shown to be effective for estimating saliency [10][8]....|
|||...To achieve scale invariance with block based methods, low-level feature saliency needs to be calculated at multiple scales [10][15]....|
|||...Comparison with other methods  To our knowledge, Ittis saliency based method (Itti) [10], Waltheras proto-object based method (proto) [27], and Wangs scanpath simulation method (WW) [28] are the only ...|
|||...Others output only saliency maps....|
|||...7, from the results for proto, we can see that sorting regions according to saliency does not provide good estimates of scanpaths....|
|||...Probabilistic multi-task learning for visual saliency estimation in video....|
|||...An eye fixation database for saliency detection in images....|
|||...Top-down visual saliency via joint crf and  dictionary learning....|
|||...Learning a saliency map using fixated locations in natural scenes....|
||20 instances in total. (in iccv2013)|
|131|Dubey_What_Makes_an_ICCV_2015_paper|...community has made great strides in understanding comparable visual properties of the world such as saliency [19, 15, 16, 7, 4, 11, 12] and importance [3], but we still do not have a clear understandi...|
|||...(2) We uncover the relationship between visual saliency and object memorability and demonstrate those instances where visual saliency directly predicts object memorability and when/why it fails to do so....|
|||...ve been a few very recent studies that explore the connection between image memorability and visual saliency [8, 34, 26], our work is the first to explore the connection between object-level memorabil...|
|||...What is the role of saliency in memorability?...|
|||...When can visual saliency predict object memorability and what are the possible differences between the two?...|
|||...Studying the relationship between saliency and memorability is paramount for understanding object memorability in greater depth....|
|||...We find this correlation to be positive and considerably high ( = 0.71), suggesting that fixation count and visual saliency may drive object memorability considerably....|
|||...In summary, saliency is a surprisingly good predictor of object memorability in simple contexts where few objects exist in an image or when an object has few interesting points, but it is a much weake...|
|||...Figure 6: Memorability prediction by saliency in complex scenes....|
|||...to visual saliency have shown that saliency is heavily influenced by center bias [21, 40], primarily due to photographer bias (also evident in Figure 7 (left)) and viewing strategy [38]....|
|||...In the previous section, we explored the relationship between visual saliency and object memorability....|
|||...To this end, we included 8 state-ofthe-art saliency methods (top performing methods according to benchmarks in [5, 4]): GB [15], AIM [7], DV [16], IT [19], GC [9], PC [35], SF [36], and FT [1] to our ...|
|||...Figure 14 shows that the H+S baseline is outperformed by most saliency methods....|
|||...The deep-net baseline model, DL-MCG performs better than all other saliency methods with only PC ( = 0.38), SF ( = 0.37), and GB ( = 0.36) showing comparable performance....|
|||...e to photographer bias (see Section 3.2), which could be a reason for the high performance of these saliency methods....|
|||...We show that the category of an object has meaningful influence on its memorability, and that visual saliency can predict object memorability to some degree....|
|||...Accuracy of the baseline and saliency algorithms on proposed benchmark....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||19 instances in total. (in iccv2015)|
|132|Hadfield_Hollywood_3D_Recognizing_2013_CVPR_paper|...The saliency content of a sub-cuboid, with origin at (u, v, w) is defined in equation 13 as c(u, v, w) for a sub-cuboid of dimensions (u, v, w)....|
|||...The descriptor  of the saliency distribution at a position (u, v, w) can then be formed, by performing N comparisons of the content of two randomly offset spatio-temporal sub-cuboids, with origins at ...|
|||...location in the sequence, a histogram may be constructed, which encodes the occurrences of relative saliency distributions within the sequence, without requiring appearance data or motion estimation....|
|||...We propose extending the standard RMD described above, by storing the saliency measurements within a 4D integral hyper-volume, so as to encode the behavior of the interest point distribution across th...|
|||...The type of saliency measure used has a surprisingly large effect on the performance, with the average performance for the best scheme being roughly double that of the worst, even using the same featu...|
|||... against a similar depth background, or within a group of people, and the inclusion of depth in the saliency measure is less valu 3personal.ee.surrey.ac.uk/Personal/S.Hadfield/  hollywood3d  able....|
|||...The best performing descriptor for each saliency measure, is shown in bold....|
|||...The previously noted relationship between saliency measures, appears to hold regardless of the feature descriptor used....|
|||...It may have been reasonable to guess, that including structural features would prove more valuable with a standard saliency measure, as the depth information had not previously been exploited....|
|||...In fact the opposite proves to be true, 4D features provide more modest gains for 3D-S and 3D-Ha (up to 20%) than they do when combined with extended saliency measures (up to 45%)....|
|||...This demonstrates that depth aware saliency measures are capable of focusing computation, into regions where structural features are particularly valuable....|
|||...  Table 3: Correct Classification rate and Average Precision for each combination of descriptor and saliency measure....|
|||...The best feature for each saliency measure is shown in bold....|
|||...In general an arbitrary threshold is selected, indeed the experiments in the previous sections employed a saliency threshold based on those suggested in previous literature....|
|||...In figure 2 the relationship between the saliency threshold and the action recognition performance, is contrasted for 4D and 3.5D interest point detectors....|
|||...Regardless of the saliency measure, the standard features descriptor and their depth aware extensions follow the same trend....|
|||...higher saliency thresholds lead to increased accuracy....|
|||...In contrast, bag of words approaches provide greater accuracy for lower saliency thresholds....|
|||... 4D Harris  Figure 2: Average Precision on the Hollywood 3D action recognition dataset, for various saliency thresholds, with 3.5D and 4D interest point detection....|
||19 instances in total. (in cvpr2013)|
|133|Jiang_SALICON_Saliency_in_2015_CVPR_paper|...SALICON: Saliency in Context  Ming Jiang  Shengsheng Huang  Juanyong Duan  Qi Zhao  Department of Electrical and Computer Engineering, National University of Singapore  mjiang@nus.edu.sg  shane.huang@...|
|||...to advance the ultimate goal in visual understanding, and (2) understand visual attention and learn saliency models, all with human attentional data at a much larger scale....|
|||...We evaluated the use of the collected data in the context of saliency prediction, and demonstrated them a good source as ground truth for the evaluation of saliency algorithms....|
|||...Several eyetracking datasets have been recently constructed and shared in the community to understand visual attention and to build computational saliency models....|
|||...ges, there are also recent datasets in focused domains like the MIT Low Resolution dataset [17] for saliency in low resolution, EyeCrowd [16] for saliency in crowd, and FiWI [28] for web page saliency...|
|||...We envision that the collection of a larger-scale eye-tracking dataset would not only improve saliency prediction with big ground truth data, but driving new research directions in visual attention st...|
|||...With the large-scale data collection, we created a Saliency in Context (SALICON) dataset, with 10,000 MS COCO images viewed by 60 observers each....|
|||...We also included the highly referred and the state-of-the-art saliency algorithms in the comparison [15, 11, 37, 3, 9, 12, 36]....|
|||...n prediction performance with mouse tracking and the highly referred/state-of-the-art computational saliency models: eye tracking (EYE), mouse map in lab (MOUSE), mouse map on AMT (MOUSE-AMT) the Itti...|
|||...With the mouse maps from the aggregated AMT data, we computed the maximum object saliency as the maximum of the map values inside each objects outline, as it does not scale with the object size [8]....|
|||... number of total instances in the scene which has instances of the particular category, and average saliency value....|
|||...Average saliency values for each of the 80 object categories in SALICON....|
|||...itatively similar, we further exploited the mouse tracking as a benchmark to evaluate computational saliency algorithms....|
|||...Comparing the saliency algorithm performance on SALICON vs. on OSIE, similar patterns are observed too....|
|||...Evaluation of saliency algorithms against mousetracking data....|
|||...We also envision SALICON to be a good source for learning and benchmarking saliency algorithms with more data....|
|||...An eye fixation database for saliency detection in images....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
|||...Learning a saliency map using fixated  locations in natural scenes....|
||19 instances in total. (in cvpr2015)|
|134|Mathialagan_VIP_Finding_Important_2015_CVPR_paper|...A number of works [6, 11, 19] have studied visual saliency  identifying which parts of an image draw viewer attention....|
|||...[14] study visual saliency in group photographs and crowded scenes....|
|||...Their objective is to build a visual saliency model that takes into account the presence of faces in the image....|
|||...Although they study the same content as our work (group photographs), the goals of  the two are different  saliency vs importance....|
|||...At a high level, saliency is about what draws the viewers attention; importance is a higher-level concept about social roles....|
|||...In order to measure how well a saliency detector performs on the importance prediction task, we used the method of Harel et al....|
|||...[10, 12] to produce saliency maps and computed the fraction of saliency intensities inside each face as a measure of its importance....|
|||...At a high level, saliency studies what draws a viewers attention in an image....|
|||...Eye-gaze tracking systems are often used to track human eye fixations and estimate pixel-level saliency maps for an image....|
|||...Saliency is potentially different from importance because saliency is controlled by low-level human visual processing, while importance involves understanding more nuanced social and semantic cues....|
|||...We have already seen in Tables 3, 5 that saliency detectors perform worse than baselines in the image-level task and worse than our model in the corpus-level task respectively....|
|||...[14] to study saliency in group photos and crowded scenes....|
|||... and (e)(f) for Corpus-Level prediction  Figure 5: Examples showing the relationship between visual saliency and person importance  Salience  significantly-more  slightly-more about-same  more  38.33%...|
|||...This results in a ranking of people according to their saliency scores....|
|||...Table 8 shows the confusion matrix of saliency vs. importance, broken down over the three strength categories....|
|||...Saliency is not the same as importance, and saliency predictors cannot be used in the place of importance predictors....|
|||...A saliency implementation in matlab....|
||17 instances in total. (in cvpr2015)|
|135|Lai_Saliency_Guided_Dictionary_CVPR_2016_paper|...To deal with the challenges caused by the label ambiguities, we design a saliency guided weight assignment scheme to boost the discriminative dictionary learning....|
|||...More specifically, with a collection of tagged images, the proposed method first conducts saliency detection and automatically infers the confidence for each semantic class to be foreground or background....|
|||... We introduce a saliency prior to guide the learning of the weights....|
|||...Intuitively, a saliency map provides us with certain information about foreground and background, which helps to reduce the label ambiguities....|
|||...A linear programming constraint is further formulated for the saliency guided weight assignment....|
|||...Moreover, the procedure of saliency detection [41] often considers both local and global contexts within an image....|
|||...Saliency Prior  In this subsection, we introduce how a saliency prior is integrated to guide weight assignment....|
|||...When an image contains multiple classes like tree, grass, sky, building, the saliency map at least helps to distinguish between foreground and background classes....|
|||...))  S(Ap), S(Ap) > Ts f bs(L(Di), I(Ap)), B(Ap) = 1 c, otherwise  (9) in which S(Ap) is the average saliency value of superpixel Ap; B(Ap) indicates if the superpixel is on the image boundary or not; ...|
|||...In the sparse representation framework, we are not able to enforce this constraint directly on  3633  (a) Color Image  (b) Saliency Map  (c) Boundary Map  (d) Our Result  (e) Ground Truth  Figure 2....|
|||...We first investigate the performance of our saliency guided weight assignment scheme....|
|||...We take the full formulation in (13), which is named the Saliency Guided Dictionary Learning (SGDL) model, as a reference, and leave the smoothness prior and the saliency prior out step by step....|
|||...However, as shown in the last row, if the saliency order of regions do not match the estimated foreground-background scores, we may get wrong labeling results....|
|||...We also compare our models to the classical or state-ofthe-art techniques summarized in Table 1 if their results are  3635  (a) Color Image  (b) Saliency Map  (c) SGDL-Sm-Sal  (d) SGDL-Sm  (e) SGDL  ...|
|||...In this paper, we have presented a Saliency Guided Dictionary Learning (SGDL) method to conduct weaklysupervised image parsing....|
|||...The spectral dictionary clustering, the saliency prior, and the smoothness prior are integrated into our model to learning dictionaries, weights, and sparse representations at the same time....|
|||...Moreover, in the current model, some errors in saliency detection are unavoidably propagated to dictionary learning....|
||17 instances in total. (in cvpr2016)|
|136|Zhang_Minimum_Barrier_Salient_ICCV_2015_paper|...2  1Boston University  2Adobe Research  Input  SO  AMC  HS  SIA  HC  FT  Ours  GT  Figure 1: Sample saliency maps of several state-of-the-art methods (SO [39], AMC [15], HS [34] and SIA [6]) and metho...|
|||...Some sample saliency maps are shown in Fig....|
|||...Related Work  Previous works in saliency detection indicate that saliency emerges from several closely related concepts such as rarity, uniqueness and local/global contrast [17, 14, 4, 2]....|
|||...While saliency detection methods are often optimized for eye fixation prediction, salient object detection aims at uniformly highlighting the salient regions with well defined boundaries....|
|||...Some saliency detection methods use a diffusion-based formulation to propagate the saliency values [21, 23, 35]....|
|||...Speedup can also be obtained by down-sampling the input image before processing [33, 37], but this will significantly affect the quality of the saliency maps....|
|||...The corresponding resultant final saliency maps are shown in the last column....|
|||...As a result, the final saliency map (right top) using the MBD suppresses the central background area more effectively than using the geodesic distance (right bottom)....|
|||...Post(cid:173)processing  We describe a series of efficient post-processing operations to enhance the quality of the final saliency map S, given either S = B or S = B+....|
|||...Some sample saliency maps are shown in Fig....|
|||...Our methods MB and MB+ often give saliency maps with better visual quality than the other methods....|
|||...1410  Figure 8: Sample saliency maps of the compared methods....|
|||...Exploiting local and global patch rarities  for saliency detection....|
|||...Adaptive partial differential equation learning for visual saliency detection....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec saliency detection....|
|||...Random walks on  graphs to model saliency in images....|
||17 instances in total. (in iccv2015)|
|137|Dang_Adaptive_Exponential_Smoothing_ICCV_2015_paper|...Introduction  Despite the success of pixel prediction, e.g., saliency detection and parsing in individual images, its extension to video pixel prediction remains a challenging problem due to the spati...|
|||...We perform two different streaming video analytics tasks, i.e., online saliency map filtering and online multiclass scene parsing, to evaluate the performance....|
|||...Online Filtering of Saliency Map  In this experiment, we evaluate our method on saliency  map filtering using UCF 101 and SegTrack datasets....|
|||...To obtain the initial dense saliency map estimations, we use the phase discrepancy method [42] for UCF 101 dataset and the inside-outside map [27] for SegTrack dataset....|
|||...We use F-measure to evaluate the saliency map quality on both datasets....|
|||...Let Sd and Sg denote the detected pixel-wise saliency map and the ground truth, respectively, the F-measure can be computed as F-measure =  Original  30.6  Ave 30.6  ST-Ave  30.7  Exp 30.6  ST-Exp  31...|
|||...Original  Ave  ST-Ave  Exp  ST-Exp  [40] Ours  62.6 64.9 59.7 67.1 60.7 62.3 65.3  17.0 20.2 24.4 21.1 23.7 17.2 22.9  Birdfall Cheetah Girl Monkey Parachute Mean 46.2 47.1 48.5 48.0 48.5 46.7 50.3  3...|
|||...It is interesting to note that our method dilates the modes of the saliency maps as shown in the 6th column of Figure 6....|
|||...If a more compact map is preferred, we can take a further step to normalize the filtered saliency map to [0, 1] and multiply it with the original saliency map....|
|||...As shown in the 7th column of Figure 6, it can select the modes from the original saliency maps....|
|||...From the results, we can see that the proposed filtering method can significantly improve the saliency map quality and is better than the compared baseline methods especially for UCF 101 dataset....|
|||...We also find that the original saliency maps of UCF 101 are usually disrupted by false alarms while SegTrack maps contain a lot of miss detections....|
|||...It is mainly for object tracking and not suitable for saliency map evaluation....|
|||...Input  Frames  Original  Saliency   Map  Filtered  Saliency   Map  Figure 4: Qualitative results of saliency map filtering on SegTrack....|
|||...Bottom Row: filtered saliency maps by our filter....|
|||...3214  Input  Frame  Original  Saliency Map  ST-Ave  ST-Exp  Our   Filtering  Ours  Original  (a)  (b)  Figure 6: Qualitative results of saliency map filtering on UCF 101....|
|||...The experimental evaluations on saliency map filtering and multiclass scene parsing validate the superiority of the proposed method compared with the state of the art....|
||17 instances in total. (in iccv2015)|
|138|cvpr18-Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features|...To supplement nondiscriminative regions, saliency maps are then considered under Bayesian framework to refine the object regions....|
|||...ions may still only focus on key part regions of objects, to supplement non-discriminative regions, saliency-guided refinement method is proposed which considers both the expanded object regions and s...|
|||...ct with the trained RegionNet to get object regions O. if t == 0 then  Refine object regions O with saliency maps to get refined object regions OR  else  OR  O  3:  4:  5:  6:  7:  8:  9:  10:  end if...|
|||..., the segmented masks contain much more object regions and are more accurate, while the accuracy of saliency maps are also limited, so in the later iterations, the saliency maps are not used to preven...|
|||...Right: (a) initial object seeds, (b) object masks predicted by RegionNet, (c) saliency map, (d) refined object regions via Bayesian framework, (e) segmentation results of PixelNet....|
|||...To address this issue, we propose to supplement object regions by incorporating saliency maps for images with single object class....|
|||...Note that we do not directly use saliency map as initial localization as previous works [31], since in some cases, salient object may not be the object class we need in semantic segmentation, and the ...|
|||...We address this by proposing saliency-guided object region supplement method which considers both the mined object regions and saliency maps under Bayesian framework....|
|||...Based on these key parts, we aim to supplement object regions with saliency maps....|
|||...Our idea is, for a region with high saliency value, if its similar with the mined object objects, then it is more likely to be part of that object....|
|||...33, 27] as:  p(obj v) =  p(obj)p(v obj)  p(obj)p(v obj) + p(bg)p(v bg)  ,  (2)  where p(obj) is the saliency map, and p(bg) = 1  p(obj), p(v obj) and p(v bg) are the feature distribution at object reg...|
|||...In our work, we use saliency map of the DRFI method [11] as in [31]....|
|||...To supplement other object regions, saliency maps are incorporated with initial object seeds....|
|||...iterations w/o saliency refinement w/ saliency refinement  1  41.8 44.4  2  46.2 51.6  3  47.7 53.3  4  51.5 55.5  5  52.1 56.2  Table 4....|
|||...Without incorporating saliency maps, some object regions will be missing and thus the performance will be limited and can not reach satisfactory accuracy....|
|||...Bayesian saliency via low and mid level cues....|
||16 instances in total. (in cvpr2018)|
|139|cvpr18-Recurrent Saliency Transformation Network  Incorporating Multi-Stage Visual Cues for Small Organ Segmentation|...Recurrent Saliency Transformation Network:  Incorporating Multi-Stage Visual Cues for Small Organ Segmentation  Qihang Yu1, Lingxi Xie2( ), Yan Wang2, Yuyin Zhou2, Elliot K. Fishman3, Alan L. Yuille2 ...|
|||...This paper presents a Recurrent Saliency Transformation Network....|
|||...The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the curre...|
|||...Motivated to alleviate these shortcomings, we propose a Recurrent Saliency Transformation Network....|
|||...The chief innovation is to relate the coarse and fine stages with a saliency transformation module, which repeatedly transforms the segmentation probability map from previous iterations as spatial pri...|
|||...The coarse-to-fine idea was also well studied in the computer vision area for saliency detection [19] or semantic segmentation [21][24]....|
|||...This is improved by the proposed Recurrent Saliency Transformation Network, see Figure 5.  proposed to focus on a smaller input region according to an estimated bounding box....|
|||...This motivates us to connect these two stages with a saliency transformation module so as to jointly optimize their parameters....|
|||...Recurrent Saliency Transformation Network  We start with training 2D deep networks for 3D segmentation1....|
|||...Different options are evaluated, including using different kernel sizes in saliency transformation, and whether to fine-tune the models using the predicted segmentations as reference maps (see the des...|
|||...As the saliency transformation module is implemented  58284  Model  Stage-wise segmentation [46]  Using 3  3 kernels in saliency transformation (basic model) Using 1  1 kernels in saliency transforma...|
|||...That is to say, we use two 3  3 convolutional layers for saliency transformation, and fine-tune the models with coarse-scaled segmentation....|
|||...At the end of each iteration, predictions from three views are fused, and thus the saliency transformation module carries these information to the next iteration....|
|||...Visualization of how recurrent saliency transformation works in coarse-to-fine segmentation (best viewed in color)....|
|||...We present the Recurrent Saliency Transformation Network, which enjoys three advantages....|
|||...Recurrent Attentional Networks for Saliency Detection....|
||16 instances in total. (in cvpr2018)|
|140|Zhang_Picking_Deep_Filter_CVPR_2016_paper|...How 1134  Positives  Weak  Detectors  Pick Deep  Filters  Random Patches  Filter  Responses  Negatives  X  Swap Train  and Validation  Mining New  e Positives  l(x i; y i ; w t) =    i  t 1  i  Stron...|
|||...The part saliency map is used to weight  1135  each Fisher Vector and pool it into final image representation, which considers the importance of Fisher Vector itself....|
|||...To deal with these issues, instead of extracting FC-CNN within a tight rectangle, we propose to compute part saliency map and pool CNN features with Spatially Weighted Fisher Vector (SWFV-CNN)....|
|||...Part saliency map....|
|||...The part saliency map is used to indicate how likely a pixel belongs to a foreground part....|
|||...Our part saliency map consists of two sources, part map and saliency map....|
|||...The saliency map [13] is a topographically arranged map that represents visual saliency of a corresponding scene....|
|||...Each entry of uk and vk can be rewritten as follows:  u jk =  N  Xi=1  ui jk =  N  Xi=1  qik N k  z ji   jk   jk  Part Saliency Map  Summation  Saliency Map  SWFV  v jk =  N  Xi=1  vi jk =  N  Xi=1  q...|
|||...We first compute part saliency map with the top detections and saliency map....|
|||...The part saliency map assign weight to each descriptor, and SWFV-CNN is the weighted combination of each Fisher Vector....|
|||...the object of interest is always the most salient region, we choose saliency map S to measure the existence probability of foreground object....|
|||...The final part saliency map M is obtained as follows:  M(p) =  S(p)Pk  i=1 Di(p) Z  ,  (3)  where Di(p) = 1 when the ith detection contains the pixel p, otherwise Di(p) = 0....|
|||...To this end, we compute a saliency map S [13] of an image....|
|||...For each region proposal x generated with selective search [25], we compute the saliency score with  s(x   S ) = Pix S i/P S ....|
|||...The regions with saliency score above a threshold (set as 0.7 in our experiments, which expands the samples by approximately 20) are chosen as augmented samples....|
||15 instances in total. (in cvpr2016)|
|141|Tu_Real-Time_Salient_Object_CVPR_2016_paper|...As a preprocessing step, a desirable saliency detection algorithm should be computational efficient for practical usage....|
|||...Most saliency detection methods are  Correspondence author....|
|||...[28] compute the saliency value of a region according to its relevance to boundary patches by manifold ranking....|
|||...In [21], they propose a saliency optimization approach based on Cellular Automata....|
|||...2336  Figure 3: An overview of our MST-based saliency detection framework....|
|||...Quantitative and Qualitative Evaluations  We compute the precision and recall scores by binarizing the saliency maps with a threshold sliding from 0 to 255 and compare the binary maps with ground truth maps....|
|||...As neither precision nor recall measure consider the true negative saliency assignments, the mean absolute error (MAE) is also introduced as a complementary....|
|||...We compare our method with twelve classic or state-ofthe-art saliency detection algorithms....|
|||...The saliency maps of different methods are provided by authors or obtained from available software....|
|||...Figure 10 shows some sample saliency maps from three datasets for reference....|
|||...2340  Input  GT  Ours  RC  MB+  BL  SO  AMC  BMS  GC  HS  MR  GS  SF  FT  Figure 10: Comparison of our saliency maps with other classic or state-of-the-art methods....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
|||...Top-down visual saliency via joint  crf and dictionary learning....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
||15 instances in total. (in cvpr2016)|
|142|Deep 360 Pilot_ Learning a Deep Agent for Piloting Through 360deg Sports Videos|...Note that saliency detection methods do not select views directly, but output a saliency score map....|
|||...[55] proposed to use an object detector [4] to extract candidate objects of interest, then rank the saliency of these candidate objects....|
|||...For 360 piloting, we propose a similar baseline which first detects objects using RCNN [50], then select the viewing angle focusing on the most salient object according to a saliency detector [64]....|
|||...mber of methods, including the state-of-the-art method AUTOCAM [53], two baseline methods combining saliency detection with the object detector [50], and a variant of deep 360 pilot without a regresso...|
|||...Then, we use a simple motion saliency proposed by [15], median flow, and HoF [11] as features to train a gradient boosting classifier to select the box that is most likely to contain the main object....|
|||...RCNN+BMS: We leverage the saliency detector proposed by Zhang et al....|
|||...With the knowledge of saliency map, we can extract the max saliency scores in each box as a score....|
|||...A deep multi-level network for saliency prediction....|
|||...Temporal spectral residual: fast motion saliency detection....|
|||...End-to-end saliency mapping via probability distribution prediction....|
|||...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
|||...Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition....|
|||...Shallow and deep convolutional networks for saliency prediction....|
|||...Learning video saliency from human gaze using candidate selection....|
|||...Learning a combined model of visual saliency for fixation prediction....|
||15 instances in total. (in cvpr2017)|
|143|Zhang_Co-Saliency_Detection_via_2015_CVPR_paper|...Compared with traditional saliency detection  [3-9], co-saliency detection additionally explore the mutual  information  among  multiple  images/frames....|
|||...Inspired  by  the  traditional  saliency  detection  algorithms,  a  variety  of  hand-crafted  features,  such as the texture descriptors [16], color histogram [2, 17],  and Gabor filter [18], are ap...|
|||...In [16], Li and  Ngan combined conventional saliency detection methods to  calculate  the  single-image  saliency  and  applied  a  co-multilayer  graph  model  to  obtain  the  multi-image  saliency....|
|||...The  final  co-saliency  was  detected  by  linearly  fusing the single-image saliency and multi-image saliency....|
|||...[2]  generated  the  intra-image  saliency  map  and  the  inter-image  using  multi-scale  segmentation  voting  and  pairwise  similarity  ranking,  respectively....|
|||...They  proposed  to  measure  the  cluster-level  saliency  by  using three bottom-up saliency cues calculated based on the  raw pixel values....|
|||... low-rank decomposition to exploit the relationship  of  the  result  maps  of  multiple  existing  saliency  and  co-saliency approaches to obtain the self-adaptive weights,  and then used these weig...|
|||...Co-saliency map generation   In  order  to  capture  the  co-salient  objects  with  finely  defined boundaries, we need to convert the window-level  co-saliency  score  to  the  superpixel-level  sal...|
|||...osed  method for co-salient object segmentation, we reported the  performance  in  segmenting  the  saliency  map  using  a  self-adaptive  threshold  T = +  as  suggested  in  [34],  where   and   ar...|
|||...4], and2LR                                                              1CBCS-S is the single image saliency detection method in [18]....|
|||...[3],  where  the  first  three  methods  are  the  state-of-the-art  co-saliency detection  methods and the last three  methods  are  the  state-of-the-art  saliency  detection  methods....|
|||...As can be seen, both the state-of-the-art saliency detection  methods and co-saliency detection methods could not solve  the  problems  in  co-saliency  detection  satisfyingly....|
|||...Bayesian saliency via low  and  mid  level  cues....|
|||...An  object-oriented  visual  saliency  detection  framework  based  on sparse coding representations....|
|||...SUN:  A  Bayesian  framework  for  saliency  using  natural statistics....|
||15 instances in total. (in cvpr2015)|
|144|Maji_Part_Discovery_from_2013_CVPR_paper|...y of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing....|
|||...We show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing....|
|||...The test set (church-corr-test) is used to evaluate the utility of parts for predicting the location of the human-clicked landmarks, a semantic saliency prediction task described in Section 5....|
|||...Landmark saliency prediction  (cid:2)  A landmark saliency map is a function s(x, y)  x,y s(x, y) = 1, which is a likelihood that a lo[0, 1], cation of the image is a landmark....|
|||...We can evaluate the likelihood of a given set of ground truth landmark locations under the saliency map as a measure of its predictive quality....|
|||...:3)  k=1  1 n    (cid:3) (x,y)Sk     ms(x, y)   Sk   (2)  According to this definition, the uniform saliency map has MAL = 1 since s(x, y) = 1/m, x, y....|
|||...Our saliency detector uses the top 30 parts sorted according to their part detection accuracy on the training set....|
|||...Each detection contributes saliency proportional to the detection score to the center of the detection window....|
|||...The contributions are accumulated across all detections to obtain the initial saliency map....|
|||...This is then smoothed with a Gaussian with  = 0.01d, where d is the length of the image diagonal, and normalized to sum to one, to obtain the final saliency map....|
|||... scale-space interest point detectors based on Differences of Gaussians (DoG) and the Itti and Koch saliency model [12]....|
|||...According to our saliency maps, the landmarks are 6.4 more likely than the DoG saliency, and 4.2 more likely than the Itti and Koch saliency....|
|||...Figure 9 shows example saliency maps for a few images for a variety of methods....|
|||...Mean Average Likelihood (MAL) of landmarks according to various saliency maps....|
||14 instances in total. (in cvpr2013)|
|145|Gan_DevNet_A_Deep_2015_CVPR_paper|...Based on the intrinsic property of CNNs, we first generate a spatial-temporal saliency map by back passing through DevNet, which then can be used to find the key frames which are most indicative to th...|
|||...Next, we exploit the intrinsic property of CNNs to generate a spatial-temporal saliency map without resorting to additional training steps....|
|||...Gradient-based Spatial-temporal Saliency Map In this section, we extend a previous method [31] which generates class saliency maps for images to the video domain....|
|||...RGB) image, we take the maximum magnitude of wc across all color channels of each pixel as the saliency value....|
|||...Thus for each event class, we can derive a single class-specific saliency score for each pixel in the video....|
|||...The computation of saliency maps is extremely fast since it only requires a single backward pass without additional training....|
|||...After obtaining the spatial-temporal saliency map, we average the saliency scores of all the pixels within a key frame to obtain a key-frame level saliency score, and then we rank the key-frame level ...|
|||...For the top ranked key frames, we use the saliency scores as guidance and apply the graph-cut algorithm [1] to segment the spatial salient regions....|
|||...Spatial-temporal saliency map....|
|||...Given the event label we are interested in, we perform a backward pass based on the DevNet model to assign to each pixel in the testing video a saliency score....|
|||...For each key frame, we compute the average of the saliency scores of all pixels and use it as the key-frame level saliency score....|
|||...09 0.2820 0.1979 0.4331 0.3735 0.1314 0.1115 0.5584 0.3929 0.3339 0.2982 0.4186 0.3329 0.3699  tial saliency maps of the selected key frames for initialization and apply graph-cut [1] to segment the d...|
|||...From left to right are top one temporal key evidence, spatial saliency map, and spatial key evidence....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||14 instances in total. (in cvpr2015)|
|146|Fong_Interpretable_Explanations_of_ICCV_2017_paper|...In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks look in an image for evidence for their predictions....|
|||...Third, we reinterpret network saliency in our framework....|
|||...We show that this provides a natural generalization of the gradient-based saliency technique of [15] by integrating information over several rounds of backpropagation in order to learn an explanation....|
|||... Model-Agnostic Explanation (LIME) framework [12] is relevant to our local explanation paradigm and saliency method (sections 3.2, 4) in that both use an functions output with respect to inputs from a...|
|||...Comparison with other saliency methods....|
|||...l image with ground truth bounding box, learned mask subtracted from 1 (our method), gradient-based saliency [15], guided backprop [16, 8], contrastive excitation backprop [20], Grad-CAM [14], and occ...|
|||...Gradient saliency maps of [15]....|
|||...box f such as a neural network, this problem is reduced but not eliminated, and can explain why the saliency map S1 is rather diffuse, with strong responses even where no obvious information can be fo...|
|||...For a linear classifier f , this results in the saliency S2(x0) = w  x0, which is large for pixels for which x0 and w are large simultaneously....|
|||...The aim of saliency is to identify which regions of an image x0 are used by the black box to produce the output value f (x0)....|
|||...and its verifiable nature (i.e., direct edits to an image), allows us to compare differing proposed saliency explanations and demonstrate that our learned masks are better on this metric....|
|||...Given the output saliency map, we normalize its intensities to lie in the range [0, 1], threshold it with h  [0 : 0.1 : 1], and fit the tightest bounding box around the resulting heatmap....|
|||...We also present a novel image saliency paradigm that learns where an algorithm looks by discovering which parts of an image most affect its output score when perturbed....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||14 instances in total. (in iccv2017)|
|147|Zhu_Pedestrian_Detection_in_2014_CVPR_paper|...Basically, tensor voting provides local dimensionality of motion patterns and its saliency at a specific scale....|
|||...igenvalue difference as motion dimensionality d and (2) the maximal eigenvalue difference as motion saliency s. Since each group is estimated using a specific neighbor size, it is straightforward to d...|
|||...Intrinsic Motion Structure Discovery  In this section, we describe how our approach discover motion dimensionality and saliency of a pixel IMS in a 4D space (x, y, vx, vy) at a specific scale....|
|||...The motion dimensionality and saliency reveal the local motion structure of a moving object....|
|||...(i  i+1)  d = arg max  (2)  i  The motion saliency is the largest difference of two consecutive eigenvalues d  d+1....|
|||..., k}  Figure 4: This example demonstrates the change of the motion dimensionality (left panel) and saliency (right panel) with respect to scale ....|
|||...Suppose we normalize s between [0, 1] and choose k bins to quantize the normalized saliency value as  s, we can build a 2D look-up table to approximate the 2D distribution of intrinsic structure at this scale....|
|||...In our case, d can only be [1, 2, 3, 4] and we quantize the saliency into 16 bins....|
|||...The feature value at a certain scale  can be represented as  (3)  f (x, ) = (d,  s)  (4)  where di and si are the intrinsic dimensionality and its corresponding saliency at scale i....|
|||...from the previous and next 4 frames of ft. To compute the motion dimensionality and saliency at a ceri, we apply the tensor voting on a group tain pixel pt of pixels, which are the neighbors of pt i i...|
|||... MIMS: we compute motion dimensionality and saliency at scale  = (0.4, 0.6.., 4) and combine them by quantizing the saliency into 16 bins and dimensionality to 4 bins (maximal 4 dimensions)....|
|||...Features  HOG  Laplacian of Gaussian (LoG)  Frame-2-Frame Flow  Intrinsic Dimensionality (ID)  Structure Saliency (SS)  ID+SS (MIMS) Diffusion Map  Per-pixel LoG+MIMS Super-pixel LoG+MIMS  Coverage  F...|
|||...Our MIMS feature encodes the local structure of the motion pattern by computing the intrinsic dimensionality and saliency of the motion manifold at a number of scales ....|
||13 instances in total. (in cvpr2014)|
|148|Perra_Adaptive_Eye-Camera_Calibration_2015_CVPR_paper|...Much research has focussed on obtaining good saliency maps that well approximate the points in an image at which an average user is inclined to look....|
|||...n a probabilistic estimate of the users gaze and its parameters by calibrating the cameras from the saliency maps obtained from the above methods and combining it with the 3D eye model....|
|||...by combining 3D saliency with a dense reconstruction of the users environment for the purposes of user localization....|
|||...In recent years, many researchers have devised models to generate better-quality saliency maps....|
|||...Among these, Graph Based Visual Saliency (GBVS) [6], Adaptive Whitening Saliency (AWS) [12], and Image Signature (ImgSig) [7] usually exhibit the best performance....|
|||...This method also provides a computational model for saliency detection in images....|
|||...Our system does not have any knowledge of the 3D geometry of the users environment so it relies on a 2D saliency map of the scene in order to pick out these interest areas for calibration....|
|||...We  Figure 7: Input scene images (2 real environment, 1 simulation) and corresponding Saliency maps obtained by combination of GBVS and face detection....|
|||...Given a video stream of the users eye, U , and scene facing camera, S, the saliency map, Mj , can be found for frame Sj [25]....|
|||...c class of faces for the salient region detection, but this setup could easily be generalized using saliency maps [12, 7, 24], which would also impose similar constraints for the eye-camera transforma...|
|||...Dynamic saliency from adaptative whitening....|
|||...3d social saliency from  head-mounted cameras....|
|||...Visual recovery of saliency maps from human attention in 3d environments....|
||13 instances in total. (in cvpr2015)|
|149|Kim_Two-Phase_Learning_for_ICCV_2017_paper|...Enhanced heat maps are then used to improve the performance of per-class saliency detection and object localization as well as semantic segmentation....|
|||...The second group of methods employ external algorithms to obtain saliency masks or object proposals....|
|||...[38] construct a new dataset consisting of images with a well-centered single object, and then apply the state-of-theart saliency detection method proposed by Jiang et al....|
|||...They extract saliency masks from the network itself by fusing feature maps from conv4 and conv5 layers....|
|||...For the background class, as in [18], we imported the network implementation proposed in [30], which generates class-agnostic saliency detection based on the image gradient....|
|||...They require either additional data from Flickr and an external saliency detector pretrained by pixel-level supervision [38] or user clicks [28] or region proposals such as selective search and MCG [25]....|
|||...The segmentation network trained in our method cov 3538  Figure 3: Object saliency detections using the first network (column 2,5, and 8) and proposed method (column 3,6, and 9)....|
|||...Per-Class Saliency Prediction Experiments  In this section, we demonstrate that the two sets of heat maps obtained via two-phase learning can synergize each other to capture the complete object....|
|||...Here, we consider the heat maps as per-class saliency maps....|
|||...Accordingly, we investigate whether those saliency maps are consistent with the ground truth segmentation masks....|
|||...In practice, 2,148 pairs of a per-class heat map and the ground truth saliency mask are collected for 1,440 images in Pascal VOC 2012 val....|
|||...Experiments on semantic segmentation, object saliency detection, and object location prediction tasks have shown the effectiveness of our two-phase learning on the challenging Pascal VOC 2012 dataset....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||13 instances in total. (in iccv2017)|
|150|Huang_Automatic_Thumbnail_Generation_ICCV_2015_paper|...Related Work  To display a photograph in limited space, prior works typically highlight the image areas of greatest saliency [14] while removing parts of the photo that would command less attention....|
|||...For cropping, most algorithms are also driven by saliency, computed through a visual attention model [14], density of interest points [2], gaze distributions [27], correlations to images with manually...|
|||...Based on a saliency map, these methods compute a crop window that encloses regions of high saliency [3, 9, 23, 27, 31, 33]....|
|||...The recent work in [13] proposes the concept of context-aware saliency, which may assign high saliency values to background areas surrounding the foreground....|
|||...Incorporating context-aware saliency into these cropping works would address the visual representativeness issue only to some degree, and it would not deal with foreground recognizability at all....|
|||...)  255  where C denotes the area within the crop, and Si is a weight computed as the proportion of saliency [7] in region i with respect to the whole image....|
|||...The saliency weight puts greater emphasis on salient regions, whose color properties are more critical to preserve....|
|||...We model this feature by taking the ratio of summed saliency within the cropping window to the summed saliency over the whole photograph....|
|||...sq is the saliency value of pixel q. SIF T () represents the set of SIFT points detected in the domain . Ca,b(q) is the point in SIF T (a) which has the minimum coordinate distance from the correspond...|
|||...It can be seen that each of the features contributes to the overall performance, and that the saliency and texture features have a relatively larger impact....|
|||...Our method relies on techniques for saliency and foreground estimation....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
||12 instances in total. (in iccv2015)|
|151|Byrne_Nested_Motion_Descriptors_2015_CVPR_paper|...Motion Visualization  We can visualize motion representation of the NMD as a saliency map using the steerable pyramid reconstruction....|
|||...A saliency map is a real valued scalar field that encodes the salience of regions in an image or video....|
|||...The nested motion descriptor can be used to compute a saliency map in a very simple manner....|
|||...Finally, reconstruct the image from this saliency pyramid....|
|||...In short, a motion saliency map is the image reconstructed from the squared response of the nested motion descriptor....|
|||...This reconstructed image is a saliency map....|
|||...Finally, a saliency video is encoded from the set of saliency maps computed from the video, and rescaled so that the maximum saliency response is encoded as red....|
|||...We show a semitransparent saliency map for motion overlayed on each frame of video....|
|||...This saliency map shows salient responses in red and non-salient in blue....|
|||...(top right) Motion saliency for HMDB rock climbing....|
|||...The top row shows the output of the motion saliency using the NMD, and the bottom row shows the same output using the NMD without the log spiral normalization....|
|||...The colors encode the saliency map such that red is salient and blue is not-salient....|
||12 instances in total. (in cvpr2015)|
|152|Shi_Chen_Boosted_Attention_Leveraging_ECCV_2018_paper|...In all of our experiments, the stimulus-based attention is obtained from a pre-trained saliency prediction network and details about the network can be found in section 5....|
|||...us-based Attention Module mainly consists of three parts, a convolutional layer Wsal pre-trained on saliency prediction for producing the stimulus-based attention features (attentional CNN features, s...|
|||...ted Attention Captioning  7  4.1 Attentional CNN Features  Instead of using the final output of the saliency prediction network (i.e., the saliency map), we propose to make use of features from interm...|
|||...Considering a fully-convolutional saliency prediction network, we denote it as the equation below (for simplicity we only take the last two layers into considerations):  S = sof tmax(Wm (WsalI))  (1) ...|
|||...Specifically, with the use of ReLU activation ensures nonnegativity, to highlight salient regions in the saliency map Wsal needs to construct the correlations between filters and stimulus-based attention (i.e....|
|||...In order to integrate stimulus-based attention, we construct a saliency prediction network with 2 convolutional layers (note that features from the last convolutional layer of a ResNet-101 are viewed ...|
|||...The first convolutional layer has 2048 filters while the second layer projects the CNN features to spatial saliency map using a single filter....|
|||...The kernel size for both layers is set as 1 and the whole saliency network can be represented as Equation 1....|
|||...Weights from the first layer of saliency prediction network is utilized to initialize the stimulus-based attention module in the proposed method (i.e....|
|||...Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: Visual saliency for image captioning in new multimedia services....|
|||...Jiang, M., Huang, S., Duan, J., Zhao, Q.: Salicon: Saliency in context....|
|||...Li, J., Xia, C., Song, Y., Fang, S., Chen, X.: A data-driven metric for comprehensive evaluation of saliency models....|
||12 instances in total. (in eccv2018)|
|153|Fast-At_ Fast Automatic Thumbnail Generation Using Deep Neural Networks|...Unlike most previous work, Fast-AT does not utilize saliency but addresses the problem directly....|
|||...In addition, it eliminates the need to conduct region search on the saliency map....|
|||...two step solution where saliency is first computed and then an optimization problem is solved to find the optimum crop....|
|||...Typically, automatic thumbnail generators utilize a saliency map as an indicator of important regions in the image to be cropped [20, 3, 21, 2]....|
|||...Region search is then performed to find the smallest region of the image that has a total saliency above a certain threshold....|
|||...0], restriction of the search space to a set of fixed size rectangles [19], and binarization of the saliency map [3]....|
|||...However, saliency can ignore the semantics of the scene and does not take the target thumbnail size into account....|
|||...Many methods address this shortcoming through a heuristic approach such as selecting a crop that contains all detected faces [20] or using an algorithm that depends on both the saliency and image class....|
|||... against 4 methods:   Scale and Object-Aware Saliency (SOAT): In this method scale and object-aware saliency is computed and a greedy algorithm is used to conduct region search over the generated sali...|
|||...We run this method at a saliency threshold of 0.7....|
|||...Namely, the saliency based methods focus on a relatively small region having large saliency....|
|||...Conclusion  We presented a solution to the automatic thumbnail generation problem that does not depend on saliency or heuristic considerations but rather attacks the problem directly....|
||12 instances in total. (in cvpr2017)|
|154|Jiang_Learning_Visual_Attention_ICCV_2017_paper|... I  is a pixel-wise subtraction of the two  As shown in Figure 2a, our network architecture follows the design of the SALICON network [14], one of the stateof-the-arts in image saliency prediction....|
|||...We compare the Fisher score with classic saliency evaluation metrics: AUC [34], shuffled AUC (SAUC) [40], linear correlation coefficients (CC) [26], information gain (IG) [19, 20], Kullback-Leibler di...|
|||...However, it has some key advantages over conventional saliency measures....|
|||...The Fisher scores are computed using the gaze features introduced in Section 3.2, while the others are location or distribution based saliency metrics....|
|||...As shown in Figure 7, we observed an increased lowerlevel saliency but decreased social attention in ASD....|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Deep gaze i boosting saliency prediction with feature maps trained on imagenet....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Atypical visual saliency in autism spectrum disorder quantified through model-based eye tracking....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
||12 instances in total. (in iccv2017)|
|155|cvpr18-Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing|...[29] proposed a method based on CNN-based class-specific saliency maps and fullyconnected CRF....|
|||...[19] proposed using a saliency model as additional information to exploit object extent....|
|||...[4] also utilized adversarial erasing manner to allow the saliency detection network to discover new salient regions of object....|
|||...For localizing background, we utilize the saliency detection technology from [12], and simply  7016  Person  Table  Cat  Bottle  S  Seed  DSRG  G  Softmax  H  Seeding Loss  Downscale  CRF  CRF  Bound...|
|||...select the regions in normalized saliency maps whose pixels are with low saliency values as background....|
|||...We use saliency maps from [12] to produce the background localization cues....|
|||...We adopt the normalized saliency value 0.06 as the threshold to obtain background localization cues (i.e....|
|||...pixels whose saliency values are smaller than 0.06 are considered as background)....|
|||...DCSP also utilizes adversarial erasing manner to allow the saliency network to discover new salient regions of object....|
|||...Exploiting saliency for object segmentation from image level labels....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||11 instances in total. (in cvpr2018)|
|156|Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper|...acked-CNN [4], patched-CNN [24] and UnaryPairwise [19]), for shadow removal (DeshadowNet [33]), for saliency detection (SRM [34] and Amulet [35]), and for semantic segmentation (PSPNet [36])....|
|||...Saliency  Detection and Semantic Segmentation  Note that deep networks designed for shadow removal, saliency detection and semantic image segmentation can be re-trained for shadow detection by using a...|
|||...by comparing our method with a recent shadow removal model, i.e., DeshadowNet [33], two recent deep saliency detection models, i.e., SRM [34] and Amulet [35], and a recent semantic segmentation model,...|
|||...4.6 Saliency Detection  Our deep model has the potential to handle other vision tasks....|
|||...Here, we take the saliency detection as an example....|
|||...To evaluate the saliency detection performance of our deep model, we first re-trained our model on MSRA10k, which is a widely-used dataset for saliency object detection, and then tested the trained mo...|
|||...Table 4: Comparison with the state-of-the-art methods on saliency detection....|
|||...Table 4 shows the quantitative comparisons between our model and several state-of-the-art saliency detectors....|
|||...From the table, we can see that our model produces the best performance on almost all the four benchmarks in terms of F and MAE, showing that our model predicts more accurate saliency maps....|
|||...: R3Net: Recurrent  residual refinement network for saliency detection....|
|||...Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional  features for accurate saliency detection....|
||11 instances in total. (in eccv2018)|
|157|Spatially Adaptive Computation Time for Residual Networks|...Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions....|
|||...maps correlate well with  1043  the position of human eye fixations by evaluating them as a visual saliency model on the cat2000 dataset [4] without any training on this dataset....|
|||...Visual saliency (cat2000 dataset)  We now show that SACT ponder cost maps correlate well with human attention....|
|||...The ground-truth saliency map is a heat map of the eye fixation positions....|
|||...Cat2000 saliency maps exhibit a strong center bias....|
|||...Most images contain a blob of saliency in the center even when there is no object of interest located there....|
|||...SACT ponder cost maps work as a visual saliency model even without explicit supervision....|
|||...model is fully convolutional, we cannot learn such bias even if we trained on the saliency data....|
|||...Table 2 presents the AUC-Judd [5] metric, the area under the ROC-curve for the saliency map as a predictor for eye fixation positions....|
|||...This model is end Figure 11: cat2000 saliency dataset....|
|||...Mit saliency benchmark....|
||11 instances in total. (in cvpr2017)|
|158|Yan_Learning_the_Change_2013_CVPR_paper|...[21] determined crops based on the summed saliency values of candidate windows....|
|||...Large non-foreground regions with low average saliency values are taken to be background regions....|
|||...Distance of saliency map centroid and detected foreground region center from nearest rule-of-thirds point....|
|||...Distribution statistics of saliency map values: average,  standard deviation, skewness, and kurtosis....|
|||...The exclusion energy function used for selecting candidates is based on crop-out, cut-through, and saliency values:  Eexclusion = Ecropout+1Ecutthrough+2Esaliency (4)  with the terms formulated as (ci...|
|||...The saliency energy represents the proportion of the original images saliency that is excluded by the crop, with cropped as the sum of saliency values that are cropped out of the imS denoting the sum ...|
|||...It can be seen that attention-based methods tend to concentrate on areas with high saliency while disregarding the overall image composition....|
|||...An incorrect foreground detection result, which is generally caused by poor saliency map estimation, will lead to low cropping quality, such as shown in Figure 8....|
|||...A model of saliency based visuIEEE Trans....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...As our work relies on existing techniques for foreground detection and saliency map construction, shortcomings in these methods can degrade the quality of our crops....|
||11 instances in total. (in cvpr2013)|
|159|Li_Learning_to_Predict_2013_ICCV_paper|...Previous saliency models can be roughly categorized into either (1) bottom-up approaches [11] where the gaze is attracted by the discontinuities of low level features, such as color, contrast and edge...|
|||...In this work we explore a third alternative: We address the question of whether measurements of head and hand movements can be used to predict gaze, without reference to saliency or activity models....|
|||...Our gaze prediction results outperform all stateof-the-art bottom-up and top-down saliency detection algorithms by a large margin on two publicly available datasets....|
|||...Saliency Detection  Most of the bottom-up saliency models are based on the Feature Integration Theory [11]....|
|||...Several methods also model saliency in a probabilistic framework, where the rarity of the feature defines its saliency [2]....|
|||...A recent review of saliency detection can be found in [2]....|
|||...Very few computational methods have addressed the topdown saliency model....|
|||...[25] combined bottom-up visual saliency with ego-motion information for egocentric gaze prediction in a walking or sitting setting....|
|||...AUC measures the consistency between a predicted saliency map and the ground truth gaze points in an image, and is widely used in the saliency detection literature....|
|||...We compare our results with five competing methods: a baseline center prior prediction using 2D Gaussian, three bottom-up saliency detection algorithms (Itti and Koch [11], GBVS [8], Hou et al....|
|||...[10]) and one topdown saliency algorithm [7]....|
||11 instances in total. (in iccv2013)|
|160|Zhao_Deep_Region_and_CVPR_2016_paper|...Specifically, we adopt a saliency map [26] to visualize the regions selected by different models with and without a region layer....|
|||...The saliency map is image-specific, and computed as the  3394  @160x160x32conv1(2,2)@20x20x32(7,3)@20x20x32(1,1)(8,8)Region layer(2,2)(7,3)@160x160x32region2patchpatchReLUconv3x3x32@20X20x32BNReLUconv...|
|||...Visualization of AU-specific saliency maps for three networks: DRML (second row), ConvNet (third row), and AlexNet [14] high....|
|||...Colors on each map indicate saliency intensity from low  magnitude of per-pixel gradients with respect to a particular AU....|
|||...For AU2, DRML has a high saliency on the forehead and strong outer brows....|
|||...The presence with slight saliency on inner brows indicates its likely co-occurrence with AU1....|
|||...AlexNet fails to identify saliency for AU6 and AU7....|
|||...AlexNet shows saliency over the whole face for AU15 and AU17....|
|||...DRML identifies strong saliency around mouth, while ConvNet emphasizes on regions of both mouth and the upper face....|
|||...4, the saliency maps of DRML show better specificity than alternative models....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||11 instances in total. (in cvpr2016)|
|161|Rubinstein_Unsupervised_Joint_Object_2013_CVPR_paper|...Although in these works no generative models were used to learn the distribution of visual objects, reliable matching and saliency are found to be helpful for object discovery....|
|||...The saliency of a pixel or a region in an image can be defined in numerous ways and exten 1  2  3  4  5  1  2  3  4  5  e c r u o S  y c n e i l a S  d e p r a w    r o b h g i e N  g n i h c t a  M ...|
|||...For each dataset, the second row shows the saliency maps, colored from black (less salient) to white (more salient); the third row shows the correspondences between images, illustrated by warping the ...|
|||...In our experiments, we used an offthe-shelf saliency measureCheng et al.s Contrast-based Saliency [3]that produced sufficiently good saliency estimates for our purposes, but our formulation is not lim...|
|||...[3] define the saliency of a pixel based on its color contrast to other pixels in the image (how different it is from the other pixels)....|
|||...Given a saliency map, (cid:2)Mi, for each image Ii, we first compute the dataset-wide normalized saliency, Mi (with values in [0, 1]), and define the term  saliency (x) =  log Mi(x)....|
|||...We use the above saliency and matching terms to define the likelihood of a pixel label:  1940 1940 1942  n o i s i c e r P  1  0.8  0.6  0.4  0.2  0  e  bik  bird  ar c  e g a r e v A  at c  air h c ...|
|||..., where the latter effectively reduces the method to segmenting every image independently using its saliency map and spatial regularization (combined in a Grabcut-style iterative optimization)....|
|||...st of the images the foreground is quite easily separated from the background based on its relative saliency alone....|
|||...For airplane, saliency plays a more important role as the uniform skies always match best regardless of the transform....|
|||...We model the sparsity and saliency properties of the common object, and construct a large-scale graphical model to jointly infer a binary mask for each image....|
||11 instances in total. (in cvpr2013)|
|162|Fried_Finding_Distractors_In_2015_CVPR_paper|...Mechanical Turk Dataset (MTurk)  For this dataset we combined several previous datasets from the saliency literature [1, 15, 18] for a total of 1073 images....|
|||...Existing computational saliency methods are thus insufficient to define visual distractors, because the main subject in a photo where people look first usually has a high saliency value....|
|||... (7) Saliency prediction methods [12, 13, 20, 22, 25]....|
|||...We also report results for previous saliency methods as well as a simple adaptation to these methods where we multiply the saliency maps by an inverted Gaussian (as described in Sec....|
|||...This comparison is rather unfair since saliency methods try to predict all salient regions and not just distractors....|
|||...We compare against saliency prediction methods as published, and the same methods attenuated with an inverted Gaussian, as described in Section 4.2....|
|||...For this application we view the distractor map as a complement to a saliency map: Whereas saliency maps give information regarding areas we would like to keep in the output image, a distractor map gi...|
|||...However, we believe that a model which combines saliency and distractor maps will produce superior results....|
|||...Studying the effect of optimizing the image quality in saliency regions at the expense of background content....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...A framework for visual saliency detection with applications to image In International Conf....|
||11 instances in total. (in cvpr2015)|
|163|Contour-Constrained Superpixels for Image and Video Processing|...thms, including image segmentation [15], video object segmentation [9], semantic segmentation [12], saliency detection [21], and stereo matching [31]....|
|||...4] and temporal superpixel [1, 5, 20, 30] methods and can be applied to object segmentation [9] and saliency detection [14, 32] effectively....|
|||...Precision-recall curves of saliency detection techniques....|
|||...Specifically, we average the saliency values of the pixels in all frames, constituting each superpixel, and then replace those saliency values with the average value....|
|||...This simple processing improves the saliency detection performance, as shown by the precision-recall curves in Figure 8....|
|||...We test two saliency detection techniques, hierarchical saliency detection (HS) [32] and deep hierarchical saliency network (DHSNet) [14] on the NTT dataset [2]....|
|||...Experimental results showed that the proposed algorithm outperforms the state-of-the-art superpixel methods and can be applied to object segmentation and saliency detection effectively....|
|||...Second, we use contour-constrained temporal superpixels to postprocess video saliency detection results....|
|||...If we apply an image saliency detection technique to each frame in a video sequence independently, the resultant saliency  References  [1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk....|
|||...DHSNet: Deep hierarchical saliency network for salient object detection....|
|||...Hierarchical saliency detec tion....|
||11 instances in total. (in cvpr2017)|
|164|cvpr18-Instance Embedding Transfer to Unsupervised Video Object Segmentation|...The motion saliency cues are built upon optical flow....|
|||...The motion saliency as well as seed track construction and ranking are explained below....|
|||...Bottom: left the optical flow; center average flow within each region; right a map of motion saliency scores....|
|||...The motion saliency for each seed, s, is the normalized distance to the nearest background motion vector,  M (s) =  1 Z  min  vbVBG    vs  vb  2 2,  where the normalization factor Z is given by  Z = m...|
|||...(7)  (8)  There are other approaches to derive motion saliency from optical flow....|
|||...For example, in [22], motion edges are obtained by applying some edge detector to optical flow and then motion saliency of some region is computed as a function of motion edge intensity....|
|||...The motion saliency proposed in this work is more efficient and works well in terms of the final segmentation performance....|
|||...We score this by linking similar seeds together across frames into a seed track and taking the average product of objectness and motion saliency scores over each track....|
|||...We analyze three variants: motion saliency alone, objectness alone, and objectness+motion saliency....|
|||...We see that combining motion saliency and objectness performs the best, outperforming motion alone and objectness alone by 4.0% and 6.4%, respectively....|
||10 instances in total. (in cvpr2018)|
|165|Yang_PatchCut_Data-Driven_Object_2015_CVPR_paper|...esults on the PASCAL images [10] by integrating shape sensitive object proposals [7] with a classic saliency map [11]....|
|||...l segmentation, and ask 6 subjects to select the salient object regions by clicking on them, so the saliency value for each segment is defined as the number of clicks it receives divided by the number...|
|||...Differently from previous experiments, we initialize the PatchCut algorithm with the saliency maps generated by the GBVS algorithm [11], and its results are denoted as GBVS PatchCut soft, GBVS PatchCu...|
|||...We mainly compare with the state-of-theart algorithm, CPMC GBVS, presented in [22] which also uses the GBVS saliency maps....|
|||...We convert the ground truth segmentation saliency maps into binary masks with three thresholds: 0.1, 0.3, 0.5....|
|||...Larger threshold means that  Figure 9: Comparing soft segmentation results at different saliency levels in terms of precision-recall curves....|
|||...Table 4 shows that our algorithm (GBVS PatchCut) performs slightly better than CPMC GBVS, especially at low saliency levels....|
|||...less objects with higher saliency values are selected in the ground truth....|
|||...We compare the GBVS PatchCut soft results with CPMC GBVS and three recent saliency algorithms, SF [28], GC [8] and PCAS [25]) in Figure....|
|||...Our algorithm (GBVS PatchCut soft) performs favorably against CPMC GBVS and clearly above other saliency algorithms....|
||10 instances in total. (in cvpr2015)|
|166|Zhang_Unconstrained_Salient_Object_CVPR_2016_paper|...A key difference between salient object detection and object class detection is that saliency greatly depends on the surrounding context....|
|||...This intrinsic property of saliency detection makes our proposal filtering process challenging....|
|||...Given a saliency map, some methods find the best detection window based on heuristics [29, 48, 45, 32]....|
|||...Traditional salient region detection methods [1, 11, 41, 50, 25] cannot be fairly evaluated in our task, as they only generate saliency maps without individuating each object....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Random walks on  graphs to model saliency in images....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Existence detection of objects in images for robot vision using saliency histogram features....|
|||...Image saliency by isocen tric curvedness and color....|
|||...Hierarchical saliency detec tion....|
||10 instances in total. (in cvpr2016)|
|167|Zhang_Salient_Object_Subitizing_2015_CVPR_paper|...e number of salient objects (1, 2, 3, and 4+) on our dataset, without resorting to any intermediate saliency map computation or salient object detection....|
|||...In [45], saliency histogram features are exploited to detect the existence of interesting objects for robot vision....|
|||...We also evaluate a spatial pyramid feature of saliency maps, in the hope that saliency maps may provide information about the composition of the foreground....|
|||...We use a state-of-the-art salient object detection model [58] to compute a saliency map for an image, and compute a spatial pyramid of a 8  8 and a 16  16 grid....|
|||...We use the state-of-the-art salient detection method of [58], and binarize its saliency maps using Otsus method [39]....|
|||...The saliency map pyramid (SalPyr) feature achieves 0.36 AP in predicting the images that have two salient objects, outperforming all the other baselines except CNN....|
|||...In contrast, the model of [55] requires computing several saliency maps, which takes over 4 seconds per image as reported in [55]....|
|||...Random walks on graphs to model saliency in images....|
|||...Existence detection of objects in images for robot In IEEE Intervision using saliency histogram features....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||10 instances in total. (in cvpr2015)|
|168|Sultani_Human_Action_Recognition_2014_CVPR_paper|...We then propose a new process to obtain a measure of confidence in each pixel of the video being a foreground region, using motion, appearance, and saliency together in a 3D MRF based framework....|
|||...[26] used saliency to remove features from nonsalient regions, however, their method requires thresholding of saliency maps while each of the remaining features contribute equally to the final representation....|
|||...In 3, we propose methods for obtaining foreground-specific action representations, using motion, appearance, and saliency in a 3D MRF based framework....|
|||...Bottom(L-R): optical flow gradient magnitude fm(x, y); magnitude of color gradient in LAB space fc(x, y); and saliency fs(x, y), resp....|
|||...Saliency  HMDB51 biking  UCF50 basketball  We propose to use visual saliency as the third cue to estimate the confidence in observing a foreground pixel....|
|||...erimentally observed that, due to large camera motion and noisy optical flow, video or motion based saliency methods do not always result in reasonable outputs....|
|||...Instead, we used graph based visual saliency [8] to capture the salient regions in each frame individually....|
|||...An example of the final saliency based foreground confidence is shown in Fig....|
|||...Coherence of Foreground Confidence  Since saliency and color gradients are computed based on a single frame, they ignore the temporal information as well as coherency....|
|||...Space-variant descriptor sampling for action  recognition based on saliency and eye movements....|
||10 instances in total. (in cvpr2014)|
|169|Alnajar_Calibration-Free_Gaze_Estimation_2013_ICCV_paper|...Some recent studies focus on visual saliency information  1550-5499/13 $31.00  2013 IEEE DOI 10.1109/ICCV.2013.24  137  in images and videos to avoid applying active human calibration....|
|||...[4, 3] treat saliency maps extracted from videos as probability distributions for gaze points....|
|||...Chen and Ji [14] use 3D models of the eye and incrementally estimate the angle between the visual and the optical axes by combining the image saliency with the 3D model....|
|||...The reason behind the use of saliency is that people look at salient regions with higher probability than other regions....|
|||...However, as shown in [12], the computational saliency models do not frequently match the actual human saccades (Figure 1)....|
|||...Examples where saliency models do not match the human fixations....|
|||...The recent work of Chen and Ji [14] uses a single camera with multiple infrared lights to reconstruct the 3D eye model while using the saliency to estimate the angle between the visual and optical axes....|
|||...[3] adopt an appearance-based gaze estimator and use visual saliency for auto-calibration....|
|||... points of the subsequent subjects to gradually autocalibrate the gaze estimator and to combine the saliency information with the template gaze patterns....|
|||...Calibration-free gaze sensing using saliency maps....|
||10 instances in total. (in iccv2013)|
|170|Luo_Switchable_Deep_Network_2014_CVPR_paper|...At the feature levels, it automatically estimates saliency maps for each test sample in order to separate background clutters from discriminative regions for pedestrian detection....|
|||...The saliency maps that separate the background clutters and the discriminative regions are shown in (b)....|
|||...At each feature level, SRBM estimates saliency maps (indicating a pixel is on the background or a pedestrian) for each test sample....|
|||...In a part layer, the saliency map also helps to localize each part in the same way....|
|||...First, we propose a unified deep model to jointly learn features, saliency maps, and mixture representations of the whole body and different body parts in a hierarchy....|
|||...This layer depresses background clutters by estimating saliency maps and handles complex pedestrian appearance variations with mixture of part templates....|
|||...For each component, mk  [0, 1]nm is the saliency map representing the discriminative regions of the pedestrian....|
|||...Switchable RBM  One of the contributions of this study is to extend RBM by modeling the mixture and saliency maps using SRBM....|
|||... = 1 x, y, h, m) = k (x  mk) + yT Uhk},  k hk + ck)), k (Wk(x  mk) + bk)  + cT  1 Z  (8)  where the saliency map of the k-th component can be considered as the correlation between the original input x...|
|||...mk denotes the saliency map....|
||10 instances in total. (in cvpr2014)|
|171|Primary Object Segmentation in Videos Based on Region Augmentation and Reduction|...[30] estimate saliency maps using geodesic distances and delineate salient objects....|
|||...They perform random walk simulation on a non-locally connected graph for all frames, by employing a saliency map as the initial distribution of the walker....|
|||...However, these saliencydependent techniques [5, 9, 30] may face difficulties, when the saliency maps are inaccurate due to background clutters or background motions....|
|||...POD algorithms also use saliency maps [15, 34] or object proposals [10]....|
|||...[34] generate candidate boxes based on saliency scores in each frame, and then find the optimal path maximizing the sum of the saliency scores....|
|||...[34] employ six kinds of saliency maps to overcome the limitations of individual saliency cues....|
|||...and (t)  i  i  are the appearance confidence and the  i  To determine the appearance confidence (t)  , we obtain a saliency map for frame I (t) using the preprocessing technique in [9]....|
|||...Based on the boundary prior, [9] estimates the initial foreground distribution, which we regard as the saliency map....|
|||...We then compute (t) the saliency values within the candidate region q(t)  i Also, we determine the edge confidence (t) , based on  by averaging  ....|
|||...Compressed domain video saliency detection using global and local spatiotemporal features....|
||10 instances in total. (in cvpr2017)|
|172|Lee_Temporal_Superpixels_Based_ICCV_2017_paper|...Furthermore, we demonstrate that the proposed algorithm is applicable to video object segmentation [10, 11, 20] and saliency detection [27, 32]....|
|||...Precision-recall curves of two saliency detection techniques, HS [32] and GS [27]....|
|||...In this work, we apply the proposed algorithms to video object segmentation (VOS) and saliency detection....|
|||...Third, we also perform post-processing on saliency detection results....|
|||...If an image saliency detection technique is applied to each frame in a video independently, the resultant saliency maps are temporally incompatible....|
|||...We average the saliency values of pixels within each temporal superpixel in all frames and replace the original saliency values with the average value....|
|||...This post-processing is applied to two saliency detection techniques, hierarchical saliency detection (HS) [32] and geodesic saliency detection (GS) [27]....|
|||...The post-processing improves the saliency detection performances of both HS and GS considerably....|
|||...Geodesic saliency using  background priors....|
|||...Hierarchical saliency detec tion....|
||10 instances in total. (in iccv2017)|
|173|cvpr18-Fast End-to-End Trainable Guided Filter|...We show the results of image retouching, multiscale detail manipulation, non-local dehazing, saliency object detection, and depth estimation from a single image in each column....|
|||...Similar challenge also occurs in computer vision tasks, such as depth estimation, saliency detection and semantic segmentation....|
|||...anging from lowlevel vision to high level-vision, namely depth estimation from a single image [34], saliency object detection [32] and semantic segmentation [22]....|
|||...For computer vision tasks, we use MonoDepth [19] for depth estimation, while using DSS [23] for saliency detection and Deeplab [9] for segmentation....|
|||...ense prediction tasks, ranging from low-level vision to high-level vision, namely depth estimation, saliency detection and segmentation....|
|||...Similar results are also observed in saliency detection and semantic segmentation....|
|||...F (higher is better) increases from  90.61% to 91.29% for saliency detection by applying the guided filter layer....|
|||...We also compare with DenseCRF [28], which is a commonly used filter for saliency detection and semantic segmentation....|
|||...Experiments show that our method is comparable to DenseCRF in saliency detection, while obtains better performance in semantic segmentation....|
|||...Visual saliency based on multiscale  deep features....|
||10 instances in total. (in cvpr2018)|
|174|Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper|...More importantly, existing proposals for 3D keypoint detection rely on geometric saliency functions that attempt to maximize repeatability rather than distinctiveness of the selected regions, which ma...|
|||...The standard approach in 2D and 3D computer vision involves defining keypoints by maximization of a handcrafted local saliency function [25, 13, 24]....|
|||...However, when deploying such detectors, the saliency is unrelated to the quality of the description to be later computed at the point....|
|||...raining images are deployed to define the positive samples used to train a regressor which learns a saliency function able to highlight the same image points under drastic illumination changes caused ...|
|||...ors of [23] proposed to learn a detector from training data instead of hand-engineering a geometric saliency in order to cope with the high variability of structures selected as salient by human users...|
|||...[10] proposed to speed up and improve the repeatability of curvature-based detectors by learning the saliency function with a Regression Forest that uses, as features, binary depth comparisons....|
|||...e limited by the suitability to the specific dataset of the geometric structures highlighted by the saliency function employed by the selected detector....|
|||...hbors of p, to simulate the effect of spatial non-maxima suppression that is usually performed over saliency values to obtain keypoints as local extrema....|
|||...Sparse points matching by combining 3D mesh saliency with statistical descriptors....|
||9 instances in total. (in iccv2015)|
|175|Cheng_BING_Binarized_Normed_2014_CVPR_paper|...According to how saliency is defined, we broadly classify the related research into three categories: fixation prediction, salient object detection, and objectness proposal generation....|
|||...[36] proposed one of the first computational models for saliency detection, which estimates center-surrounded difference across multi-scale image features....|
|||...[41] combined local, regional, and global saliency measurements  in a CRF framework....|
|||...More recent research also tried to produce high quality saliency maps in a filtering based framework [46], using efficient data representation [12], or consider hierarchical structures [55]....|
|||...d on the same benchmark and shown to outperform other alternative proposal methods [6,21,25,30,50], saliency measures [33, 36], interesting point detectors [44], and HOG detector [17] (see [3] for the...|
|||...Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study....|
|||...Salientshape: Group saliency in image collections....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Hierarchical saliency detec tion....|
||9 instances in total. (in cvpr2014)|
|176|Wang_Deep_Cropping_via_ICCV_2017_paper|...Early attention models [16, 2] are typically based on various low-level features (e.g., color, intensity, orientation), operating and combining them at multiple scales to form a saliency map....|
|||...We also use AIC as a baseline, which is obtained via equipping crop window researching method [3] with top-performing saliency detection method....|
|||...apply context-aware saliency [11] and optimal parameters, as suggested by [3], for maximizing its performance....|
|||...Boosting bottom-up and top-down visual features  for saliency estimation....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
|||...Correspondence  driven saliency transfer....|
|||...Stereoscopic thumbIEEE  nail creation via efficient stereo saliency detection....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
||9 instances in total. (in iccv2017)|
|177|Kolkin_Training_Deep_Networks_ICCV_2017_paper|...she@adobe.com Adobe Research Seattle, WA, USA  Abstract  In many computer vision tasks, for example saliency prediction or semantic segmentation, the desired output is a foreground map that predicts p...|
|||...ecomes impor [17]  SZNCE (ours)  SZNF w 1 (ours)  GT  Input  Figure 1: A comparison of the previous saliency prediction state of the art with our SZN model predictions with the traditional log-loss (S...|
|||...Deep networks were used in [31] to learn local patch features to predict the saliency score at the center of the patch....|
|||...Additionally, training our model is three times faster than competitive saliency methods, making it much easier to scale to larger training sets....|
|||...We also use saliency to explore the effectiveness of the proposed objective function, compared to other reweighting schemes....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...Visual saliency detection based on multiscale deep cnn features....|
|||...Deep networks for saliency detection via local estimation and global search....|
|||...Hierarchical saliency detection....|
||9 instances in total. (in iccv2017)|
|178|Chandra_Dense_and_Low-Rank_ICCV_2017_paper|...results on three challenging benchmarks, namely semantic segmentation, human part segmentation, and saliency estimation....|
|||...4.2), and saliency estimation (Sec....|
|||...We show the pairwise terms the dot product Ap,p = Ap Ap,p for the segmentation task in (b), human part estimation in (c), and saliency estimation in (d)....|
|||...Saliency estimation results: we report the Maximal Fmeasure (MF) on the PASCAL Saliency dataset of [23], and the HKU-IS dataset of [21]....|
|||...The Pascal-S saliency dataset contains pixel-wise saliency for 850 images....|
|||...Our baselines for the saliency estimation task include (a) the Local Estimation and Global Search (LEGS) framework [36], (b) the multi-context network [39], (c) the multiscale deep features network [2...|
|||...aches on three challenging public benchmarks for semantic segmentation, human part segmentation and saliency estimation....|
|||...Visual saliency based on multiscale deep  features....|
|||...PISA: pixelwise image saliency by aggregating complementary appearance contrast measures with edge-preserving coherence....|
||9 instances in total. (in iccv2017)|
|179|Zhang_A_Self-Paced_Multiple-Instance_ICCV_2015_paper|...As one extension of the traditional saliency detection [2,  3],  co-saliency  detection  additionally  explores  the  global  information at the group level....|
|||...[19]  firstly  defined visual co-saliency as  the visual saliency of image  pixels or regions in the context of other images....|
|||...[17]  proposed  a  co-multilayer  graph  model  to  explore  the  multi-image  saliency  and  established  the  first  public co-saliency dataset....|
|||...[1]  firstly  defined  the  intra-image  saliency and inter-image saliency and then integrated them  to obtain the final co-saliency of the image group....|
|||...[23] used  rank  constraint  to  exploit  the  relationship  of  multiple  pre-designed  saliency  cues  and  the  self-adaptive weight to generate the final co-saliency map....|
|||...= 1  k  n k  i  = 1  k ( ) v i  ,   (2)   step  aims  to  update  the  classifiers  for  detecting  saliency  areas....|
|||...Consequently,  any  off-the-shelf  singe-image saliency detection approach can be adopted to  roughly  initialize  the  training  samples....|
|||...To  demonstrate  the  robustness  of  our  method,  we  used  some  state-of-the-art  single-image  saliency  methods  [21,  24,  37,  44,  45]  for  initialization  and  reported  the  performance  a...|
|||...Looking  beyond the image: unsupervised learning for object saliency  and detection....|
||9 instances in total. (in iccv2015)|
|180|Kim_Interpretable_Learning_for_ICCV_2017_paper|...) Coarse-grained decoder by visual attention mechanism, and (3) Fine-grained decoder: causal visual saliency detection and refinement of attention map....|
|||...Fine-Grained Decoder3Strideof 2Strideof 2Strideof 2Strideof 1Strideof 15555160804080362433204048331020643310206410201102064t+11020PreprocessingLSTMVisual Saliency detectionand causality checkClusterin...|
|||...To this end, we assess a decrease in performance when a local visual saliency on an input raw image is masked out....|
|||...We then apply a clustering algorithm to find a local visual saliency by grouping 3D particles P into clusters {C} (see Figure 2 (D))....|
|||...(C) We randomly sample 3D N = 500 particles over the attention map, and (D) we apply a density-based clustering algorithm (DBSCAN [10]) to find a local visual saliency by grouping particles into clusters....|
|||...(E, F) For each cluster c  C, we compute a convex hull H(c) to define its region, and mask out the visual saliency to see causal effects on prediction accuracy (see E, F for clusters 1 and 5, respectively)....|
|||...We compute a convex hull H(c) to find a local, and mask out each visual saliency by assigning zero values for all pixels lying inside each convex hull....|
|||...Each causal visual saliency is generated by warping into a fixed spatial resolution (=6464) as shown in Figure 2 (G, H)....|
|||...We observe our model can produce a simpler and more accurate map of visual saliency by filtering out spurious attention blobs....|
||9 instances in total. (in iccv2017)|
|181|Making 360deg Video Watchable in 2D_ Learning Videography for Click Free Viewing|...Compared to both the existing solution as well as strong baselines driven by saliency or center biases, our methods autozoom improves results by up to 43.4%....|
|||...Related Work  Video saliency Saliency studies visual content that attracts viewers attention [17, 20, 21, 30, 35, 43], where attention is usually measured by gaze fixations under free viewing settings....|
|||...Both video saliency and Pano2Vid try to predict spatial locations in videos....|
|||...Also, saliency usually depends on local image content whereas Pano2Vid depends on the content and composition of the entire FOV....|
|||... SALIENCY  replace the capture-worthiness scores in AUTOCAM by saliency scores.5 The saliency is computed by a popular method [17] over the 360 video frame in equirectangular projection....|
|||...The overlap is approximated by  5We also considered a saliency baseline that permits zoom like our  4http://vision.cs.utexas.edu/projects/Pano2Vid  method, but it fared worse than all others....|
|||...Although SALIENCY is content-dependent, it captures content that attracts gaze, which appears to be a poor proxy for the Pano2Vid task....|
|||...Because saliency scores are computed more densely than the capture-worthiness scores, the correlation of the scores between neighbor directions are stronger, and the algorithm can generate more trajec...|
|||...Learning video saliency from human gaze using candidate selection....|
||9 instances in total. (in cvpr2017)|
|182|Wang_Motionlets_Mid-level_3D_2013_CVPR_paper|...Specifically, we first estimate motion saliency using spatiotemporal orientation energies [1], and extract 3D regions with high motion saliency....|
|||...Low-level Features, left: the motion saliency of eight orientations; right: dense HOE and HOG features of three video clips from KTH, HMDB51 and UCF 50.  and adding the computational cost....|
|||...tation energies from the background and noise influence:  (4)  The resulting eight energies can be seen as measures of motion saliency along eight different orientations (Figure 2)....|
|||...Extraction of Motion Salient Regions  In the first step, we extract 3D video regions with high motion saliency as seeds for constructing motionlets....|
|||...or each volume , we use the summation of spatiotemporal orientation energies as a measure of motion saliency (See Left of Figure 4),  (cid:2)  (cid:2)  s() =  Ei(x)....|
|||...(6)  x  iAll{s,o}  Then we use a threshold  to convert motion saliency map into a binary one,  (cid:7)  B() =  1 0  if s() > ,  otherwis....|
|||...(7)  Empirically, we set  as 0.9 times of saliency maximum....|
|||...Finding Motionlet Candidates  The 3D regions generated from motion saliency serve as the seeds for constructing motionlet....|
|||...For computational cost, we extract motion saliency for about 30s and 3000 motionlets match for about 40s for each video on average on HMDB51 and UCF50 on a PC with E5645 CPU(2.4 GHZ) and 8G RAM....|
||9 instances in total. (in cvpr2013)|
|183|Li_Harvesting_Mid-level_Visual_2013_CVPR_paper|...ommon patterns from retrieved images, which have a high degree of relevance to the query words; (4) saliency detection [9] saliency detection helps to reduce the search space by finding potential cand...|
|||...ed learning problem into a weakly supervised learning approach; (2) a system is designed to utilize saliency detection to create bags of image patches, from which mixture concepts are learned; (3) con...|
|||...arning for the mid-level representations; [28] uses an iterative procedure, while our method adopts saliency detection, miSVM and Kmeans in a novel way; in addition, our method significantly outperfor...|
|||...In [37], saliency detection is utilized to create bags of image patches, but only one object is assumed in each image for the task of object discovery....|
|||...3 shows sample saliency detection results (the top 5 saliency windows for each image) by [9], a window based saliency detection method....|
|||...3, we observe that within the top 5 saliency windows, objects such as airplanes, birds, caterpillars, crosses, dogs, and horses are covered by the saliency windows....|
|||...This illustrates the benefit of the use of saliency detection: it helps to identify the regions and parts with more significance naturally....|
|||...Although the saliency assumption is reasonable, not all category images satisfy this assumption....|
|||...As the two codebooks are created without using the saliency assumption and the multiple instance learning framework, they serve as two good baselines....|
||9 instances in total. (in cvpr2013)|
|184|cvpr18-Learning Deep Sketch Abstraction|... our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of v...|
|||...e develop a computational model that learns to abstract concrete input sketches and estimate stroke saliency by finding the most compact subset of input strokes for which the sketch is still recogniza...|
|||...Using our abstraction model, we can address a number of problems: (1) Modeling sketch stroke saliency: We can estimate stroke saliency as a byproduct of learning to (2) Category-level produce brief re...|
|||...The agent is trained with reinforcement learning, and learns to estimate the saliency of each stroke in order to achieve its goal of compactly encoding a recognizable sketch....|
|||...d the basic reward by proposing a more elaborate reward computation, aiming to learn the underlying saliency of strokesegments by integrating the classification rank information at each time step t. T...|
|||...Measuring sketch stroke saliency Using Eq....|
|||...9, we can compute a saliency value S for each stroke in a sketch, indicating how it contributes towards the overall recognizability of the sketch....|
|||...Some example stroke saliency maps ob 8019  Figure 5: Examples of sketch abstraction and stroke saliency....|
|||...We observe that high saliency strokes correspond to the more distinctive visual characteristics of the object category....|
||9 instances in total. (in cvpr2018)|
|185|Chai_Symbiotic_Segmentation_and_2013_ICCV_paper|...and also on the powerful GrabCut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them....|
|||...With the introduction of a third (consistency) energy term EC that takes a pre-trained saliency model S we penalize the cases where the foreground segmentation f and the part locations p do not agree....|
|||...An example of a set of saliency maps is shown in the center of the bottom row of Fig....|
|||...This map is resized to the size of a saliency map st, which is denoted as t....|
|||...Updating part localizations p. When finding the best part localization p (given the DPM W, the saliency model S and the foreground segmentation f), EGC can be ignored and we are left with the original...|
|||...(x) denotes the binary foreground-background label at position x. n(pt, st) describes a real valued saliency map of the same size as the input image....|
|||...Learning the Model  The DPM model W and the saliency model S are trained using a set I of training images....|
|||...We learn the model progressively, starting with the HOG-filters and saliency mask corresponding to the root, and then proceeding to the parts....|
|||...Learning the saliency model S. Given the part localizations and the GrabCut segmentations of all training images, we set the saliency mask for each part to be the pixel-wise mean of all segmentation m...|
||9 instances in total. (in iccv2013)|
|186|Kohei_Uehara_Visual_Question_Generation_ECCV_2018_paper|...In order to solve this problem, we calculate the saliency of each proposed region in the image as a criterion for selecting the target region....|
|||...Thus, to select salient objects in the image, we propose using a saliency map....|
|||...The saliency map is a plot obtained by estimating the saliency for each pixel in  6  K. Uehara et al....|
|||...We calculate the saliency map using the method of Zhu et al....|
|||...This method estimates low saliency for background pixels and high saliency for foreground pixels....|
|||...First, we preprocessed the image by applying mask based on saliency map and applied non-maximum suppression to reduce the large number of object regions....|
|||...Then, the saliency of each proposed object region is expressed by:  Iregion = I(p)    I(p)   Ssalient Sregion  (2)  where, I(p) is the saliency value of each pixel,  is the threshold value, Ssalient i...|
|||...The region with the highest saliency is selected as the target region....|
|||...Zhu, W., Liang, S., Wei, Y., Sun, J.: Saliency Optimization from Robust Back ground Detection....|
||9 instances in total. (in eccv2018)|
|187|Choi_Visual_Tracking_Using_CVPR_2016_paper|...The scheme to estimate the attentional features in the AtCF is similar to saliency detection....|
|||...Top-down saliency [14, 17, 18, 44] is obtained by a classifier pre-trained by the dataset of salient objects and bottom-up saliency [12, 15, 16, 19, 20, 30, 31] is estimated by utilizing only the inpu...|
|||...Bottom-up saliency has been applied to foreground detection [30], activity recognition [31], classification and segmentation problems [21, 3234] and saliency has been applied to a tracker [35]....|
|||...[35], the proposed framework does not find the target location directly from saliency map but use the saliency map as a weight map for correlation filters....|
|||...2  [19] C. Gong, D. Tao, W. Liu, S. J. Maybank, M. Fang, K. Fu, and J. Yang, Saliency Propagation from Simple to Difficult, CVPR, 2015....|
|||...2  [14] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, Deep Networks for Saliency Detection via Local Estimation and Global Search, CVPR, 2015....|
|||...2  [17] R. Zhao, W. Ouyang, H. Li, and X. Wang, Saliency Detec tion by Multi-Context Deep Learning, CVPR, 2015....|
|||...2  [34] B. Heo, H. Jeong, J. Kim, S. Choi, and J. Y. Choi, Weighted Pooling Based on Visual Saliency for Image Classification, ISVC, 2014....|
|||...2  4329  [35] S. Hong, T. You, S. Kwak, and B. Han, Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network, ICML, 2015....|
||9 instances in total. (in cvpr2016)|
|188|Bo_Xiong_Snap_Angle_Prediction_ECCV_2018_paper|...Furthermore, the proposed snap angles respect high-level image contentdetected foreground objectsas opposed to typical lower-level cues like line straightness [12, 10] or low-level saliency metrics [8]....|
|||...Specifically, we compute the panoramas saliency map [40] in equirectangular form and blur it with a Gaussian kernel....|
|||...We then identify the P  P pixel square with the highest total saliency value, and predict the snap angle as the center of the square....|
|||...Unlike the other methods, this baseline is not iterative, since the maximal saliency region does not change with rotations....|
|||...When T = 1, SALIENCY is better than RANDOM but it underperforms our method and UNIFORM....|
|||...SALIENCY likely has difficulty capturing important objects in panoramas, since the saliency model is trained with standard field-of-view images....|
|||...Following the interface of [3], we present crowdworkers the panorama and instruct  Snap Angle Prediction for 360 Panorama  11  CANONICAL RANDOM SALIENCY P2V-ADAPTED OURS UPPERBOUND  Concert  Ski  Par...|
|||...Here we consider PANO2VID(P2V) [1]-ADAPTED and SALIENCY as defined in Sec....|
||8 instances in total. (in eccv2018)|
|189|Margolin_What_Makes_a_2013_CVPR_paper|...However, the saliency of this patch should be totally different, when the images have different patch distributions....|
|||...Our final saliency map S(px) is a simple product be tween the distinctness map and the Gaussian weight map:  S(px) = G(px)  D(px)....|
|||...(5)  We present a few examples of our saliency detection in Figure 8....|
|||...The two distinctness maps are combined (d) and then integrated with priors of image organization (e), to obtain our final saliency results in (f)....|
|||...As can be seen, the final saliency maps are more accurate than each of the components....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Static and space-time visual saliency detec tion by self-resemblance....|
||8 instances in total. (in cvpr2013)|
|190|Ahmed_Semantic_Object_Selection_2014_CVPR_paper|...Unlike saliency methods, our method can select objects that are small and potentially not salient in the input image as well as objects in images with several salient objects....|
|||...In [28], the saliency is determined by optimizing an energy function which encourages pixels to be salient if they are contained in regions that have high contrast to all other regions....|
|||...The method in [4] uses similar contrast and loca tion terms, and then computes a binary segmentation by including the computed saliency in a variant of GrabCut [20]....|
|||...Because this method heavily relies on saliency, it is largely restricted to working well on images where saliency methods also work well....|
|||...b) Top 1st column shows object proposals, 2nd column shows best exemplar warped onto the object proposal, and 3rdcolumn shows the saliency mask for the warped exemplars....|
|||...We use saliency to estimate the object location....|
|||...We obtain soft segmentation mask on the exemplar image by computing saliency map of [7], which gives a score in [0, 1] to each pixel in the exemplar image....|
|||...Hierarchical saliency detec tion....|
||8 instances in total. (in cvpr2014)|
|191|cvpr18-Automatic 3D Indoor Scene Modeling From Single Panorama|...To better isolate the objects from room layouts, we fuse the results of saliency [31] and object detection algorithms [18]....|
|||...3, the sofa (circled) is not detected, while the saliency map includes it....|
|||...Recall that object detection information and saliency map are recovered in each perspective subview, as shown in Fig....|
|||...The object mask in the panorama is then computed using saliency and object detection information that have been transferred and merged from the sub-views....|
|||...Examples of how the saliency map and object detection information complement each other, with highlights within dashed red circles....|
|||...eometry context, orientation map, and approximate surface normal) as well as semantic cues (namely, saliency and object detection map)....|
|||...We ap where EM denotes the object mask, Eo, Es denotes the object detection output and saliency map, respectively....|
|||...Our system is highly dependent on the accuracy of semantic cues; failures occur where both object and saliency detection fail....|
||8 instances in total. (in cvpr2018)|
|192|Singh_First_Person_Action_CVPR_2016_paper|...proach for egocentric videos and use region cues indicative of high-level saliency such as the nearness to hands, gaze, and frequency of occurrence....|
|||... Gaze or saliency map is usually captured with the help of separate eye tracking sensors.To make our approach generic we do not want to use extra sensors....|
|||...We note that there is hand-eye coordination in any first person action, and therefore saliency map obtained from the gaze resembles the dominantly moving parts in the scene....|
|||...We use dominant flow regions to generate a saliency map per frame....|
|||...The saliency values are normalized to the range [0, 255] and encoded as grayscale image for input to the network....|
|||...Camera motion (x and y direc tions separately) and saliency map are encoded as grayscale images....|
|||...H: Hand masks, C: Camera/Head motion, M: Saliency Map, S: Deep learned Spatial descriptors, T: Deep learned Temporal descriptors....|
|||...H: Hand masks, C: Camera/Head motion, M: Saliency Map, S: Deep learned Spatial descriptors, T: Deep learned Temporal descriptors....|
||8 instances in total. (in cvpr2016)|
|193|Object Co-Skeletonization With Co-Segmentation|...Note that at the beginning it is not robust to build up the GMM appearance models in this manner since the initial skeleton extracted based on saliency is not reliable at all....|
|||...Implementation Details  We use the saliency extraction method [2] for initialization of our framework in our experiments....|
|||...Ours(0): our initialization baseline using Otsu thresholded saliency maps [2] for segmentation and [21] for skeleton....|
|||...We replace the saliency initialization with ground truth initialization for training images....|
|||...n between our supervised method (groundtruth initialization) and our weakly supervised method (with saliency initialization), we denote the results of our supervised approach as Ours (S)....|
|||...Group saliency propagation for large scale and quick image co-segmentation....|
|||...Image cosegmentation via saliency co-fusion....|
||7 instances in total. (in cvpr2017)|
|194|Luo_Actionness-Assisted_Recognition_of_ICCV_2015_paper|...51, we contend that there is still significant inadequacy in adopting these conventional notions of saliency for the domain of action recognition....|
|||...The dominant paradigm in saliency research is still that of picture viewing in which the interpretation of the perceiver is supposed to be neutral....|
|||...Even if we extend the saliency feature channel with optic flow (like in various saliency works dealing with video), it is still basically a series of optic flows between two frames strung togeth er....|
|||...In sum, current conspicuity-based saliency models lack explanatory power for the aforementioned dynamic and situated aspects of saliency....|
|||...Given a video clip V with T frames, for the tth(t  [1, T ]) frame, we obtain its image saliency map St I by the wellknown GVBS algorithm [14]....|
|||...Since GVBS is pixel based, we take the average saliency value within a superpixel as the saliency value of the superpixel Sa....|
|||...For future work, one can probably integrate a face detector in the saliency attribute (perhaps even a generic face that is not agent-specific), so that the face will feature more prominently in the ac...|
||7 instances in total. (in iccv2015)|
|195|cvpr18-A Common Framework for Interactive Texture Transfer|...The structure channels are then extracted automatically by content-aware saliency detection and propagated from the source style image to the target as a prior....|
|||...The structure mask (i) or (j) is acquired by the computation of saliency maps (c), (d) or (g), (h)....|
|||...[18] proposed a saliency detection with content-awareness....|
|||...Following their method, we compute a saliency map for the source semantic map as Msem and the other one for the source stylized image as Msty....|
|||...e structure mask  Mstruct(p) =(1, Msty(p) lMsem(p)>  otherwise  0,  ,  (1)  where p is the pixel in saliency map Msty and Msem, we set l as 10 for a sharp saliency decrease of boundary pixels....|
|||...l is set to a higher value to decrease boundary saliency since boundary patches mainly depend on semantic annotations rather than structure guidance....|
|||... is a saliency threshold between 0 and 1....|
||7 instances in total. (in cvpr2018)|
|196|Fu_Object-based_Multiple_Foreground_2014_CVPR_paper|...(2)  The factors that influence this energy are the objectness score O(u), motion score M (u), and saliency score S(u) of the candidate u....|
|||...However, in practice a foreground object may not always be moving in the video, so we additionally consider a static saliency cue and take the maximum between the dynamic motion and static saliency cues in Eq....|
|||...ess score O(u), which is designed to identify extracted regions that are object-like and whole, the saliency score S(u) relates to visually salient stimuli, which has often been used to find regions o...|
|||...For this, rather than performing saliency detection for single images, we compute the co-saliency map on multiple images as described in [8], which takes consistency throughout the video into account....|
|||... state-of-the-art methods that are the most closely related works published in recent years: (1) Co-saliency detection (CoSal) in [8], which is based on bottom-up saliency cues and employs a global co...|
|||...However, the bottom-up saliency cue used in [8] can become less effective in complex videos (e.g., Tiger), as also mentioned in [8]....|
|||...Object co-segmentation based  on shortest path algorithm and saliency model....|
||7 instances in total. (in cvpr2014)|
|197|Fu_Object-Based_RGBD_Image_2015_CVPR_paper|...On the other hand, depthbased saliency methods [27, 22, 28] can effectively distinguish salient objects from backgrounds of similar color,  3.2....|
|||...In (d), RGBD saliency detection (e.g., [28]) generally distinguishes the foreground from the complex background, but it lacks a way to identify common objects in multiple images....|
|||...In our work, we take advantage of both approaches to combine the depth and co-saliency cues from multiple RGBD images by integrating them within the saliency map fusion framework of [6]....|
|||...The method in [6] exploits the low-rank relationship of multiple saliency sub-maps and obtains selfadaptive weights to generate a final co-saliency map....|
|||...We combine five kinds of sub-maps, specifically single-image RGB saliency [37, 38] and RGB co-saliency [12] and RGBD saliency [28, 9]....|
|||...Depth enhanced saliency detection method....|
|||...Object co-segmentation based on shortest path algorithm and saliency model....|
||7 instances in total. (in cvpr2015)|
|198|Sun_Exploring_Implicit_Image_2013_CVPR_paper|...Discussion Relation to Saliency People might be confused when being asked to point out both the representative and the salient parts of an image....|
|||...As shown in the results, AIM saliency map highlights the unique objects, e.g....|
|||...Comparisons between bottom-up saliency and the proposed representativeness....|
|||...For each row, we show the input image, the AIM saliency map [31], and the visualized response map of our representativeness....|
|||...Unlike the saliency method, our model locates those regions which contain both salient and discriminative contents such as the golden roof of the Chinese Palace Museum and the huge pillars of the Germ...|
|||...Figure 6 shows some comparisons between our representativeness model and AIM saliency (Attention by Information Maximization [31]) on natural images....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||7 instances in total. (in cvpr2013)|
|199|Ruochen_Fan_Associating_Inter-Image_Salient_ECCV_2018_paper|...This may be done using a saliency detector [4, 20, 22, 42] or attention models [4, 42], for example....|
|||...With the rapid development of saliency detection algorithms, some saliency extractors, such as MSRNet [24] and S4Net [12], are now not only able to predict gray-level salient objects but also instance...|
|||...ect detectors, in this paper, we propose to carry out the instance distinguishing task in the early saliency detection stage, with the help of S4Net, greatly simplifying the learning pipeline....|
|||...[19] advanced this approach by combining the saliency maps [18] with attention maps [45]....|
|||...[4] considered linking saliency and attention cues together, but they adopted different strategies to acquire semantic objects....|
|||...learning, saliency detectors are now available that can predict saliency maps along with instance bounding boxes....|
|||...Given training images labelled only with keywords, we use an instance-level saliency segmentation network, S4Net [12], to extract salient instances from every image....|
||7 instances in total. (in eccv2018)|
|200|Kim_Automatic_Content-Aware_Projection_ICCV_2017_paper|...Here, note that, although the proposed framework includes the contents analysis step, we mainly focus on the projec 4744  Content Analysis Step(Sec 3.1)Input 360ImageLine ExtractionScene Saliency Esti...|
|||...To extract salient objects, we compute scene saliency as the combination of appearance and motion saliency of the image as  Sscene  i  = wSappear  i  + (1  w)Smotion  i  ,  (1)  i  , Sappear  where Ss...|
|||...To estimate motion saliency Smotion, we exploit the method proposed in [5] with optical flow [8] as an input....|
|||...We assume that objects have higher scene saliency than the background....|
|||...Note that any other saliency detection methods can be applied to our projection method....|
|||...However, it can be improved by substituting it with faster saliency detection algorithms....|
||6 instances in total. (in iccv2017)|
|201|Jain_Active_Image_Segmentation_CVPR_2016_paper|...First we generate the generic object proposals and compute a saliency map using [18]....|
|||...Next we obtain two ranked lists of these proposals using saliency and objectness scores [8], respectively....|
|||...e  2One could choose from a variety of features; we employ off-the-shelf  CNN-based descriptors and saliency metrics (see Sec....|
|||...The variables s and m weight the influence of the saliency and matching terms, respectively....|
|||...The saliency term is defined using the saliency region feature (Rs  ij) as:  s(Yij) = YijRs  ij + (1  Yij)(1  Rs  ij),  (3)  so that we favor assigning Yij = 1 if Rij is very salient....|
|||...For saliency Rs ij, we average the regions pixel-level saliency values from [18]....|
||6 instances in total. (in cvpr2016)|
|202|cvpr18-Inferring Shared Attention in Social Scene Videos|...We didnt use saliency models (like [21, 37]) because shared attention is more influenced by social group interaction instead of visual importance, and people engaged in shared attention are not free-v...|
|||...Given a video clip and the annotations of head and eye location, their model combines gaze pathway, saliency pathway and transformation pathway to predict where a person is looking even when the objec...|
|||...After that, they proposed a method to predict social saliency from images or videos captured by multiple first-person view cameras [23]....|
|||...The replacement of region proposal module with a saliency model impairs our model performance because the shared attention of people in a social interaction may not be the most visually salient object...|
|||...Social saliency prediction....|
|||...Sun: A bayesian framework for saliency using natural statistics....|
||6 instances in total. (in cvpr2018)|
|203|Tang_GrabCut_in_One_2013_ICCV_paper|...l applications including interactive binary segmentation in Sec.3.1, shape matching in Sec.3.2, and saliency detection in Sec.3.3....|
|||...Then let <A> (A(p))  sp  ESalience(S) =  (cid:2)  (17)  p  denote an energy term measuring the saliency of a given segment....|
|||...Let E3(S) be the energy combining the saliency and smoothness terms  E3(S) = ESalience(S) +  S ,  (18)  and E4(S) be the energy with the appearance overlap term  E4(S) = E3(S)  (cid:7)S    S(cid:7)L1 ....|
|||...15 shows qualitative results for our saliency segmentation with and without the appearance overlap term....|
|||...In several applications including interactive image segmentation, shape matching and saliency region detection we achieve the state-of-the-art results....|
|||...tation examples: (a) Original image, (b) Saliency map from [17] with bright intensity denoting high saliency, (c-d) Graph cut segmentation without and with appearance overlap penalty term, (e) Ground ...|
||6 instances in total. (in iccv2013)|
|204|cvpr18-Reinforcement Cutting-Agent Learning for Video Object Segmentation|...ted training exemplars, they typically estimate informative cues, such as boundaries, motion, video saliency and object detection etc., then segment video object according to the estimated cues....|
|||...The used saliency detection datasets include MSRA10K [6], PASCAL-S [21], SOD [27] and ECSSD [32]....|
|||...Having trained the CEN with image saliency datasets, we fine-tune the model with the annotated first frame in the video sequence, so as to alleviate the difference between the saliency datasets and th...|
|||...Train CPN  Although we desire to train the CPN in the DRL manner, the only available training data is static saliency detection data rather than the annotated sequential video data....|
|||...Thus, we add noise to the static saliency detection data so as to simulate the object location variation among neighbor video frames....|
|||...After training CPN on static saliency detection  datasets, we fine-tune it with the annotated first frame in the video sequence, so as to enhance the video specific object knowledge....|
||6 instances in total. (in cvpr2018)|
|205|Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper|...(b) Visual explanation is a saliency map....|
|||...To evaluate this we rely on two visual explanation techniques: Grad-CAM [30] and saliency maps generated by occluding image regions in a sliding window fashion....|
|||...To obtain saliency maps, we resize an input image to 299  299 and uniformly divide it into 32  32 pixel regions, obtaining a 10  10 grid (the bottom/rightmost cells being smaller)....|
|||...This is similar to the top-down saliency approach of [26], who zero-out all the intermediate feature descriptors but one....|
|||...Results on the MSCOCO-Balanced set are presented in Table 3 (a) and (b), for the Grad-CAM and saliency maps, respectively....|
|||...Ramanishka, V., Das, A., Zhang, J., Saenko, K.: Top-down visual saliency guided by captions....|
||6 instances in total. (in eccv2018)|
|206|Chen_Automatic_Image_Cropping_CVPR_2016_paper|...Pixel saliency values calculated using Ittis model [9] was combined with face and text detection results for generating the attention map....|
|||...extended this work by using summed saliency values within cropping rectangles for determining the best cropping position [20]....|
|||...acquired image saliency values by means of human-computer interaction [18]....|
|||...combined visual saliency information with face and skin color detection results for placing bounding box in image cropping [7]....|
|||...A model of saliency based visual attension of rapid scene analysis....|
|||...Top-down visual saliency via joint  crf and dictionary learning....|
||6 instances in total. (in cvpr2016)|
|207|Peng_A_Mixed_Bag_2015_CVPR_paper|...Difference of areas of the most/least saliency regions....|
|||...Color difference of the most/least saliency regions....|
|||...Difference of the sum of edge magnitude of the most/least saliency regions....|
|||...Cascaded CIECAM02 color histograms (lightness, chroma, hue, brightness, and saturation) in the most/least saliency regions....|
|||...Cascaded edge histograms (8 (8)-bin edge direction (magnitude) in RGB and gray channels) in the most/least saliency regions....|
|||... of angle (4 bins), the ratio of major and minor axes (4 bins), and area (4 bins) in the most/least saliency regions....|
||6 instances in total. (in cvpr2015)|
|208|Kendall_PoseNet_A_Convolutional_ICCV_2015_paper|...11 shows example saliency maps produced by PoseNet....|
|||...The saliency map, as used in [21], is the magnitude of the gradient of the loss function with respect to the pixel intensities....|
|||...This would agree with our own findings that a network trained to output position and orientation outperforms a network trained to output only po Figure 11: Saliency maps....|
|||...This figure shows the saliency map superimposed on the input image....|
|||...The saliency maps suggest that the convnet exploits not only distinctive point features (`a la SIFT), but also large textureless patches, which can be as informative, if not more so, to the pose....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||6 instances in total. (in iccv2015)|
|209|Faktor_Co-segmentation_by_Composition_2013_ICCV_paper|...Recently, [17] suggested to combine visual saliency and dense pixel correspondences across images for the purpose of co-segmentation....|
|||...However, we use the statistical significance of the shared regions to initialize the co-segmentation and not visual saliency like [17] does....|
|||...We compare our results to two baselines: (i) Grab-cut initialized with a central window of size 25% of the image (ii) Saliency map of [10]  Let p  I be a pixel....|
|||...Similarly, saliency based segmentation will not suffice either, since the co-occuring object is not necessarily salient in the image, and there can be other salient  image parts (e.g....|
|||...4d  the saliency maps were generated using [10])....|
|||...Moreover, initializing the co-segmentation using saliency maps will also be problematic, since the co-segments are not necessarily salient in the image, as there are many other distracting objects in ...|
||6 instances in total. (in iccv2013)|
|210|Wang_Visual_Tracking_With_ICCV_2015_paper|...Their associated saliency maps (bottom row) are mainly focused on the target region....|
|||...Their saliency maps (bottom row) present spatial information of the category....|
|||...We also use the approach in [26] to obtain the saliency maps of CNN features....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...Deep networks for saliency detection via local estimation and global search....|
||6 instances in total. (in iccv2015)|
|211|BRISKS_ Binary Features for Spherical Images on a Geodesic Grid|...Feature saliency can be computed for corners by finding the largest value for the threshold tscore(p) at which the point is still considered a corner....|
|||...Non-maxima suppression  We use the feature saliency score to apply non-maxima suppression....|
|||...For every detected corner, we compute the corner saliency for adjacent pixels (even if they were not detected as corners) at the same scale....|
|||...We also compute corner saliency for neighbours across scale (if there are two neighbours at the next coarser scale, we take the one with higher saliency as the neighbour)....|
|||...We require a feature to have maximum saliency amongst all of its neighbours and remove any that do not....|
|||...The computed saliency scores are saved as they are used subsequently for position refinement....|
||6 instances in total. (in cvpr2017)|
|212|Nathan_Silberman_ExplainGAN_Model_Explanation_ECCV_2018_paper|...Existing approaches to model interpretation, including saliency and explanation-by-nearest neighbor, fail to visually illustrate examples of transformations required for a specific input to alter a mo...|
|||...sing traditional criteria by demonstrating that our models inferred masks are highly competitive as saliency maps when compared to state-of-the-art attribution approaches (Section 5.4)....|
|||...5.4 Explanation via Pixel-Wise Attribution  Many post-hoc explanation methods that use attribution or saliency rely on visual, qualitative comparisons of attribution maps....|
|||...Our model is not designed for attribution / saliency as it produces a binary, rather than continuous mask, which is also paired to a particular transformation image....|
|||...While not explicitly trained to act as a saliency map, ExplainGANs maps are very competitive at demonstrating saliency....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: visualising image classification models and saliency maps (2014)....|
||6 instances in total. (in eccv2018)|
|213|Su_Multi-View_Convolutional_Neural_ICCV_2015_paper|...taken from fc8 layer) for its ground truth class c. Following [33], saliency maps can be defined as the derivatives of Fc w.r.t....|
|||...2 can be computed using backpropagation with all the network parameters fixed, and can then be rearranged to form saliency maps for individual views....|
|||...Examples of saliency maps are shown in Fig....|
|||...Top three views with the highest saliency are highlighted in blue and the relative magnitudes of gradient energy for each view is shown on top....|
|||...The saliency maps are computed by back-propagating the gradients of the class score onto the image via the view-pooling layer....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||6 instances in total. (in iccv2015)|
|214|Cao_Look_and_Think_ICCV_2015_paper|...Inspired by visualizations of CNNs [33, 24], a more feasible and cognitive manner for detection / localization could be derived by utilizing the saliency maps generated in feedback visualizations....|
|||...We adopt the same saliency extraction strategy as [24] that a single class saliency value Mk for class k at pixel (i, j) is computed across all color channels: Mk(i, j) =  Method Localization Error (%...|
|||...Different from [24], which uses GraphCut [32], this requires saliency maps of higher quality, but only takes less computation....|
|||...alian greyhound and sunglass), column (b) shows the 5 calculated bounding box areas using the top-5 saliency maps, column (c) shows the cropped images obtained from the red box which ConvNets predict ...|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||5 instances in total. (in iccv2015)|
|215|cvpr18-AMNet  Memorability Estimation With Attention|...In order to build better computational models to learn and predict memorability, researchers analyzed the relationship between memorability and various visual factors [19], image classes [12] and saliency [7]....|
|||...Mancas and Le Meur [24] studied the link between saliency and memorability and found that the most memorable images have uniquely localized regions, while less memorable either do not have precise reg...|
|||...[4] applied an attention driven spatial pooling pipeline based on SIFT [22] and HOG [5] features and bottom-up and object-level saliency detectors....|
|||...An eye fixation database for saliency detection in images....|
|||...SUN: A bayesian framework for saliency using natural statistics....|
||5 instances in total. (in cvpr2018)|
|216|A-Lamp_ Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural Network for Photo Aesthetic Assessment|..., and im 4536  age classification [36, 10], as well as contemporary tasks in image captioning [1], saliency detection [32], style recognition [8, 14] and photo aesthetics assessment [21, 23, 39, 28, ...|
|||...The task of saliency detection is to identify the most important and informative part of a scene....|
|||...Therefore, it is natural to adopt saliency map for selecting regions that human usually pay more attention to....|
|||...In addition to saliency map, we also encourage diversification within a set of patches....|
|||...The saliency value is obtained by a graph-based saliency detection approach [44]....|
||5 instances in total. (in cvpr2017)|
|217|Online Video Object Segmentation via Convolutional Trident Network|...To discover visually important objects in a video, [11, 35, 53] employ saliency detection techniques....|
|||...Papazoglou and Ferrari [35] compute a motion saliency map, called the inside-outside map, using optical flow boundaries....|
|||...Faktor and Irani [11] exploit both motion saliency and visual saliency to separate a segment track from the background....|
|||...Liu and Han [29] propose a saliency detection algorithm, which predicts a multi-scale saliency map at each layer in a decoder....|
|||...DHSNet: Deep hierarchical saliency network for salient object detection....|
||5 instances in total. (in cvpr2017)|
|218|SPFTN_ A Self-Paced Fine-Tuning Network for Segmenting Objects in Weakly Labelled Videos|...As we know, DNN has achieved tremendous success in various problems like object detection [6] and saliency prediction [7]....|
|||...the MSRA 10K dataset [5] (containing random objects like flower and traffic sign) under the task of saliency detection [7, 17], which could guide the network to encode general saliency priors from the...|
|||...Here a larger pk indicates more consistency between the segmentation proposal and saliency mask....|
|||...6) Pre-training on salient object dataset could encode helpful saliency priors, which benefits learning under the weak supervision....|
|||...Revealing event saliency in unconstrained video collection....|
||5 instances in total. (in cvpr2017)|
|219|Li_Primary_Video_Object_ICCV_2017_paper|...To speed up the training process and reduce the number of parameters, we down-sample all images to 280  280 and their groundtruth saliency maps into 140  140....|
|||...Deep saliency with encoded In CVPR,  low level distance map and high level features....|
|||...Visual saliency based on multiscale deep  features....|
|||...Hierarchical saliency detec tion....|
|||...Deep networks for saliency detection via local estimation and global search....|
||5 instances in total. (in iccv2017)|
|220|Yang_Semantic_Filtering_CVPR_2016_paper|...In this section, we propose to numerically evaluate the improvement over the state-ofthe-art saliency detection algorithm when the original image is processed by the state-of-the-art filters....|
|||...We choose the ECSSD dataset [46] which is known to be difficult and Minimum Barrier Saliency (MBS) detection algorithm [53]4 which is the latest algorithm that outperforms all the others on this dataset....|
|||...The precision-recall curves which evaluate the overall performance of a saliency detection method  3The default parameters included in the implementations published by  the authors of [44, 54, 45] were used....|
|||...Precision-recall curves for saliency detection....|
|||...Hierarchical saliency detec tion....|
||5 instances in total. (in cvpr2016)|
|221|Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper|...This challenge is usually addressed using visual attention techniques that output saliency maps....|
|||...This is different than most attention works (with one label per image) that extract saliency maps of the same spatial/channel size of the given feature representation....|
|||...We observed in our experiments that this confidence-weighting branch boosts the performance by a small amount and helps the attention mechanism learn better saliency heatmaps (Figure 3 right)....|
|||...Combining the output saliency masks from different scales can be done either at a prediction level (i.e., averaging the logits) or at a feature level [42]....|
|||...ed as:  L = Lw + La1 + La2 ,  (7)  where La1 is applied to the first attention module that extracts saliency maps of spatial resolution 14  14, and La2 is similarly applied to the second attention mod...|
||5 instances in total. (in eccv2018)|
|222|Minho_Shim_Teaching_Machines_to_ECCV_2018_paper|...Saliency on the class foul with corresponding video frame: saliency of CNN+GRU trained (a) without masking, (b) with on-screen socreboard masking....|
|||...6.2 Saliency Analysis  As an attempt to understand what the neural net is looking at when recognizing actions, we provide further analysis through the saliency map [38]....|
|||...A saliency result for CNN+GRU is shown in Fig....|
|||...A saliency results with the scoreboard masked is shown in Fig....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising  image classification models and saliency maps....|
||5 instances in total. (in eccv2018)|
|223|cvpr18-Deflecting Adversarial Attacks With Pixel Deflection|...Jacobian-based Saliency Map Attack (JSMA) [31] estimates the saliency of each image pixel w.r.t....|
|||...This is a targeted attack, and saliency is designed to find the pixel which increases the classifiers output for the target class while tending to decrease the output for other classes....|
|||...We prefer to use weakly supervised localization over saliency maps [17], as saliency maps are trained on human eye fixations and thus do not always capture object classes [25]....|
|||...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
|||...Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet....|
||5 instances in total. (in cvpr2018)|
|224|Lee_Visual_Tracking_Using_2014_CVPR_paper|...Similarly, we compute the saliency sS(i) of the ith patch  in 1, with respect to the shrunken region S 1 ,   K(cid:88)  j=1   ,  (a)  (b)  sS(i) = min  (cid:107)c(i)  cS(j)(cid:107)  (5)  where cS(j) ...|
|||...In contrast, a background patch often has a large saliency sS(i)....|
|||...The notion of pertinence is related to the saliency based on the histogram contrast [6]....|
|||...In [6], the saliency of a pixel is proportional to its rarity, which is defined as the sum of the differences from the pixel to the other pixels....|
|||...We then compute the saliency sE(i) of the ith patch in 1, with respect to the expanded region E  1 and E  1 , as  sE(i) = min  (cid:107)c(i)  cE(j)(cid:107)  (4)   K(cid:88)  j=1   ,  10105.1....|
||5 instances in total. (in cvpr2014)|
|225|Guo_Video_Co-segmentation_for_2013_ICCV_paper|... of regions are defined to be co-salient if they exhibit strong internal coherence and strong local saliency w.r.t the background, and the correspondences between the regions are supported by high app...|
|||...The foreground trajectories are those with high motion saliency w.r.t....|
|||...Use si t to represent the saliency of tri at time t. We measure si t using the median value of the distances between tri and all the others, i.e.,  {(ui  uj t )2 + (vi  yi t and vi t = yi t+T  t = xi ...|
|||...6, its algorithm is sensitive to wrong initial segmentation caused by those background contents that confuse the objectness and saliency measures....|
|||...Image matching via saliency region corre spondences....|
||5 instances in total. (in iccv2013)|
|226|Kwak_Unsupervised_Object_Discovery_ICCV_2015_paper|...Given neighbor frames N (vt) where an object in vt may appear, the corresponding region saliency is defined as the sum of maxpooled match scores from R  u to r:  g(rt Rt, Ru) = X  vuN (vt)  max ruRu  ...|
|||...The region saliency g(rt) is high when r matches the neighbor frames well in terms of both appearance and geometric consistency....|
|||...While useful as an evidence for foreground regions, the region saliency of Eq....|
|||...The similarity is then computed as the sum of all region saliency scores given by the matching....|
|||...For example, F(A) means foreground saliency based only on appearance (i.e., a), and T(A,M) indicates temporal smoothness based on both of appearance and motion (i.e., a + m = )....|
||5 instances in total. (in iccv2015)|
|227|Automatic Discovery, Association Estimation and Learning of Semantic Attributes for a Thousand Categories|...Capturing word saliency directly is hard due to word polysemy and since word importance depends on the context....|
|||...We define a saliency cost function as:  C(S) = XwiS  (1 +  XTkinsig(T )  p(Tk wi)),  (5)  where  controls the contribution of the insignificance score of a word to the cost function....|
|||...618  Model  Relevance (%)   Junk (%)   Saliency (%)   mRmR MinCorr LLC-fs MCFS  Ours  20.8 14.4 29.1 18.6  44.5  53.0 20.6 42.9 13.6  2.6  33.9 46.9 43.1 52.5  71.0  Table 2: Saliency scores of the s...|
|||...Vocabulary saliency Here, we explore how the selected vocabulary correlates with human understanding of salient semantic attributes....|
|||...Out of the 4 categories, we are interested in the positive and junk categories since they describe the semantic saliency of the words....|
||5 instances in total. (in cvpr2017)|
|228|Qian_Multi-Scale_Deep_Learning_ICCV_2017_paper|...Deep saliency modelling Visual saliency has been studied extensively [19, 20]....|
|||...In this work, we use saliency-based learning strategy in a saliency-based learning fusion layer to exploit both visual saliency and attention mechanism....|
|||...Specifically, with the multi-scale stream layers, the saliency features of multiple scales are computed in multi-channel (e.g....|
|||...Such a simple saliency learning architecture is shown to be very effective in our experiments....|
|||...3), for each branch, the saliency-based learning fusion layer combines four data streams into a single data stream with selectively learning learned from the saliency of each data stream....|
||5 instances in total. (in iccv2017)|
|229|Fine-Grained Image Classification via Combining Vision and Language|...The vision stream first localizes the object of image via saliency extraction and co-segmentation, and then learns deep representations of the original image and its discriminative object via deep con...|
|||...Object localization  In this paper, we apply an automatic object localization method based on saliency extraction and co-segmentation proposed in TSC [4], which allows to localize the object in a weak...|
|||...Saliency extraction is to localize the object preliminarily with the saliency map generated by  5996  CNN model to obtain the prediction, which is the result of the vision stream....|
|||...bounding boxes of objects, and the red rectangles indicate the object regions generated by jointly applying saliency extraction and cosegmentation....|
|||...However, only through saliency extraction, the object region is not accurate enough so that cosegmentation is conducted to make the object region more accurate for fine-grained image classification....|
||5 instances in total. (in cvpr2017)|
|230|Zhang_SOM_Semantic_Obviousness_2015_CVPR_paper|...Unlike previous methods which use saliency response to weight different image regions, the patches get an automatic weight in our method....|
|||...[23] proposed a NR-IQA algorithm based on saliency map analysis....|
|||...[17] shows that the improvement of IQA algorithms is not guaranteed if saliency response is simply used as a weighting term....|
|||...Full reference image quality assessment based on saliency map analysis....|
||4 instances in total. (in cvpr2015)|
|231|Webly Supervised Semantic Segmentation|...The difference is that [20] builds general objectness measure for all classes while [19] focuses on class-specific saliency maps....|
|||...Instead of inferring rough location cues for each class using objectness or saliency maps, we train a shallow segmentation network from web images to automatically generate segmentation masks for each class....|
|||...Since images from Wk are relatively easy to segment, the quality of the segmentation masks, which are generated by saliency and CRF algorithms without using human annotations, is thus acceptable....|
|||...Figure 3: Sample images from Wcar and the corresponding segmentation masks generated from saliency combined with a dense CRF....|
||4 instances in total. (in cvpr2017)|
|232|cvpr18-Learning Attentions  Residual Attentional Siamese Network for High Performance Online Visual Tracking|...For instance, CNN-SVM tracker [23] utilized CNN model with saliency map and SVM....|
|||...On the other hand, RTT [7] drew attention to possible targets by a multidirectional RNN to generate saliency and CSR-DCF [33] constructed a foreground spatial reliability map by using color histograms...|
|||...They both utilize saliency to regularize correlation filters with hand-crafted features....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
||4 instances in total. (in cvpr2018)|
|233|Gaze Embeddings for Zero-Shot Image Classification|...On the other hand, in [17] gaze has been used to evaluate saliency algorithms on video sequences....|
|||...Saliency histogram (35.8%) is a discretization of a saliency map [8] using a spatial grid over the image....|
|||...A dataset and evaluation methodology for visual saliency in video....|
|||...Learning saliency maps  for object categorization....|
||4 instances in total. (in cvpr2017)|
|234|Varun_Jampani_Superpixel_Sampling_Networks_ECCV_2018_paper|...d in computer vision algorithms such as object detection [35,42], semantic segmentation [15,34,13], saliency estimation [18,30,43,46], optical flow estimation [20,28,37,41], depth estimation [6], trac...|
|||...Perazzi, F., Kr ahenb uhl, P., Pritch, Y., Hornung, A.: Saliency filters: Contrast based filtering for salient region detection....|
|||...: Saliency detection via graphbased manifold ranking....|
|||...Zhu, W., Liang, S., Wei, Y., Sun, J.: Saliency optimization from robust background detection....|
||4 instances in total. (in eccv2018)|
|235|cvpr18-Visual Feature Attribution Using Wasserstein GANs|...Another class of techniques creates saliency maps by backpropagating back to the input image....|
|||...[27] use the backpropbased saliency technique proposed by [51] to pinpoint lumbar degradations, and Baumgartner et al....|
|||...Image super resolution using generative adversarial networks and local saliency maps for retinal image analysis....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||4 instances in total. (in cvpr2018)|
|236|cvpr18-Learning to Understand Image Blur|...rization, we build another four baselines based on the different combinations of the blur map (Bm), saliency map (Sm), and content feature map (Cm) to conduct extensive ablation studies....|
|||...Other baselines share the same pipeline with different combination of the blur map, saliency map, and content feature map....|
|||...All the baselines separately generate blur map, saliency map, or content feature map, and then perform blur type classification....|
|||...To be specific, saliency map is generated by training the attention map estimation branch of ABC-FuseNet on the salient object segmentation datasets [35]....|
||4 instances in total. (in cvpr2018)|
|237|cvpr18-Learning-Compression Algorithms for Neural Net Pruning|...ly has the form of magnitude pruning, which gives support to using magnitude as a measure of weight saliency (as opposed to, say, curvature)....|
|||...Many saliency criteria exist, such as magnitude  wi , curvature using the diagonal [23] or all the Hessian entries [17, 18], and sensitivity of the loss to removing wi....|
|||...While most saliency methods are simple and fast, their performance is limited: they are local (the saliency estimate for each wi is valid at the reference net but not away from it), greedy (weights ar...|
|||...Interestingly, although saliency and penalty methods appear very different, we will show they are related in our LC algorithm: an iterative form of magnitude-based pruning arises in a principled way f...|
||4 instances in total. (in cvpr2018)|
|238|cvpr18-Harmonious Attention Network for Person Re-Identification|...One common strategy is local patch calibration and saliency weighting in pairwise image matching [48, 28, 51, 39]....|
|||...To overcome this limitation, attention selection techniques have been developed for improving re-id by localised patch matching [28, 51] and saliency weighting [39, 48]....|
|||...Soft spatial-channel attention learning aims to produce a saliency weight map Al  Rhwc of the same size as X....|
|||...Unsupervised learning of generative topic saliency for person re-identification....|
||4 instances in total. (in cvpr2018)|
|239|Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper|...ing gaze is an active research topic in computer vision, with applications in affect analysis [22], saliency detection [42,48,49] and action recognition [31,36], to name a few....|
|||...We hypothesize that saliency information of the scene could prove useful in this context....|
|||...Parks, D., Borji, A., Itti, L.: Augmented saliency model using automatic 3D head pose detection and learned gaze following in natural scenes....|
|||...Rudoy, D., Goldman, D.B., Shechtman, E., Zelnik-Manor, L.: Learning video saliency from human gaze using candidate selection....|
||4 instances in total. (in eccv2018)|
|240|cvpr18-Direction-Aware Spatial Context Features for Shadow Detection|... Third, we evaluate our network on two benchmark sets and compare it with several state-of-the-art methods on shadow detection, saliency detection, and semantic image segmentation....|
|||...ency Detection and Se(cid:173)  mantic Segmentation Methods  In general, deep networks designed for saliency detection and semantic image segmentation may also be used for shadow detection by training...|
|||...Hence, we conduct another experiment by using two recent deep models for saliency detection, i.e., SRM [26] and Amulet [28], and a recent deep model for semantic image segmentation, i.e., PSPNet [29]....|
|||...In future, we plan to explore the potential of our network for other applications such as saliency detection and semantic segmentation, and further enhance its capability for detecting time-varying sh...|
||4 instances in total. (in cvpr2018)|
|241|Yagiz_Aksoy_A_Dataset_of_ECCV_2018_paper|...He and Lau [11] provide a dataset of 120 flash/no-flash photograph pairs captured with a DSLR camera and a tripod for the application of saliency detection....|
|||...In addition to image processing, flash/noflash pairs have been used to improve image matting [31], automatic object segmentation [30], image deblurring [35], saliency detection [11], and stereo matching [34]....|
|||...Previous literature in flash photography shows that our dataset can be utilized for studying white balance, enhancement of flash photographs, saliency and more....|
|||...: Saliency detection with flash and no-flash image pairs....|
||4 instances in total. (in eccv2018)|
|242|Anurag_Arnab_Weakly-_and_Semi-Supervised_ECCV_2018_paper|...The salient parts of an image are thing classes in popular saliency datasets [34 36] and this prior therefore does not help at all in segmenting stuff as in our case....|
|||...Note that saliency priors, used by many works such as [10, 31, 32] on Pascal VOC, are not suitable for stuff classes as popular saliency datasets [3436] only consider things to be salient....|
|||...: Saliency detection via graph-based  manifold ranking....|
|||...Shi, J., Yan, Q., Xu, L., Jia, J.: Hierarchical image saliency detection on extended cssd....|
||4 instances in total. (in eccv2018)|
|243|Wu_SCaLE_Supervised_and_2013_CVPR_paper|...Since the visual objects that need to be recognized typically belong to a salient foreground region of an image, we first perform saliency detection as in [9] and then threshold the resulting saliency map....|
|||...We further obtain a bounding box for the pixels with saliency above the threshold....|
|||...During the computation of a histogram, the feature value at a pixel is cast into a bin after being multiplied by the saliency value of that pixel....|
|||...Instead, it is only necessary to precompute and save the saliency map and its associated bounding box for each training image, once and for all....|
||4 instances in total. (in cvpr2013)|
|244|Object Region Mining With Adversarial Erasing_ A Simple Classification to Semantic Segmentation Approach|...[29] presented a simple to complex learning method, in which an initial segmentation model is trained with simple images using saliency maps for supervision....|
|||...Based on the generated saliency maps, the regions whose pixels are with low saliency values are selected as background....|
|||...We use saliency maps from [9] to produce the background localization cues....|
|||...sofa or table), we adopt the normalized saliency value 0.06 as the threshold to obtain background localization cues (i.e....|
||4 instances in total. (in cvpr2017)|
|245|Yunchao_Wei_TS2C_Tight_Box_ECCV_2018_paper|...Motivated by [19, 36, 38, 40], we leverage the saliency detection technology [41] to produce the saliency map for each training image....|
|||...Based on the generated saliency map, we choose the pixels with low normalized saliency values (i.e....|
|||...However, both the class-specific confidence map and the saliency map are not accurate enough to guarantee a high-quality segmentation mask....|
|||...Lai, B., Gong, X.: Saliency guided end-to-end learning for weakly supervised object  detection....|
||4 instances in total. (in eccv2018)|
|246|Ubernet_ Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory|...er a unified CNN architecture that jointly handles (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary dete...|
|||...s systematically evaluated on the following tasks: (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) semantic part segmentation (f) semantic boundary d...|
|||...set with common annotations for tasks as diverse as human part segmentation, normal estimation, and saliency estimation....|
|||...Visual saliency based on multiscale  deep features....|
||4 instances in total. (in cvpr2017)|
|247|cvpr18-Learning to Look Around  Intelligently Exploring Unseen Environments for Unknown Tasks|... peek-saliency moves to the most salient view within reach at each timestep, using a popular saliency metric [23]....|
|||...To avoid getting stuck in a local saliency maximum, it does not revisit seen views....|
|||...On ModelNet data, peek-saliency performs poorly, likely because saliency fails to differentiate well between the syn thetic CAD model views; what is informative about an objects shape is much more com...|
|||...Recall that peek-saliency is not actually a viable solution; it cheats by trying out all moves and measuring saliency before selecting a move at each timestep....|
||4 instances in total. (in cvpr2018)|
|248|Panda_Weakly_Supervised_Summarization_ICCV_2017_paper|...A very recent work [46] generates spatio-temporal saliency maps using an encoder-decoder network with human annotated captions which are harder to obtain compared to video class labels....|
|||...image saliency detection algorithm....|
|||...Top-down  visual saliency guided by captions....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||4 instances in total. (in iccv2017)|
|249|Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper|...We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped....|
|||...Finally, we study the skipping behavior of the learned skipping policy and reveal the relation between image scale and saliency and the number of layers skipped....|
|||...4.2, we decipher the dynamic essence of SkipNets with extensive qualitative study and analysis to reveal the relation between image scale and saliency and number of layers skipped....|
|||...Interestingly, we find that images within each cluster share similar characteristics with respect to saliency and clarity....|
||4 instances in total. (in eccv2018)|
|250|Shi_Sampling_Strategies_for_2013_CVPR_paper|...They measured the eye movement of human observers watching videos, and used the data to produce an empirical saliency map....|
|||...By using such saliency maps, they pruned 20-50% of the dense features and achieved better results....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
|||...Space-variant descriptor sampling for action recognition based on saliency and eye movements....|
||4 instances in total. (in cvpr2013)|
|251|cvpr18-CSRNet  Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes|...semantic segmentation tasks [7, 8, 9, 10, 11] and the significant progress they have made in visual saliency [12]....|
|||...It integrates the information of saliency during the learning process....|
|||...Inspired by the works [10, 11, 40], we try to deploy dilated convolutional layers as the back-end for  1093  extracting deeper information of saliency as well as maintaining the output resolution....|
|||...Shallow and deep convolutional networks for saliency prediction....|
||4 instances in total. (in cvpr2018)|
|252|cvpr18-Demo2Vec  Reasoning Object Affordances From Online Videos|...Here, we follow the annotation routine from previous works on visual saliency [1, 2, 9]....|
|||...The model should not only fixate on specific object parts as it may seem to be the saliency regardless of the demonstration video....|
|||...Mit saliency benchmark....|
|||...A benchmark of computational models of saliency to predict human fixations....|
||4 instances in total. (in cvpr2018)|
|253|Gkioxari_Finding_Action_Tubes_2015_CVPR_paper|...We use motion saliency to eliminate re gions that are not likely to contain the action....|
|||...One could use more complicated techniques, such as action saliency detectors trained on human eye fixations and low level cues [29]....|
|||...Our motion saliency algorithm is extremely simple....|
|||...Dynamic Eye Movement Datasets and Learned Saliency Models for Visual Action Recognition....|
||4 instances in total. (in cvpr2015)|
|254|Vadivel_Eye_Tracking_Assisted_2015_CVPR_paper|...Quantitative analysis of human-model agreement in visual saliency modeling: A Image Processing, IEEE Transactions comparative study....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recogIn Computer VisionECCV 2012, pages 842856....|
|||...An eye fixation database for saliency detection in images....|
|||...Space-variant descriptor sampling for action recognition based on saliency and eye movements....|
||4 instances in total. (in cvpr2015)|
|255|cvpr18-Depth-Aware Stereo Video Retargeting|...We calculate si by averaging the saliency values of 3D points in the i-th depth trajectory, where the saliency values are defined in Sec....|
|||... z  z  t  X X  t  i  X X  i  D(gz,t k )  kwz,t  k   hz,t  k   wz,t  k  hz,t  k )k2  z,t k ,  is the saliency value of gz,t k  (10) where sz,t calculated by averk aging the saliency values of all pixel...|
|||...Similar to [15], we calculate pixel saliency z,t k using a weighted sum of the image-based saliency[8][40] and the disparity-based saliency [15]....|
|||...Efficient scale-space spatiotemporal saliency tracking for distortionfree video retargeting....|
||4 instances in total. (in cvpr2018)|
|256|cvpr18-TieNet  Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays|...[32] proposed to correlate the entire image or saliency regions with MeSH terms....|
|||...Afterwards we discuss two enhancements we develop and integrate, i.e., attention-encoded text embedding (AETE) and saliency weighted global average pooling (SW-GAP)....|
|||...(4)  t  This map is encoded with all spatial saliency regions guided by the text attention....|
|||...When only the reports are used, the framework will not have the saliency weighted global average pooling path....|
||4 instances in total. (in cvpr2018)|
|257|Wang_Constructing_Canonical_Regions_CVPR_2016_paper|...d using the displayed information, including geometric information such as curvatures [2, 23], mesh saliency [18] and surface regions of interest determined by some measurement [19], visual informatio...|
|||...They are the methods by evaluating view entropy (VE) [34], mesh saliency (MS)[18], and viewpoint saliency Kullback-Leibler distance (vSKL)[27]....|
|||...To compute saliency values for mesh saliency and viewpoint saliency Kullback-Leibler distances, we employed the CUDA computing platform for fast computation....|
|||...In the statistics, we do not include the time cost of measurements that produce reusable results during preprocessing, such as mesh saliency computed during preprocessing....|
||4 instances in total. (in cvpr2016)|
|258|Yang_Shen_Egocentric_Activity_Prediction_ECCV_2018_paper|...This has led to methods for computation of image saliency [14] which use low-level image features such as color contrast or motion to provide a good explanation of how humans orient their attention....|
|||...However, those lowlevel saliency models performed worse in fixation location prediction compared with those methods based on object-level information [3, 6]....|
|||...[8] using the observed gaze; (b) Two-stream CNN results with object-cnn, SVMfusion and joint training [22]; (c) 2D and 3D Ego ConvNet results (H: Hand mask, C: Camera/Head motion, M: Saliency map) [28]....|
|||...For our baselines, we add the same random noise on the hand mask, saliency map and optical flow....|
||4 instances in total. (in eccv2018)|
|259|Garcia_Person_Re-Identification_Ranking_ICCV_2015_paper|...ese representations have been combined with reference sets [3], patch matching strategies [42, 28], saliency learning [36] and joint attributes [16]; (ii) feature transformations addressing the reiden...|
|||...In the latter, the saliency similarity is computed between the probe and the gallery only, not between galleries themselves....|
|||...Unsupervised Learning of Generative Topic Saliency for Person Re-identification....|
||3 instances in total. (in iccv2015)|
|260|Haque_High_Quality_Photometric_2014_CVPR_paper|...For every pixel p in the depth map, we T where S(q) is a patch centered on p. Following [10], we define an edge saliency measure based on the eigen-values of N ....|
|||...For eigen-values 3  2  1, our edge saliency is given as 21 which is large for strong edges and small for flat regions....|
|||...2(a) shows a representative weighting function obtained using our edge saliency measure where the depth information is given lower weightage at sharp changes or discontinuities in depth....|
||3 instances in total. (in cvpr2014)|
|261|Lei_Chen_Part-Activated_Deep_Reinforcement_ECCV_2018_paper|...To address the problem of the noise from the unrelated parts for actions, we propose a part-activated deep reinforcement learning method to select the saliency parts of features on the human body....|
|||...However, our PA-DRL achieved a higher performance with exploiting the structural information and mining the saliency information of human....|
|||...Lai, S., Zheng, W.S., Hu, J.F., Zhang, J.: Global-local temporal saliency action  prediction....|
||3 instances in total. (in eccv2018)|
|262|Margolin_How_to_Evaluate_2014_CVPR_paper|...Fusing generic objectness and visual saliency for salient object detection....|
|||...A benchmark of computational models of saliency to predict human fixations....|
|||...Geodesic saliency using back ground priors....|
||3 instances in total. (in cvpr2014)|
|263|Wang_Unsupervised_Multi-Class_Joint_2014_CVPR_paper|...The objective function in the continuous phase considers the saliency of each segmentation function, the mutual exclusiveness of different segmentation functions on the same image, as well as the cons...|
|||...(15)  (k,k){IiCkC  k}  The final term evaluates the saliency of each segmentation....|
|||...After computing the segmentation function sik, we comikLisik (agreement with normal pute the saliency score sT ized cuts)....|
||3 instances in total. (in cvpr2014)|
|264|Feng_Interactive_Segmentation_on_CVPR_2016_paper|...Experiments performed on the RGBD saliency dataset [12] and a stereo dataset demonstrates better results than [5]....|
|||...We use RGBD Saliency dataset as a testbed to further conduct comparisons with two additional methods....|
|||...Geodesic saliency using background priors....|
||3 instances in total. (in cvpr2016)|
|265|cvpr18-Normalized Cut Loss for Weakly-Supervised CNN Segmentation|...For simplicity, we consider binary segmentation for MSRA10K saliency dataset [14], which contain simple images with good color clustering....|
|||...Note that here our goal is NOT saliency segmentation, but color clustering using neural networks....|
|||...We fine-tune from pretrained saliency networks....|
||3 instances in total. (in cvpr2018)|
|266|cvpr18-Compare and Contrast  Learning Prominent Visual Differences|...Image Saliency Works modeling saliency (e.g., [13, 16, 33]) have used attributes to predict what regions people tend to look at in images....|
|||...Although saliency may have an influence on prominence, it refers to low-level regions in single  1268  images, whereas prominence is a linguistic, pairwise concept, and the result of a combination of...|
|||...Shallow and deep convolutional networks for saliency prediction....|
||3 instances in total. (in cvpr2018)|
|267|Jain_Predicting_Sufficient_Annotation_2013_ICCV_paper|...Given a novel image, we apply a saliency detector to coarsely estimate the foreground....|
|||...ur masks as input to our method, showing the impact of our features in the absence of errors in the saliency step....|
|||...We attribute this to the complex composition of certain images in IIS that makes saliency detection fail....|
||3 instances in total. (in iccv2013)|
|268|Redi_6_Seconds_of_2014_CVPR_paper|...distance between spectral residual [9] saliency maps of adjacent frames Avg....|
|||...We extract the 462-dimensional namely the Saliency Moments feature [26] from video frames, a holistic representation of the content of an image scene based on the shape of the salient region, which ha...|
|||...We first compute a saliency map of each frame and then retain, as a movement feature, the average of the distances between the maps of neighboring frames:  M = 1/Nf  Nf 1 X  i=1  (SM (Fi), SM (Fi+1)) ...|
||3 instances in total. (in cvpr2014)|
|269|Yonetani_Recognizing_Micro-Actions_and_CVPR_2016_paper|...3d social saliency from head-mounted cameras....|
|||...Predicting primary gaze behavior using social saliency fields....|
|||...Social saliency prediction....|
||3 instances in total. (in cvpr2016)|
|270|Perazzi_A_Benchmark_Dataset_CVPR_2016_paper|...We extract per-frame saliency from CIE-Lab images (SF-LAB, [34]) and from inter-frame motion (SFMOT, [34]), while we use ground-truth to select the hypotheses of the object proposal generator (MCG, [3...|
|||...ifficulty to the class of unsupervised methods, such as NLC and SAL, which adopt distinctive motion saliency as the underlying assumption to predict the object location....|
|||...method which has none or negligible loss of performance in both circumstances, possibly because the saliency computation is still reliable on a subset of the frames, and their random-walk matrix being...|
||3 instances in total. (in cvpr2016)|
|271|Peng_Tang_Weakly_Supervised_Region_ECCV_2018_paper|...By contrast, the later layers tend to respond to more semantic features such as objects or object parts, and the response maps from these layers are similar to the saliency map....|
|||...Dabkowski, P., Gal, Y.: Real time image saliency for black box classifiers....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||3 instances in total. (in eccv2018)|
|272|Le_PDM-ENLOR_Learning_Ensemble_2013_CVPR_paper|...Our method first detects a set of reference points which were selected based on their saliency during training....|
|||...A set of reference points, which were selected during training phase based on a saliency criteria, are detected using PASM-CTX....|
|||...[26], was modified to similaritysaliency concept [20] to evaluate the degree of saliency in appearance for the corresponding anatomical point in multiple images....|
||3 instances in total. (in cvpr2013)|
|273|Temporal Attention-Gated Model for Robust Sequence Classification|...Note that at is the saliency score represented as a scalar value instead of a vector, hence  in the figure means multiplication between a scalar and a vector....|
|||...Temporal Attention Module  The goal of this module is to estimate the saliency and relevance of each sequence observation....|
|||...This saliency score should not only be based on the input observation at the current time step, but also take into consideration information from neighboring observations in both directions....|
||3 instances in total. (in cvpr2017)|
|274|Park_Egocentric_Future_Localization_CVPR_2016_paper|...Our method does not rely on prior processes such as semantic segmentation, object detection, or saliency prediction, which are often fragile to real world scenes or need manual annotations....|
|||...Social saliency prediction....|
|||...3D social saliency from  head-mounted cameras....|
||3 instances in total. (in cvpr2016)|
|275|Santhosh_Kumar_Ramakrishnan_Sidekick_Policy_Learning_ECCV_2018_paper|...Models of saliency and attention allow a system to prioritize portions of its observation to reduce clutter or save computation [42,4,45,68,67]....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
|||...: Saliency detection via graphbased manifold ranking....|
||3 instances in total. (in eccv2018)|
|276|Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper|...category-specific saliency on each location of an image, while the decoder performs foreground segmentation using the saliency map based on category-independent segmentation knowledge....|
|||...Given a feature extracted from the encoder, the attention model estimates adaptive spatial saliency of each category associated with input image (Section 4.2)....|
|||...Decoder  The attention model described in the previous section generates a set of adaptive saliency maps for each category {l}lL , which provides useful information for localization....|
||3 instances in total. (in cvpr2016)|
|277|DeepPermNet_ Visual Permutation Learning|...We also compute the saliency maps of different attributes using the method proposed by Simonyan et al....|
|||...We perform max pooling across channels to generate the saliency maps....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||3 instances in total. (in cvpr2017)|
|278|cvpr18-Referring Relationships|...Next, we report the KL-divergence, which measures the dissimilarity between the two saliency maps and heavily penalizes false positives....|
|||...Mit saliency benchmark, 2015....|
|||...Where should saliency models look next?...|
||3 instances in total. (in cvpr2018)|
|279|Qi_Hedged_Deep_Tracking_CVPR_2016_paper|...[18] construct a discriminative model with features from the first fully-connected layer of R-CNN [11] and a generative model with saliency map for visual tracking....|
|||...This is because CNN-SVM takes advantages of both a RCNN feature based discriminative model (features of f c6 being used) and a back-project saliency map based generative model....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
||3 instances in total. (in cvpr2016)|
|280|Predicting Behaviors of Basketball Players From First Person Videos|...Predicting primary gaze  behavior using social saliency fields....|
|||...3D social saliency from  head-mounted cameras....|
|||...Social saliency prediction....|
||3 instances in total. (in cvpr2017)|
|281|cvpr18-Modifying Non-Local Variations Across Multiple Views|...Other methods, use the correspondence information to force physical constraints on the mutual saliency maps [4][30]....|
|||...Stereoscopic visual saliency prediction based on stereo contrast and stereo focus....|
|||...Co-salient object detection based on deep saliency networks and seed propagation over an integrated graph....|
||3 instances in total. (in cvpr2018)|
|282|Yin_Li_In_the_Eye_ECCV_2018_paper|...Mathe and Sminchesescu [24] proposed to recognize actions by sampling local descriptors from a predicted saliency map....|
|||...For our model, it is helpful to reparameterize gt as a 2D saliency map gt(m, n), where the value of the gaze position are set to one and all others are zero....|
|||...Park, H.S., Jain, E., Sheikh, Y.: 3D social saliency from head-mounted cameras....|
||3 instances in total. (in eccv2018)|
|283|Linchao_Zhu_Compound_Memory_Networks_ECCV_2018_paper|...Second, we introduce a series of hidden saliency descriptors as constituent keys in the memory slots of CMN....|
|||...The multisaliency embedding algorithm learns a hidden saliency descriptor for each genre, which is then stacked as a video representation in CMN....|
|||...Each component hj is used to detect one saliency in a video....|
||3 instances in total. (in eccv2018)|
|284|Matzen_BubbLeNet_Foveated_Imaging_ICCV_2015_paper|...Once we have a saliency map, discovering discriminative elements is much easier since we can use that map as a distribution for sampling candidate visual elements....|
|||...A naive approach to exploring the saliency of a normal image is to generate a large number of bubble images and pass each through the CNN, recording the score for the true class label....|
|||...py (foveal best), foveal performance for each FoveaNet improves significantly, indicating that this saliency landscape is something that we can explore using SGD and backprop to drive  3 In terms of I...|
||3 instances in total. (in iccv2015)|
|285|Xie_InterActive_Inter-Layer_Activeness_CVPR_2016_paper|...As t increases, neurons have larger receptive fields and capture less local details, thus the weighting map is more similar to saliency detection results....|
|||...On the one hand, with the last configuration, neuron activeness provides strong clues for saliency detection....|
|||...Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps....|
||3 instances in total. (in cvpr2016)|
|286|Chen_Multi-Object_Tracking_via_2014_CVPR_paper|...This occurs, for example, due to imperfections in our foreground saliency mask....|
|||...For the basketball videos, we follow [9] and extract foreground features based on on motion saliency of dense point trajectories....|
|||...For PETS2009-S2L1, we use a trained background model together with motion saliency to estimate the foreground features....|
||3 instances in total. (in cvpr2014)|
|287|cvpr18-Fast Video Object Segmentation by Reference-Guided Mask Propagation|...The main sources of information include visual saliency [42] and difference in motion (e.g....|
|||...We used the saliency detection datasets [34, 8] to segment foreground objects and the Pascal VOC dataset [10, 14] for background images....|
|||...Strategy 2 simulates more complex changes and also covers a larger variety of object classes as the saliency detection datasets have more diverse class of objects than the Pascal VOC dataset....|
||3 instances in total. (in cvpr2018)|
|288|Soomro_Unsupervised_Action_Discovery_ICCV_2017_paper|...Knapsack Value: Let the value of each supervoxel be defined by its score of belonging to the foreground action,  n. Each supervoxel in a video contains discriminative information towards an action, ou...|
|||...We define the action distinctness as a combination of  humanness, saliency and motion boundary as follows:  (v  n, Bn, n, x  n) = hmShm(v  + salSsal(v  n, x  n) + mbSmb(v  n, Bn, n) n, x  n),  (4)  wh...|
|||...are the functions to compute supervoxel saliency and motion boundary, respectively....|
||3 instances in total. (in iccv2017)|
|289|Haoshu_Fang_Pairwise_Body-Part_Attention_ECCV_2018_paper|...Early works motivated by human perception are saliency detection [22, 19, 15]....|
|||...Goferman, S., Zelnik-Manor, L., Tal, A.: Context-aware saliency detection....|
|||...Hou, X., Zhang, L.: Saliency detection: A spectral residual approach....|
||3 instances in total. (in eccv2018)|
|290|Kendall_End-To-End_Learning_of_ICCV_2017_paper|...(c) Saliency map (red = stronger saliency)  4.3....|
|||...Model Saliency  (d) What the network sees (input attenuated by saliency)  Figure 4: Saliency map visualization which shows the models effective receptive field for a selected output pixel (indicated b...|
|||...In Figure 4 we show some examples of the models saliency with respect to a predicted pixels disparity....|
||3 instances in total. (in iccv2017)|
|291|Learning Features by Watching Objects Move|...First uNLC computes a per-frame saliency map based on motion by looking for either pixels that move in a mostly static frame or, if the frame contains significant motion, pixels that move in a directi...|
|||...Per-pixel saliency is then averaged over superpixels [1]....|
|||...Finally, it uses a nearest neighbor voting scheme to propagate the saliency across frames....|
||3 instances in total. (in cvpr2017)|
|292|Zhu_Submodular_Object_Recognition_2014_CVPR_paper|...The saliency of a region is modeled in terms of its appearance and spatial location, and salient region detection is achieved by maximizing a submodular objective function....|
|||...By including the entropy term, we observe that the category consistency of selected segments is considered as well as the saliency of each segment....|
|||...On the other hand, if  is too large, pursuing segments purity while considering less on their visual saliency is harmful to the performance....|
||3 instances in total. (in cvpr2014)|
|293|Cho_Weakly-_and_Self-Supervised_ICCV_2017_paper|...The saliency is conventionally generated from edge map or hand-crafted features, which are restricted by fixed design principles and limit  (a)  (b)  (c)  (b) Linear scaling....|
|||...Their method iteratively computes optimal scaling factors for local regions and updates a warped image assisted by edge and saliency map....|
|||...Deep saliency with encoded low level distance map and high level features....|
||3 instances in total. (in iccv2017)|
|294|Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper|...During inference, we use the output from the attention layer to determine the saliency likelihood of an input 3D point....|
|||...The attention score wk is a positive number that indicates the saliency of the input cluster Ck....|
|||... by making use of a triplet loss that takes into account individual descriptor similarities and the saliency of input 3D points....|
||3 instances in total. (in eccv2018)|
|295|cvpr18-Revisiting Dilated Convolution  A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation|...Motivated by [14, 33, 34] , we utilize the saliency detection method [37] to produce saliency maps for training images and take the pixels with low saliency values as background....|
|||...To obtain the object-related region based on the dense localization map, the pixels belonging to the top 30% of the unique largest value are selected as object regions. Saliency maps produced by [37] ...|
|||...Following the settings of [33], we set the pixels with normalized saliency values smaller than 0.06 as background....|
||3 instances in total. (in cvpr2018)|
|296|Mingtao_Feng_3D_Face_Reconstruction_ECCV_2018_paper|...ture light field images have been exploited to improve the performance of many applications such as saliency detection [32], hyperspectral light field imaging [57], material classification [53], image...|
|||...Li, N., Sun, B., Yu, J.: A weighted sparse coding framework for saliency detection....|
|||...Li, N., Ye, J., Ji, Y., Ling, H., Yu, J.: Saliency detection on light field....|
||3 instances in total. (in eccv2018)|
|297|Yufei_Wang_ConceptMask_Large-Scale_Segmentation_ECCV_2018_paper|...groundtruth, one concern is that the test set is not sufficient to test concept segmentation, and a saliency object detection is enough....|
|||...Here by showing the performance of the saliency detection model is poor, we demonstrate that our test dataset is valid for evaluating our task....|
|||...For COCO-80, the IoU for the saliency detection result is very low, whereas for the other test dataset, the saliency performance is higher than that in COCO80....|
||3 instances in total. (in eccv2018)|
|298|Wang_STCT_Sequentially_Training_CVPR_2016_paper|...[14] predicts saliency maps using deep features....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
|||...Deep networks for saliency detection via local estimation and global search....|
||3 instances in total. (in cvpr2016)|
|299|3D Point Cloud Registration for Localization Using a Deep Neural Network Auto-Encoder|...Irrelevant SPs are filtered out by three criteria: density, geometric properties, and saliency levels....|
|||...A base of k eigenvectors are calculated from the depth vectors of the global point cloud SP (similarly to the saliency detection, but here k > 3)....|
|||...s captured online from an unknown position within the global scene, the SP division, normalization, saliency detection, and DAE dimension reduction stages can be carried out in parallel for each SP in...|
||3 instances in total. (in cvpr2017)|
|300|Robust Interpolation of Correspondences for Large Displacement Optical Flow|...The applications include motion segmentation [28], video saliency detection [32], action recognition [38], driver assistance [15, 25], etc....|
|||...Notice how the flat regions with low saliency (see the background in the first column) and the motion discontinuities (see the leg of the character in the third column) are handled properly by our RicFlow....|
|||...Learning video saliency from human gaze using candidate selection....|
||3 instances in total. (in cvpr2017)|
|301|Zhu_Joint_Bi-Layer_Optimization_ICCV_2017_paper|...Second, we explored saliency detection with and without rain using [31]....|
|||...Then, we compare the saliency detection accuracy using the AUC-Judd metric1....|
|||...Hierarchical saliency detection....|
||3 instances in total. (in iccv2017)|
|302|Wang_Action_Recognition_with_2013_ICCV_paper|...Here we only compare to their results when the saliency map is extracted automatically....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
|||...Space-variant descriptor sampling for action recognition based on saliency and eye movements....|
||3 instances in total. (in iccv2013)|
|303|Ionescu_How_Hard_Can_CVPR_2016_paper|...There are many computer vision works analyzing global image properties such as saliency [17, 19, 26, 30, 31], memorability [20, 21], photo quality [29] and  12157  Figure 1....|
|||...A framework for visual saliency detection with applications to image thumbnailing....|
|||...Bottom-up saliency is a dis tern Analysis....|
||3 instances in total. (in cvpr2016)|
|304|cvpr18-Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network|...object detection [11], image classification [29], image denoising [32], image super-resolution [6], saliency detection [13, 33] and object tracking [14, 24]....|
|||...Visual saliency detection based on multiscale IEEE Transactions on Image Processing,  deep CNN features....|
|||...Learning uncertain convolutional features for accurate saliency detection....|
||3 instances in total. (in cvpr2018)|
|305|Zhou_Learning_Deep_Features_CVPR_2016_paper|...Class activation maps from CNN-GAPs and the class-specific saliency map from the backpropagation methods....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||2 instances in total. (in cvpr2016)|
|306|Automatic Understanding of Image and Video Advertisements|...End-to-end saliency mapping via probability distribution prediction....|
|||...Discriminative spatial saliency for image classification....|
||2 instances in total. (in cvpr2017)|
|307|cvpr18-Convolutional Neural Networks With Alternately Updated Clique|...It is proved fruitful in many applications, including image recognition [37, 8], image captioning [3], imagetext matching [29], and saliency detection [24]....|
|||...Recurrent attentional networks for saliency detection....|
||2 instances in total. (in cvpr2018)|
|308|Wang_Multiple_Granularity_Descriptors_ICCV_2015_paper|...Region Discovery  Generating Saliency Heatmap We first create multiple granularity detection networks....|
|||...Our goal is to uncover saliency in their hidden layers to guide selection of ROIs....|
||2 instances in total. (in iccv2015)|
|309|Alayrac_Joint_Discovery_of_ICCV_2017_paper|...r actions that was shown to be beneficial in a weakly supervised setting [7] (referred to as action saliency constraint)....|
|||...While the saliency approach (taking only the most confident detection per video) was useful for action modeling in [7], it is less suitable for our setup where multiple tracklets can be in the same state....|
||2 instances in total. (in iccv2017)|
|310|Siyang_Li_Unsupervised_Video_Object_ECCV_2018_paper|...In MP [31], a binary segmentation neural network uses the optical flow vector as the input and produces a motion saliency map....|
|||...The flow patterns are flipped for the two scenarios yet both expect high motion saliency on objects....|
||2 instances in total. (in eccv2018)|
|311|Shervin_Ardeshir_Integrating_Egocentric_Videos_ECCV_2018_paper|...Zhao, R., Oyang, W., Wang, X.: Person re-identification by saliency learning....|
|||...Park, Hyun, E.J., Sheikh., Y.: Predicting primary gaze behavior using social saliency fields....|
||2 instances in total. (in eccv2018)|
|312|cvpr18-Geometry-Aware Learning of Maps for Camera Localization|...We also computed saliency maps s(x, y) = 1 I(x,y)   (magnitude gradient of the mean of the 6-element output w.r.t....|
|||...We find that compared to PoseNet, MapNet+ focuses more on geometrically meaningful regions and its saliency map is more consistent over time....|
||2 instances in total. (in cvpr2018)|
|313|Supervising Neural Attention Models for Video Captioning by Human Gaze Data|...The ShallowNet [19] is one of the state-ofthe-art methods for saliency or fixation prediction....|
|||...Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition....|
||2 instances in total. (in cvpr2017)|
|314|He_Oriented_Object_Proposals_ICCV_2015_paper|...[1] first sample a set of candidate windows according to a saliency map, and then measure objectness scores of the candidate windows by combining different cues, including multi-scale saliency, color ...|
|||...certain objects, e.g., saliency map, are not appropriate....|
||2 instances in total. (in iccv2015)|
|315|Quan_Object_Co-Segmentation_via_CVPR_2016_paper|...[1]  proposed  to  establish  reliable  correspondences  between  pixels  in  different  images  based  on  the  extracted  saliency  regions....|
|||...Geodesic  saliency   using background priors....|
||2 instances in total. (in cvpr2016)|
|316|Kuo_DeepBox_Learning_Objectness_ICCV_2015_paper|...[14, 10]  Most object proposal methods rely on simple bottom-up grouping and saliency cues....|
|||...This ranking is typically based on low level region features such as saliency [1], and is sometimes learnt [2, 4]....|
||2 instances in total. (in iccv2015)|
|317|Chen_Enriching_Visual_Knowledge_2014_CVPR_paper|...Finally, approaches have tried using other kind of priors including bounding boxes [20], context [19, 18], saliency [8] and object probability [2, 17, 15]....|
|||...They model the sparsity and saliency properties of the common object in images, and construct a large-scale graphical model to jointly infer an object mask for each image....|
||2 instances in total. (in cvpr2014)|
|318|Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper|...region dynamics [3], the waterfall transform [4], extinction values [5], region saliency [6], and (, )-connected components [7]....|
|||...Najman, L., Schmitt, M.: Geodesic saliency of watershed contours and hierarchical IEEE Trans....|
||2 instances in total. (in eccv2018)|
|319|Wang_Additive_Nearest_Neighbor_ICCV_2015_paper|...In particularly, we perform an over-segmentation operator on the image firstly, and then apply a saliency detection method [1] to estimate the importance of each segmented region....|
|||...Right panel shows example of non-uniform spatial sampling + saliency detection....|
||2 instances in total. (in iccv2015)|
|320|Parikshit_Sakurikar_Single_Image_Scene_ECCV_2018_paper|...Several other light-field datasets also exist such as the Stanford light-field archive [31] and the light-field saliency dataset [17]....|
|||...Li, N., Ye, J., Ji, Y., Ling, H., Yu, J.: Saliency detection on light field....|
||2 instances in total. (in eccv2018)|
|321|Li_Delving_Into_Egocentric_2015_CVPR_paper|...Most recently, Mathe and Sminchesescu [23] demonstrate promising results for recognizing actions by sampling local descriptors from a predicted saliency map....|
|||...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
||2 instances in total. (in cvpr2015)|
|322|Shen_Person_Re-Identification_With_ICCV_2015_paper|...To reduce the effect of patch-wise mismatching, some saliency-based approaches [33, 32] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to co...|
|||...Unsupervised learning of generative topic saliency for person re-identification....|
||2 instances in total. (in iccv2015)|
|323|Jie_Zhang_Geometric_Constrained_Joint_ECCV_2018_paper|...Pixel color indicates task-related saliency with respect to input images....|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||2 instances in total. (in eccv2018)|
|324|Fragkiadaki_Learning_to_Segment_2015_CVPR_paper|...l as spatio-temporal tube proposals, and compare with alternative CNN architectures, centersurround saliency and static image objectness....|
|||...mage only CNN (imgCNN), a flow only CNN (flowCNN), our implementation of a standard center-surround saliency measure from optical flow magnitude (center-surround) [14], and an objectness detector usin...|
||2 instances in total. (in cvpr2015)|
|325|cvpr18-CRRN  Multi-Scale Guided Concurrent Reflection Removal Network|...puter vision tasks can be well generalized to inverse imaging tasks such as shadow removal [19] and saliency detection [18]....|
|||...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
||2 instances in total. (in cvpr2018)|
|326|Zhang_Summary_Transfer_Exemplar-Based_CVPR_2016_paper|...Another strategy is to predict object and event saliency [16, 24, 32, 36], or to pose summarization as an anomaly detection problem [19, 49]....|
|||...Some prior work includes supervised learning components (e.g., to generate regions with learned saliency metrics [24], train classifiers for canonical viewpoints [17], or recognize fragments of a part...|
||2 instances in total. (in cvpr2016)|
|327|Cho_Improving_Person_Re-Identification_CVPR_2016_paper|...For this reason, many works have been focused on appearance modeling and learning such as feature learning [7, 25], metric learning [10,18], and saliency learning [24] for the efficient re-id task....|
|||...Similar to the metric learning methods, a saliency learning method was also proposed by R. Zhao et al....|
||2 instances in total. (in cvpr2016)|
|328|cvpr18-Tags2Parts  Discovering Semantic Regions From Shape Tags|...For comparison we use these ablated alternatives:   The saliency map of the trained WU-Net, com puted as the gradient of output w.r.t....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||2 instances in total. (in cvpr2018)|
|329|Yan_Wang_Spatial_Pyramid_Calibration_ECCV_2018_paper|...agation to find the neurons that contribute most to the classification result [43][39], introducing saliency [30][26] or attention [2] into the network, and investigating local properties (e.g., smoot...|
|||...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||2 instances in total. (in eccv2018)|
|330|Multi-Attention Network for One Shot Learning|...Our method is also related to saliency detection [11]....|
|||...Recurrent attentional net works for saliency detection....|
||2 instances in total. (in cvpr2017)|
|331|cvpr18-Tell Me Where to Look  Guided Attention Inference Network|...Dhsnet: Deep hierarchical saliency net work for salient object detection....|
|||...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||2 instances in total. (in cvpr2018)|
|332|Lee_Detecting_Curved_Symmetric_2013_ICCV_paper|...Geometric saliency of curve In  correspondences and grouping of symmetric contours....|
|||...View-based object recognition using saliency maps....|
||2 instances in total. (in iccv2013)|
|333|BranchOut_ Regularization for Online Ensemble Tracking With Convolutional Neural Networks|...CNN-SVM [16] combines a pretrained CNN and online SVMs to obtain target-specific saliency maps for tracking and segmentation....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
||2 instances in total. (in cvpr2017)|
|334|cvpr18-Reflection Removal for Large-Scale 3D Point Clouds|...of 3D real world scene, which are recently used for the research issues of data compression [2] and saliency detection [26, 28]....|
|||...Supervoxel-based saliency detection for large-scale colored 3D point clouds....|
||2 instances in total. (in cvpr2018)|
|335|Yu_Learning_Reconstruction-Based_Remote_CVPR_2016_paper|...In [3], Chen and Ji proposed to utilize saliency map to incrementally learn a distribution of the person dependent parameters and the gaze....|
|||...proposed to use saliency map [14]....|
||2 instances in total. (in cvpr2016)|
|336|Qiao_ScaleNet_Guiding_Object_ICCV_2017_paper|...ds, including superpixel grouping based [1, 20, 42], edge or gradient computation based [7, 46] and saliency and attention detection based [2, 4, 5, 24, 28], are less effective and require a large num...|
|||...Fusing generic objectness and visual saliency for salient object detection....|
||2 instances in total. (in iccv2017)|
|337|Khosla_Understanding_and_Predicting_ICCV_2015_paper|...], AVA dataset [27], affective images dataset [25] (consisting of Art and Abstract datasets), image saliency datasets (MIT1003 [14] and NUSEF [28]), SUN [34], image popularity dataset [16], Abnormal O...|
|||...An eye fixation database for saliency detection in images....|
||2 instances in total. (in iccv2015)|
|338|Paulin_Transformation_Pursuit_for_2014_CVPR_paper|...a saliency map)....|
|||...This is equivalent to weighting patches with the saliency map [25]....|
||2 instances in total. (in cvpr2014)|
|339|cvpr18-SeedNet  Automatic Seed Generation With Deep Reinforcement Learning for Robust Interactive Segmentation|...First, we use the MSRA10K saliency dataset [11] to train and compare our results against the initial results from the initial seed....|
|||...As this system is learned using the saliency dataset, MSRA10K, we test our agent on various single-object binary segmentation datasets instead of the validation images of the MSRA10K datasets....|
||2 instances in total. (in cvpr2018)|
|340|Li_Video_Segmentation_by_2013_ICCV_paper|...These advances have made unsupervised segmentation applicable to difficult high-level tasks, such as semantic segmentation [23, 2, 10] and video saliency reasoning [19]....|
|||...A few new approaches rely on multiple per-frame figure-ground segmentations: [20] utilizes motion saliency to detect the right segments to track, then run successive graph cuts on clips propagating fr...|
||2 instances in total. (in iccv2013)|
|341|Liu_Understanding_Image_Structure_2015_CVPR_paper|...The results of ALC are shown in Table 3, compared with several baseline methods, including the saliency based methods [14], feature detection based method using HoG [6], random chance, and variances o...|
|||...risons show that: 1) Saliency: Due to the intrinsic that our method could be viewed as a structured saliency measurement, it performs much better than all the salience detection methods, i.e., Salienc...|
||2 instances in total. (in cvpr2015)|
|342|cvpr18-Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification|...Person re-identification  by saliency learning....|
|||...Unsupervised learning of generative topic saliency for person re-identification....|
||2 instances in total. (in cvpr2018)|
|343|Daniel_Jakubovitz_Improving_DNN_Robustness_ECCV_2018_paper|...Such methods include DeepFool [19], Fast Gradient Sign Method (FGSM) [8], Jacobian-based Saliency Map Attack (JSMA) [23], Universal Perturbations [20], Adversarial Transformation Networks [2], and more [3]....|
|||...1: Test accuracy for FGSM attack on MNIST (left) and CIFAR-10 (right) for different values of o  4.3 JSMA evaluation  The JSMA (Jacobian-based Saliency Map Attack) [23] attack relies on the computatio...|
||2 instances in total. (in eccv2018)|
|344|Sattar_Prediction_of_Search_2015_CVPR_paper|...For example, Oyekoya and Stendiford compared similarity measures based on a visual saliency model  as well as real human gaze patterns, indicating better performance for gaze [30]....|
|||...In this work we propose to use sampling from the saliency map as a sampling strategy....|
||2 instances in total. (in cvpr2015)|
|345|cvpr18-Context-Aware Deep Feature Compression for High-Speed Visual Tracking|...Neural networks specialising in local and global information have also been utilised in the saliency map estimation task [34,43]....|
|||...Deep networks for saliency detection via local estimation and global search....|
||2 instances in total. (in cvpr2018)|
|346|cvpr18-Where and Why Are They Looking  Jointly Inferring Human Attention and Intentions in Complex Tasks|...Visual saliency [13] describes image regions which attract the attention of observers outside the image....|
|||...Augmented saliency model using automatic 3d head pose detection and learned gaze following in natural scenes....|
||2 instances in total. (in cvpr2018)|
|347|Bertasius_Unsupervised_Learning_of_ICCV_2017_paper|...To achieve this goal, we formulate an important object  1Figure-ground segmentation, and saliency detection are a line of work  that addresses the relationship with the photographer....|
|||...Furthermore, we include three popular visual saliency methods: (1) Judd [11], (2) GBVS [9], and (3) Deep Contrast Saliency method [13]....|
||2 instances in total. (in iccv2017)|
|348|Yonetani_Ego-Surfing_First-Person_Videos_2015_CVPR_paper|...3D Social Saliency from Head-mounted Cameras....|
|||...IEEE InBehavior using Social Saliency Fields....|
||2 instances in total. (in cvpr2015)|
|349|cvpr18-A Low Power, High Throughput, Fully Event-Based Stereo System|... throughput Convolutional neural networks (CNNs) [22], character recognition [53], optic flow [11], saliency [3], and gesture recognition [2]....|
|||...Visual saliency on networks of neurosynaptic cores....|
||2 instances in total. (in cvpr2018)|
|350|Jas_Image_Specificity_2015_CVPR_paper|...Bottom-up saliency models [20, 22] study which image features predict eye fixations....|
|||... Lins similarity [33] or word2vec5 when measuring specificity, exploring the potential of low-level saliency and objectness maps in predicting specificity, studying specificity in more controlled sett...|
||2 instances in total. (in cvpr2015)|
|351|cvpr18-Feature Quantization for Defending Against Distortion of Images|...We employ gradient methods to obtain a saliency map that records pixel-wise classification scores....|
|||...We employ a pre-trained Plain-18 [17] network to compute the saliency map....|
||2 instances in total. (in cvpr2018)|
|352|Chang_Propagated_Image_Filtering_2015_CVPR_paper|...[15] proposed an algorithm which is able to fuse images, with the weights determined by the corresponding saliency or detailed image information....|
|||...3By courtesy of http://www.flickr.com/photos/hhoyer/7105107291  4By courtesy of http://www.imgfsr.com/ifsr ifs1.html  Figure 8: Input images and saliency weights recovered by the  corresponding images....|
||2 instances in total. (in cvpr2015)|
|353|cvpr18-Learning Pixel-Level Semantic Affinity With Image-Level Supervision for Weakly Supervised Semantic Segmentation|...They substantially outperform the approaches based on the same level of supervision but with extra data and annotations like segmentation labels in MS-COCO [20], class-agnostic bounding boxes in MSRA ...|
|||...Exploiting saliency for object segmentation from image level labels....|
||2 instances in total. (in cvpr2018)|
|354|Tao_Siamese_Instance_Search_CVPR_2016_paper|...[17] focuses on learning target-specific saliency map using pre-trained ImageNet network....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
||2 instances in total. (in cvpr2016)|
|355|cvpr18-A2-RL  Aesthetics Aware Reinforcement Learning for Image Cropping|...This method computes the saliency maps with algorithms from [30], and search the best cropping window by maximizing the difference of average saliency between the cropping window and other region....|
|||...Large-scale optimization of hierarchical features for saliency prediction in natural images....|
||2 instances in total. (in cvpr2018)|
|356|Wang_Super-Trajectory_for_Video_ICCV_2017_paper|...The works [10, 43, 46] introduce saliency information [45] as prior knowledge to infer the object....|
|||...Correspondence  driven saliency transfer....|
||2 instances in total. (in iccv2017)|
|357|Yun_Studying_Relationships_between_2013_CVPR_paper|...These latter influences can all be considered saliency factors affecting object importance....|
|||...Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes....|
||2 instances in total. (in cvpr2013)|
|358|Sun_Human_Action_Recognition_ICCV_2015_paper|...The shown saliency maps suggest that learned parameters in FSTCN can capture the most representative regions of action categories....|
|||...The visualization of the saliency map which maximize the output score, from left to right, from top to down, the category is smile, clap, pull-up and climbing....|
||2 instances in total. (in iccv2015)|
|359|NIKOLAOS_ZIOULIS_OmniDepth_Dense_Depth_ECCV_2018_paper|...Likewise, in SalNet360 [22], saliency predictions on the cubes faces are refined using their spherical coordinates and then merged back to 360o....|
|||...Monroy, R., Lutz, S., Chalasani, T., Smolic, A.: Salnet360: Saliency maps for  omni-directional images with cnn....|
||2 instances in total. (in eccv2018)|
|360|Tang_Co-localization_in_Real-World_2014_CVPR_paper|...We compute an off-the-shelf saliency map for each image [6, 23], and for each box we compute the average saliency within the box, weighted by the size of the box, and stack these values into the nb di...|
|||...(2)  Although objectness also provides scores for each box, we found that the saliency measure used in objectness is dated and does not work as well....|
||2 instances in total. (in cvpr2014)|
|361|Borji_Human_vs._Computer_2014_CVPR_paper|...Lastly, siagianItti07 is constructed from the maps of a saliency model [24, 29, 30]....|
|||...Quantitative analysis of human-model agree ment in visual saliency modeling: a comparative study....|
||2 instances in total. (in cvpr2014)|
|362|Koh_CDTS_Collaborative_Detection_ICCV_2017_paper|...In [9, 36, 38], saliency maps are used for the initial estimation of a primary object....|
|||...These saliency-based techniques are vulnerable to inaccurate saliency detection results due to background clutters or background motions....|
||2 instances in total. (in iccv2017)|
|363|Gatys_Image_Style_Transfer_CVPR_2016_paper|...Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet....|
|||...Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps....|
||2 instances in total. (in cvpr2016)|
|364|cvpr18-Learning to Localize Sound Source in Visual Scenes|...This localization method corresponds to visual saliency which is not interactively estimated according to the given sound....|
|||...Additionally, this work would open many potential directions for future research, i.e., multi-modal retrieval, sound based saliency or representation learning and its applications....|
||2 instances in total. (in cvpr2018)|
|365|Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper|...These include grammaticality, saliency (covering main aspects), correctness/truthfulness, etc....|
|||...That is, the IDF provides a measure of word saliency by discounting popular words that are likely to be less visually informative....|
||2 instances in total. (in cvpr2015)|
|366|Shi_Transferring_a_Semantic_2015_CVPR_paper|...Related generative models include [55] which used unsupervised topic models to estimate saliency for re-id, and [16, 11] which addressed data driven attribute discovery and learning....|
|||...Unsupervised learning of generative topic saliency for person re-identification....|
||2 instances in total. (in cvpr2015)|
|367|Superpixel-Based Tracking-By-Segmentation Using Markov Chains|...AMC has been studied for several computer vision tasks, which include image matching [7], image segmentation [15], co-activity detection [41] and saliency detection [19]....|
|||...The other datasets are constructed for other kinds of tasks such as foreground and background segmentation [25, 24, 23] and video saliency detection [11]....|
||2 instances in total. (in cvpr2017)|
|368|cvpr18-Bootstrapping the Performance of Webly Supervised Semantic Segmentation|...In [11, 30], a network is firstly trained with simple images from the internet and the corresponding masks estimated using saliency detection....|
|||...Exploiting saliency for object segmentation from image level labels....|
||2 instances in total. (in cvpr2018)|
|369|Teng_Robust_Object_Tracking_ICCV_2017_paper|...me CNN based tracking methods combine a CNN model with conventional tracking techniques such as the saliency map and SVM used in the DSCNN tracker [23], correlation filters employed in the HCFT tracke...|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
||2 instances in total. (in iccv2017)|
|370|Zhu_A_Text_Detection_CVPR_2016_paper|...The saliency map of the coarse detector will then be used as a reference for fine detection....|
|||...Region Growing  The thresholded fine detection saliency map provides seeds for a flood-filling type of region growing, in order to form CCs....|
||2 instances in total. (in cvpr2016)|
|371|Yu_Cross-View_Asymmetric_Metric_ICCV_2017_paper|...Unsupervised learning of generative topic saliency for person re-identification....|
|||...Person re-identification  by saliency learning....|
||2 instances in total. (in iccv2017)|
|372|Hua_Edge-aware_Gradient_Domain_2014_CVPR_paper|...(2) and (6) respectively, and sd(p) is the context-aware saliency value of p for detecting the image regions that represent the scene [16]....|
|||...Context-aware saliency detection....|
||2 instances in total. (in cvpr2014)|
|373|cvpr18-Image-Image Domain Adaptation With Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification|...Some methods are based on saliency statistics [50, 44]....|
|||...Unsupervised learning of generative topic saliency for person re-identification....|
||2 instances in total. (in cvpr2018)|
|374|Zhang_3D_Fragment_Reassembly_ICCV_2015_paper|...The pipeline is practically effective in composing small pieces that lack geometric saliency and have small overlap regions with adjacent pieces....|
|||...Sparse points matching by combining 3d mesh saliency with statistical descriptors....|
||2 instances in total. (in iccv2015)|
|375|Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper|...[18] learns target-specific saliency map using a pre-trained CNN....|
|||...Online tracking by learning discriminative saliency map with convolutional neural network....|
||2 instances in total. (in iccv2015)|
|376|Lu_Online_Object_Tracking_2014_CVPR_paper|...arning [4], the struck [14], the circulant structure-based kernel method [15], and the discriminant saliency based tracking [25]....|
|||...Biologically inspired object tracking using center-surround saliency mechanisms....|
||2 instances in total. (in cvpr2014)|
|377|Multi-Context Attention for Human Pose Estimation|...], object recognition [2, 17, 6, 40], image captioning [47, 41], image question answering [46], and saliency detection [26]....|
|||...Recurrent attentional net works for saliency detection....|
||2 instances in total. (in cvpr2017)|
|378|cvpr18-Weakly Supervised Instance Segmentation Using Class Peak Response|...gh effective, these  Figure 2: Compared to existing weakly supervised methods which aim to obtain a saliency map (middle) for each class, the proposed approach extracts fine-detailed representation (r...|
|||...Based on the deep responses, top-down attention methods are proposed to generate refined class saliency maps by exploring visual attention evidence [4, 43]....|
||2 instances in total. (in cvpr2018)|
|379|Wang_Multi-Label_Image_Recognition_ICCV_2017_paper|...Visual attention model  Attention model has been recently applied to various computer vision tasks, including image classification [21, 1], saliency detection [19], and image captioning [28]....|
|||...tional networks for saliency detection....|
||2 instances in total. (in iccv2017)|
|380|Manen_Prime_Object_Proposals_2013_ICCV_paper|...al extensions, including additional cues and using discriminative training [21], fusion with region saliency [4] and generalization to video using motion segmentation [25]....|
|||...Fusing generic objectness and visual saliency for salient object detection....|
||2 instances in total. (in iccv2013)|
|381|Perazzi_Fully_Connected_Object_ICCV_2015_paper|...Relying on saliency detection this method might encounter difficulties to segment complex objects of multiple colors....|
|||...Finally, Faktor and Irani (NLC, [17]) consolidate an initial foreground estimate based on saliency using a Markov chain....|
||2 instances in total. (in iccv2015)|
|382|Beery_Recognition_in_Terra_ECCV_2018_paper|...[16] demonstrates a graph-cut method that uses background modeling and foreground object saliency to segment foreground in camera trap sequences....|
|||...(5) Camouflage: decreases saliency in animals natural habitat....|
||2 instances in total. (in eccv2018)|
|383|cvpr18-Glimpse Clouds  Human Activity Recognition From Unstructured Feature Points|...Attention vs. saliency vs. random  We evaluated whether a sequential attention process contributes to performance, or whether the gain is solely explained from the sampling of local features in the sp...|
|||...Recurrent attentional networks for saliency detection....|
||2 instances in total. (in cvpr2018)|
|384|Zhang_Parallax-tolerant_Image_Stitching_2014_CVPR_paper|...Here ws measures the saliency value of the triangle (cid:3)  V1  V3 using the same method as [14]....|
|||...We use this saliency weight to distribute more distortion to less salient regions than those salient ones....|
||2 instances in total. (in cvpr2014)|
|385|Kim_SOWP_Spatially_Ordered_ICCV_2015_paper|...RWR has many applications, such as data mining [35], and saliency detection [25], as well as interactive image segmentation....|
|||...Multiscale saliency detection using random walk with restart....|
||2 instances in total. (in iccv2015)|
|386|Li_Image_Understanding_from_2013_CVPR_paper|...ttention is influenced by two main sources of input: bottom-up visual attention driven by low-level saliency image features and top-down process in which cognitive processes, guided by the viewing tas...|
|||...Modeling spatio-temporal saliency to predicit gaze direction for short videos....|
||2 instances in total. (in cvpr2013)|
|387|Simon_Neural_Activation_Constellations_ICCV_2015_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in iccv2015)|
|388|Quan_Sparse_Coding_for_CVPR_2016_paper|...Ensemble dictionary learnIMAGE VISION COMPUT,  ing for saliency detection....|
||1 instances in total. (in cvpr2016)|
|389|Liu_GRASP_Recurring_Patterns_2013_CVPR_paper|...The potential applications of an automated recurring pattern discovery tool are enormous, ranging from image registration, segmentation, people/product counting, surveillance to saliency perception....|
||1 instances in total. (in cvpr2013)|
|390|Zhu_Soft_Proposal_Networks_ICCV_2017_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in iccv2017)|
|391|Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper|...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in eccv2018)|
|392|Lu_Story-Driven_Summarization_for_2013_CVPR_paper|...This application of our work may be useful for video retrieval or video saliency detection applications....|
||1 instances in total. (in cvpr2013)|
|393|Singh_Hide-And-Seek_Forcing_a_ICCV_2017_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in iccv2017)|
|394|Cinbis_Multi-fold_MIL_Training_2014_CVPR_paper|...[28] instead estimate an unsupervised patch-level saliency map  for a given image by measuring the average similarity of each patch to the other patches in a retrieved set of similar images....|
||1 instances in total. (in cvpr2014)|
|395|Wang_Improving_Human_Action_CVPR_2016_paper|...They often measure the actionness by fusing different feature channels such as space-time saliency [24], optical flow [7], body configuration [16] and deep learning features [9], sometimes with human ...|
||1 instances in total. (in cvpr2016)|
|396|Liu_Stepwise_Metric_Promotion_ICCV_2017_paper|...Another saliency matching approach, Unsupervised Saliency Learning (USL) [47], matches persons by building dense correspondence between image pairs and learning human salience on patch level....|
||1 instances in total. (in iccv2017)|
|397|Video Segmentation via Multiple Granularity Analysis|...Recently, MIL successfully pushed forward online visual tracking [48, 3], saliency detection [47] and image retrieval [23]....|
||1 instances in total. (in cvpr2017)|
|398|Edo_Collins_Deep_Feature_Factorization_ECCV_2018_paper|...[28] are unsupervised and rely on a Markov random field formulation, where the unary features are based on surface image features and various saliency heuristics....|
||1 instances in total. (in eccv2018)|
|399|Khoreva_Weakly_Supervised_Object_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|400|Hanyu_Wang_Learning_3D_Keypoint_ECCV_2018_paper|...In future work, we would like to extend our flexible approach to other data-driven 3D vision applications, e.g., shape segmentation, 3D saliency detection, etc....|
||1 instances in total. (in eccv2018)|
|401|Shi_Bayesian_Joint_Topic_2013_ICCV_paper|...arance of individual classes (e.g., by obtaining the opinion of a generic object detector or object saliency model [1] on images labelled with class c), then this can be encoded via the appearance pri...|
||1 instances in total. (in iccv2013)|
|402|Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper|...A deep multi-level network for saliency prediction....|
||1 instances in total. (in iccv2017)|
|403|Relja_Arandjelovic_Objects_that_Sound_ECCV_2018_paper|...Furthermore, to completely reject the saliency hypothesis  in the case of an image depicting a piano and a flute, it is possible to play a flute sound and the network will pick the flute, while if a p...|
||1 instances in total. (in eccv2018)|
|404|cvpr18-SBNet  Sparse Blocks Network for Fast Inference|...re such masks are not directly available from the inputs, we can predict them in the form of visual saliency [16] or objectness prior [20] by using another relatively cheap network or even a part of t...|
||1 instances in total. (in cvpr2018)|
|405|Caicedo_Active_Object_Localization_ICCV_2015_paper|...These models are generally based on a saliency map that aggregates low-level features to identify interesting regions....|
||1 instances in total. (in iccv2015)|
|406|Multi-Task Correlation Particle Filter for Robust Object Tracking|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2017)|
|407|Sakurikar_Composite_Focus_Measure_ICCV_2017_paper|...We use 100 focal stacks from the light-field saliency dataset [16] representing everyday scenes and having focal resolution from 3 slices to 12....|
||1 instances in total. (in iccv2017)|
|408|Zhang_Casual_Stereoscopic_Panorama_2015_CVPR_paper|...wi is the average saliency value inside the triangle defined by the three vertices and is computed using the same method as [15]....|
||1 instances in total. (in cvpr2015)|
|409|Joo_Panoptic_Studio_A_ICCV_2015_paper|...Predicting primary gaze  behavior using social saliency fields....|
||1 instances in total. (in iccv2015)|
|410|Zheng_Shou_AutoLoc_Weakly-supervised_Temporal_ECCV_2018_paper|...Karaman, S., Seidenari, L., Bimbo, A.D.: Fast saliency based pooling of fisher  encoded dense trajectories....|
||1 instances in total. (in eccv2018)|
|411|Yijun_Li_A_Closed-form_Solution_ECCV_2018_paper|...: Saliency detection via graph based manifold ranking....|
||1 instances in total. (in eccv2018)|
|412|Abdullah_Abuolaim_Revisiting_Autofocus_for_ECCV_2018_paper|...Li, N., Ye, J., Ji, Y., Ling, H., Yu, J.: Saliency detection on light field....|
||1 instances in total. (in eccv2018)|
|413|Guoliang_Kang_Deep_Adversarial_Attention_ECCV_2018_paper|...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in eccv2018)|
|414|Cheng_SegFlow_Joint_Learning_ICCV_2017_paper|...Several methods have been proposed to generate object segmentation via saliency [31, 11, 42], optical flow [4, 28] or superpixel [17, 46, 13]....|
||1 instances in total. (in iccv2017)|
|415|Nagpal_Face_Sketch_Matching_ICCV_2017_paper|...Composite sketch recognition using saliency and attribute feedback....|
||1 instances in total. (in iccv2017)|
|416|Low-Rank Bilinear Pooling for Fine-Grained Classification|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|417|Kolaman_Amplitude_Modulated_Video_CVPR_2016_paper|...2, 5  [7] S. He and R. W. Lau, Saliency detection with flash and no-flash image pairs, in Computer VisionECCV 2014....|
||1 instances in total. (in cvpr2016)|
|418|Wu_Watch-n-Patch_Unsupervised_Understanding_2015_CVPR_paper|...Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition....|
||1 instances in total. (in cvpr2015)|
|419|Busta_Deep_TextSpotter_An_ICCV_2017_paper|...Given the saliency maps, word bounding boxes are then obtained by the run length smoothing algorithm....|
||1 instances in total. (in iccv2017)|
|420|Adrien_Kaiser_Proxy_Clouds_for_ECCV_2018_paper|...For flat cells whose distribution has two or more modes, we do not perform any projection in order to keep the saliency of the surface....|
||1 instances in total. (in eccv2018)|
|421|Mao_Generation_and_Comprehension_CVPR_2016_paper|...However, in real applications, region saliency p(R I) should be taken into account....|
||1 instances in total. (in cvpr2016)|
|422|cvpr18-Features for Multi-Target Multi-Camera Tracking and Re-Identification|...Discriminative power is improved by saliency information [50, 80] or by learning features specific to body parts [14, 20, 21, 26, 27, 39, 42], either in the image [6, 7, 24] or back-projected onto an ...|
||1 instances in total. (in cvpr2018)|
|423|Wei-Sheng_Lai_Real-Time_Blind_Video_ECCV_2018_paper|...Temporal filtering is an efficient approach to extend image-based algorithms to videos, e.g., tone-mapping [1], color transfer [5], and visual saliency [25] to generate temporally consistent results....|
||1 instances in total. (in eccv2018)|
|424|Sun_Sparsifying_Neural_Network_CVPR_2016_paper|...For OBD, parameters with the largest saliency values defined by the second order derivatives of parameters are reserved, and then the sparsified model is re-trained....|
||1 instances in total. (in cvpr2016)|
|425|Wang_Understanding_and_Diagnosing_ICCV_2015_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in iccv2015)|
|426|Lu_Localize_Me_Anywhere_ICCV_2015_paper|...Complex event detection using semantic saliency and nearly-isotonic svm....|
||1 instances in total. (in iccv2015)|
|427|Nam_Learning_Multi-Domain_Convolutional_CVPR_2016_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2016)|
|428|FusionSeg_ Learning to Combine Motion and Appearance for Fully Automatic Segmentation of Generic Objects in Videos|...which normalizes the flow by applying a saliency detection method [21] to the flow image itself....|
||1 instances in total. (in cvpr2017)|
|429|Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper|...propose a saliency based video summarization method, which trains a linear regression model to predict the importance score for each frame in egocentric videos [12]....|
||1 instances in total. (in cvpr2015)|
|430|Song_CREST_Convolutional_Residual_ICCV_2017_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in iccv2017)|
|431|Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2016)|
|432|Tokmakov_Learning_Video_Object_ICCV_2017_paper|...The final object segmentation is obtained by integrating these appearance models with other cues, e.g., saliency maps [45], shape estimates [25], pairwise constraints [31]....|
||1 instances in total. (in iccv2017)|
|433|Ramanathan_Learning_Semantic_Relationships_2015_CVPR_paper|...Discriminative spaIn Computer Vision tial saliency for image classification....|
||1 instances in total. (in cvpr2015)|
|434|Tan_Multipoint_Filtering_with_2014_CVPR_paper|...This attribute makes it a powerful method in many computer vision and graphics applications including stereo matching [3, 4], optical flow estimation [4], colorization [5], and saliency detection [6]....|
||1 instances in total. (in cvpr2014)|
|435|cvpr18-Recurrent Pixel Embedding for Instance Grouping|...Geodesic saliency of watershed contours and hierarchical segmentation....|
||1 instances in total. (in cvpr2018)|
|436|Deep Quantization_ Encoding Convolutional Activations With Deep Generative Model|...For instance, a saliency weight is learnt and assigned to each local region in [48], and a spatial transformer is trained to reduce the effect of translation and rotation as preprocess in [10]....|
||1 instances in total. (in cvpr2017)|
|437|Lin_Visualizing_and_Understanding_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|438|Oquab_Is_Object_Localization_2015_CVPR_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2015)|
|439|Bidirectional Multirate Reconstruction for Temporal Modeling in Videos|...Complex event detection using semantic saliency and nearly-isotonic svm....|
||1 instances in total. (in cvpr2017)|
|440|Fanello_Ask_the_Image_2014_CVPR_paper|...sed on a combination of multiple features (SIFT,HOG,LBP) and object detectors coupled with a visual saliency map....|
||1 instances in total. (in cvpr2014)|
|441|Misra_Watch_and_Learn_2015_CVPR_paper|...2) We do not assume strong motion or appearance saliency of objects, thus discovering static object instances as well....|
||1 instances in total. (in cvpr2015)|
|442|cvpr18-Future Person Localization in First-Person Videos|...3D Social Saliency from Head-mounted Cameras....|
||1 instances in total. (in cvpr2018)|
|443|Wang_Joint_Learning_of_CVPR_2016_paper|...analysis (PCCA) [28], aPRDC [23], PRDC [43], enriched BiCov (eBiCov) [25], PRSVM [2], and ELF [10], saliency matching (SalMatch) [40], patch matching (PatMatch) [40], locallyadaptive decision function...|
||1 instances in total. (in cvpr2016)|
|444|Ghodrati_DeepProposal_Hunting_Objects_ICCV_2015_paper|...[1] propose an objectness measure based on image saliency and other cues like color and edges....|
||1 instances in total. (in iccv2015)|
|445|Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes|...Hierarchical saliency detection....|
||1 instances in total. (in cvpr2017)|
|446|Li_A_Geodesic-Preserving_Method_2015_CVPR_paper|...For fair comparisons, we adopt the same saliency map as in [7] that weights the shape-preserving terms in Eqn.(7)....|
||1 instances in total. (in cvpr2015)|
|447|Chu_Online_Multi-Object_Tracking_ICCV_2017_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in iccv2017)|
|448|cvpr18-End-to-End Learning of Keypoint Detector and Descriptor for Pose Invariant 3D Matching|...mages, xm t = (xt, yt) are 2D coordinates on the image plane, sm is the score which t signifies the saliency level of the keypoint, and f m is the t corresponding feature vector....|
||1 instances in total. (in cvpr2018)|
|449|Yang_End-To-End_Learning_of_CVPR_2016_paper|...Deep networks for saliency detection via local estimation and global search....|
||1 instances in total. (in cvpr2016)|
|450|Veeriah_Differential_Recurrent_Neural_ICCV_2015_paper|...This inspired us to develop a new family of LSTM model that automatically learns the dynamic saliency of the actions performed....|
||1 instances in total. (in iccv2015)|
|451|Abhimanyu_Dubey_Improving_Fine-Grained_Visual_ECCV_2018_paper|...To measure the regions the CNN localizes on, we utilize Gradient-Weighted Class Activation Mapping (Grad-CAM) [53], a method that provides a heatmap of visual saliency as produced by the network....|
||1 instances in total. (in eccv2018)|
|452|Forecasting Interactive Dynamics of Pedestrians With Fictitious Play|...3d social saliency from head mounted cameras....|
||1 instances in total. (in cvpr2017)|
|453|Vedantam_Learning_Common_Sense_ICCV_2015_paper|...also link the semantics of a scene to memorability and saliency of objects [36]....|
||1 instances in total. (in iccv2015)|
|454|cvpr18-Adversarial Complementary Learning for Weakly Supervised Object Localization|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|455|Weakly Supervised Cascaded Convolutional Networks|...To handle the problem of not being able to generate enough candidate proposals because of fixed shape and size, object saliency [9, 28, 29] based approaches were proposed to extract region proposals....|
||1 instances in total. (in cvpr2017)|
|456|Ilchae_Jung_Real-Time_MDNet_ECCV_2018_paper|...In addition, while its multi-domain learning framework concentrates on saliency of target against background in each domain, it is not optimized to distinguish potential target instances across multip...|
||1 instances in total. (in eccv2018)|
|457|Xu_Learning_Receptive_Fields_2014_CVPR_paper|...Biologically plausible saliency mechanisms improve feedforward object recognition....|
||1 instances in total. (in cvpr2014)|
|458|Fu_Robust_Image_Segmentation_ICCV_2015_paper|...To overcome this limitation, contour-based methods, such as gPb-OWT-UCM [2], and saliency driven total variation (SDTV) [7] were developed to find connected regions blocked by detected contours....|
||1 instances in total. (in iccv2015)|
|459|Ferstl_Image_Guided_Depth_2013_ICCV_paper|...They used a combination of different weighting terms of a least squares optimization including segmentation, image gradients, edge saliency and non-local means for depth upsampling....|
||1 instances in total. (in iccv2013)|
|460|Oneata_Action_and_Event_2013_ICCV_paper|...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
||1 instances in total. (in iccv2013)|
|461|Sultani_What_If_We_CVPR_2016_paper|...We use this normalize score to represent motion saliency of each proposal....|
||1 instances in total. (in cvpr2016)|
|462|Wang_Supervised_Kernel_Descriptors_2013_CVPR_paper|...Discriminative spatial saliency for image  classification....|
||1 instances in total. (in cvpr2013)|
|463|DeepNav_ Learning to Navigate Large Cities|...Visual saliency based on multiscale deep features....|
||1 instances in total. (in cvpr2017)|
|464|cvpr18-Interpret Neural Networks by Identifying Critical Data Routing Paths|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|465|Cheng_Person_Re-Identification_by_CVPR_2016_paper|...Complex event detection using semantic saliency and nearly-isotonic svm....|
||1 instances in total. (in cvpr2016)|
|466|cvpr18-What Do Deep Networks Like to See |...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|467|Asynchronous Temporal Fields for Action Recognition|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|468|Yuan_Temporal_Action_Localization_CVPR_2016_paper|...The CNN [19] has been dominating in image classification [15, 29], and its intermediate features generated by hidden layers have been widely used in object detection, segmentation and saliency applications....|
||1 instances in total. (in cvpr2016)|
|469|Su_Reasoning_About_Fine-Grained_ICCV_2017_paper|...Nevertheless, attribute saliency is a signal we did not model explicitly and may be used to train better speakers and listeners (e.g., see Turakhia and Parikh [42])....|
||1 instances in total. (in iccv2017)|
|470|Learning to Rank Retargeted Images|...factor is measured by a bidirectional similarity metric Q3 that takes into account the influence of saliency [28]:  Q3 = 0.5  0.5  1 Ns  1 Nt  PU Is  SU minV It D(U,V )  maxU Is (SU minV It D(U,V )) +...|
||1 instances in total. (in cvpr2017)|
|471|cvpr18-Analysis of Hand Segmentation in the Wild|...CRFs are well known for being useful in refining pixel-level predictions for computer vision problems such as saliency detection and semantic segmentation....|
||1 instances in total. (in cvpr2018)|
|472|cvpr18-Social GAN  Socially Acceptable Trajectories With Generative Adversarial Networks|...Social saliency prediction....|
||1 instances in total. (in cvpr2018)|
|473|Zitnick_Bringing_Semantics_into_2013_CVPR_paper|...Absolute spatial location: It is known that the position of an object is related to its perceived saliency [33] and can even convey its identity [23]....|
||1 instances in total. (in cvpr2013)|
|474|Yihua_Cheng_Appearance-Based_Gaze_Estimation_ECCV_2018_paper|...ecent intelligent systems, with direct applications ranging from human-computer interaction [1, 2], saliency detection [3] to video surveillance [4]....|
||1 instances in total. (in eccv2018)|
|475|Cao_Egocentric_Gesture_Recognition_ICCV_2017_paper|...Ego ConveNet [24] uses hand-crafted egocentric cues (including hand masks, head motions and saliency maps) as input to a 2D CNN or 3D CNN model....|
||1 instances in total. (in iccv2017)|
|476|San_Biagio_Heterogeneous_Auto-similarities_of_2013_ICCV_paper|...rent computer vision tasks, i.e., to optimally align an image to a given template [19] or to detect saliency regions in an image [9], [10]....|
||1 instances in total. (in iccv2013)|
|477|Deza_Understanding_Image_Virality_2015_CVPR_paper|...However, an image that may seem more viral visualized through saliency [22] (e.g....|
||1 instances in total. (in cvpr2015)|
|478|Shou_Temporal_Action_Localization_CVPR_2016_paper|...[17] used FV encoding of iDT with weighted saliency based pooling, and conducted late fusion with frame-level CNN features....|
||1 instances in total. (in cvpr2016)|
|479|cvpr18-Unsupervised Cross-Dataset Person Re-Identification by Transfer Learning of Spatial-Temporal Patterns|...Unsupervised learning of generative topic saliency for person re-identification....|
||1 instances in total. (in cvpr2018)|
|480|Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper|...sed to learn the most discriminative attribute that characterizes a particular individual, in which saliency detection is utilized to drive automatically the PTZ camera to (2) Methods on focus on cert...|
||1 instances in total. (in cvpr2015)|
|481|Xiong_Storyline_Representation_of_ICCV_2015_paper|...3d social saliency from  head-mounted cameras....|
||1 instances in total. (in iccv2015)|
|482|Shen_Shadow_Optimization_From_2015_CVPR_paper|...We are also inspired by the work on saliency estimation [26]....|
||1 instances in total. (in cvpr2015)|
|483|Sijia_Cai_Weakly-supervised_Video_Summarization_ECCV_2018_paper|...autoencoder for learning the latent semantics from web videos, and an encoder-attention-decoder for saliency estimation of raw video and summary generation....|
||1 instances in total. (in eccv2018)|
|484|cvpr18-Bidirectional Retrieval Made Simple|...They introduce a multimodal contextmodulated attention scheme at each time-step, which is capable of focusing on a text-image pair by predicting pairwise instance-aware saliency maps....|
||1 instances in total. (in cvpr2018)|
|485|Benjamin_Coors_SphereNet_Learning_Spherical_ECCV_2018_paper|...Monroy, R., Lutz, S., Chalasani, T., Smolic, A.: Salnet360: Saliency maps for omni directional images with cnn....|
||1 instances in total. (in eccv2018)|
|486|Gu_An_Empirical_Study_ICCV_2017_paper|...Recurrent attentional net works for saliency detection....|
||1 instances in total. (in iccv2017)|
|487|cvpr18-Texture Mapping for 3D Reconstruction With RGB-D Sensor|...In the future, we would like to import the visual saliency information [24] into our framework for more detailed texture recovery....|
||1 instances in total. (in cvpr2018)|
|488|Yang_Clothing_Co-Parsing_by_2014_CVPR_paper|...(4) is determined by the automated saliency detection [25]....|
||1 instances in total. (in cvpr2014)|
|489|A Unified Approach of Multi-Scale Deep and Hand-Crafted Features for Defocus Estimation|...Deep saliency with encoded low level distance map and high level features....|
||1 instances in total. (in cvpr2017)|
|490|Weakly-Supervised Visual Grounding of Phrases With Linguistic Structures|...This result demonstrates that our model learns to focus on the right concepts instead of simply computing a language-independent saliency map....|
||1 instances in total. (in cvpr2017)|
|491|Saleh_Bringing_Background_Into_ICCV_2017_paper|...Exploiting saliency for object segmentation from image level labels....|
||1 instances in total. (in iccv2017)|
|492|Action-Decision Networks for Visual Tracking With Deep Reinforcement Learning|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2017)|
|493|Liao_Person_Re-Identification_by_2015_CVPR_paper|...Recently, saliency information has been investigated for person re-identification [47, 46, 29], leading to a novel feature representation....|
||1 instances in total. (in cvpr2015)|
|494|cvpr18-Feedback-Prop  Convolutional Neural Network Inference Under Partial Evidence|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|495|Surveillance Video Parsing With Single Frame Supervision|...The reason is that DecoupledNet first obtains the saliency map of each classified label....|
||1 instances in total. (in cvpr2017)|
|496|Barman_SHaPE_A_Novel_ICCV_2017_paper|...In more recent years, feature extraction has been done by exploiting properties such as visual saliency [36], custom pictorial structures [6] and the use of regionlets [33]....|
||1 instances in total. (in iccv2017)|
|497|Yan_A_Matrix_Decomposition_ICCV_2015_paper|...Visual saliency detection via sparsity  pursuit....|
||1 instances in total. (in iccv2015)|
|498|Safa_Messaoud_Structural_Consistency_and_ECCV_2018_paper|...nt computer vision applications including semantic segmentation [3941], human part segmentation and saliency estimation [40, 41], image labeling [42] and image denoising [43, 44]....|
||1 instances in total. (in eccv2018)|
|499|Rochan_Weakly_Supervised_Localization_2015_CVPR_paper|...Looking beyong the image unsupervised learning for object saliency and detection....|
||1 instances in total. (in cvpr2015)|
|500|Krafka_Eye_Tracking_for_CVPR_2016_paper|...Turkergaze: Crowdsourcing saliency with webcam based eye tracking....|
||1 instances in total. (in cvpr2016)|
|501|Shi_Discriminative_Blur_Detection_2014_CVPR_paper|...Hierarchical saliency detec tion....|
||1 instances in total. (in cvpr2014)|
|502|Sun_Learning_Discriminative_Part_2013_ICCV_paper|...Discriminative spatial saliency for image  classification....|
||1 instances in total. (in iccv2013)|
|503|Xiao_Single_Target_Tracking_2015_CVPR_paper|...However, this method ignores feature saliency from the local background regions....|
||1 instances in total. (in cvpr2015)|
|504|Bach_Analyzing_Classifiers_Fisher_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|505|Shankar_DEEP-CARVING_Discovering_Visual_2015_CVPR_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2015)|
|506|Peng_Unsupervised_Cross-Dataset_Transfer_CVPR_2016_paper|...Unsupervised learning of generative topic saliency for person reidentification....|
||1 instances in total. (in cvpr2016)|
|507|Williem_Robust_Light_Field_CVPR_2016_paper|...d in the recent literatures, such as refocusing [17], depth estimation [4, 15, 11, 19, 20, 22, 23], saliency detection [14], matting [5], calibration [2, 6, 7], editing [10], etc....|
||1 instances in total. (in cvpr2016)|
|508|Arjun_Nitin_Bhagoji_Practical_Black-box_Attacks_ECCV_2018_paper|...Narodytska & Kasiviswanathan [22] propose a greedy local search for high-impact pixels in input saliency maps to generate adversarial examples....|
||1 instances in total. (in eccv2018)|
|509|Shen_Multi-Stage_Multi-Recursive-Input_Fully_ICCV_2017_paper|...Geodesic saliency of watershed contours and hierarchical segmentation....|
||1 instances in total. (in iccv2017)|
|510|Lee_Multiple_Random_Walkers_2015_CVPR_paper|...[5] used saliency models to exclude regions that infrequently appear across images....|
||1 instances in total. (in cvpr2015)|
|511|Zhu_Action_Recognition_with_2013_ICCV_paper|...Generally, the VOIs can be extracted by saliency detector or densely sampling from a regular grid in spatial-temporal domain....|
||1 instances in total. (in iccv2013)|
|512|Zhao_Unsupervised_Salience_Learning_2013_CVPR_paper|...Exploiting local and global patch rarities  for saliency detection....|
||1 instances in total. (in cvpr2013)|
|513|Misra_Seeing_Through_the_CVPR_2016_paper|...a study of human explicit saliency judgment....|
||1 instances in total. (in cvpr2016)|
|514|Video Propagation Networks|...Unsupervised techniques such as [25, 48, 45, 55, 77, 80, 72, 23] use some prior information about the foreground objects such as distinctive motion, saliency etc....|
||1 instances in total. (in cvpr2017)|
|515|Aubry_Understanding_Deep_Features_ICCV_2015_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in iccv2015)|
|516|Karianakis_An_Empirical_Evaluation_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|517|Large Margin Object Tracking With Circulant Feature Maps|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2017)|
|518|Tianwei_Lin_BSN_Boundary_Sensitive_ECCV_2018_paper|...Karaman, S., Seidenari, L., Del Bimbo, A.: Fast saliency based pooling of fisher encoded  dense trajectories....|
||1 instances in total. (in eccv2018)|
|519|Jain_What_do_15000_2015_CVPR_paper|...Space-variant descriptor sampling for action recognition based on saliency and eye movements....|
||1 instances in total. (in cvpr2015)|
|520|Xiao_Learning_Deep_Feature_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|521|Pang_Finding_the_Best_2013_ICCV_paper|...Biologically inspired object tracking using center-surround saliency mechanisms....|
||1 instances in total. (in iccv2013)|
|522|Sadovnik_Its_Not_Polite_2013_CVPR_paper|...Our guesser model still does not completely mimic a human because it does not consider factors such as saliency or relative attributes....|
||1 instances in total. (in cvpr2013)|
|523|Mining Object Parts From CNNs via Active Question-Answering|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|524|Li_Weakly_Supervised_Object_CVPR_2016_paper|...Looking beyond the image: Unsupervised learning for object saliency and detection....|
||1 instances in total. (in cvpr2016)|
|525|Khan_Separating_Objects_and_2015_CVPR_paper|...We use the projected area of a cuboid as its saliency measure to rank the ground-truth objects....|
||1 instances in total. (in cvpr2015)|
|526|Yang_Efficient_Illuminant_Estimation_2015_CVPR_paper|... evaluate our hypothesis on two irrelevant natural datasets collected for other applications, e.g., saliency region detection [1, 36].These datasets include thousands of images captured by different c...|
||1 instances in total. (in cvpr2015)|
|527|Guo_Learning_Dynamic_Siamese_ICCV_2017_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in iccv2017)|
|528|Sujoy_Paul_W-TALC_Weakly-supervised_Temporal_ECCV_2018_paper|...Karaman, S., Seidenari, L., Del Bimbo, A.: Fast saliency based pooling of fisher encoded  dense trajectories....|
||1 instances in total. (in eccv2018)|
|529|Strandmark_Shortest_Paths_with_2013_ICCV_paper|...Other applications where curvature plays an important role include saliency [17], inpainting [9], stereo [21], region-based image segmentation [15] and surface reconstruction [19]....|
||1 instances in total. (in iccv2013)|
|530|Xiaolin_Zhang_Self-produced_Guidance_for_ECCV_2018_paper|...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in eccv2018)|
|531|Thomas_Seeing_Behind_the_CVPR_2016_paper|...Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks....|
||1 instances in total. (in cvpr2016)|
|532|cvpr18-Transparency by Design  Closing the Gap Between Performance and Interpretability in Visual Reasoning|...Picanet: Learning pixel-wise contextual attention in convnets and its application in saliency detection....|
||1 instances in total. (in cvpr2018)|
|533|Learning Video Object Segmentation From Static Images|...Instead we combine images and annotations from several saliency segmentation datasets (ECSSD [45], MSRA10K [9], SOD [31], and PASCAL-S [27]), resulting in an aggregated set of 11 282 training images....|
||1 instances in total. (in cvpr2017)|
|534|Wang_Relaxed_Multiple-Instance_SVM_ICCV_2015_paper|...Looking beyond the image: Unsupervised learning for object saliency and detection....|
||1 instances in total. (in iccv2015)|
|535|Di_Chen_Person_Search_via_ECCV_2018_paper|...Zhao, R., Oyang, W., Wang, X.: Person re-identification by saliency learning....|
||1 instances in total. (in eccv2018)|
|536|Fanyi_Xiao_Object_Detection_with_ECCV_2018_paper|...t after it has moved to a different spatial position (third row), which is manifested by a trail of saliency on the memory map due to overlaying multiple unaligned maps (fourth row)....|
||1 instances in total. (in eccv2018)|
|537|The Incremental Multiresolution Matrix Factorization Algorithm|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|538|Gupta_KL_Divergence_Based_2015_CVPR_paper|...In the future, we would like to introduce saliency based costs into the framework to identify stable regions and prune them from subsequent merging....|
||1 instances in total. (in cvpr2015)|
|539|One-To-Many Network for Visually Pleasing Compression Artifacts Reduction|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|540|Guo_Robust_Object_Co-detection_2013_CVPR_paper|...We have recently seen several successful similar formulations of this toward applications including image segmentation [8] and saliency detection [24]....|
||1 instances in total. (in cvpr2013)|
|541|Liu_PatchMatch-Based_Automatic_Lattice_ICCV_2015_paper|...Instead of using a global threshold on the saliency score, we perform thresholding in a block-wise manner to allow it to adapt locally....|
||1 instances in total. (in iccv2015)|
|542|Meng_From_Keyframes_to_CVPR_2016_paper|...By learning to predict important object regions in egocentric videos using egocentric and saliency cues, concise visual summaries for egocentric videos can be produced driven by those regions [21]....|
||1 instances in total. (in cvpr2016)|
|543|cvpr18-Natural and Effective Obfuscation by Head Inpainting|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|544|Jang_Primary_Object_Segmentation_CVPR_2016_paper|...Geodesic saliency using  background priors....|
||1 instances in total. (in cvpr2016)|
|545|Reflection Removal Using Low-Rank Matrix Completion|...Double low rank matrix recovery for saliency fusion....|
||1 instances in total. (in cvpr2017)|
|546|Xiangyu_Xu_Rendering_Portraitures_from_ECCV_2018_paper|...Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object  detection....|
||1 instances in total. (in eccv2018)|
|547|cvpr18-Fast and Accurate Online Video Object Segmentation via Tracking Parts|...Existing approaches often rely on visual cues such as superpixels, saliency maps or optical flow to obtain  7416  initial object regions, and need to process the entire video in batch mode for refini...|
||1 instances in total. (in cvpr2018)|
|548|SCA-CNN_ Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning|...ces in generated sentences and ground-truth sentences, where this consistency is weighted by n-gram saliency and rarity....|
||1 instances in total. (in cvpr2017)|
|549|Nguyen_Deep_Neural_Networks_2015_CVPR_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2015)|
|550|cvpr18-Video Representation Learning Using Discriminative Pooling|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|551|Wu_Deep_Multiple_Instance_2015_CVPR_paper|...These approaches either adopt saliency information [13], train generic object models to harvest objectness [4], or turn to more adaptive segmentation systems [33, 41], all of which can be viewed as ef...|
||1 instances in total. (in cvpr2015)|
|552|cvpr18-Connecting Pixels to Privacy and Utility  Automatic Redaction of Private Information in Images|...Exploiting saliency for object segmentation from image level labels....|
||1 instances in total. (in cvpr2018)|
|553|cvpr18-Attend and Interact  Higher-Order Object Interactions for Video Understanding|...Top-down visual saliency guided by captions....|
||1 instances in total. (in cvpr2018)|
|554|Ouyang_DeepID-Net_Deformable_Deep_2015_CVPR_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2015)|
|555|cvpr18-End-to-End Deep Kronecker-Product Matching for Person Re-Identification|...Person re-identification by saliency learning....|
||1 instances in total. (in cvpr2018)|
|556|Alahi_Social_LSTM_Human_CVPR_2016_paper|...Social saliency prediction....|
||1 instances in total. (in cvpr2016)|
|557|Liu_Recurrent_Multimodal_Interaction_ICCV_2017_paper|...Recurrent attentional networks for saliency detection....|
||1 instances in total. (in iccv2017)|
|558|Simon_Reflection_Removal_for_2015_CVPR_paper|...with inconsistent color and ghost artifact because the complex outdoor scene distracts the gradient saliency detection....|
||1 instances in total. (in cvpr2015)|
|559|Weinzaepfel_Learning_to_Detect_2015_CVPR_paper|...Using statistical tests on histograms and structural saliency based postprocessing, this work develops a method to recover and segment motion boundaries in synthetic footage....|
||1 instances in total. (in cvpr2015)|
|560|Karthikeyan_From_Where_and_2013_ICCV_paper|...Context-aware saliency detection....|
||1 instances in total. (in iccv2013)|
|561|cvpr18-Conditional Image-to-Image Translation|...Introduction  Image-to-image translation covers a large variety of computer vision problems, including image stylization [4], segmentation [13] and saliency detection [5]....|
||1 instances in total. (in cvpr2018)|
|562|Wang_Image_Co-segmentation_via_2013_ICCV_paper|...Related Work  Earlier work on joint segmentation mainly compared the visual features of image pairs, such as foreground color histogram [18], SIFT [14], saliency [5], and Gabor features [9]....|
||1 instances in total. (in iccv2013)|
|563|Samuel_Albanie_Semi-convolutional_Operators_for_ECCV_2018_paper|...: Deep networks for saliency detection via  local estimation and global search....|
||1 instances in total. (in eccv2018)|
|564|cvpr18-End-to-End Flow Correlation Tracking With Spatial-Temporal Attention|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2018)|
|565|Saleh_Object-Centric_Anomaly_Detection_2013_CVPR_paper|...Our goal is different from saliency detection; abnormalities in objects are not necessarily aligned with definitions of saliency in the literature....|
||1 instances in total. (in cvpr2013)|
|566|Chatzis_A_Nonparametric_Bayesian_ICCV_2015_paper|...l approaches, such  as optical flow-based ones (e.g., HOG3D [15], HOG/HOF [35]), methods maximizing saliency functions in the spatiotemporal domain (e.g., Cuboid [5] and ESURF [37]), methods based on ...|
||1 instances in total. (in iccv2015)|
|567|Kabra_Understanding_Classifier_Errors_2015_CVPR_paper|...with the use of saliency maps [18]....|
||1 instances in total. (in cvpr2015)|
|568|Sun_DL-SFA_Deeply-Learned_Slow_2014_CVPR_paper|...Feature detection extracts interesting or saliency points by applying Harris (spacial)[24], Harris3D [15] (spacial-temporal), temporal Gabor filter [5], or Hessian [32] detector....|
||1 instances in total. (in cvpr2014)|
|569|Sun_Lattice_Long_Short-Term_ICCV_2017_paper|...To visually verify the effect of L2STM, we use BPTT to visualize saliency regions for specific video sequences, i.e., back-propagating the neuron of that action category in the classifier layer to the...|
||1 instances in total. (in iccv2017)|
|570|Workman_Understanding_and_Mapping_ICCV_2017_paper|...This process leads to a saliency map over the input image....|
||1 instances in total. (in iccv2017)|
|571|cvpr18-Good View Hunting  Learning Photo Composition From Dense View Pairs|...We reimplemented [19] using a state-of-theart saliency method [47] and added a face detector....|
||1 instances in total. (in cvpr2018)|
|572|Bertasius_Am_I_a_ICCV_2017_paper|...Social saliency prediction....|
||1 instances in total. (in iccv2017)|
|573|Marin_Thin_Structure_Estimation_ICCV_2015_paper|...Extracting salient curves from images: An analysis of the saliency network....|
||1 instances in total. (in iccv2015)|
|574|Wu_Harnessing_Object_and_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|575|Borji_iLab-20M_A_Large-Scale_CVPR_2016_paper|...-grained recognition [39], multi-view 3D shape recognition [56], activity recognition [53, 28], and saliency prediction [32]....|
||1 instances in total. (in cvpr2016)|
|576|Zhou_Measuring_Crowd_Collectiveness_2013_CVPR_paper|...Collectiveness also provides useful information in crowd saliency detection and abnormality detection....|
||1 instances in total. (in cvpr2013)|
|577|Mai_Composition-Preserving_Deep_Photo_CVPR_2016_paper|...Visual saliency based on multiscale deep features....|
||1 instances in total. (in cvpr2016)|
|578|Minxian_Li_Unsupervised_Person_Re-identification_ECCV_2018_paper|...Zhao, R., Oyang, W., Wang, X.: Person re-identification by saliency learning....|
||1 instances in total. (in eccv2018)|
|579|Koh_POD_Discovering_Primary_CVPR_2016_paper|...[34] detected saliency maps using geodesic distances to discover a salient object....|
||1 instances in total. (in cvpr2016)|
|580|cvpr18-Mask-Guided Contrastive Attention Model for Person Re-Identification|...Person re-identification by saliency learning....|
||1 instances in total. (in cvpr2018)|
|581|Jiang_A_Linear_Approach_2013_CVPR_paper|...Ties of saliency are broken randomly....|
||1 instances in total. (in cvpr2013)|
|582|Alayrac_Unsupervised_Learning_From_CVPR_2016_paper|... ties, allowing the choice of an adaptive number of main instruction steps when there is not enough saliency for the last steps....|
||1 instances in total. (in cvpr2016)|
|583|LSTM Self-Supervision for Detailed Behavior Analysis|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2017)|
|584|cvpr18-Differential Attention for Visual Question Answering|...However, as noted by [3], using a saliency based method [10] that is trained on eye tracking data to obtain a measure of where people look in a task independent manner results in more correlation with...|
||1 instances in total. (in cvpr2018)|
|585|Efficient Diffusion on Region Manifolds_ Recovering Small Objects With Compact CNN Representations|...puter vision problems, such as semi-supervised classification [63], seeded image segmentation [20], saliency detection [33, 6], clustering [12] and image retrieval [28, 14, 60, 13, 58]....|
||1 instances in total. (in cvpr2017)|
|586|Shakeri_Moving_Object_Detection_ICCV_2017_paper|...They also used a motion saliency map to distinguish the foreground object from background motion....|
||1 instances in total. (in iccv2017)|
|587|Ahmed_An_Improved_Deep_2015_CVPR_paper|...We compare our approach against mid-level filters (mFilter) [31], saliency matching (SalMatch) [29], patch matching (PatMatch) [29], generic metric [15], ITML [4], LMNN [24], eSDC [30], SDALF [5], l2-...|
||1 instances in total. (in cvpr2015)|
|588|Du_RPAN_An_End-To-End_ICCV_2017_paper|...1, human poses of different actors are closely related to the saliency regions in the average of convolutional feature maps estimated by CNN, and different joints of human pose can also be highly acti...|
||1 instances in total. (in iccv2017)|
|589|Gopalan_Hierarchical_Sparse_Coding_2015_CVPR_paper|...Learning hierarchical image repIn British  resentation with sparsity, saliency and locality....|
||1 instances in total. (in cvpr2015)|
|590|Zhao_Quasi_Real-Time_Summarization_2014_CVPR_paper|...Moreover, [15] proposes a saliency based method, which trains a linear regression model to predict importance score for each frame in egocentric videos [15]....|
||1 instances in total. (in cvpr2014)|
|591|cvpr18-Multimodal Visual Concept Learning With Weakly Supervised Techniques|...The problems that span from this field, such as activity recognition, saliency and scene analysis, comprise detecting events and extracting high level semantics in realistic video sequences....|
||1 instances in total. (in cvpr2018)|
|592|cvpr18-VITAL  VIsual Tracking via Adversarial Learning|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in cvpr2018)|
|593|Lam_HC-Search_for_Structured_2015_CVPR_paper|...This sampling is realized by randomly picking a threshold on the saliency of region boundaries present in the segmentation....|
||1 instances in total. (in cvpr2015)|
|594|Training Object Class Detectors With Click Supervision|...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
||1 instances in total. (in cvpr2017)|
|595|Lin_Co-Interest_Person_Detection_ICCV_2015_paper|...3D social saliency from  head-mounted cameras....|
||1 instances in total. (in iccv2015)|
|596|Wang_Action_Recognition_With_2015_CVPR_paper|...[39] proposed a Hessian detector, which is a spatio-temporal extension of Hessian saliency measure used for blob detection in images....|
||1 instances in total. (in cvpr2015)|
|597|Chang_They_Are_Not_CVPR_2016_paper|...Complex event detection using semantic saliency and nearlyisotonic SVM....|
||1 instances in total. (in cvpr2016)|
|598|Noh_Learning_Deconvolution_Network_ICCV_2015_paper|...Online tracking by learning discriminative saliency map with convolutional neural network....|
||1 instances in total. (in iccv2015)|
|599|MDNet_ A Semantically and Visually Interpretable Medical Image Diagnosis Network|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|600|cvpr18-Learning Intelligent Dialogs for Bounding Box Annotation|...Dynamic eye movement datasets and learnt saliency models for visual action recognition....|
||1 instances in total. (in cvpr2018)|
|601|cvpr18-Gated Fusion Network for Single Image Dehazing|...However, these methods need complex blending based on luminance, chromatic and saliency maps....|
||1 instances in total. (in cvpr2018)|
|602|One-Shot Metric Learning for Person Re-Identification|...Unsupervised learning of generative topic saliency for person re-identification....|
||1 instances in total. (in cvpr2017)|
|603|cvpr18-Learning Convolutional Networks for Content-Weighted Image Compression|...[24] exploit segmentation information in image super-resolution, and saliency was introduced to the image compression system by Prakash et al....|
||1 instances in total. (in cvpr2018)|
|604|Ronghang_Hu_Explainable_Neural_Computation_ECCV_2018_paper|...Ramanishka, V., Das, A., Zhang, J., Saenko, K.: Top-down visual saliency guided by captions....|
||1 instances in total. (in eccv2018)|
|605|Wang_Actions__Transformations_CVPR_2016_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2016)|
|606|Kong_HyperNet_Towards_Accurate_CVPR_2016_paper|...These methods unusually adopt cues like superpixels [33], edges [36][5], saliency [1] and shapes [2][23] as features ....|
||1 instances in total. (in cvpr2016)|
|607|Konda_Reddy_Mopuri_Ask_Acquire_and_ECCV_2018_paper|...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in eccv2018)|
|608|Charles_Herrmann_Object-centered_image_stitching_ECCV_2018_paper|...In addition, objects can be prioritized based on saliency measures or category (i.e....|
||1 instances in total. (in eccv2018)|
|609|Synthesizing Normalized Faces From Facial Identity Features|...2  [9] K. Simonyan, A. Vedaldi, and A. Zisserman, Deep inside convolutional networks: Visualising image classification models and saliency maps, CoRR, vol....|
||1 instances in total. (in cvpr2017)|
|610|Plug & Play Generative Networks_ Conditional Iterative Generation of Images in Latent Space|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|611|Gandhi_Decomposing_Bag_of_2013_ICCV_paper|...[25] on spatial saliency also partitions the image into regular cells and assigns weights to them....|
||1 instances in total. (in iccv2013)|
|612|Lee_Learning_to_Combine_ICCV_2015_paper|...At that point, the concept of an objectness detector was introduced [2], resurrecting interest in bottom-up saliency and attention....|
||1 instances in total. (in iccv2015)|
|613|Zheng_Tag_Taxonomy_Aware_2013_CVPR_paper|...Top-down visual saliency via joint  crf and dictionary learning....|
||1 instances in total. (in cvpr2013)|
|614|Gregoire_Payen_de_La_Garanderie_Eliminating_the_Dreaded_ECCV_2018_paper|...ation has been applied directly to equirectangular panoramic images to provide object detection and saliency in the context of virtual cinematography [34,42] using pre-trained detectors such as Faster...|
||1 instances in total. (in eccv2018)|
|615|Shitala_Prasad_Using_Object_Information_ECCV_2018_paper|...proposed a cascade of boosted classifiers with a saliency map to create bounding boxes for text detection [28]....|
||1 instances in total. (in eccv2018)|
|616|Multimodal Transfer_ A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|617|Ze_Yang_Learning_to_Navigate_ECCV_2018_paper|...[47] propose a two-step approach to learn a bunch of part detectors and part saliency maps....|
||1 instances in total. (in eccv2018)|
|618|cvpr18-What Have We Learned From Deep Representations for Action Recognition |...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|619|Yang_Coarse-To-Fine_Region_Selection_2015_CVPR_paper|...Detectors each have a peak threshold Tp measuring the saliency of regions detected (and controlling the number of regions detected)....|
||1 instances in total. (in cvpr2015)|
|620|cvpr18-Interpretable Convolutional Neural Networks|...Deep inside convolutional networks: visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
|621|Altwaijry_Learning_to_Match_CVPR_2016_paper|...In essence, the attention mechanism embodies a saliency detector....|
||1 instances in total. (in cvpr2016)|
|622|Yang_From_Facial_Parts_ICCV_2015_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in iccv2015)|
|623|Zhang_Low-Rank_Sparse_Coding_2013_ICCV_paper|...It exploits codebook locality by setting the code to a saliency degree based on the nearest codebook bases (cid:2)bj to (cid:2)xi....|
||1 instances in total. (in iccv2013)|
|624|Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|625|Yuan_Temporal_Dynamic_Graph_ICCV_2017_paper|...Revealing event saliency in unconstrained video collection....|
||1 instances in total. (in iccv2017)|
|626|Escorcia_On_the_Relationship_2015_CVPR_paper|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2015)|
|627|4D Light Field Superpixel and Segmentation|...e benefited many problems in computer vision, such as depth and scene flow estimation [30, 28, 23], saliency detection [14], super resolution [3] and material recognition [29]....|
||1 instances in total. (in cvpr2017)|
|628|Lin_Depth_Recovery_From_ICCV_2015_paper|...It has been shown that given the simultaneous multiple views, LFs enable improved image analysis, e.g., stereo reconstruction [20], refocusing [29], saliency detection [23], and scene classification [39]....|
||1 instances in total. (in iccv2015)|
|629|Meng_Tang_On_Regularized_Losses_ECCV_2018_paper|...This experiment is on a simple saliency dataset [11] where color clustering is obvious and likely to help....|
||1 instances in total. (in eccv2018)|
|630|Li_Leveraging_Weak_Semantic_ICCV_2017_paper|...Complex event detection using semantic saliency and nearly-isotonicSVM....|
||1 instances in total. (in iccv2017)|
|631|Woojae_Kim_Deep_Video_Quality_ECCV_2018_paper|...Kim, H., Lee, S., Bovik, A.C.: Saliency prediction on stereoscopic videos....|
||1 instances in total. (in eccv2018)|
|632|Fatemeh_Sadat_Saleh_Effective_Use_of_ECCV_2018_paper|...Oh, S.J., Benenson, R., Khoreva, A., Akata, Z., Fritz, M., Schiele, B.: Exploiting saliency for object segmentation from image level labels....|
||1 instances in total. (in eccv2018)|
|633|Apoorv_Vyas_Out-of-Distribution_Detection_Using_ECCV_2018_paper|...Xu, P., Ehinger, K.A., Zhang, Y., Finkelstein, A., Kulkarni, S.R., Xiao, J.: Turkergaze: Crowdsourcing saliency with webcam based eye tracking (2015), arXiv preprint arXiv:1504.06755  21....|
||1 instances in total. (in eccv2018)|
|634|Unsupervised Adaptive Re-Identification in Open World Dynamic Camera Networks|...Representative methods along this direction use either hand-crafted appearance features [47, 41, 46, 11] or saliency statistics [71] for matching persons without requiring huge amount of labeled data....|
||1 instances in total. (in cvpr2017)|
|635|Park_Force_From_Motion_CVPR_2016_paper|...3D social saliency from  head-mounted cameras....|
||1 instances in total. (in cvpr2016)|
|636|Qingqiu_Huang_Person_Search_in_ECCV_2018_paper|...Different kinds of LP-based approaches have been proposed for face recognition [18, 48], semantic segmentation [33], object detection [36], saliency detection [20] in the computer vision community....|
||1 instances in total. (in eccv2018)|
|637|Sindagi_Generating_High-Quality_Crowd_ICCV_2017_paper|...Several recent works for semantic segmentation [21], scene parsing [51] and visual saliency [52] have demonstrated that incorporating contextual information can provide significant improvements in the results....|
||1 instances in total. (in iccv2017)|
|638|Jue_Wang_Learning_Discriminative_Video_ECCV_2018_paper|...Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in eccv2018)|
|639|Bertasius_DeepEdge_A_Multi-Scale_2015_CVPR_paper|...Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection....|
||1 instances in total. (in cvpr2015)|
|640|Identifying First-Person Camera Wearers in Third-Person Videos|...Predicting primary gaze behavior using social saliency fields....|
||1 instances in total. (in cvpr2017)|
|641|cvpr18-Context Encoding for Semantic Segmentation|...d semantics to predict scaling factors of featuremap channels, which provides a mechanism to assign saliency by emphasizing or de-emphasizing individual featuremaps conditioned on scene context....|
||1 instances in total. (in cvpr2018)|
|642|cvpr18-Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning|...Given an uncertain pixel q with a high saliency score (Sq > 0.3), if the maximum element in its label vector yq is larger than a threshold (=0.6) and this element does not correspond to the background...|
||1 instances in total. (in cvpr2018)|
|643|Semantic Image Inpainting With Deep Generative Models|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2017)|
|644|Pan_Soft-Segmentation_Guided_Object_CVPR_2016_paper|...If we solve the sub-problem (12) on a super-pixel level and consider each pixel as a graph node, this becomes the saliency detection problem considered in [49]....|
||1 instances in total. (in cvpr2016)|
|645|cvpr18-MoNet  Deep Motion Exploitation for Video Object Segmentation|...Related Work  Unsupervised VOS methods aim to segment a primary object without human inputs, by utilizing visual saliency [8, 33] and motion cues [16, 20]....|
||1 instances in total. (in cvpr2018)|
|646|Murray_Generalized_Max_Pooling_2014_CVPR_paper|...Clearly, they are reminiscent of saliency maps computed to predict fixations of the human gaze....|
||1 instances in total. (in cvpr2014)|
|647|cvpr18-Net2Vec  Quantifying and Explaining How Concepts Are Encoded by Filters in Deep Neural Networks|...Deep inside convolutional networks: Visualising image classification models and saliency maps....|
||1 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2014)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2013)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in eccv2018)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in iccv2015)|
||0 instances in total. (in cvpr2018)|
||0 instances in total. (in cvpr2017)|
||0 instances in total. (in cvpr2015)|
||0 instances in total. (in iccv2017)|
||0 instances in total. (in iccv2013)|
||0 instances in total. (in cvpr2016)|
||0 instances in total. (in cvpr2018)|
