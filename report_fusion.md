|Index|Title|sentence|
|---|---|---|
|1|Liu_Sample-Specific_Late_Fusion_2013_CVPR_paper|of Electrical Engineering, Columbia University, USA  Dong Liu, Kuan-Ting Lai, Guangnan Ye, Ming-Syan Chen, Shih-Fu Chang Graduate Institute of Electrical Engineering, National Taiwan University, Taiwan Research Center for Information Technology Innovation, Academia Sinica, Taiwan {dongliu,gnye,sfchang}@ee.columbia.edu, {ktlai,mschen}@arbor.ee.ntu.edu.tw  Abstract  Late fusion addresses the problem of combining the prediction scores of multiple classifiers, in which each score is predicted by a classifier trained with a specific feature.|
|||However, the existing methods generally use a fixed fusion weight for all the scores of a classifier, and thus fail to optimally determine the fusion weight for the individual samples.|
|||Specifically, we cast the problem into an information propagation process which propagates the fusion weights learned on the labeled samples to individual unlabeled samples, while enforcing that positive samples have higher fusion scores than negative samples.|
|||In this process, we identify the optimal fusion weights for each sample and push positive samples to top positions in the fusion score rank list.|
|||To the best knowledge, this is the first method supporting sample-specific fusion weight learning.|
|||Introduction  Recently, multi-feature late fusion has been advocated in the computer vision community, and its effectiveness has been demonstrated in various applications such as object recognition [22, 24], biometric analysis [15], video event detection [14, 24].|
|||Given multiple classifiers trained with different low-level features, late fusion tries to combine the prediction scores of all classifiers (the prediction score of each sample generated by a classifier indicates the confidence of classifying the sample as positive).|
|||, n), where the images with green and red borders are respectively labeled as positive and negative while the others are unlabeled, we want to learn a fusion weight vector wi for each sample.|
|||The problem is cast into an information propagation procedure which propagates the fusion weights of the labeled images to the individual unlabeled ones along a graph built on low-level features.|
|||During the propagation, we use an infinite push constraint to ensure the positive samples have higher fusion scores than the negative samples.|
|||The fusion scores w(cid:2) i si can be used to rank the images where the positive images will appear at the top positions of the rank list.|
|||individual classifier and also produces highly comparative results to multi-feature early fusion methods [21, 24].|
|||The simplest approach to late fusion is to estimate a fixed weight for each classifier and then use a weighted summation of the prediction scores as the fusion result.|
|||Therefore, instead of using a fixed weight for each classifier, a promising alternative is to estimate the specific fusion weights for each sample to  alleviate the individual prediction errors from the imperfect classifiers and achieve robust fusion.|
|||Discovering the sample specific fusion weights is a nontrivial task due to the following issues.|
|||First, given the prediction scores of a test sample, since its label information is unavailable, it is unknown how to determine the sample specific fusion weights for such an unlabeled sample.|
|||Second, to get a robust late fusion result, we need to maximally ensure positive samples have the highest fusion scores in the fusion result.|
|||In this paper, we address the above issues by proposing the Sample Specific Late Fusion (SSLF) method, which learns the optimal sample-specific fusion weights from supervision information while directly enforcing that positive samples have the highest fusion scores in the fusion result.|
|||Specifically, we define the fusion process as an information propagation procedure which propagates the fusion weights learned on the individual labeled samples to the individual unlabeled ones.|
|||The propagation is guided by a graph built on low-level features of all samples, which enforces visually similar samples have similar fusion scores and offers the capability to infer fusion weights for unlabeled samples.|
|||To ensure most positive samples have the highest fusion scores as possible, we use the L norm infinite push constraint to minimize the number of positive samples scored lower than the highest-scored negative sample.|
|||By this propagation process, we identify the optimal sample-specific fusion weights and push positive samples to have the highest fusion scores.|
|||[15] employed the Gaussian mixture model to approximate the score distributions of the classifier, and then performed score fusion using likelihood ratio test.|
|||[22] developed a supervised late fusion method which tried to minimize the classification error rates under L1 constraints on the fusion weights.|
|||However, these works focus on classifier-level fusion which determines a fixed weight for all prediction scores of a specific classifier.|
|||Such fusion methods blindly treat the prediction scores of a classifier as equally important and cannot optimally determine the fusion weights for each sample.|
|||[14] proposed a local expert forest model for late fusion, which partitioned the score space into local regions and learned the local fusion weights in each region.|
|||However, the learning can only be performed on the training samples whose label information is provided, and hence cannot be applied to learn the fusion weights on  the test samples.|
|||One promising work that tries to obtain sample specific fusion scores is the low rank late fusion method proposed by Ye et al [24].|
|||Finally, a score vector is extracted from the rank-2 matrix as the late fusion result.|
|||Our method is related to instance-specific metric learning [26], which aims at deriving a proper distance metric for each instance rather than optimally determining the fusion weights of each instance for ranking.|
|||Such prediction scores cannot reflect the classifiers prediction capabilities on unseen samples, defeating the value of a fusion method.|
|||, wm i ] j i being the fusion weight of s  to learn a sample-specific fusion function We want fi(si) = w(cid:3) i si for each sample (i = 1, .|
|||, l + u), where (cid:3) is a non-negative fusion weight vecwi = [w1 j tor with w i .|
|||Obviously, we can directly derive the fusion weights of the labeled samples based on their label information.|
|||However, it is non-trivial to learn fusion weights for the unlabeled samples since there is no supervision information that can be directly applied.|
|||Our late fusion method is formulated as follows:  (W) + (cid:4)({fi}l i=1;P,N ), wi  0, i = 1, .|
|||, wl+u] consists of l + u fusion weight vectors to be derived for both labeled and unlabeled samples, and  is a trade-off parameter among the two competing terms.|
|||The first term is a regularization term designed for the purpose of fusion weight propagation: i si  w(cid:3) Eij(w(cid:3) j sj) (cid:5) (cid:6) (cid:6)(cid:3) (W)  l+u(cid:4) (cid:5) (W)  (W) =  (3)  i,j=1  =  L  2  ,  2 GU 1  where E = U 1 2 is a normalized weight matrix of G. The matrix U is a diagonal matrix where the (i, i)entry is the i-th row or column sum of G. L = (I  E) (cid:5) (cid:6) is the graph laplacian [2].|
|||(3) enforces a smooth fusion score propagation over the graph structure, making similar samples have similar fusion scores.|
|||This essentially ensures positive samples have higher fusion scores than the negative, leading to more accurate fusion results.|
|||(2) Average Late Fusion (ALF), we directly average the prediction scores from all the classifiers as the fusion results.|
|||The final fusion score vector can be extracted from the rank-2 matrix by matrix  805805805807807  decomposition.|
|||(4) Fixed Weight Late Fusion (FWLF), instead of learning sample-specific fusion functions, we learn a fixed fusion function f (s) = w(cid:3)s for all the samples.|
|||Following previous work on late fusion [24], we employ the probabilistic outputs of the one-vs-all SVM classifier as the prediction scores, in which each value measures the possibility of classifying a sample as positive.|
|||Example images and their rank positions in the fusion score rank list obtained from different fusion methods.|
|||For each method, the rank list is obtained by ranking all 4, 952 test images in descending order based on the fusion scores.|
|||From the results, we have the following observations: (1) The proposed SSLF method con 806806806808808  sistently beats all the other baseline methods by a large margin, which demonstrates its effectiveness in determining the optimal fusion weights for each sample.|
|||(2) The LRLF, FWLF and SSLF late fusion methods all outperform the ALF method.|
|||(3) The sample level late fusion methods including LRLF and SSLF outperform the FWLF.|
|||The reason may be that FWLF only tries to learn uniform fusion weights for all the samples and hence cannot discover the optimal fusion weights for each sample.|
|||This clearly demonstrates that our method is able to assign higher fusion scores to the positive samples.|
|||In our experiments, we also observe that prediction scores from more reliable classifiers tend to have higher fusion weights than the scores from the less reliable classifiers.|
|||Figure 3 shows the rank positions of some example images after ranking the 4, 952 test images based on fusion scores of different methods.|
|||Figure 4 shows the image ranking results of different fusion methods.|
|||Top 15 images ranked with the fusion scores of different methods.|
|||Following the experiment setting on PASCAL VOC07, we uniformly split the training samples into 5 folds and use 4 folds for SVM training and 1 fold for learning fusion weight.|
|||Based on the neighborhood (cid:3)q set, the late fusion score can be determined as f (z) = (cid:3)si, where G(z, xi) is the similar(cid:3)si is the fusion score of xi ity between z and xi, and (w i ) obtained on the original dataset.|
|||Conclusions  We have introduced a sample-specific late fusion method to learn the optimal fusion weights for each sample.|
|||The proposed method works in a transductive setting which propagates the fusion weights of the labeled samples to the individual unlabeled samples, while leveraging the infinite push constraint to enforce positive samples to have higher fusion scores than negative samples.|
|||For future work, we will pursue the sample-specific late fusion for multi-class and multi-label visual recognition tasks.|
|||Local expert forest of score fusion for video  event classification.|
|||Early versus late fusion in semantic  video analysis.|
|||Optimal classifier fusion in a non bayesian probabilistic framework.|
|||Robust late fusion with rank  mininization.|
||66 instances in total. (in cvpr2013)|
|2|Task-Driven Dynamic Fusion_ Reducing Ambiguity in Video Description|Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description  Xishan Zhang12, Ke Gao1, Yongdong Zhang12, Dongming Zhang1, Jintao Li1,and Qi Tian3  1 Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China  2 University of Chinese Academy of Sciences, Beijing, China  3 Department of Computer Science, University of Texas at San Antonio  {zhangxishan,kegao,zhyd,dmzhang,jtli}@ict.ac.cn,qitian@cs.utsa.edu  Abstract  Integrating complementary features from multiple channels is expected to solve the description ambiguity problem in video captioning, whereas inappropriate fusion strategies often harm rather than help the performance.|
|||Existing static fusion methods in video captioning such as concatenation and summation cannot attend to appropriate feature channels, thus fail to adaptively support the recognition of various kinds of visual entities such as actions and objects. This paper contributes to: 1)The first in-depth study of the weakness inherent in data-driven static fusion methods for video captioning.|
|||2) The establishment of a task-driven dynamic fusion (TDDF) method.|
|||It can adaptively choose different fusion patterns according to model status.|
|||Extensive experiments conducted on two well-known benchmarks demonstrate that our dynamic fusion method outperforms the state-of-the-art results on MSVD with METEOR scores 0.333, and achieves superior METEOR scores 0.278 on MSR-VTT-10K.|
|||Compared to single features, the relative improvement derived from our fusion method are 10.0% and 5.7% respectively on two datasets.|
|||While different fusion methods such as concatenation and summation have been used in video captioning, the relative increase obtained by fusing multiple-channel visual features is only 0.1%1.7% [11] or even -0.7% [26].|
|||It reveals that existing visual fusion strategies in video captioning have not made full use of each channel of features and their correlation.|
|||Therefore, in the data-driven fusion method such as feature concatenation, the appearance features are often enhanced, while motion features are suppressed.|
|||Such static fusion models cannot adaptively support the recognition of three different kinds of visual entities, which result in description ambiguity, including recognition error and detail deficiency.|
|||To alleviate description ambiguity, we propose a taskdriven dynamic fusion approach which can adaptively attend to certain visual cues based on the current model status, so that the generated visual representation will be most relevant to the current word.|
|||The fusion model consists of three different fusion patterns, which support the recognition of three kinds of visual entities separately.|
|||Three different fusion patterns are designed to support the recognition of appearance-centric, motion-centric and correlation-centric entities.|
|||In summary, we make the following contributions:   In-depth study of the weakness inherent in data-driven static fusion methods for video captioning.|
|||Existing static fusion methods cannot adaptively support the recognition of various kinds of visual entities, which results in description ambiguity, including recognition error and detail deficiency.|
||| A task-driven dynamic fusion (TDDF) model is proposed to adaptively choose different fusion patterns according to task status.|
|||The dynamic fusion model can attend to certain visual cues that are most relevant to the current word.|
||| Extensive experiments conducted on two well-known video captioning benchmarks, MSVD and MSR-VTT10K demonstrate that our dynamic fusion method achieves noticeable gains by appropriately integrating multiple-channel features.|
|||Our proposed in-depth study of the fusion of motion and appearance information in video captioning generates a joint representation by promoting individual feature channels and correlating complimentary features according to task status.|
|||Feature Fusion: All the existing feature fusion methods in video caption are static fusion, which means the visual fusion model is not affected by the previous generated target words.|
|||The work includes score-level decision fusion [31, 28] and early-stage feature combination [11, 19, 3, 22, 26].|
|||Decision fusion is achieved by averaging a set of network predictors.|
|||However, the decision fusion is not data-driven, since discrepant prediction capabilities on different samples of the individual features are neglected.|
|||Our proposed dynamic fusion model can adaptively choose different fusion patterns according to task status.|
|||Illustration of task-driven dynamic fusion (TDDF) in video captioning.|
|||However, there is a significant difference between our dynamic fusion and the wildly used attention mechanism.The attention mechanism deals with homogeneous features extracted from different samples (frames or regions).|
|||Our dynamic fusion deals with heterogeneous features even from the same sample.|
|||Our dynamic fusion is built upon attention and extends it one step further, which automatically determines whether the appearance of the dog, or the movement of the dog, or the combination should be focused.|
|||We further enhance the encoder part by adding the taskdriven dynamic fusion layer as shown in Figure.|
|||Illustration of the task-driven dynamic fusion unit.|
|||We first introduce two kinds of basic shallow fusion functions: concatenation fusion and sum or max fusion.|
|||The fusion function is  VMS() = WF([VM(), VS()] )  = WFl VM() + WFr VS()  (11)  where motion features and appearance features are concatenated together [VM(), VS()]  R+ .|
|||The concatenation fusion is capable of modeling correlations within and across features.|
|||However, the fusion parameters are fixed once learned.|
|||The fusion function is the element-wise sum VMS() = VM()  +VS() or the element-wise max VMS() = max{VM(), VS()}.|
|||These parameter-free fusion functions usually applied to features of same kind so the element-wise addition or max  3716  is reasonable.|
|||Sum fusion is applied to the shortcut connection in Residual Network [10] and the combination layers in FractalNet [14].|
|||However, different from concatenation fusion, sum or max fusion can hardly model the correlation between different dimensions of heterogeneous features.|
|||We propose a fusion function that is the element-wise weighted-sum of feature channels VMS() = () 2 VS().|
|||Therefore, the sum or max fusion can be transformed to a special case of the dynamic weighted-sum fusion.|
|||The idea of dynamic fusion is similar to the idea of attention mechanism [33, 36, 35] in the sense that both of them deal with how well the inputs are related to the target words.|
|||Dynamic fusion deals with heterogeneous feature channel  with  =  (ht1, ) :=  (ht1).|
|||The fusion weights are determined by the type of the feature  instead of the content of the feature.|
|||In design of task-driven dynamic fusion (TDDF) unit, we take advantage of the above three kinds of fusion functions, which are related and complimentary.|
|||Then, the followed concatenation fusion Layer 3 is used to combine the refined motion and appearance features, and generates the correlation pathway.|
|||At last, we apply a dynamic fusion Layer 4 on the top of motion, appearance and correlation pathways.|
|||The three pathways correspond to three different fusion patterns that are designed to support the recognition of appearance-centric, motion-centric and correlation-centric entities in video description.|
|||The proposed task-driven dynamic fusion unit is shown in Figure 3, Layer 2 and Layer 3 are fully connected layers with tanh function as activation.|
|||Experimental Results  Baseline Methods: First, we compare our task-driven dynamic visual fusion method (TDDF) with single feature methods, denoted as VGG, GoogLeNet, and C3D.|
|||Then,  as stated in Section 3.3, our TDDF unit takes advantage of static fusion, so we compare to these methods: concatenation fusion denoted as CON, sum fusion denoted as SUM and max fusion denoted as MAX.|
|||As some work also fuses multiple features and reports the results before and after fusion, we present their relative improvement by fusion methods.|
|||Our task-driven dynamic visual fusion method achieves the best METEOR and CIDEr scores among all the methods.|
|||by all the fusion method.|
|||The baseline static fusion methods also have improvement over the single feature methods.|
|||Although CON, MAX3 and SUM-3 considered the feature correlation through a concatenation fusion layer, they still perform worse than our fusion method.|
|||Their relative improvement obtained by the fusion method is less than our fusion method.|
|||Therefore, the fusion methods in the task of video caption is worth exploring.|
|||Though s() is not the final fusion weights a(), it serves as a automatic switch to give us intuition in what features the model is focusing on to predict the current word.|
|||Conclusion  Existing static fusion methods cannot adaptively support the recognition of various kinds of visual entities, so the relative increase obtained by fusing multiple-channel visual features is limited.|
|||In this paper, we propose a taskdriven dynamic visual fusion method for video captioning, which achieves state-of-the-art performance on popular benchmarks.|
|||Our method adaptively chooses different fusion patterns according to task status.|
|||Three different fusion patterns are designed to support the recognition of three visual entities respectively, including appearancecentric, motion-centric and correlation-centric entities.|
|||The dynamic fusion model can attend to certain visual cues that are most relevant to the current word, thus reducing ambiguity in video description.|
|||Convolutional two-stream network fusion for video action recognition.|
||65 instances in total. (in cvpr2017)|
|3|Mai_Kernel_Fusion_for_2015_CVPR_paper|We discuss various kernel fusion models and find that kernel fusion using Gaussian Conditional Random Fields performs best.|
|||[28]  (f) Kernel Fusion  Figure 1: Kernel fusion and image deblurring examples.|
|||By combing multiple kernel estimations from different image deblurring methods, our kernel fusion method can produce the final kernel that is more accurate and leads to better deblurring results than each individual one.|
|||The problem is challenging in that the fusion process needs to capture the complex relation among individual estimations, as well as how they relate to the underlying true blur kernel.|
|||Classical fusion methods such as (weighted) averaging cannot lead to good fusion results, as shown in Section 2.1.|
|||Therefore, we develop data-driven approaches to kernel fusion that learns how individual kernels contribute to the final fusion result and how they interact with each other.|
|||After examining various kernel fusion models, we find that kernel fusion using Gaussian Conditional Random Fields (GCRF) performs best.|
|||Our goal is to combine multiple kernel estimations such that the fusion result can outperform each individual one.|
|||Therein, we develop data-driven approaches which can effectively learn the good kernel fusion models from training data.|
|||In this section, we examine various strategies to construct the fusion model fc.|
|||Specifically, this combination strategy takes the average of all individual kernel estimations to obtained the fusion result.|
||| AV G  k  =  1  N  N(cid:2)  i=1  ki  (3)  FD Fergus HQ KE RD RM SBDB Text AVG WAVG  1  0.8  0.6  0.4  0.2  e t a R   s s e c c u S  0    0  1  2  3  4  5  6  7  8  9 Error Ratio     10  11  12  13  14  15  Figure 2: Classical fusion method performance.|
|||Classical fusion methods such as kernel averaging (AVG) and weighted kernel averaging (WAVG) apply the fixed combination function regardless of the input.|
|||When applying in kernel fusion, those fusion models perform worse than the best individual method in the dataset (RM).|
|||To evaluate the performance of the two classical kernel fusion strategies AVG and WAVG described above, we compared them against each of the eight individual kernel estimation methods by comparing their deconvolution results.|
|||Figure 2 shows the ER curves for the deconvolution results obtained from eight different kernel estimation meth ods as well as those from the kernel averaging fusion (AVG) and weighted kernel averaging fusion (WAVG) strategies.|
|||The curves show that the AVG kernel fusion strategy leads to worse results than the best individual method (the RM method in this case).|
|||In addition, while the WAVG fusion strategy can improve the kernel fusion quality over AVG by weighing each individual method in the fusion, it is still not able to outperform the best individual method.|
|||However, in our context, the fusion model should adapt to different input, because the relation among individual kernel estimations as well as the relation between the estimated kernel and true underlying kernel is likely to change from image to image.|
|||In this paper, we develop data-driven approaches to learn such a fusion model from training data.|
|||After training, the trained RF model can be used to perform element-wise kernel fusion for any new given image.|
|||Kernel Fusion using Gaussian Conditional  Random Fields  While the above element-wise fusion strategy can generate promising kernel fusion results (as demonstrated in Sec 1https://code.google.com/p/randomforest-matlab/  tion 3.2), it has an inherent limitation.|
|||As the element-wise kernel fusion model is trained to predict the kernel value at each kernel element individually, it does not capture the relationship between the values of adjacent kernel elements.|
|||Our second method addresses this problem by modeling kernel fusion using a Gaussian Conditional Random Fields (GCRF) framework.|
|||We use the GCRF with an eight-neighboring system to model the relationship between the kernel fusion value between neighboring kernel elements.|
|||We train the model parameters by minimizing the difference between the kernel fusion results and the ground-truth kernels over the training data.|
|||Dataset  As our method relies on a large amount of training data to train the kernel fusion models, we established a large synthetically blurred image set.|
|||Kernel Fusion Performance  We evaluate the effectiveness of our fusion method by comparing the quality of kernel estimation from our method against that from each individual methods.|
|||Our data-driven kernel fusion methods (RF and GCRF) can produce kernel estimation results which can better resemble the true kernel compared to each individual method.|
|||We can observe from the figure that by learning the fusion model directly from the training data, the kernel estimations from our kernel fusion methods can better resemble the ground-truth kernels significantly than each individual method.|
|||Compared to each individual deblurring method, our kernel fusion methods can more accurately recover the underlying kernel and thus can significantly improve the final deconvolution results.|
|||Our data-driven kernel fusion method can effectively combine multiple kernel estimation into the final kernel estimation that outperforms each individual one.|
|||Our kernel fusion methods lead to kernel estimation results with significantly higher success rates than each individual method.|
|||kernel fusion approaches, our GCRF-based fusion model can better capture the spatial relationship between neighboring kernel element values and therefore consistently improve the performance over our element-wise(RF)model by a large margin.|
|||In the remaining experiments, we will only experiment with our GCRF-based fusion method.|
|||3.2.3 Generalization Performance  Previous experiments demonstrate the effectiveness of our data-driven fusion methods.|
|||Leveraging the training data, our method is able to learn the relation between the true underlying kernels and their noisy estimation from individual deblurring method and effectively capture that relation in the resulted fusion model.|
|||In this experiment, for each blurred image in the dataset, we train our fusion models using training images blurred by the different kernels, and use the trained fusion model to produce the final kernel estimation on that image.|
|||We note that this curve is consistent with that in Figure 5, which demonstrates the robustness of our kernel fusion method in generalizing to new blur kernels.|
|||To further evaluate how our kernel fusion method generalize for images that are different on those that are used to train the fusion model, we perform the experiment on  (b) Cho et al.|
|||By combing multiple kernel estimations from different image deblurring methods, our kernel fusion can produce the final kernel estimation that better resemble the ground-truth kernel and leads to better deblurring results compared to each individual one.|
|||We train the fusion models on our dataset and test on the new dataset (We only consider the images where all eight individual methods can generate kernel estimation results without crashing, which results in 450 images).|
|||8 shows the result consistent with that from the previous experiment: kernel fusion can improve the deblurring results over each individual one.|
|||Visually examine the results, we observe that when some individual methods can generate reasonable deblurring results, kernel fusion can take advantage of those  e t a R   s s e c c u S  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1     0 0  FD FG HQ KE RD RM SBDB Text Ours  1  2  3  4  5  6     10  11  12  13  14  15  8  7 9 Error Ratio  Figure 8: Generalization to images from the dataset provided by [30], which has no overlap with our dataset.|
|||Our kernel fusion method can outperform each individual method on the new images.|
|||It is interesting to examine how effective that approach is compared to our kernel fusion approach.|
|||When a few individual methods can generate reasonable deblurring results, kernel fusion can take advantage of those good methods to generate better results.|
|||Because our kernel fusion framework depends solely on the kernel estimation results of individual method, when all the method severely fail to estimate the true kernel, our method may fail too.|
|||7  8  6 Error Ratio  Figure 10: Comparison between kernel fusion and bestkernel-selection.|
|||Figure 10 compares the deconvolution performance of our kernel fusion method to that of the oracle best-kernelselection approach.|
|||This result shows the advantages of our kernel fusion approach.|
|||Specifically, we develop datadriven approaches to kernel fusion and find that the Gaussian Conditional Random Field based fusion model works the best.|
|||By effectively combining multiple kernel estimation together, our kernel fusion method can consistently outperform each individual deblurring method in estimating blur kernels, and lead to superior deconvolution results.|
||54 instances in total. (in cvpr2015)|
|4|Finlayson_POP_Image_Fusion_ICCV_2015_paper|POP image fusion derivative domain image fusion without reintegration  Graham D. Finlayson  Alex E. Hayes  University of East Anglia  University of East Anglia  Norwich, UK.|
|||Often, and intuitively, image fusion is carried out in the derivative domain.|
|||Experiments demonstrate our method delivers state of the art image fusion performance.|
|||Introduction  Image fusion has applications in many problem doincluding multispectral photography[7], medimains, cal imaging[34], remote sensing[23] and computational photography[18].|
|||In image fusion we seek to combine image details present in N input images into one output image.|
|||Image gradients are a natural and versatile way of representing image detail information[8], and have been used as a basis for several image fusion techniques including [32]  and [37].|
|||Other image fusion methods include those based on wavelet decomposition[25], the Laplacian pyramid[31] and neural networks[17].|
|||The seminal image fusion method of Socolinsky and Wolff (SW) uses the structure tensor to find a 1-D set of equivalent gradients, which in terms of their orientation and magnitude, approximate the tensor derived from a multichannel image as closely as possible in a least-squares sense[30].|
|||We prove that the projection of the original multichannel image in the direction of the Principal characteristic vector of the Outer Product (POP) tensor results in the same equivalent gradient field defined  1334  (a) Input 1  (b) Input 2  (c) DWT db4  (d) DWT b1.3  (e) SW  (f) POP  235  230  225  220  215  210  205  200  195  190  y t i s n e  t  n  i   l  e x P  i  256  255.8  255.6  255.4  255.2  255  254.8  254.6  254.4  254.2  y t i s n e  t  n  i   l  e x P  i  185  80  85  90  100  95 105 Pixel ycoordinate  110  115  120  254  80  85  90  100  95 105 Pixel ycoordinate  110  115  120  (g) SW plot  (h) POP plot  Image fusion example:  Figure 1: (a) and (b) are fused by wavelet-based methods (c) and (d), resulting in severe image artifacts.|
|||A comparison of POP image fusion with antecedent methods is shown in fig.|
|||We ran a standard DWT image fusion implementation using the CM (choose maximum) selection method, which is simple and one of the best performing in a comparison[25].|
|||Clearly neither the basic wavelet method nor the Socolinsky and Wolff method (1e) work on this image fusion example.|
|||However the POP image fusion method (1f) discussed in detail in section 3 succeeds in fusing the images without artifact.|
|||Our POP image fusion method is presented in section 3.|
|||The singular value decomposition (SVD) of J uncovers structure that is useful both for the understanding of Socolinsky and Wolffs image fusion method and also our own POP image fusion algorithm presented in the next section:  J = U SV T  (3)  In Eq.|
|||Often the look-up-table reintegration theorem delivers surprisingly good image fusion (it looks like the Socolinsky and Wolff image but without the artifacts).|
|||POP image fusion  The derived gradient in the Socolinsky and Wolff method is a mathematically well founded fusion of all the available gradient information from a multichannel image.|
|||The basic premise of our method is that we can carry out image fusion without the need to reintegrate.|
|||The POP image fusion theorem is for a single image point and assumes the underlying multichannel image is continuous.|
|||We wish to understand whether we can sensibly apply the POP image fusion theorem at all image locations and even when the underlying image is not continuous.|
|||In POP image fusion the scalar output image O(x) is calculated as a simple per pixel dot product.|
|||The initial projector image derived in POP image fusion is shown in (b) note how edgy and sparse it is and after bilateral filtering and normalization (steps 3 and 4) in (c).|
|||5), fused as separate R, G and B channels for a total of 3 fusion steps, takes 54.93 seconds at full resolution, and 2.82 seconds when calculated on 68  102 downsampled thumbnail images, using a MATLAB implementation of our method.|
|||Experiments  We compare our method against two state-of-the-art algorithms, the image fusion method of Eynard et al., based on using the graph Laplacian to find an M to N channel color mapping, and the Spectral Edge (SE) method of Connah et al.|
|||In 2f we show the output of the Socolinsky and Wolff image fusion algorithm.|
|||In contrast POP image fusion which produces an output 2e (see Fig 2 caption for description of the intermediate steps) the initial projection directions are diffused with the bilateral filtering step enforcing the projection directions calculated at a pixel to be considered in concert with other image regions.|
|||produces a higher level of discrimination, as their fusion changes the output color values, whereas our method only affects luminance.|
|||(a) RGB  (b) Band 5  (c) Band 7  (d) SW  (e) SE  (f) POP  Figure 4: Remote sensing image fusion Landsat 5[1]: original RGB image, bands 1-3 (a), infrared bands 5 (b) and 7 (c) capture extra detail, which is fused with the RGB by the SW (d), SE (e) and POP (f) methods.|
|||We apply POP image fusion 3 times we fuse the R-channel with the NIR, the G-channel with the NIR and the B-channel with the NIR.|
|||The POP Image fusion result is shown in fig.|
|||Multifocus image fusion  Multifocus image fusion is another potential application, which has typically been studied using greyscale images with different focal settings[17][18].|
|||Standard multifocus image fusion involves fusing two greyscale input images with different focal settings.|
|||Table 1 shows a comparison of the performance of the POP image fusion method on this task, on several standard multifocus image pairs, using standard image fusion quality metrics.|
|||(f) POP  Figure 6: Multifocus Fusion: four color input images with different points of focus captured with one exposure using a plenoptic camera, and the fusion results of Eynard et al.|
|||This fusion result creates an output image which combines the most salient details of all the time-lapse images.|
|||[11]) from different parts of the day and night, and results of POP fusion and the method of Eynard et al.|
|||We believe the POP result is a more natural fusion in either case.|
|||(f) POP  Figure 7: Time-lapse Photography Fusion of Multiple Illuminations: four color input images captured at different times of day and night, and the fusion results of Eynard et al.|
|||We call this the Principal characteristic vector of the Outer Product image fusion method.|
|||We have compared our method to state of the art methods for RGB-NIR image fusion, image optimization for color-deficient viewers, and remote sensing, and provided illustrative results for multifocus image fusion based on plenoptic imaging and the fusion of time-lapse images.|
|||Many thanks to Davide Eynard, for providing code for the Laplacian colormaps method, as well as example fusion results.|
|||Also, thank you to Spectral Edge Ltd., for providing the image fusion results of their method used in this paper.|
|||Multifocus image fusion using region segmentation and spatial frequency.|
|||Medical image fusion with adaptive local geometrical structure and wavelet transform.|
|||Remote sensing image fusion using the curvelet transform.|
|||Image fusion for enhanced visualization: a variational approach.|
|||Image fusion by a ratio of low-pass pyramid.|
|||Salience preserving image fusion with dynamic range compression.|
|||Medical image fusion using m-pcnn.|
|||Objective image fusion performance measure.|
|||Multi-scale weighted gradientInformation Fusion,  based fusion for multi-focus image.|
||52 instances in total. (in iccv2015)|
|5|Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper|Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.|
|||3.4) for spatiotemporal fusion of two stream networks that achieves state of the art performance in Sec.|
|||[11] study several approaches for temporal sampling, including early fusion (letting the first layer filters operate over frames as in [10]), slow fusion (consecutively increasing the temporal receptive field as the layers increase) and late fusion (merging fully connected layers of two separate networks that operate on temporally distant frames).|
|||This architecture has two main drawbacks: (i) it is not able to learn the pixel-wise correspondences between spatial and temporal features (since fusion is only on the classification scores), and (ii) it is limited in temporal scale as the spatial ConvNet operates only on single frames and the temporal  1934  ConvNet only on a stack of L temporally adjacent optical flow frames (e.g.|
|||A fusion function f : xa t  RHW D and xb  t , xb t ,  yt fuses two feature t  RH W D maps xa , at time t, to produce an output map yt  RH W D , where W, H and D are the width, height and number of channels of the respective feature maps.|
|||The advantage of bilinear fusion is that every channel of one network is combined (as a product) with every channel of the other network.|
|||Discussion: These operations illustrate a range of possible fusion methods.|
|||Injecting fusion layers can have significant impact on the number of parameters and layers in a two-stream network, especially if only the network which is fused into is kept and the other network tower is truncated, as illustrated in Fig.|
|||Table 1 shows how the number of layers and parameters are affected by different fusion methods for the case of two VGG-M-2048 models (used in [22]) containing five convolution layers followed by three fullyconnected layers each.|
|||Conv fusion has slightly more parameters (97.58M) compared to sum and max fusion (97.31M) due to the additional filter that is used for channel-wise fusion and dimensionality reduction.|
|||Many more parameters are involved in concatenation fusion, which does not involve dimensionality reduction after fusion and therefore doubles the number of parameters in the first fully connected layer.|
|||Where to fuse the networks  t  RHW D and xb  As noted above, fusion can be applied at any point in the two networks, with the only constraint that the two input t  RH W D, at time t, maps xa have the same spatial dimensions; i.e.|
|||Table 2 compares the number of parameters for fusion at different layers in the two networks for the case of a VGGM model.|
|||Two examples of where a fusion layer can be placed.|
|||The left example shows fusion after the fourth conv-layer.|
|||The right figure shows fusion at two layers (after conv5 and after fc8) where both network towers are kept, one as a hybrid spatiotemporal net and one as a purely spatial network.|
|||(c) 3D conv + 3D pooling additionally performs a convolution with a fusion kernel that spans the feature channels, space and time before 3D pooling.|
|||For example, if three  1936  Spatiotemporal Loss   Temporal Loss  y  t  *  y  t  x  3D Conv fusion + 3D Pooling  x  3D Pooling  ...  t  Time t  t +  ...  Figure 4.|
|||Our spatiotemporal fusion ConvNet applies two-stream ConvNets, that capture short-term information at a fine temporal scale (t  L 2 ), to temporally adjacent inputs at a coarse temporal scale (t + T  ).|
|||The two streams are fused by a 3D filter that is able to learn correspondences between highly abstract features of the spatial stream (blue) and temporal stream (green), as well as local weighted combinations in x, y, t. The resulting features from the fusion stream and the temporal stream are 3D-pooled in space and time to learn spatiotemporal (top left) and purely temporal (top right) features for recognising the input video.|
|||Proposed architecture  We now bring together the ideas from the previous sections to propose a new spatio-temporal fusion architecture and motivate our choices based on our empirical evaluation in Sec.|
|||The choice of the spatial fusion method, layer and temporal fusion is based on the experiments in sections 4.2, 4.3 and 4.5, respectively.|
|||The temporal fusion layer receives T temporal chunks that are  frames apart; i.e.|
|||We only propagate back to the injected fusion layer, since full backpropagation did not result in an improvement.|
|||For Conv fusion, we found that careful initialisation of the injected fusion layer (as in (4)) is very important.|
|||3.4, the 3D Conv fusion kernel f has dimension 3  3  3  1024  512 and T = 5, i.e.|
|||Sum fusion at the softmax layer corresponds to averaging the two networks predictions and therefore includes the parameters of both 8-layer VGG-M models.|
|||Performing fusion at ReLU5 using Conv or Sum fusion does not significantly lower classification accuracy.|
|||Moreover, this requires only half of the parameters in the softmax fusion network.|
|||After the fusion layer a single processing stream is used.|
|||We compare different fusion strategies in Table 1 where we report the average accuracy on the first split of UCF101.|
|||Conv fusion performs best and is slightly better than Bilinear fusion and simple fusion via summation.|
|||This is interesting, since this, as well as the high result of Sum-fusion, suggest that simply summing the feature maps is already a good fusion technique and learning a randomly initialised combination does not lead to significantly different/better results.|
|||For all the fusion methods shown in Table 1, fusion at FC layers results in lower performance compared to ReLU5, with the ordering of the methods being the same as in Table 1, except for bilinear fusion which is not possible at FC layers.|
|||Among all FC layers, FC8 performs better than FC7 and FC6, with Conv fusion at 85.9%, followed by Sum fusion at 85.1%.|
|||Conv fusion is used and the fusion layers are initialised  #layers  #parameters  Accuracy 82.25% 83.43% 82.55% 85.96% 86.04% 81.55%  Fusion Layers ReLU2 ReLU3 ReLU4 ReLU5 ReLU5 + FC8 ReLU3 + ReLU5 + FC6  91.90M 93.08M 95.48M 97.57M 181,68M 190,06M Table 2.|
|||Performance comparison for Conv fusion (4) at different fusion layers.|
|||An earlier fusion (than after conv5) results in weaker performance.|
|||Late fusion is implemented by averaging the prediction layer outputs.|
|||Interestingly, fusing and truncating one net at ReLU5 achieves around the same classification accuracy on the first split of UCF101 (85.96% vs 86.04%) as an additional fusion at the prediction layer (FC8), but at a much lower number of total parameters (97.57M vs 181.68M).|
|||Spatiotemporal two-stream fusion on UCF101 (split1) and HMDB51 (split1).|
|||The + after a fusion layer indicates that both networks and their loss are kept after fusing, as this performs better than truncating one network.|
|||Different temporal fusion strategies are shown in Table 4.|
|||In the first row of Table 4 we observe that conv fusion performs better than averaging the softmax output (cf.|
|||Next, we find that applying 3D pooling instead of using 2D pooling after the fusion layer increases performance on both datasets, with larger gains on HMDB51.|
|||Note also that using a single stream after temporal fusion achieves 91.8%, compared to maintaining two streams and achieving 92.5%, but with far fewer parameters and a simpler architecture.|
|||As a final experiment, we explore what benefit results from a late fusion of hand-crafted IDT features [33] with our representation.|
|||Conclusion  We have proposed a new spatiotemporal architecture for two stream networks with a novel convolutional fusion layer between the networks, and a novel temporal fusion layer (incorporating 3D convolutions and pooling).|
|||Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice.|
||50 instances in total. (in cvpr2016)|
|6|Hu_Attribute-Enhanced_Face_Recognition_ICCV_2017_paper|We present the first work to systematically explore how the fusion of face recognition features (FRF) and facial attribute features (FAF) can enhance face recognition performance in various challenging scenarios.|
|||Despite the promise of FAF, we find that in practice existing fusion methods fail to leverage FAF to boost face recognition performance in some challenging scenarios.|
|||Thus, we develop a powerful tensor-based framework which formulates feature fusion as a tensor optimisation problem.|
|||Thus these two features are potentially complementary, if a suitable fusion method can be devised.|
|||To the best of our knowledge, we are the first to systematically explore the fusion of FAF and FRF in various face recognition scenarios.|
|||We empirically show that this fusion can greatly enhance face recognition performance.|
|||Though facial attributes are an important cue for face recognition, in practice, we find the existing fusion methods including early (feature) or late (score) fusion cannot reliably improve the performance [34].|
|||In this work, we propose a novel tensor-based fusion framework that is uniquely capable of fusing the very asymmetric FAF and FRF.|
|||Our framework provides a more powerful and robust fusion approach than existing strategies by learning from all interactions between the two feature views.|
||| A rich tensor-based fusion framework is proposed.|
|||We show the low-rank Tucker-decomposition of this tensor-based fusion has an equivalent Gated Twostream Neural Network (GTNN), allowing easy yet effective optimisation by neural network learning.|
|||The code is available: https://github.com/yanghuadr/ Neural-Tensor-Fusion-Network   We achieve state-of-the-art face recognition performance using the fusion of face (newly designed LeanFace deep learning feature) and attribute-based features on three popular databases: MultiPIE (controlled environment), CASIA NIR-VIS2.0 (cross-modality environment) and LFW (uncontrolled environment).|
|||Existing fusion approaches can be classified into feature-level (early fusion) and score-level (late fusion).|
|||Score-level fusion is to fuse the similarity scores after computation based on each view either by simple averaging [37] or stacking another classifier [48, 37].|
|||Feature-level fusion can be achieved by either simple feature aggregation or subspace learning.|
|||For aggregation approaches, fusion is usually performed by simply element wise averaging or product (the dimension of features have to be the same) or concatenation [28].|
|||Unsupervised fusion does not use the identity (label) information to learn the subspace, such as Canonical Correlational Analysis (CCA) [35] and Bilinear Models (BLM) [45].|
|||In comparison, supervised fusion uses the identity information such as Linear Discriminant Analysis (LDA) [3] and Locality Preserving Projections (LPP) [9].|
|||However, aside from differing applications and objectives, the key difference is that we establish a novel equivalence between a rich Tucker [46] decomposed low-rank fusion tensor, and a gated twostream neural network.|
|||This allows us achieve expressive fusion, while maintaining tractable computation and a small number of parameters; and crucially permits easy optimisation of the fusion tensor through standard toolboxes.|
|||However in practice, we find that existing fusion methods often cannot effectively combine these asymmetric features so as to improve performance.|
|||This motivates us to design a more powerful fusion method, as detailed in Section 3.|
|||Based on our neural tensor fusion method, in Section 5 we systematically explore the fusion of FAF and FRF in various face recognition environments, showing that FAF can greatly enhance recognition performance.|
|||The proposed fusion method we present here performs significantly better than the existing ones introduced in Section 2.|
|||In this section, we detail our tensor-based fusion strategy.|
|||Here we propose instead a non-linear fusion method via the following formulation  y(i) = W 1 x(i) 3 z(i)  (2)  where W is the fusion model parameters in the form of a third-order tensor of size D  C  B.|
|||Optimisation  The proposed tensor W provides a rich fusion model.|
|||network is identity-supervised at train time, and feature in the fusion layer used as representation for verification.|
|||Discussion  Compared with the fusion methods introduced in Section 2, we summarise the advantages of our tensor-based fusion method as follows:  3747  Figure 3: LeanFace.|
|||Unlike linear methods based on averaging, concatenation, linear subspace learning [8, 27], or LDA [3], our fusion method is non-linear, which is more powerful to model complex problems.|
|||Thanks to the lowrank modelling, our method achieves such powerful nonlinear fusion with few parameters and thus it is robust to overfitting.|
|||Once each network is trained, the features extracted from the penultimate fully-connected layers of LeanFace (256D) and AttNet (256D) are extracted as x and z, and input to GTNN for fusion and then face recognition.|
|||Then, we compare our GTNN method with other fusion methods on CASIA NIR-VIS 2.0 database [22] in Section 5.2 and LFW database [12] in Section 5.3, respectively.|
|||Clearly, the fusion of FRF and FAF, namely GTNN (LBP, AttNet) and GTNN (LeanFace, AttNet), works much better than using FRF only, showing the complementary power of facial features to face recognition features.|
|||So there is little room for fusion of AttrNet to provide benefit.|
|||(%)  C-CBFD+LDA [25]  Dictionary Learning [13]  Gabor+RBM [53] Light CNN [49]  LeanFace  AttNet  GTNN (LeanFace, AttNet)  81.8 78.46 86.16 91.88 97.27 2.38 99.94  Comparison with other fusion methods.|
|||Simple concatenation and average fusion achieve the same accuracy 97.27% as using LeanFace feature only.|
|||Another three unsupervised fusion methods: score fusion, CCA [35] and BLM [45] achieve the accuracy between using LeanFace only and AttNet only.|
|||The three supervised fusion methods achieve higher accuracy than LeanFace, showing the importance of label information for fusion.|
|||Supervised fusion methods can also be viewed as metric learning, which has been proven effective for various face recognition scenarios [9, 3, 44].|
|||Table 6: Comparisons with the state-of-the-art on LFW  Table 5: Comparison with fusion methods on NIR-VIS 2.0  Method  Err.Rate(%) Acc.|
|||To train the fusion methods, we use 0.1M images of 1.5K subjects (non-overlapping with LFW subjects) from our training data for LeanFace.|
|||Although LeanFace almost saturates the LFW database, the fusion of attribute feature further reduces the error rate by 19%.|
|||Comparison with other fusion methods.|
|||In Table 7, all alternative methods fail to improve the fused performance  DeepFace [44] VGGFace [29] Center loss [47] DeepID2+ [42]  FaceNet [32]  LeanFace  AttNet  GTNN (LeanFace, AttNet)  2.65 1.05 0.72 0.53 0.37 0.43 20.93 0.35  97.35 98.95 99.28 99.47 99.63 99.57 79.07 99.65  Table 7: Comparison with fusion methods on LFW  Method  Err.Rate(%) Acc.|
|||We presented a powerful nonlinear tensor-based fusion method that can synergistically combine attribute-derived features with both hand-crafted and deep conventional features.|
||47 instances in total. (in iccv2017)|
|7|Bai_Ensemble_Diffusion_for_ICCV_2017_paper|This stimulates a great research interest of considering similarity fusion in the framework of diffusion process (i.e., fusion with diffusion) for robust retrieval.|
|||In this paper, we firstly revisit representative methods about fusion with diffusion, and provide new insights which are ignored by previous researchers.|
|||Therefore, enormous efforts are also devoted to similarity fusion for the sake of leveraging complementary nature among those distinct feature modalities.|
|||To inherit the property of manifold-preserving from diffusion process, fusion is usually reconsidered within the framework of diffusion process, which leads to a new methodology called Fusion with Diffusion.|
|||Existing fusion with diffusion methods either utilize a naive solution by simply combining the edge weights of multiple affinity graphs (Sec.|
|||3.1), or consider a homogeneous fusion (Sec.|
|||The validity of those fusion with diffusion methods (both  774  existing and newly proposed ones) is evaluated on various retrieval tasks.|
|||Naive fusion simply averages the input similarities.|
|||Fusion with Diffusion  Different from diffusion process that works with only one affinity graph, fusion with diffusion can tackle M  2 affinity graphs Gv = (X, W v)M v=1 simultaneously.|
|||Since most fusion with diffusion methods stem from cer tain variants of diffusion process, we will use  Another typical fusion strategy is tensor product fusion.|
|||(3), we  formulate the tensor product graph fusion as  A(t+1) = S(2)A(t)S(1)T  + (1  )I,  (5)  where S(1) and S(2) are the transition matrices associated with two similarities, respectively.|
|||775  kind of similarity fusion better captures the manifold structure.|
|||Though formulated in an iterative model, tensor product fusion can be theoretically explained using an optimization framework.|
|||Tensor product fusion considers the complementary structures of two different affinity graphs.|
|||Regularized Ensemble Diffusion  Insofar as we can conclude, learning the weights of multiple similarities has not been treated seriously by most existing fusion with diffusion methods.|
|||Inspired by [6] where a weight learning paradigm can be exerted on affinity graphs to assist neighborhood structure mining, the proposed Regularized Ensemble Diffusion (RED) makes viable the automatic weight learning for fusion with diffusion.|
|||Compared with naive fusion and tensor product fusion, RED is robust to noisy features by adaptively tuning the weights .|
|||As the table suggests, the biggest defect of tensor product fusion is that it can only deal with M = 2 similarities, limiting its promotion where multiple similarities (M  2) are accessible.|
|||Secondly, RED is the most robust to noise similarities owing to the weight learning mechanism, followed by naive  777  fusion and tensor product fusion (see the experiments).|
|||The robustness of naive fusion comes from a statistical assumption that if two objects are similar in most similarity spaces, they are true matching pairs.|
|||The time complexity of naive fusion is the lowest.|
|||Tensor product fusion has to select 2 similarities each time to fulfill the diffusion step.|
|||Since td, ta, k, M  N , we can conclude that the time complexity of all the above fusion with diffusion methods is dominated by O(N 3), which is equivalent to the standard diffusion process [10].|
|||3.2, i.e., the essence of the iterative solver of tensor product fusion is to recover a closed-form solution of an optimization problem, which measures the smoothness of the joint graph manifold.|
|||Second, tensor product fusion totally fails when one of the two to-be-fused similarities is not discriminative enough.|
|||For instance, the fusion of Volumetric CNN and PANORAMA achieves mAP 67.16, significantly lower than the baseline mAP 79.53 of Volumetric CNN.|
|||Table 4 shows the comparison of different fusion methods.|
|||The summary of fusion with diffusion methods.|
|||The mAPs (%) of tensor product fusion on ModelNet40 dataset.|
|||As tensor product fusion can only deal with two similarities, its performances are given in an interval.|
|||product fusion (its highest AUC and mAP are 86.00 and 85.12 respectively) outperforms naive fusion slightly, since it considers the complementary structures between two homogenous graphs.|
|||These results also indicate that if more noisy similarities are fused, the performance difference between RED and other fusion methods will be more dramatic.|
|||The performance comparison of different fusion methods on Holidays and Ukbench dataset.|
|||Table 8 compares the results of different fusion methods.|
|||As a representative algorithm, Graph Fusion considers a naive fusion of multiple similarities with equal weights.|
|||As the focus of this paper, the first kind aims at feature fusion or diffusion, including LCMD [28], CDM [20], kNN Re-ranking [38], and Hello Neighbor [36].|
|||4.2), and plot the retrieval performances of different fusion with diffusion methods by varying the number of fused noisy similarities in Fig.|
|||In contrast, naive fusion and tensor product fusion encounter a sharp decrease in performance.|
|||When 5 noisy similarities are fused, naive fusion only achieves mAP 14.82.|
|||Conclusion  In this paper, we focus on similarity fusion in the framework of diffusion process for retrieval.|
|||Re-ranking by multifeature fusion with diffusion for image retrieval.|
|||Query specific fusion for image retrieval.|
|||TPAMI,  Query specific rank fusion for image retrieval.|
|||Query-adaptive late fusion for image search and person reidentification.|
||45 instances in total. (in iccv2017)|
|8|Spatiotemporal Pyramid Network for Video Action Recognition|From the architecture perspective, our network constitutes hierarchical fusion strategies which can be trained as a whole using a unified spatiotemporal loss.|
|||An overview of our spatiotemporal pyramid network, which constitutes a multi-level fusion pyramid of spatial features, long-term temporal features and spatiotemporal attended features.|
|||We bring in the compact bilinear fusion strategy, which captures full interactions across spatial and temporal features, while significantly reduces the number of parameters of traditional bilinear fusion methods from millions to just several thousands.|
|||[12] compare multiple CNN connectivity methods in time, including late fusion, early fusion and slow fusion.|
|||They propose a spatiotemporal fusion method and claim that the two-stream networks should be fused at the last convolutional layer.|
|||First and foremost, we propose a multi-layer pyramid fusion architecture, replacing a 3D convolutional layer and a pooling layer in [6], to combine the spatial and temporal features at different abstraction levels.|
|||In contrast, our fusion network is trained end-to-end with one single spatiotemporal loss function.|
|||Spatiotemporal Pyramid Network  The spatiotemporal pyramid network supports long-term temporal fusion and a visual attention mechanism.|
|||Also, we propose a new spatiotemporal compact bilinear operator to enable a unified modeling of various fusion strategies.|
|||Spatiotemporal Compact Bilinear Fusion  The fusion of spatial and temporal features in compact representations proves to be the key to learning highquality spatiotemporal features for video recognition.|
|||A good fusion strategy should maximally preserve the spatial and temporal information while maximize their interaction.|
|||Typical fusion methods including element-wise sum, concatenation, and bilinear fusion have been extensively evaluated in the convolutional two-stream fusion framework [6].|
|||Bilinear fusion allows all spatial and temporal features in different dimensions to interact with each other in a multiplicative way.|
|||Specifically, denote by x and y the spatial and temporal feature vectors respectively, the bilinear fusion is defined as z = vec(xy), where  denotes the outer product xyT, and vec denotes the vectorization of a vector.|
|||To circumvent the curse of dimensionality, we propose a Spatiotemporal Compact Bilinear (STCB) operator to enable various fusion strategies.|
|||We invoke the algorithm with m pathways of spatial and/or temporal features that need to be fused, which enables spatiotemporal fusion into compact representations.|
|||For the fusion method, we exploit STCB and make it support a scalable number of input feature maps.|
|||Another difference between our method and [6] is that their temporal fusion includes fusing the features of multiple RGB frames as well, while we only combine optical flow representations.|
|||We observe that compact bilinear fusion can preserve the temporal cues to supervise the spatiotemporal attention module.|
|||Spatiotemporal Attention  The second level of our spatiotemporal fusion pyramid is a variant of the attention model, which is originally proposed in multi-modal tasks [36, 35, 18].|
|||We design our architecture by injecting the proposed fusion layers between the convolutional and the fully connected layers.|
|||These features are then fed into the next fusion level, the spatiotemporal attention subnet 1532  work (red layers), where we use another STCB to fuse the spatial feature maps with the corresponding motion representations, and offer the attention cues of salient activities.|
|||At the top of the fusion pyramid, all the three previous outcomes are used: the original spatial and temporal features through average pooling, as well as the resulting attended features through the attention module.|
|||All models but the VGGnet one follow the same architecture, that the fusion layer is put between the last convolutional layer (i.e.|
|||Our experiments show that such a late fusion architecture outperforms its alternatives in which the fusion layer is moved forward.|
|||Accuracy of various fusion methods on UCF101 (Split 1).|
|||First, among all these fusion strategies, spatiotemporal compact bilinear fusion presents the best performance.|
|||It is the first time that compact bilinear fusion is demonstrated effective for merging multi-path optical flow representations.|
|||The columns in Table 4 denotes the number of pathways before the fusion layer.|
|||Among all these models, a 3-path network with spatiotemporal compact bilinear fusion outperforms the others.|
|||Moreover, this set of experiments testify the value of compact bilinear fusion again.|
|||We then try to merge temporal and spatial features in advance, while in this scenario the compact bilinear fusion performs surprisingly well.|
|||We feed the attention module with representations generated by various fusion methods.|
|||Ablation Results  To testify the individual effect of fusion approaches we discuss above, we stack them one by one and test the overall performance.|
|||Furthermore, the proposed multi-path temporal fusion method results in another 0.4 points performance gain.|
|||Both based on VGG-16, our result (93.2%) is still competitive to the original two-stream fusion [6] (92.5%).|
|||On the contrary, it offers the fusion pyramid some useful and additional cues for accurate predictions.|
|||pact bilinear fusion at the top of the pyramid can further increase the discriminative performance.|
|||From the architecture perspective, our network is hierarchical, consisting of multiple fusion strategies at different abstraction levels.|
|||These fusion modules are trained as a whole to maximally complementing each other.|
|||A series of ablation studies validate the importance of each fusion technique.|
|||We extensively show its benefit over other fusion methods, such as concatenation and element-wise sum.|
|||Convolutional two-stream network fusion for video action recognition.|
||44 instances in total. (in cvpr2017)|
|9|Tekin_Learning_to_Fuse_ICCV_2017_paper|At the heart of our framework is a trainable fusion scheme that learns how to fuse the information optimally instead of being hand-designed.|
|||Our contributions can be summarized as follows:   We introduce a discriminative fusion framework to  13941  simultaneously exploit 2D joint location confidence maps and 3D image cues for 3D human pose estimation.|
||| We introduce a novel trainable fusion scheme, which automatically learns where and how to fuse these two sources of information.|
|||In contrast to these approaches, our approach automatically learns where and how to combine the two information sources with a trainable fusion framework.|
|||The fusion strategies combine 2D joint location confidence maps with 3D cues directly extracted from the input image.|
|||2, there is a whole range of ways to perform the fusion of these two data streams, ranging from early to late fusion with no obvious way to choose the best, which might well be problem-dependent anyway.|
|||To solve this conundrum, we rely on the fusion architecture depicted by Fig.|
|||3, which involves introducing a third fusion stream that combines the feature maps produced by the two data streams in a trainable way.|
|||Each layer of the fusion stream acts on a linear combination of the previous fusion layer with the concatenation of the two data stream outputs.|
|||In effect, different weight values for these linear combinations correspond to different fusion strategies.|
|||Let {Zl}L+1  l=0 be the feature maps of the fusion stream.|
|||The feature map Zl is the output of layer l, but, unlike in the data streams, the input to layer l + 1 is a linear combination of Zl with Il and Xl given by  (1  wl)  concat(Il, Xl) + wl  Zl,  1  l  L,  (1)  where concat(, ) is the concatenation of the given feature maps along the channel axis, and wl is the l-th element of the fusion weights w  [0, 1]L controlling the mixture.|
|||In essence, the fusion weights w control where and how the fusion of the data streams occurs.|
|||Different settings of these weights lead to different fusion strategies.|
|||If the fusion weights are all set to one, w = 1, the two data streams are ignored, and only the fusion one is considered to compute the output.|
|||Since the fusion stream takes the concatenation of the image I0 and the confidence maps X0 as input, this is equivalent to the early fusion architecture of Fig.|
|||In our formalism, this can be achieved by setting the fusion weights to wl = I[l > ], where I is the indicator function.|
|||Ultimately, the complete fusion network encodes a function f (i, x; , w) = ZL+1|I0=i,X0=x mapping from an image i and confidence maps x to the 3D joint locations, parametrized by layer weights  and fusion weights w. With manually-defined fusion weights, given a set of N training pairs (in, xn) with corresponding ground-truth joint positions yn, the parameters  can be learnt by minimizing the square loss expressed as  L() =  N  X  n=1  kf (in, xn; , w)  ynk2  2 .|
|||To address this issue, we introduce a trainable fusion approach, which aims to learn  from data jointly with the network parameters.|
|||As above,  determines the stage at which fusion occurs and  controls how sharp the transition between weights with value 0 and with value 1 is.|
|||3943  im  cm  conv  fc  concat  weighted average  draw weight  im  cm  (1-w1) (1-w1)  (1-w2)  (1-w2)  (1-w3)  (1-w3)  (1-w4)  (1-w4)  (1-w5)  (1-w5)  (1-w6)  (1-w6)  (1-w7)  (1-w7)  w1 w1  w2  w2  w3  w3  w4  w4  w5  w5  w6  w6  w7  w7  conv  fc  concat  weighted average  draw weight  s t h g e w  i  layers  Figure 3: Trainable fusion architecture.|
|||The combined feature maps of the image and confidence map stream are fed into the fusion stream and linearly combined with the outputs of the previous fusion layer.|
|||mixes the data and fusion streams in equal proportions at every layer.|
|||In practice, mixing the data and fusion streams at every layer is not desirable.|
|||Furthermore, after training, a model with binary weights can be pruned, by removing the inactive layers in each stream, that is all layers l from the fusion stream where wl  0, and all layers l from the data streams where wl  1.|
|||Altogether, this loss lets us simultaneously find the most suitable fusion layer  for the given data and the corresponding network parameters , while encouraging a sharp fusion function to mimic the behavior of the indicator function.|
|||In our experiments, we fine-tuned the hourglass network initially trained on the MPII dataset [4] using the training data specific to each experiment as a preliminary step to training our fusion network.|
|||In Table 1, we compare the results of our trainable fusion approach with those of the following stateof-the-art single image-based methods: KDE regression from HOG features to 3D poses [30], jointly training a 2D body part detector and a 3D pose regressor [38, 45], the maximum-margin structured learning framework of [39, 40], the deep structured prediction approach of [64], pose regression with kinematic constraints [76], pose estimation with mocap guided data augmentation [53], volumetric pose prediction approach of [46] and lifting 2D heatmap predictions to 3D human pose [66].|
|||In Table 3, we present the performance of our fusion approach on the HumanEva-I dataset [61].|
|||[65] Ours  65.1 73.3 34.2 35.8 37.5 27.24  48.6 59.0 30.9 32.4 25.1 14.26  73.5 99.4 49.1 41.6 49.2 31.74  62.4 77.2 38.07 36.6 37.3 24.41  Table 3: Quantitative results of our fusion approach on the Walking sequences of the HumanEva-I dataset [61].|
|||Method:  3D Pose Error  Image-Only CM-Only Early Fusion Late Fusion Trainable Fusion  124.13 79.28 76.41 74.12 69.73  Table 5: Comparison of different fusion strategies and single-stream baselines on Human3.6m.|
|||The fusion networks perform better than those that use only the image or only the confidence map as input.|
|||Our trainable fusion achieves the best accuracy overall.|
|||First, we compare our trainable fusion approach to early fusion, depicted in Fig.|
|||Note also that, in general, all fusion strategies yield accurate pose estimates.|
|||5, we depict the evolution throughout the training iterations of (a) the parameters  and  that define the weight vector in our trainable fusion framework as given by Eq.|
|||An increasing , which is the typical behavior, corresponds to fusion occurring in the later stages of the network.|
|||7 the squared Pearson correlation coefficients between all pairs of features of the confidence map stream and of the image stream at the last convolutional layer of our trainable fusion network.|
|||We have also introduced an ap Figure 7: Squared Pearson correlation coefficients (R2) between each pair of the features learned at the last convolutional layer of our trainable fusion network computed from 128 randomly selected images in Human3.6m.|
|||Furthermore, our trainable fusion strategy could be applied to other fusion problems, which is what we intend to do in future work.|
||41 instances in total. (in iccv2017)|
|10|cvpr18-Fusing Crowd Density Maps and Visual Object Trackers for People Tracking in Crowd Scenes|To train the fusion CNN, we propose a two-stage strategy to gradually optimize the parameters. The first stage is to train a preliminary model in batch mode with image patches selected around the targets, and the second stage is to fine-tune the preliminary model using the real frame-by-frame tracking process.|
|||Our density fusion framework combines the S-KCF response map with the crowd density map, and effectively suppresses the irrelevant responses and detects the target accurately (top-right).|
|||1 presents an example for people tracking using different trackers: KCF [14], S-KCF (ours), long-term correlation tracker (LCT) [20], density-aware [25], and our proposed fusion tracker.|
|||We propose a density fusion framework, based on a CNN, that combines the S-KCF tracker response and the estimated crowd density map to improve people tracking in crowd scenes.|
|||Our density fusion framework can also be used with other appearance-based trackers to improve their accuracy in crowded scenes.|
|||To train the fusion CNN, we propose a two-stage training strategy to gradually optimize the parameters in an end-to-end fashion.|
|||Methodology  Our density fusion framework has three main parts: visual tracking model (S-KCF), crowd density estimation, and fusion neural network.|
|||The response map, image patch and the corresponding density map are fused together using a fusion CNN, yielding a final fused response map.|
|||The proposed density fusion framework.|
|||The fusion CNN takes in the image patch, S-KCF response map, and crowd density map and produces a refined fused response map, whose maximum value indicates the location of the target.|
|||Fusion CNN  The fusion CNN combines the tracker response map, the crowd density map, and the image patch to produce a refined (fused) response map, where the maximum value indicates the target position.|
|||The structure of our fusion CNN is shown in Fig.|
|||Our fusion network has 3 convolutional layers (Conv1-Conv3).|
|||The structure of the fusion CNN.|
|||The fusion network has three input channels, and can effectively fuse the appearance information (image patch), with the crowd density map, and the visual tracker response map.|
|||cording to the fusion response maps (see Fig.|
|||Because of the interplay between the output of one frame with the input in the next frame, we adopt a two-stage training procedure to gradually optimize the fusion CNN.|
|||In the first stage, we train the fusion CNN in batch mode, where each frame is treated independently.|
|||In the second stage, we run the fusion CNN, and use the fusion response map to predict target position.|
|||This is iterated over frames, and the samples used for fine-tune training the fusion CNN.|
|||Two(cid:173)stage Training Strategy for Fusion CNN  Training the fusion CNN requires the response maps generated by KCF, but the KCF model is also updated ac 4.1.|
|||We randomly select 80% of the unique people for training the fusion CNN, and the remaining 20% are held out for testing.|
|||For the PETS2009 dataset, the density map CNN is only trained on the PETS2009 data, while the fusion CNN is fine-tuned from the network learned from the UCSD dataset.|
|||To show the general effectiveness of using density map fusion, we train a separate fusion CNN for each tracker, using the two-stage training procedure (denoted as FusionCNNv2), and evaluate the tracking performance of the fused response map.|
|||To show the effectiveness of two-stage training, we compare with a fusion CNN using only the first stage of training, denoted as FusionCNN-v1.|
|||We implement the fusion CNN using the Caffe [16] framework.|
|||When combined with crowd density using our density fusion framework (FusionCNN-v2), all the trackers can be improved significantly (e.g., KCF improves P@10 from 0.4235 to 0.5501, while S-KCF improves from 0.4356 to 0.5999).|
|||The density-aware method [25] for fusion does not perform as well as our fusion method.|
|||We also compare our CNN fusion method with the  Tracking results P@10 on the PETS2009 dataset are  5358  Table 2.|
|||On UCSD, the running times of fusion CNN with KCF, S-KCF, LCT and DSST are 19, 18, 10, 23 fps.|
|||The fusion model can improve visual trackers more than incorporating color and intensity cues (e.g., for UCSD, DSST tracker improves from 0.4058 to 0.4364 using HOG+A features, while it improves from 0.4085 to 0.5300 when fused with our fusion CNN).|
|||The last row of the table (Fusion / HOG+A) shows the fusion results using trackers with HOG+A features.|
|||Here, we report the tracking results when using uniform fusion model trained on all crowd levels in Table 5.|
|||Overall, the uniform fusion model performs a little worse than the separate model (the average P@10 of 0.3245 vs 0.3326, and the average IoU = 0.5 of 0.3381 vs 0.3559).|
|||However, the uniform fusion model can still significantly improve S-KCF tracker (the average P@10 improves from 0.2447 to 0.3245, and the average IoU = 0.5 improves from 0.2807 to 0.3381).|
|||We also evaluate training and testing across crowdlevels, where the fusion model is trained only on either L1, L2 or L3.|
|||Finally, we evaluate the cross-scene generalization ability of the fusion model.|
|||Tracking results on UCSD dataset for architecture variations of our fusion CNN.|
|||Comparison of fusion CNN architectures  In this subsection we compare different variations of the fusion CNN architecture.|
|||We fuse the appearance-based tracker and crowd density map together with a three-layer fusion CNN to produce a refined response map.|
||41 instances in total. (in cvpr2018)|
|11|Cross-Modality Binary Code Learning via Fusion Similarity Hashing|Few methods consider to preserve the fusion similarity among multi-modal instances instead, which can explicitly capture their heterogeneous correlation in cross-modality retrieval.|
|||In this paper, we propose a hashing scheme, termed Fusion Similarity Hashing (FSH), which explicitly embeds the graphbased fusion similarity across modalities into a common Hamming space.|
|||Inspired by the fusion by diffusion, our core idea is to construct an undirected asymmetric graph to model the fusion similarity among different modalities, upon which a graph hashing scheme with alternating optimization is introduced to learn binary codes that embeds such fusion similarity.|
|||We argue that such fusion similarity are more important for measuring the  7380                                                      The Framework of our proposed Fusion Similarity Figure 1.|
|||Hashing (FSH).FSH explicitly embeds the graph based fusion similarity across modalities into a common Hamming space.|
|||In particular, it is shown that binary code learning by fusion similarity is more robust to noise compared with that by indirectly preserving intraand inter-modal similarity respectively.|
|||The biggest concern lies in the efficiency issue in building the fusion model, i.e., the fusion graph, which typically needs relaxation on the eigen decomposition of the graph Laplacian, resulting in significant performance degeneration with the growth of hash bits.|
|||To address the above problems, we propose a novel cross-modality hashing method, termed Fusion Similarity Hashing (FSH), which makes the attempt towards directly preserving the fusion similarity from the multiple modalities to a common Hamming space.|
|||Such fusion similarity is robust to noise in capturing multi-modal relationship among instances.|
|||Different from the existing work of crossmodality hashing [12, 23], we argue that it is the fusion similarity, rather than the individual intra-modal similarity, that should be preserved in the common Hamming space.|
|||To that effect, an asymmetrical fusion graph is built, which simultaneously captures the intrinsic relations according to heterogeneous and homogenous data with a low storage cost afterwards.. After that, we design an efficient objective function to learn accurate binary codes and the corresponding hash functions in an alternating optimizing way.|
|||FSH first builds the similarity matrix in each modality, and then combining them to construct a matrix that reflect the fusion similarity.|
|||To handle such problem, we propose an alternating optimization algorithm, which also updates the fusion parameters, so as to find the optimal fusion graph to  generate more discriminated hash codes.|
|||We do this by minimizing the quantization error between the fusion similarity matrix G and the Hamming similarity matrix G , which can be written as:  G  G 2  ,  (2)  where    is the Frobenius norm of the matrix.|
|||Therefore, the key issue falls in the construction quality of the fusion similarity matrix G  R.|
|||Inspired by the Neighbor Set Similarity (NSS) [1], it is straightforward to define our fusion similarity in the following: Given two instances with two modalities  = {  ,  } and  = { }, the bi-modal NSS in the -th modality   ,   7381  can be defined as:  S    (),    () =  1 2       ()  (, ),     ( )  (3) where    () returns the -nearest neighbor index numbers according to the -th modal similarity measure.|
|||And the fusion similarity G(, ) across such two modalities can be defined via:  G(, ) = {S1 1   (),  2   (), S2 2   (),  1   ()},  (4)  It is quite intuitive to extend the above bi-modal fusion similarity to multi-modal case, i.e.|
|||We then use the SIAG to rewrite the fusion similarity:  G(, ) =       L(, ),  1   =1 1 2   =  L(, ) =  S   ,(),    ,(),  (6)  S   ,(),    ,() =  1 2              (, ),  where     index numbers of -nearest anchors in the -th modality.|
|||Such hash function learning can be easily integrated into the overall crossmodality similarity persevering, which is rewritten as:  B,W=B B G2 min   +B B G2    +   =1 B  (X)2    (8)  .. B  {1, 1},   =1  = 1, 0    1,  where  is a tradeoff parameter to control the weights between minimizing the binary quantization and preserving the fusion similarity.|
|||Therefore, to handle this problem, the symmetric fusion similarity matrix G can be approximated by Cholesky decomposition G = UU , where U  R, and U can be represented as the low-rank approximated to the high-dimensional matrix G. Although such decomposition makes the storage efficiency, it is still too complexity to calculate such decomposition.|
|||min    Then, we assume Bs = (B G)  {1, 1} to be the binary anchors, and U to be the affinity matrix that measures the fusion similarity between data points and anchors binary codes Bs.|
|||We then omit the first constant item and replace the third item with matrix A =   ) is the fusion similarity at the -th modality among anchor points.|
|||The new fusion similarity matrix G is updated upon the definition in Eq.6, which combines each modal asymmetric similarity to construct the matrix that reflect more accurate fusion similarity.|
|||For the asymmetric similarity matrix G  R, its multiplication G G  R approximately estimates the fusion similarity of anchor points, which is defined as A =  Proof.|
|||Except these, we further compare the FSH with a simple fusion graph construction, which just fuse the anchor graph in each modality, and we refer this as FSH-S which is commonly a strong baseline for both two crossmodality retrieval tasks4.|
|||It is worth noting that, for both tasks, our FSH shows advantage on precision with all hash bits, which is mainly due to the fact that more accurate similarity got from fusion similarity with SIAG can find more optimal binary codes from multi-modality.|
|||We further compare the simple fusion way, named FSH-S, with the baselines [12, 21, 5, 10].|
|||The results in Fig.2 and Tab.1 show that the simple fusion model can also achieve second best  5The percentage of mAP growth is obtained by the means of improve ment on all hash bits.|
|||However, FSH is overall better than FSH-S. As shown in [1], NSS is robust to noise with different similarity measures, and such similar scheme in fusion construction makes the proposed FSH more robust for cross-modality retrieval task.|
|||This demonstrates that the fusion similarity has advantageous to produce more distinguished binary code on the modality with weak expression power, which subsequently enhances the performance of single-modality retrieval.|
|||It is shown that a large size of anchors with small parameter  will bring more noise to the fusion graph, which decreases the performance of the proposed FSH.|
|||As a conclusion, for the proposed FSH, asymmetric fusion graph with little anchor points can enhance the performance of cross-modality retrieval, which solves the large-scale problem of binary code learning efficiently.|
|||In this framework, combining neighbor set similarity with sample important anchor graph can be embedded to the fusion graph matrix, leading to the learning of more discriminative binary codes.|
|||The core idea is to directly preserve the fusion sim This work is supported by the National Key Technology R&D Program (No.|
|||Unsupervised metric fusion by cross diffusion.|
|||Data fusion through cross-modality metric learning using similarity-sensitive hashing.|
||37 instances in total. (in cvpr2017)|
|12|Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper|This fusion usually happens at a coarse level, with significant resolution loss.|
|||The proposed continuous fusion layer is capable of encoding dense accurate geometric relationships between positions under the two modalities.|
|||Our proposed continuous fusion layer can be considered as a special case that connects points between different modalities.|
|||Since most self-driving cars are equipped with both LIDAR and cameras, sensor fusion between these modalities is desirable in order to further boost performance.|
|||Continuous fusion layers are used to fuse the image features onto the BEV feature maps.|
|||We design the continuous fusion layer to bridge multiple intermediate layers on both sides in order to perform multi-sensor fusion at multiple scales.|
|||After that we propose a deep multi-sensor detection architecture using this new continuous fusion layer.|
|||2: Continuous fusion layer: given a target pixel on BEV image, we first extract K nearest LIDAR points (Step 1); we then project the 3D points onto the camera image plane (Step 2-Step 3); this helps retrieve corresponding image features (Step 4); finally we feed the image feature + continuous geometry offset into a MLP to generate feature for the target pixel (Step 5).|
|||Continuous Fusion Layer: Our proposed continuous fusion layer exploits continuous convolutions to overcome the two aforementioned problems, namely the sparsity in the observations and the handling of the spatially-discrete features in camera view image.|
|||Given the input camera image feature map and a set of LIDAR points, the target of the continuous fusion layer is to create a dense BEV feature map where each discrete pixel contains features generated from the camera image.|
|||One difficulty of image-BEV fusion is that not all the discrete pixels on BEV space are observable in the camera.|
|||We use four continuous fusion layers to fuse multiple scales of image features into BEV network from lower level to higher level.|
|||Fusion Layers: Four continuous fusion layers are used to fuse multi-scale image features into the four residual groups of the BEV network.|
|||We initialize the image network with ImageNet pre-trained weights and initialize the BEV network and continuous fusion layers using Xavier initialization [14].|
|||Note that there is no direct supervision on the image stream; instead, error is propagated along the bridge of continuous fusion layer from the BEV feature space.|
|||We train a 3D multi-sensor fusion detection model, where all seven regression terms in Equation 3 are used.|
|||We compare our continuous fusion model with a LIDAR only model (LIDAR input), a sparse fusion model (no KNN pooling) and a discrete fusion model (no geometric feature).|
|||Our detector runs at > 15 frames per second, much faster than all other LIDAR based and fusion based methods.|
|||4.2 Ablation Study on KITTI  Continuous fusion has two components which enables the dense accurate fusion between image and LIDAR.|
|||We investigate these components by comparing the continuous fusion model with a set of derived models.|
|||The first derived model is a LIDAR BEV only model, which uses the BEV stream of the continuous fusion model as its backbone net and the same detection header.|
|||All continuous fusion models significantly outperform the BEV model in all six metrics, which demonstrates the great advantage of our model.|
|||The second derived model is a discrete fusion model, which has neither KNN pooling nor geometric feature.|
|||Continuous fusion models outperform the discrete fusion model in all metrics.|
|||For BEV detection, the discrete fusion model even has similar scores as the BEV  Deep Continuous Fusion  11  Vehicle  Pedestrian  Bicyclist  n/a  n/a  Model  AP0.5 AP0.7 AP0.3 AP0.5 AP0.3 AP0.5 91.35 79.37 n/a n/a 93.26 81.41 78.87 72.46 70.97 57.63 Ours (Continuous Fusion) 94.94 83.89 82.32 75.34 74.08 59.83  Ours (BEV only)  PIXOR  Table 3: Evaluation of multi-class BEV object detection on TOR4D dataset.|
|||We compare the continuous fusion model with the BEV baseline, and a recent LIDAR based detector PIXOR [37].|
|||When geometric feature is removed from MLP input, the performance of the continuous fusion model significantly drops.|
|||However, even when offsets are absent, the continuous fusion model still outperforms the discrete one, which justifies the importance of interpolation by KNN pooling.|
|||Continuous fusion layer has two hyper-parameters, the maximum neighbor distance d and number of nearest neighbors k. Setting a threshold on the distance to selected neighbors prevents propagation of wrong information from far away neighbors.|
|||Our continuous fusion model, its BEV baseline, and PIXOR [37] are compared.|
|||The fusion model achieves more gains for long range detection.|
|||Evaluation results We compare the continuous fusion model with two baseline models.|
|||Our continuous fusion model significantly outperforms the other two LIDAR based methods on all classes(Table 3).|
|||The continuous fusion model outperforms BEV and PIXOR [37] at most ranges, and achieves more gains for long range detection.|
|||The BEV and image pairs and detected bounding boxes by our continuous fusion model are shown.|
|||High resolution images can be readily incorporated into our model, thanks to the flexibility of continuous fusion layer.|
||37 instances in total. (in eccv2018)|
|13|Video2Shop_ Exact Matching Clothes in Videos to Online Shopping Images|These features are then fed into the similarity network to perform pair-wise matching between clothing regions from videos and shopping images, in which a reconfigurable deep tree structure is proposed to automatically learn the fusion strategy.|
|||The proposed approach attempts to allocate fusion nodes to summarize the single similarity located in different viewpoints.|
|||There are two types of nodes involved in the tree structure, i.e., similarity network node (SNN) and fusion node (FN), corresponding to the leaves and the branches in the tree.|
|||After that, these results are passed to fusion nodes, which generate a scalar output controlling the weight of similarity fusion.|
|||These fusion nodes will be passed layer by layer to fuse the results of internal results.|
|||Here, each low-level fusion node is connected to a specific SNN.|
|||The output of the low-level fusion node gij is a weighted score normalized by the scores of all fusion nodes connecting to the same top-level fusion node:  gij =  (4)  ei,j  Pi ei,j  Similarly, for the top-level fusion node F Nj , an intermeT (xj), where xj diate variable j is computed as: j = vj is an average pooling vector from multiple low-level fusion nodes, which are connected to F Nj , vj is the parameters of this fusion node.|
|||The fusion score gj is normalized by the scores of all top-level fusion nodes as: gj = ej Pj ej .|
|||Once the similarity network converges, the fusion strategy is obtained.|
|||The learning is implemented in a two-step iteration approach, where similar network nodes and fusion nodes will be mutually enhanced.|
|||The feature representation network and similar network nodes are first learned, and then the fusion nodes are learned when similar network nodes are fixed.|
|||Once the individual SNN is calculated, the fusion scores of all fusion nodes will be generated with a a tree-like structure.|
|||In this network, multiple lowlevel fusion nodes are connected to a higher-level fusion node, which forms a tree-like structure.|
|||The low-level fusion nodes refer to the leaves while the top-level is the side of the root.|
|||For a low-level fusion node F Nij ,  ( yklog (yk)+(1  yk) log (1  yk))+ kWik2  (5) where Wi is the parameters of i-th SNN, yk is the output of single similarity network with xk as the input, which is defined in Eqn.|
|||gj and gij are the fusion scores of higher and lowlevel fusion nodes.|
|||6 is that the similarity of all similar network nodes are passed to multiply layers of fusion nodes to generate the results of global similarity.|
|||6, the posterior probabilities of fusion nodes are defined.|
|||With Bayes rule, the posterior probabilities at the top-level fusion nodes and low-level nodes are denoted as follows:  hj =  gj Pi gijpi(y) Pj gj Pi gijpi(y)  hij =  gijpi(y)  Pi gij pij(y)  (7)  (8)  and  With these posterior probabilities, a gradient descent learning algorithm is developed for Eqn.|
|||The log likelihood function of a training sample is obtained as:  l = lnXj  gj Xi  gijpi(y)  (9)  In this case, by differentiating l with respect to the parameters, the following gradient descent learning rules for the parameters of top-level and low-level fusion nodes are obtained as:   vj = (hj  gj)xj  (t)   vij = hj(hij  gij)xij  (10)  (11)  where  is a learning rate.|
|||vj and vij are the parameters of high-level and low-level fusion nodes, respectively.|
|||To form a deeper tree, each SNN is expanded recursively into a fusion node and a set of subSNN networks.|
|||In our experiment, we have five-level deep tree structure and the number of fusion nodes in each level is 32, 16, 8, 4, 2, respectively.|
|||7-8; 6: Train fusion nodes as Eqn.|
|||Structure Selection of Similarity Network  To investigate the structure of similarity network, we vary the number of levels and the fusion nodes in similarity network, while keeping all other common settings fixed.|
|||We evaluate two types of architectures: 1) Homogeneous branches: all fusion nodes have the same number of branches; 2) Varying branches: the number of branches is inconsistent across layers.|
|||For homogeneous setting, one-level flat structure with 32 fusion nodes to hierarchical structure with five levels (62 fusion nodes) are tested.|
|||As the training proceeds, the parameters in the fusion nodes begin to grow in magnitude, which means that the weights of fusion nodes are becoming more and more reasonable.|
|||However, the improvement is not obvious after 4 epochs, since the weights of fusion nodes tend to be stable.|
|||As the training proceeds, the parameters in the fusion nodes begin to grow in magnitude.|
|||When the fusion notes begin to take action, the performance of the system is boosted.|
|||We also notice that the general performance is increased when more levels of fusion nodes are involved.|
|||It indicates that the similar network becomes stable when the levels of fusion nodes are more than three.|
|||Performance of Similarity Learning Network  In order to verify the effectiveness of our similarity network, we compare the performance of the proposed method with other methods when fusion nodes are not included.|
|||This is mainly because AsymNet can handle the temporal dynamic variety existing in videos, and it integrates discriminative information of video frames by automatically adjusting the fusion strategy.|
||36 instances in total. (in cvpr2017)|
|14|Liu_Learning_a_Recurrent_ICCV_2017_paper|In this work, we introduce a novel bridge between the modality-specific representations by creating a co-embedding space based on a recurrent residual fusion (RRF) block.|
|||Then, a fusion module is used to integrate the intermediate recurrent outputs and generates a more powerful representation.|
|||To address this issue, we propose a deep matching network using recurrent residual fusion (RRF) as building blocks for improving feature embeddings.|
|||The third component is the use of a fusion module, which aims to integrate intermediate recurrent outputs to generate a more powerful fused output.|
|||The fusion module facilitates making use of more complementary information in the intermediate layers and explicitly transferring their effects to the final output.|
|||We provide two efficient fusion modules: sum-pooling fusion and convolutional fusion.|
|||Deep fusion networks.|
|||Considering complementary representations from intermediate layers in deep networks, multi-layer (or multi-scale) fusion approaches have been well-studied and applied in many vision tasks, such as image-level classification [43, 23] and pixel-level prediction [24, 41].|
|||However, few works investigated the use of deep fusion networks for image and text matching.|
|||In this work, our fusion network is able to integrate the intermediate outputs of recurrent residual learning.|
|||Although the general idea of using residual blocks in RNNs has been studied in recent works [21, 44] for image classification and image super-resolution, our proposed RRF that acts as a deep fusion model aims to explore the intermediate features in recurrent residual learning.|
|||2) with three components: an identity connection, a recurrent connection and a fusion module.|
|||Built upon recurrent residual learning, we develop a fusion module (in red) to integrate the intermediate output vectors from each recurrent step.|
|||Specifically, there are two types of fusion modules: the sum-pooling fusion simply fixes equal weights, but the convolutional fusion can learn adaptive weights (drawn in different colors).|
|||Therefore, we develop a fusion module to explicitly aggregate the intermediate layers involved in the recurrent procedure.|
|||Figure 2 highlights the fusion module in red.|
|||Specifically, several new side branches (dot lines in red) are generated from intermediate layers and then merged into a fusion module.|
|||As the intermediate layers have the same dimension, the fusion module is able to integrate them without adding extra new transition layers.|
|||In a fusion module, T + 1 side outputs are stacked as a layer S. S has 1  N  (T + 1) size, where N is the dimension of each side output.|
|||Based on S, we employ two fusion methods to compute a fused output vector: sumpooling fusion and convolutional fusion.|
|||(5)  The sum-pooling fusion supposes that each side branch has the same importance without learning any weights.|
|||Therefore, we use a convolutional layer in the fusion module to learn adaptive weights (or importance) for better fusing side branches.|
|||Different from other deep fusion networks in which different layers are aggregated, RRF delves into improving the discrimination of one layer over recurrence.|
|||Method  Baseline  RRF-Net, T=1 RRF-Net, T=2 RRF-Net, T=3 RRF-Net, T=4  Image to Text R@1 45.0 46.4 46.9 47.6 46.2  R@5 75.5 76.1 76.8 77.4 76.6  Text to Image R@5 R@1 66.5 33.6 34.3 67.3 67.7 34.8 68.3 35.4 35.1 67.6  Table 2: Evaluation for fusion modules on the Flickr30K test set.|
|||The convolutional fusion shows better results by learning adaptive weights.|
|||Method  RRF-Net w/o fusion module  RRF-Net with sum fusion RRF-Net with conv fusion  Image to Text R@5 R@1 45.8 75.9 76.8 47.1 47.6 77.4  Text to Image R@5 R@1 34.2 67.1 67.6 35.0 35.4 68.3  5.|
|||Evaluation for fusion modules.|
|||Recall that we define two types of fusion modules.|
|||First, we trained a RRF-Net model without using any fusion module, which is actually a recurrent residual model in Fig.|
|||By comparison, we can see that using fusion modules can achieve remarkable improvements.|
|||Moreover, the advantage of the sum-pooling fusion is that it is parameter-free, however, the convolution fusion yields better results than the sumpooling fusion due to learning adaptive weights.|
|||Furthermore, we analyzed T + 1 adaptive weights learned in the convolutional fusion module.|
|||These results provide deeper insights towards the convolutional fusion module.|
|||On the exploration of convolutional fusion networks for visual recognition.|
||35 instances in total. (in iccv2017)|
|15|Wenhao_Jiang_Recurrent_Fusion_Network_ECCV_2018_paper|In this paper, to exploit the complementary information from multiple encoders, we propose a novel recurrent fusion network (RFNet) for the image captioning task.|
|||The fusion process in our model can exploit the interactions among the outputs of the image encoders and generate new compact and informative representations for the decoder.|
|||Multiple CNNs are employed as encoders and a recurrent fusion procedure is inserted after the encoders to form better representations for the decoder.|
|||The fusion procedure consists of two stages.|
|||In this paper, to exploit complementary information from multiple encoders, we propose a recurrent fusion network (RFNet) for image captioning.|
|||1, introduces a fusion procedure between the encoders and decoder.|
|||The fusion procedure performs a given number of RNN steps and outputs the hidden states as thought vectors.|
|||Our fusion procedure consists of two stages.|
|||The common ensemble technique in image captioning is regarded as an output fusion technique, combining the output of the decoder at each time step [18, 19, 24].|
|||Our method can be regarded as a kind of intermediate fusion methods.|
|||The fusion stage I contains M review components.|
|||The fusion stage II is a review component that performs the multi-attention mechanism on the multiple sets of thought vectors from fusion stage I.|
|||The parameters of LSTM units in the fusion procedure are all different.|
|||4 Our Method  In this section, we propose our recurrent fusion network (RFNet) for image captioning.|
|||The fusion process in RFNet consists of two stages.|
|||The fusion procedure of RFNet consists of two stages, specifically fusion stage I and II.|
|||The hidden states and memory cells after the last step of fusion stage I are aggregated to form the initial hidden state and the memory cell for fusion stage II.|
|||In our model, the LSTM unit LSTM(m) (, ) can be different for different t and m. Hence, M  T1 LSTMs are used in fusion stage I.|
|||In fusion stage I, the interactions among review components are realized via Eq.|
|||4.3 Fusion Stage II  The hidden state and memory cell of fusion stage II are initialized with h(1) T1 and c(1) T1  .|
|||The hidden states of fusion stage II are collected to form the thought vector  set:  C = {hT1+1,    , hT1+T2 } ,  (14)  which will be used as the input of the attention model in the decoder.|
|||Recurrent Fusion Network for Image Captioning  9  4.4 Decoder  The decoder translates the information generated by the fusion procedure into natural sentences.|
|||The initial hidden state and memory cell are inherited from the last step of fusion stage II directly.|
|||(6), the complete loss function of our model is expressed as:  Lall = L +    M + 1Ld(C) +  M  m=1  LdB(m) ,  (19)  where  is a trade-off parameter, and B(m) and C are sets of thought vectors from fusion stages I and II.|
|||With the recurrent fusion strategy, RFNet can extract useful information from different encoders to remedy the lacking of information about objects and attributes in the representations.|
|||To study the effects of the two fusion stages, we present the performance of the following models:   RFNetI denotes RFNet without fusion stage I, with only the fusion stage II preserved.|
||| RFNetII denotes RFNet without fusion stage II.|
||| RFNetinter denotes RFNet without the interactions in fusion stage I.|
|||Each component in the fusion stage I is independent.|
|||Therefore, with the specifically designed recurrent fusion strategy, our proposed RFNet provides the best performance.|
|||Ablation study of fusion stages I and II in our RFNet.|
|||  0  1  10  CIDEr  105.2  107.2  107.3  100  104.7  6 Conclusions  In this paper, we proposed a novel recurrent fusion network (RFNet), to exploit complementary information of multiple image representations for image captioning.|
|||In the RFNet, a recurrent fusion procedure is inserted between the encoders and the decoder.This recurrent fusion procedure consists of two stages, and each stage can be regarded as a special RNN.|
|||Wang, J., Jiang, W., Ma, L., Liu, W., Xu, Y.: Bidirectional attentive fusion with context gating for dense video captioning.|
||35 instances in total. (in eccv2018)|
|16|Xu_Feature_Weighting_via_2013_ICCV_paper|To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously.|
|||Compared to state-of-the-art fusion algorithms, our approach achieves promising improvements on HMDB [8] action recognition dataset and CCV [5] video classification dataset.|
|||In addition, experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection.|
|||As for event detection tasks, reports from teams with top performance [26, 14, 15] in TRECVID MED competition show that fusion, either feature-level fusion or decision-level fusion brings performance gain into the detection tasks.|
|||Fusion mechanisms can be grouped into two types which are feature-level fusion and decision-level fusion.|
|||The other fusion mechanism is decision-level fusion, which adopts classifiers to features and then fuses the results based on the confidence scores.|
|||The most widely used decision-level fusion method is to assign average weights to confidence scores from each feature, which may restrain the overall performance due to the inconsistency and incomparability of confidence scores from different models.|
|||Another issue in decision-level fusion is the difference of thresholds among confidence scores from different models.|
|||If the effects of the difference of thresholds among predictive results are ignored, it would degrade the discriminative ability of the fusion result.|
|||Inspired by [9], we combine the early fusion result at the decision-level fusion.|
|||The experiment shows that it is beneficial to exploit the unlabeled data for multiple feature fusion when the labeled data are few.|
|||The adaptive decision-level fusion assigns lower weights to specific scores if the confidence scores are near the threshold while assigns higher weights to videos if the confidence scores are very far away from the threshold.|
|||Thresholds are set before the fusion stage.|
|||An illustration of our Feature Weighting via Optimal Thresholding (FWOT) fusion method  1  0.5  0  0.5  1 1  0.5  0  0.5  1  1  0.5  0  0.5  1 1  0.5  0  0.5  1  1  0.5  0  0.5  1 1  a = 0.5 Figure 2. f (x) = tanh x  a = 0.2  a of different parameters  0.5  0  0.5  1  a = 0.1  3.|
|||Then we show the detailed steps to obtain the optimal fusion function.|
|||However, the sgn() function here makes the fusion process inflexible, since videos with much higher confidence scores than the threshold and those with  3435 3442  confidence scores a little bit higher than the threshold would contribute equally to the fusion result.|
|||Thus the final fusion function can be formulated as,  f (x) =  wi tanh  i=1  ai  fi(x)  bi  .|
|||Furthermore, we define a function gD : R as:  To step further, we define a function  : R  (cid:3)  (cid:3)  gD(x) = (D (cid:5) F (x)),  (3) and (cid:5) is the Hadamard Denoting the fusion classifier as f (x) =  fi(x)bij  aik  where Fijk(x) = tanh product.|
|||We compare the result with state-of-the-art fusion algorithms, including Early Kernel Fusion (EKF) [18], Multiple Kernel Learning (MKL) [16], and LPBoost [4].|
|||Other late fusion method like linear SVM on top of normalized decision scores from all the different features has similar optimization goal and consistent performance with the LPBoost.|
|||Thus in the late fusion comparison algorithms, we only report the result of LPBoost.|
|||The top row shows the performance of the best individual feature, and others indicate performance of fusion methods.|
|||Before the fusion stage, we train a multi-class SVM classifier for each visual feature with one-vs-all approach.|
|||Results are shown in Table 3, in which we list the performance of the best individual feature Dense Trajectories to show the improvement of the fusion methods over the individual feature.|
|||Experiment on Columbia Consumer Video  dataset  For the video classification task , we use Columbia Consumer Video dataset (CCV) [5] to compare the performance of different fusion methods.|
|||The top row shows the performance of the best individual feature, and others indicate performance of fusion methods.|
|||Note that in the Adaptive Late Fusion (ALF) algorithm, thresholds are set before the fusion process, and bad thresholds would lead to weak performance of ALF method.|
|||O collections shows that ALF may suffer from the difficulty of getting a good detection threshold and show unstable performance in the fusion stage.|
|||On the contrary, our method learns proper thresholds in the process of weighting fusion, which makes the fusion method more robust in the event detection system.|
|||We can see that our fusion method outperforms other state-of-the-art fusion algorithms in 8 out of 10 events in TRECVID MED 11 DEV-O collection.|
|||In addition, we achieve the best performance among different fusion methods on a large-scale video dataset TRECVID MED 2011 (including DEV-T and DEV-O collections) using both Average Precision and Pmiss@TER=12.5 metrics.|
|||Multimodal feature fusion for robust event detection in web videos.|
|||Multi-feature fusion via hierarchical regression for multimedia analysis.|
||34 instances in total. (in iccv2013)|
|17|Deep Affordance-Grounded Sensorimotor Object Recognition|In particular, object perception is based on the fusion of sensory (object appearance) and motor (human-object interaction) information.|
||| Extensive quantitative evaluation of the proposed fusion methods and comparison with traditional probabilistic fusion approaches.|
|||Related Work  Most sensorimotor object recognition works have so far relied on simple fusion schemes (e.g using simple Bayesian models or the product rule), hard assumptions (e.g.|
|||Eventually, appearance and affordance information are combined to yield improved object recognition, following various fusion strategies.|
|||The visual front-end module (left) processes the captured data, providing three information streams (middle) that are then fed into a single-stream or fusion DL model (right).|
|||Fusion architectures  Prior to the detailed description of the evaluated sensorimotor information fusion principles, it needs to be noted that these are implemented within two general NN architectures, namely the Generalized Template-Matching (GTM) and the Generalized Spatio-Temporal (GST) one.|
|||4.4.1 Late fusion  Late fusion refers to the combination of information at the end of the processing pipeline of each stream.|
|||The FC layer fusion is performed by concatenating the FC features of both streams.|
|||It was experimentally shown that fusion after the RL6 layer was advantageous, compared to concatenating at the output of the FC6 layer  Figure 3.|
|||Detailed topology of the GTM architecture for: a) late fusion at FC layer, b) late fusion at last CONV layer, c) slow fusion, and d) multi-level slow fusion.|
|||Regarding fusion at the last CONV layer, the RL53 activations of both appearance and affordance CNNs are stacked.|
|||For the GST architecture, the late fusion scheme considers only the concatenation of the features of the last FC layers of the appearance CNN and the affordance LSTM model, as depicted in Fig.|
|||In this context, an asynchronous late fusion approach is also investigated for the GST architecture.|
|||Specifically, the GST late fusion scheme (Fig.|
|||the internal state vector h(t) of the last LSTM layer] is provided with a time-delay factor, denoted by  > 0, compared to the FC features of the appearance stream; in other words, the features of the affordance stream at time t   are combined with the appearance features at time t.  4.4.2 Slow fusion  Slow fusion for the GTM architecture corresponds to the case of combining the CONV feature maps of the appearance and affordance CNNs in an intermediate layer (i.e.|
|||The actual fusion operator is materialized by simple stacking of the two feature maps.|
|||For the GST architecture, the slow fusion scheme considers only the concatenation of the features of the RL7 layer of the appearance and the affordance CNNs models, followed by an LSTM model, as can be seen in Fig.|
|||In order to simulate the complex information exchange routes at different levels of granularity between the two streams, a multi-level slow fusion scheme is also examined.|
|||GTM and GST architectures evaluation  In Table 4, evaluation results from the application of different GTM-based fusion schemes (Section 4.4) are given.|
|||From the presented results, it can be seen that for the case of late fusion combination of CONV features (i.e.|
|||However, single-level slow fusion tends to exhibit lower recognition performance than late fusion.|
|||Building on the evaluation outcomes of the single-level slow and late fusion schemes, multi-level slow fusion architectures are also evaluated.|
|||This is mainly due to the preservation of the spatial correspondence (initial fusion at the CONV level), coupled with the additional correlations learned by the fusion at the FC level.|
|||, RL5af f  3  3  Experimental results from the application of the GSTbased fusion schemes (Section 4.4) are reported in Table 5.|
|||It can be observed that asynchronous fusion leads to decreased performance, compared to the synchronous case, while increasing values of the delay parameter  lead to a drop in the recognition rate.|
|||Moreover, the slow fusion approach results to a significant decrease of the object recognition performance.|
|||architectures: GATF T (param), where the Generalized Architecture Type, GAT  {GTM, GST}, and the Fusion Type, FT  {LS, LA, SSL, SM L}  {Late Synchronous, Late Asynchronous, Slow Single Level, Slow Multi Level} and param indicates the specific parameters for each particular fusion scheme (as detailed above).|
|||From the results presented in Table 3 (only overall classification accuracy is  1http://torch.ch/  Method Appearance CNN Affordance CNN affordance recognition Affordance CNN-LSTM affordance recognition  object recognition  Task  85.12 81.92 69.27  Accuracy (%)  GST-based fusion architecture [after fusion] Accuracy (%)  Table 3.|
|||GTM-based fusion architecture [after fusion] Accuracy (%)  GTMLS(FC6) GTMLS(RL53) [1 CONV, 1 FC] GTMLS(RL53) [1 CONV, 2 FC] GTMLS(RL53) [2 CONV, 1 FC] GTMLS(RL53) [2 CONV, 2 FC] GTMSSL(RL3app GTMSSL(RL4app GTMSSL(RL4app GTMSSL(RL5app GTMSM L(RL5app GTMSM L(RL5app  , RL3af f ) , RL4af f ) , RL4af f ) , RL5af f ) , RL5af f , RL5af f  3  3  1  1  3  3  1  3  3  3  3  3  , RL6) , RL6)  87.40 87.65 88.24 87.64 86.40  78.74 87.20 85.82 88.13  88.23 89.43  Table 4.|
|||Two generalized neuro-biologically and neuro-physiologically grounded neural network architectures, implementing multiple fusion schemes for sensorimotor object recognition were presented and evaluated.|
|||The proposed sensorimotor multi-level slow fusion approach was experimentally shown to outperform similar probabilistic fusion methods of the literature.|
||32 instances in total. (in cvpr2017)|
|18|Prabhakar_DeepFuse_A_Deep_ICCV_2017_paper|The proposed approach uses a novel CNN architecture trained to learn the fusion operation without reference ground truth image.|
|||The contributions of this work are as follows:  A CNN based unsupervised image fusion algorithm  for fusing exposure stacked static image pairs.|
|||Section 3, we present our CNN based exposure fusion algorithm and discuss the details of experiments.|
|||[16] perform exposure fusion using simple quality metrics such as contrast and saturation.|
|||[9], [10] different approaches to exposure fusion have been reported.|
|||[22] proposed a fusion technique using quality metrics such as local contrast and color consistency.|
|||The random walk approach they perform gives a global optimum solution to the fusion problem set in a probabilistic fashion.|
|||Proposed Method  In this work, we propose an image fusion framework using CNNs.|
|||Architecture of proposed image fusion CNN illustrated for input exposure stack with images of size h  w. The pre-fusion layers C1 and C2 that share same weights, extract low-level features from input images.|
|||Since the fusion happens in the pixel domain itself, this type of architecture does not make use of feature learning ability of CNNs to a great extent.|
|||The proposed network architecture for image fusion is illustrated in Fig.|
|||The proposed architecture has three components: feature extraction layers, a fusion layer and re construction layers.|
|||This is understandable as the network needs more number of iterations to figure out appropriate fusion weights.|
|||Let {yk}={yk|k=1,2} denote the image patches extracted at a pixel location p from input image pairs and yf denote the patch extracted from CNN output fused image at same location p. The objective is to compute a score to define the fusion performance given yk input patches and yf fused image patch.|
|||Thus, different fusion strategies are followed in literature for Y and Cb/Cr fusion ([18], [24], [26]).|
|||Although number of other IQA models for general image fusion have also been reported, none of them makes adequate quality predictions of subjective opinions [15].|
|||This experiment can test the capability of CNN to learn complex fusion rules from data itself without the help of MEF SSIM loss function.|
|||[16] is a simple and effective weighting based image fusion technique with multi resolution blending to produce smooth results.|
|||However, it suffers from following shortcomings: (a) it picks best parts of each image for fusion using hand  4719  (a) UE input  (b) OE input  (c) Li et al.|
|||proposed a patch based fusion algorithm that fuses patches from input images based on their patch strength.|
|||In summary, the advantages offered by DF are as follows: 1) Better fusion quality: produces better fusion result even for extreme exposure image pairs, 2) SSIM over l1 : In [29], the authors report that l1 loss outperforms SSIM loss function.|
|||3) Generalizability to other fusion tasks: The proposed fusion is generic in nature and could be easily adapted to other fusion problems as well.|
|||Application to Multi-Focus Fusion  In this section, we discuss the possibility of applying our DeepFuse model for solving other image fusion problems.|
|||Exposure fusion usIEEE Trans.|
|||Generalized random walks for fusion of multi-exposure images.|
|||Extreme learning machine based exposure fusion for displaying HDR scenes.|
|||Exposure fusion based on steerable pyramid for displaying high dynamic range scenes.|
|||Fast multi-exposure image fusion with median filter and recursive filter.|
|||Image fusion with guided filtering.|
|||Multi-focus image fusion with  dense SIFT.|
|||Ghosting-free multiexposure image fusion in gradient domain.|
||32 instances in total. (in iccv2017)|
|19|Zhang_Supervision_by_Fusion_ICCV_2017_paper|The key insight is supervision by fusion, i.e., generating useful supervisory signals from the fusion process of weak but fast unsupervised saliency models.|
|||Based on this insight, we combine an intra-image fusion stream and a inter-image fusion stream in the proposed framework to generate the learning curriculum and pseudo ground-truth for supervising the training of the deep salient object detector.|
|||Moreover, some modern fusion models can not only integrate the weak saliency models to obtain stronger saliency prediction but also automatically infer the reliabilities for each weak saliency model under the condition of the given image context at the same time.|
|||Examples to show that directly training the deep salient object detector in the native way cannot work well, while the proposed supervision by fusion strategy can guide a more robust unsupervised learning procedure.|
|||As such saliency maps can be extracted in parallel, which is efficient, and the fusion process itself is not time-consuming2, this strategy tends to be a cost-effective way to improve the unsupervised learning performance.|
|||Secondly, during the fusion process, we can simultaneously infer the difficulty of each training data (both training images and more detailed superpixel regions), which provides a natural way to reflect their corresponding learning confidence.|
|||In addition, after each learning iteration, the fusion process will again be used to update the difficulties of the training samples based on the current weak saliency predictions, which essentially forms the learning curriculum dynamically.|
|||3) and inter-image fusion is performed on all the training images to obtain the imagelevel confidence and image-level fusion map (the green  2The GLAD fusion model [36] takes 10 minutes for 1 million images  using a single core of a Xeon 2.8 GHz processor.|
|||2) We reveal the insight of supervision by fusion, i.e., generating reliable supervisory signals from the fusion process of weak saliency models in iterative learning stages.|
|||Through fusion, we can use the obtained fusion map to provide more reliable supervision and the sample confidence weights to generate the dynamic learning curriculum.|
|||Then, the superpixellevel confidence maps {bn} and superpixel-level fusion maps {SFMn} are obtained by intra-image fusion (see Sec.|
|||2.1), while the image-level confidence weights {n} and image-level fusion maps {IFMn} are obtained by interimage fusion (see Sec.|
|||(1)  Afterwards, for inferring the superpixel-level reliabilities of the weak salient object detectors {am n } and the difficulties of various superpixel regions {bn,i}, we adopt the GLAD fusion model [36], which is a probabilistic model providing a principled way to approach the fusion problem.|
|||Given the observed weak saliency labels {lm n,i}, GLAD infers the  4The intra-image fusion can also perform on pixel-level, while this pa per uses superpixels for pursuing lower computational cost.|
|||Finally, the superpixel-level learning confidence maps {bn} can be formed by the inferred {bn,i} and the superpixel-level fusion maps can be obtained by5:  SFMn =  M  Xm=1  n  WSMm am n ,  (3)  2.2.|
|||Inter-Image Fusion  Different from intra-image fusion, inter-image fusion integrates the weak saliency maps from the entire training image collection instead of the content of each single image, and the fusion process considers the global image scenes rather than the local superpixel regions.|
|||Afterwards, we adopt the GLAD fusion model to infer the imagelevel reliabilities of the weak salient object detectors {m} and the difficulties of various training images {n}.|
|||Finally, the image-level fusion maps can be obtained by6:  IFMn =  M  Xm=1  m  WSMm n ,  (6)  2.3.|
|||The  5Here we name the fusion results obtained from the intra-image fusion process as the superpixel-level fusion maps because the basic computational unit in the intra-image fusion is each superpixel region.|
|||6Here we name the fusion results obtained from the inter-image fusion process as the image-level fusion maps because the basic computational unit in the inter-image fusion is each image.|
|||For training such deep salient object detector without using any human annotation, we introduce three channels of supervisory signals, including the superpixel-level fusion maps {SFMn}, the image-level fusion maps {IFMn}, and the learning confidence maps {LCMn}, where LCMn = bn  n  7.|
|||LCMn,d  [0, 1] indicates the d-th element of the learning confidence map LCMn, SF Mn,d  {0, 1} and IF Mn,d  {0, 1} indicate the d-th element of the binarized superpixellevel fusion map SFMn and image-level fusion map IFMn, respectively, (In|W)d  [0, 1] indicates the d-th element of the predicted saliency map (In|W), r() is the squared l2-norm, and  is the weight decay factor, d indicates the pixel index for the fusion maps and the confidence maps.|
|||(7), the first term penalizes the predictions which are inconsistent with the superpixel-level fusion maps, while the second term penalizes the predictions which are inconsistent with the image-level fusion maps.|
|||As either the superpixel-level fusion maps or the image-level fusion maps are not perfect, collaborating these two kinds of fusion maps could provide supervision that is complementary to each other, leading to more effective learning performance as demonstrated in Sec.|
|||Besides, the proposed learning scheme also has some interesting differences compared with SPL and CL: 1) Instead of inferring the self-paced learning weights via the internal force (the learnt classifier itself), the learning weights of the proposed learning scheme are inferred by the external force (the fusion process).|
|||The only difference is that the supervision of DHS comes from the pixel-wise human annotation whereas the supervision of our approach is automatically generated by the supervision by fusion frame work.|
|||AP  -SFM .750 -b .754 -IFM .749 .754  OURS  .789  F  .647 .658 .649 .674  .676  SOD  MAE  SOV  .154 .150 .150 .148  .140  .520 .526 .525 .534  .545  ECSSD  DUT-O  AP  .852 .864 .844 .876  .889  F  .760 .768 .757 .781  .787  MAE  SOV  .099 .093 .096 .095  .085  .657 .662 .657 .660  .677  AP  .640 .667 .649 .677  .715  F  .541 .561 .552 .579  .583  MAE  SOV  .162 .148 .152 .142  .135  .471 .487 .481 .498  .505  AP  .754 .759 .750 .777  .791  PASCAL-S F MAE  .649 .664 .653 .679  .680  .151 .148 .149 .144  .141  SOV  .528 .541 .534 .549  .549  demonstrates the rationality of the proposed supervision by fusion strategy as well as the established unsupervised learning framework.|
|||7, where -SFM, -b,-IFM,indicate the learning framework without using the supervisory signal of the superpixellevel fusion map, the superpixel-level confidence map, the image-level fusion map, and the image-level confidence weight, respectively.|
|||From the experimental results, we can observe that:  1) All the four types of supervisory signals are beneficial to the proposed learning framework as without using each of them can cause obvious performance drop, especially in terms of AP and F; 2) Basically, the average performance drop (i.e., the importance) of the used supervisory signals follows: SFM (0.4839) > IFM (0.431) > b (0.312) >  (0.186), which demonstrates that the superpixel-level fusion map and the image-level fusion map provide the major supervision for the network while the superpixel-level confidence map and the image-level confidence weight provide additionally helpful supervision; 3) Consistent and obvious performance gain can be obtained by the full learning framework as compared with other baselines, which  9The reported number is the average performance drop across all the  evaluation metrics and datasets.|
|||It revealed the insight of supervision by fusion and established a novel two-stream framework to generate useful supervisory signals through the intra-image fusion and interimage fusion processes.|
||31 instances in total. (in iccv2017)|
|20|cvpr18-Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection|To  this end, we design a novel complementarity-aware fusion  (CA-Fuse)  module  when  adopting  the  Convolutional  Neural  Network  (CNN).|
|||The  proposed  RGB-D  fusion  network  disambiguates  both  cross-modal  and  cross-level  fusion  processes  and  enables  more  sufficient fusion results.|
|||(a)  Early  fusion  scheme  adopted in [13] and (b) late fusion scheme adopted in [14].|
|||Although  encouraging  performance has been achieved by these networks, there is      Figure  2:  The  architecture  of  the  proposed  progressively  complementarity-aware  fusion  network  for  RGB-D  salient  object  detection.|
|||Most  of  previous  RGB-D  fusion  networks  explore the cross-modal complementarity by a two-stream  architecture  shown  in  Fig.|
|||Most  of  RGB-D  fusion  networks  [19,  25,  27]  combine  RGB  and  depth  modalities  by  only  fusing  their  deep  CNN  features  (i.e.,  late  fusion),  while  we  believe  that  the  cross-modal  complement  for  saliency  detection  exists  across  multiple  levels, which are not well-explored by previous works.|
|||supply  more   features   spatial   In  our  view,  addressing  these  problems  will  enable  the  multi-modal  fusion  network  to  capture  cross-modal  and  cross-level  complement  more  sufficiently.|
|||To  this  end,  in  this  progressively  complementarity-aware fusion network (shown in Fig.|
|||Hence,  the  multi-modal  fusion  process  will  be  complementarity-aware  in  terms  of  both  cross-modal  and  cross-level  views,  resulting  in  sufficient  multi-modal  multi-level  fusion.|
|||the   In  summary,   the  proposed  RGB-D  salient  object   detection network enjoys several distinguished benefits:   1)  The  cross-modal  complementarity  can  be  explicitly   3052     Figure 3: The architectures of different multi-modal fusion modules.|
|||Result  fusion  methods  include  summation [35, 42], multiplication [31] and designed rules  [33].|
|||However, due to the lack of cross-modal interactions  in the feature-extraction  stage,  the  result  fusion  scheme  is  insufficient to leverage underlying cooperative information  during the unimodal prediction course.|
|||[19]  use  a  two-stream  late  fusion  architecture  to  fuse  RGB-D  deep  features.|
|||Nonetheless,  in  its  multi-modal  fusion  stage,  it  still  follows  the  paradigm  of  direct  feature  concatenation  without  any  explicit  formulation on the cross-modal complementarity.|
|||Draw inspiration from  unimodal  networks  [44]  and  [45],  in  which  deep  supervisions  are  introduced  to  facilitate  convergence  and  generate  hierarchical  representations,  we  consider  that  an  effective solution is to introduce intermediate supervisions  on  top  of  each  multi-modal  fusion  level  (Fig.|
|||The  added  intermediate  supervision  can  act  as  instruction  to  encourage  multi-modal  fusion  in  each  level  timely,  thus   3054  Module   CA-Fuse 6     Adaptation layers   1     2     Transition   layer         384, 11   CA-Fuse 5   384, 11   384, 11   384, 11   CA-Fuse 4   384, 33   384, 33   256, 11   CA-Fuse 3   192, 33   192, 33   128, 11   CA-Fuse 2   128, 33   12833     Table  1:  Illustration  of  the  parameters  of  the  intra-level  adaptation  layers  inside  the  CA-Fuse  module  and  the  transition layer between two neighboring CA-Fuse modules.|
|||reducing the multi-level fusion uncertainty.|
|||Although  this  strategy  is  able  to  ease  the  multi-level  multi-modal  fusion  process  effectively,  the  multi-modal  fusion component in each level still does not go beyond the  traditional direct concatenation scheme, which in our view,  is  unlikely  the  cross-modal  complementary  information.|
|||To  address  this  problem,  we  further  tailor  a  complementarity-aware  fusion  (CA-Fuse)  module  (Fig.|
|||By  this  way,  the   RP ,   m  DP  and   RDP  m  Figure 4: Visual comparison of using different multi-modal fusion modules shown in Fig.|
|||Then  the  enhanced   will  be  selected  by  a  transition  layer  (detailed   m  RF  m  DF  and    along  with  the  selected  features    from the m+1 layer are concatenated and fused by one   convolutional   layer   to   learn   cooperative    and make integrated predictions    features  + m m RDF 11   1  ,  m  RDF  representations         P  m RD  =  m  RD  (    F F F    ,  ,  m D  m R  + 1 m , m RD  ),     (1)     where  m  RD denotes  the  parameters  of  the  fusion  layer  and   3055     Side out   NLPR   NJUD   STEREO   Fig.|
|||3(c)   2   3   4   5   6   0.836  0.850  0.845  0.862  0.864  0.872   0.839  0.851  0.843  0.860  0.864  0.871   0.838  0.846  0.837  0.854  0.863  0.869   0.813  0.821  0.809  0.833  0.848  0.856   0.808  0.817  0.813  0.829  0.846  0.855   Table  2:  F-measure  scores  on  three  datasets  with  adopting  different multi-modal fusion modules in Fig.|
|||3(b) BPDC in Fig.4), the multi-modal fusion network is  basically able to learn level-specific predictions.|
|||Nonetheless,  owing  to  that  the  multi-modal  feature  fusion  component  is  still  implemented  by  direct  concatenation, the Fig.|
|||This visualization  reveals the contribution of each layer clearly and verifies the  effectiveness of the proposed cross-level fusion strategy.|
|||In  these  challenging  cases,  most  of  other  methods  are  unlikely  to  locate  the  salient  object  due  to  the  lack  of  high-level  contextual reasoning or robust multi-modal fusion strategy.|
|||The  introduced  cross-modal/level  connections  and  modal/  level-wise supervisions explicitly encourage the capture of  complementary  information  from  the  counterpart,  thus  reducing fusion ambiguity and increasing fusion sufficiency.|
|||Comprehensive experiments demonstrate the effectiveness  of  the  proposed  multi-modal  multi-level  fusion  strategies,  which  may  also  benefit  other  RGB-D  systems  and  even  other multi-model fusion problems.|
||29 instances in total. (in cvpr2018)|
|21|Zheng_Query-Adaptive_Late_Fusion_2015_CVPR_paper|Query-Adaptive Late Fusion for Image Search and Person Re-identification  Liang Zheng1, Shengjin Wang1, Lu Tian1, Fei He1, Ziqiong Liu1, and Qi Tian2  1State Key Laboratory of Intelligent Technology and Systems;  1Tsinghua National Laboratory for Information Science and Technology;  1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China  2University of Texas at San Antonio, TX, 78249, USA  liangzheng06@gmail.com wgsgj@tsinghua.edu.cn qitian@cs.utsa.edu  Abstract  Feature fusion has been proven effective [35, 36] in image search.|
|||Towards this goal, this paper proposes a simple yet effective late fusion method at score level.|
|||We show that our method is robust to parameter changes, and outperforms two popular fusion schemes, especially on the resistance to bad features.|
|||Recently, the fusion of multiple features [35, 36, 40] has been pushing the state-of-the-art forward.|
|||This problem is not trivial: some state-of-the-art fusion methods [40, 35], as will be shown, suffer from the fusion of black sheep features.|
|||Another issue that should be paid attention to includes the amenability of fusion method to database updating.|
|||It requires that the fusion algorithm be independent on the test database, so that its effectiveness can be preserved in an updated database.|
|||In light of the above analysis, this paper proposes a score-level fusion scheme based on a simple motivation (Fig.|
|||On the other hand, late fusion refers to fusion at score or decision levels [20, 8, 29, 9].|
|||Feature fusion has been demonstrated as effective in image search.|
|||Three features obtain APs of 0.9083, 0.6671, and 0.0025, respectively, and the fusion result is AP = 1.|
|||First, previous works in biometric multi-modality fusion [16, 1] demonstrate that the product rule has very similar, if not superior, performance to the sum rule.|
|||Results on benchmarks with different fusion methods.|
|||We find that the fusion accuracy increases steadily with Q.|
|||When a large number of bad features are present, it is desirable that fusion result not be influenced too much.|
|||Comparison with other fusion schemes.|
|||In order to further verify the strength of our method, results of two stateof-the-art fusion schemes, i.e., Graph Fusion [35] and CoIndexing [36] are presented in Table 2, Fig.|
|||9 indicate that graph fusion is sensitive to parameter kNN, which, in order to obtain fine accuracy, should be consistent with the average number of groundtruth images in the dataset.|
|||On Ukbench, our result is lower than graph fusion only when kNN = 4, which is the ideal parameter setting on Ukbench.|
|||Moreover, when bad features, such as GIST and Random are used, graph fusion does not have a fall back mechanism (in fact, it treats all the features as equally important), and the resulting performance could be worse than BoW.|
|||This method is similar to graph fusion in that both require to query all the database images in an offline manner.|
|||Specifically, although LBP and HOG yield low matching rate, the fusion of both features still improves recognition accuracy.|
|||Conclusion  This paper proposes a score-level fusion scheme featured by two advantages.|
|||This enables safe fusion in that ineffective features are unlikely to exert negative impact on the overall accuracy.|
|||This makes the fusion scheme compatible to dynamic databases.|
|||Optimal classifier fusion in a non-bayesian probabilistic framework.|
|||Query specific fusion for image retrieval.|
||28 instances in total. (in cvpr2015)|
|22|Real-Time Video Super-Resolution With Spatio-Temporal Networks and Motion Compensation|Specifically, we discuss the use of early fusion, slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames.|
|||We study different treatments of the temporal dimension with early fusion, slow fusion and 3D convolutions, which have been previously suggested to extend classification from images to videos [23, 37].|
||| Comparing early fusion, slow fusion and 3D convolutions as alternative architectures for discovering spatio-temporal correlations.|
|||In early fusion (a), the temporal depth of the networks input filters matches the number of input frames collapsing all temporal information in the first layer.|
|||In slow fusion (b), the first layers merge frames in groups smaller than the input number of frames.|
|||This architecture, termed slow fusion, has shown better performance than early fusion for video classification [23].|
|||2b we show a slow fusion network where D0 = 5 and the rate of fusion is defined by dl = 2 for l  3 or dl = 1 otherwise, meaning that at each layer only two consecutive frames or filter activations are merged until the networks temporal depth shrinks to 1.|
|||Note that early fusion is an special case of slow fusion.|
|||2.2.3  3D convolutions  Another variation of slow fusion is to force layer weights to be shared across the temporal dimension, which has computational advantages.|
|||(8)  bias & activation  2  |{z}  In measuring the complexity of slow fusion networks with weight sharing we look at steady-state operation where the output of some layers is reused from one frame to the following.|
|||As seen previously, early fusion networks attain a higher accuracy at a marginal 3% increase in operations relative to the single frame models, and as expected, slow fusion architectures provide efficiency advantages.|
|||Slow fusion is faster than early fusion because it uses fewer features in the initial layers.|
|||(8), slow fusion uses dl = 2 in the first layers and nl = 24/Dl, which results in fewer operations than dl = 1, nl = 24 as used in early fusion.|
|||While the 7 layer network sees a considerable decrease in accuracy using slow fusion relative to early fusion, the 9 layer network can benefit from the same accuracy while reducing its complexity with slow fusion by about 30%.|
|||Using 7 layers with E5 nevertheless shows better performance and faster operation than S5-SW with 9 layers, and in all cases we found that early or slow fusion consistently outperformed slow fusion with shared weights in this performance and efficiency trade-off.|
|||Motion compensated video SR  In this section, the proposed frame motion compensation is combined with an early fusion network of temporal depth D0 = 3.|
|||This results in a network that will  4783  Figure 4: CDVL 3 SR using single frame models (SF) and multi frame early fusion models (E3-7).|
|||This ensures that the number of features per hidden layer in early and slow fusion networks is always the same.|
|||We compare single frame models (SF) against early fusion spatio-temporal models using 3, 5 and 7 input frames (E3, E5 and E7).|
|||The increase in complexity from early fusion is marginal because only the first layer contributes to an increase of operations.|
|||We assume networks with an input of 5 frames and slow fusion models with fil timised with Eq.|
|||Results for 3 SR on CDVL are compared in Table 3 against a single frame (SF) model and early fusion without motion compensation (E3).|
|||To demonstrate its benefits in efficiency and quality we evaluate two early fusion models: a 5 layer 3 frame network (5L-E3) and a 9 layer 3 frame network with motion compensation (9LE3-MC).|
|||# Layers  6  SF E3  E3-MC  37.718 37.842 37.928  7  37.780 37.889 37.961  8  37.812 37.956 38.019  9  37.800 37.980 38.060  Table 3: PSNR for CDVL 3 SR using single frame (SF) and 3 frame early fusion without and with motion compensation (E3, E3-MC).|
|||The early fusion motion compensated SR network (E3MC) is initialised with a compensation and a SR network pretrained separately, and the full model is then jointly op 3.4.2 Efficiency comparison  The complexity of methods in Table 4 is determined by network and input image sizes.|
|||Conclusion  In this paper we combine the efficiency advantages of sub-pixel convolutions with temporal fusion strategies to present real-time spatio-temporal models for video SR.|
||27 instances in total. (in cvpr2017)|
|23|cvpr18-PointFusion  Deep Sensor Fusion for 3D Bounding Box Estimation|The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors.|
|||In this paper, we show that our simple and generic sensor fusion method is able to handle datasets with distinctive environments and sensor types and perform better or on-par with state-of-the-art methods on the respective datasets.|
|||stage pipeline, which preprocesses each sensor modality separately and then performs a late fusion or decision-level fusion step using an expert-designed tracking system such as a Kalman filter [4, 7].|
|||Inspired by the successes of deep learning for handling diverse raw sensory input, we propose an early fusion model for 3D box estimation, which directly learns to combine image and depth information optimally.|
|||Our fusion sub-network features a novel dense 3D box prediction architecture, in which for each input 3D point, the network predicts the corner locations of a 3D box relative to the point.|
|||We present two fusion network formulations: a vanilla global architecture that directly regresses the box corner locations (D), and a novel dense architecture that predicts the spatial offset of each of the 8 corners relative to an input point, as illustrated in (C): for each input point, the network predicts the spatial offset (white arrows) from a corner (red dot) to the input point (blue), and selects the prediction with the highest score as the final prediction (E).|
|||2D-3D fusion Our paper is most related to recent methods that fuse image and lidar data.|
|||In addition, the current setup allows us to plug in any stateof-the-art detector without modifying the fusion network.|
|||2B), and a fusion network that combines both and outputs a 3D bounding box for the object in the crop.|
|||We describe two variants of the fusion network: a vanilla global architecture (Fig.|
|||2C) and a novel dense fusion network (Fig.|
|||Below, we go into the details of our point cloud and fusion sub-components.|
|||Fusion Network  The fusion network takes as input an image feature extracted using a standard CNN and the corresponding point cloud feature produced by the PointNet sub-network.|
|||Below we propose two fusion network formulations, a vanilla global fusion network, and a novel dense fusion network.|
|||Global fusion network As shown in Fig.|
|||We experimented with a number of fusion functions and found that a concatenation of the two vectors, followed by applying a number of fully connected layers, results in optimal performance.|
|||The loss function with the global fusion network is then:  L = X  i  smoothL1(x    i , xi) + Lstn,  (1)    where x i are the ground-truth box corners, xi are the predicted corner locations and Lstn is the spatial transformation regularization loss introduced in [23] to enforce the orthogonality of the learned spatial transform matrix.|
|||A major drawback of the global fusion network is that the variance of the regression target x i is directly dependent on the particular scenario.|
|||These ideas motivate our dense fusion network, which is described below.|
|||Dense fusion network The main idea behind this model is to use the input 3D points as dense spatial anchors.|
|||The dense fusion network processes this input using several layers and outputs a 3D bounding box prediction along with a score for each point.|
|||Concretely, the loss function of the dense fusion network is:  L =  1 N X  i  smoothL1(x  i  offset, x  i  offset) + Lscore + Lstn,  (2) i  where N is the number of the input points, x offset is the offset between the ground truth box corner locations and the i-th input point, and x offset contains the predicted offsets.|
|||Effect of fusion Both car-only and all-category evaluation results show that fusing lidar and image information always yields significant gains over lidar-only architectures, but the gains vary across classes.|
|||We observe that the fusion model is better at estimating the dimension and orientation of objects than the lidar-only model.|
|||Second, we introduce a novel dense fusion network, which combines the image and point cloud representations.|
|||A multi-sensor fusion system for moving object detection and tracking in urban driving environments.|
||27 instances in total. (in cvpr2018)|
|24|Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper|Furthermore, we investigate several audio-visual feature fusion methods and propose a novel dual multimodal residual fusion network that achieves the best fusion results.|
|||Our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, audio-visual event localization in a noisy condition can still achieve promising results, the audio-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish audio-visual unrelated videos, temporal alignment is important for audio-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization.|
|||Feature fusion is one of the most important part for multimodal learning [8], and many different fusion models have been developed, such as statistical models [15], Multiple Kernel Learning (MKL) [19,44], Graphical models [20,38].|
|||One timestep is illustrated, and note that the fusion network and FC are shared for all timesteps.|
|||4.2 and a novel dual multimodal residual fusion network in Sec.|
|||4.1 Audio-Visual Event Localization Network  t , ..., vk  Our network mainly consists of five modules: feature extraction, audio-guided visual attention, temporal modeling, multimodal fusion and temporal labeling (see Fig.|
|||To better incorporate the two modalities, we  t  Audio-Visual Event Localization in Unconstrained Videos  7  introduce a multimodal fusion network (see details in Sec.|
|||The audiovisual representation h t is learned by a multimodal fusion network with audio and visual hidden state output vectors hv t as inputs.|
|||4.3 Audio-Visual Feature Fusion  Our fusion method is designed based on the philosophy in [51], which processes multiple features separately and then learns a joint representation using a middle layer.|
|||Given audio and visual features ha  t and hv t compute the updated audio and visual features:  from LSTMs, the DMRN will  t + f (ha  t , hv  t )) ,  ha t = (ha hv t = (hv  t + f (ha  t , hv  t )) ,  (5)  (6)  t and hv  where f () is an additive fusion function, and the average of ha is used as the joint representation h t for labeling the video segment.|
|||Simply, we can stack multiple residual blocks to learn a deep fusion network with updated ha t and hv t as inputs of new residual blocks.|
|||We argue that the network becomes harder to train with increasing parameters and one block is enough to handle this simple fusion task well.|
|||We empirically find that late fusion (fusion after temporal modeling) is much better than early fusion (fusion before temporal modeling).|
|||We compare our fusion method: DMRN with several network-based multimodal fusion methods: Additive, Maxpooling (MP), Gated, Multimodal Bilinear (MB), and Gated Multimodal Bilinear (GMB) in [28], Gated Multimodal Unit (GMU) in [4], Concatenation (Concat), and MRN [29].|
|||Here, early fusion methods directly fuse audio features from pre-trained CNNs and attended visual features; late fusion methods fuse audio and visual features from outputs of two LSTMs; and decision fusion methods fuse the two modalities before Softmax  Audio-Visual Event Localization in Unconstrained Videos  11  Fig.|
|||To further enhance the performance of DMRN, we also introduce a variant model of DMRN called dual multimodal residual fusion ensemble (DMRFE)  12  Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu  Table 1: Event localization prediction accuracy (%) on AVE dataset.|
|||Table 2 shows event localization performance of different fusion methods.|
|||We  Audio-Visual Event Localization in Unconstrained Videos  13  Table 2: Event localization prediction accuracy (%) of different feature fusion methods on AVE dataset.|
|||Table 2 shows audio-visual event localization prediction accuracy of different multimodal feature fusion methods on AVE dataset.|
|||Our DMRN model in the late fusion setting can achieve better performance than all compared methods, and our DMRFE model can further improve performance.|
|||We also observe that late fusion is better than early fusion and decision fusion.|
|||The superiority of late fusion over early fusion demonstrates that temporal modeling before audio-visual fusion is useful.|
|||We know that auditory and visual modalities are not completely aligned, and temporal modeling can implicitly learn certain alignments between the two modalities, which is helpful for the audio-visual feature fusion task.|
|||The decision fusion can be regard as a type of late fusion but using lower dimension (same as the category number) features.|
|||14  Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu  The late fusion outperforms the decision fusion, which validates that processing multiple features separately and then learning joint representation using a middle layer rather than the bottom layer is an efficient fusion way.|
|||: Learning joint statistical models for audio-visual fusion and segregation.|
||27 instances in total. (in eccv2018)|
|25|cvpr18-Unsupervised Cross-Dataset Person Re-Identification by Transfer Learning of Spatial-Temporal Patterns|Figure 1: The TFusion model consists of 4 steps: (1) Train the visual classifier C in the labeled source dataset (Section 4.2); (2) Using C to learn the pedestrians spatio-temporal patterns in the unlabeled target dataset (Section 4.3); (3) Construct the fusion model F (Section 4.4); (4) Incrementally optimize C by using the ranking results of F in the unlabeled target dataset (Section 4.6).|
|||Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved fusion classifier F for Re-ID in the target dataset.|
|||During the iterative optimization procedure, both of the visual classifier C and the fusion classifier F are updated in a mutual promotion way.|
||| We propose a Bayesian fusion model, which combines the spatio-temporal patterns learned and the visual features to achieve high performance of person Re-ID in the unlabeled target datasets.|
||| We propose a learning-to-rank based mutual promotion procedure, which uses the fusion classifier to teach the weaker visual classifier by the ranking results on unlabeled dataset.|
|||Then we combine the patterns with the visual features to build a more precise fusion classifier.|
|||A Bayesian fusion model F is proposed to combine the visual classifier C and the newly learned spatio-temporal patterns for precise discrimination of pedestrian images.|
|||In this step, we leverage the fusion model F to further optimize the visual classifier C based on the learning-to-rank scheme.|
|||Firstly, given any surveillance image Si, the fusion model F is applied to rank the images in the unlabeled target dataset according to the similarity with Si.|
|||In this way, all of the visual classifier C, the fusion model F , and the spatio-temporal patterns can achieve collaborative optimization.|
|||Bayesian Fusion model  As represented in the last section, the spatio-temporal pattern P r(ij, ci, cj|(Si) = (Sj)), which is estimated from the visual classifier C, provides a new perspective to discriminate surveillance images besides the visual features used in C. This motivates us to propose a fusion model, which combines the visual features with the spatio-temporal  7951  Figure 3: Incremental optimization by the learning-to-rank scheme.|
|||(9), we can construct a fusion classifier F , which takes the visual features and spatio-temporal information of two images as input, and outputs their matching probability.|
|||The following Theorem 1 shows the performance of the fusion model:  E   n < Ep + En.|
|||Theorem 1 : If Ep + En < 1 and  +  < 1, we have p + E  Theorem 1 means that the error rate of the fusion model F may be lower than the original visual classifier C under the conditions of Ep + En < 1 and  +  < 1.|
|||1, the fusion model F is derived from the visual classifier C by integrating with spatio-temporal patterns.|
|||Subsequently, the improvement of C may also derive a better fusion model F .|
|||(11) in the fusion model,  and  are two tunable parameters.|
|||The following Fusion Model F  column shows that the performance of the fusion model, which integrates with the spatio-temporal patterns, gains significant improvement compared with the original visual classifier C.  The Incremental Optimization step in Table.|
|||This proves the effectiveness of the learning-to-rank scheme to transfer knowledge from the fusion model F to the visual classifier C  in the unlabeled target dataset.|
|||1 also shows that the performance of the fusion model F achieves significant improvement after the incremental learning.|
|||The fusion with pedestrians spatio-temporal pattern can significantly improve the Re-ID performance.|
|||This proves again the effectiveness of the fusion with spatio-temporal information.|
|||(11),  and  are two tunable parameters in the fusion model.|
|||Theorem 1 proves that when  +  < 1, the fusion model F may have chance to perform better than the original visual classifier C. Thus, we try different combinations of  and , which satisfy  +  < 1, and test the performance of the fusion model.|
|||In each iteration, the fusion model F is used to train the visual classifier C, and  subsequently a more accurate C can derive a better F .|
||26 instances in total. (in cvpr2018)|
|26|Locality-Sensitive Deconvolution Networks With Gated Fusion for RGB-D Indoor Semantic Segmentation|Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets.|
|||Thanks to recent consumer depth  fridge  (a) Imprecise boundaries due to the large context when labeling each pixel (see the fridge)  box  (b) Misclassified objects due to the improper fusion of RGB and depth (see the box)  Figure 1.|
|||Here a two-stream DeconvNet is used to represent RGB and depth, followed by score fusion with equal-weight sum just like the FCN model [19].|
|||3029  Towards RGB-D fusion, a simple sum fusion with equal weights is adopted by [19] to combine the predictions of RGB and depth FCN models.|
|||We adapt DeconvNet to RGB-D indoor scene segmentation with the same fusion way of FCN, which achieves large performance gain compared to FCN in our experiments.|
|||Instead of the simple score fusion with equal weights for the two modalities like [19], we devise a gated fusion layer to automatically learn the varying contributions of each modality for classifying different categories in different scenes.|
|||An effective fusion of the two complementary modalities can improve the performance of semantic segmentation.|
|||Due to the computational cost, only two-layer deconvolution networks are used; 3) the final gated fusion layer.|
|||Towards the popular convolutional neural networks (CNN), three levels of fusion are often used: Couprie et al.|
|||find the late fusion can be more effective to benefit from the complementarities of the two modalities, compared to other fusion levels.|
|||This paper adopts the late fusion version, but embeds a gate fusion layer to further adapt our model to the varying contributions of the two modalities for recognition of different categories in different scenes.|
|||As shown in the experiments, the proposed fusion way can achieve performance gains for those confused categories.|
|||LSD-GF is composed of three parts: the frontend fully convolutional networks (FCN), the intermediate locality-sensitive deconvolution networks (LS-DeconvNet), and the final gated fusion layer.|
|||Gated Fusion  The gated fusion layer is proposed to effectively combine RGB and depth for semantic segmentation.|
|||Finally, we generate the gated fusion probability map as   Pfusion =  Prgb +  Pdepth.|
|||In the first stage, we train two independent locality-sensitive DeconvNets on RGB and depth for semantic segmentation without the gated fusion layer.|
|||In the second stage, we add the gated fusion layer, and then finetune the whole networks on the synchronized RGB and depth data.|
|||Note that the only differences between DeconvNet and the proposed approach are that we replace the conventional deconvolution networks with simple sum fusion by the locality-sensitive deconvolution networks with gated fusion.|
|||We owe the improvements to two factors: 1) the local visual and geometrical cues from raw data embedded into the deconvolution networks can effectively alleviate the imprecise boundary representation from the frontend FCN model with large context; 2) the gated fusion layer can effectively combine the two complementary modalities for accurate object recognition.|
|||To discover the importance of the proposed localitysensitive DeconvNet and the gated fusion of LSD-GF, we conduct an ablation study via removing or replacing each component independently or both together for semantic segmentation on the NYU-Depth v2 dataset.|
|||For each comparison pair, the only difference is with and without locality-sensitive module; 2) Gated fusion is superior to the sum fusion, as well as some other popular equal-weight score fusion like pixelwise production and Dempster-Shafer (DS) [26] (comparing e  h and i  l).|
|||These objects need to effectively weigh the contributions of RGB an depth for recognition; 3) Cascading the locality-sensitive deconvolution networks and the gated fusion can achieve the best result, i.e., 45.9% mean IOU.|
|||Specifically, rows (1)(3) of the figure show some examples to witness the effectiveness of the proposed gated fusion, e.g., it helps to correctly recognize the box on the sofa (emphasize appearance), the faraway fridge against the cabinet (emphasize shape), and  3035  RGB  HHA  GT  LSD-GF  w/o gated fusion  w/o locality-sensitive  w/o both  (1)  (2)  (3)  (4)  (5)  (6)  (7)  (8)  floor  wall  bed  cabinet  table  chair  sofa  door  bookshelf window  picture  blinds  curtain  shelves  pillow floormat  ceiling  clothes  fridge  paper  shower  towel  board  nightstand  sink  toilet  lamp  bag  counter  desk  dresser  miror  books  tv  box  person  bathtub  oprops  ofurn  ostuct  background  Figure 4.|
|||For the scene image in each row, we show: (column 1) the RGB image; (column 2) the HHA image; (column 3) the ground truth of semantic segmentation; (column 4) the result of our LSD-GF approach, i.e., l in Table 3; (column 5) the result of LSD-GF whose gated fusion is replaced by sum fusion, i.e., i in Table 3; (column 6) the result of LSD-GF whose locality-sensitive module is removed, i.e., h in Table 3; (column 7) the result of LSD-GF whose locality-sensitive is removed and the gated fusion is replaced by sum fusion, i.e., e in Table 3.|
|||LSDGF is composed of two main components: 1) the locality sensitive deconvolution networks, which are designed for simultaneously upsamping the coarse fully convolutional maps and refining object boundaries; 2) gated fusion, which can adapt to the varying contributions of RGB and depth for better fusion of the two modalities for object recognition.|
||26 instances in total. (in cvpr2017)|
|27|Veksler_Efficient_Parallel_Optimization_2015_CVPR_paper|We present an efficient parallel method for optimizing Potts energy based on the extension of hierarchical fusion algorithm.|
|||Our approach is based on extending the hierarchical fusion (h-fusion) algorithm of [9], which is based on the fusion algorithm of [21].|
|||In [21] they also propose a graph-cut based parallel optimization that uses a mixture of expansion and fusion moves.|
|||Fusion Algorithm  We now describe fusion moves, introduced in [20, 21].|
|||The idea behind fusion moves is to stitch solutions from multiple algorithms to obtain the best combined result.|
|||We labeling x is a fusion of x1, x2 if p, xp  {x1 use F use(x1, x2) to denote the set of all fusions of x1, x2.|
|||Algorithm 2: FUSION 1 x(cid:48) := arbitrary labeling 2 repeat 3 4 5 until converged 6 return x(cid:48)  for each i  {1, 2, ..., n}  x(cid:48) := argminxF use(x(cid:48),xi) f (x)  Like an expansion move, a fusion move can be formulated as binary energy minimization.|
|||If yp = 1, pixel p is assigned label x2 p. We formulate the following binary energy h(y):  hp(0) = fp(x1 p) hp(1) = fp(x2 p)  hpq(0, 0) = fpq(x1 hpq(0, 1) = fpq(x1 hpq(1, 0) = fpq(x2 hpq(1, 1) = fpq(x2  p, x1 q) p, x2 q) p, x1 q) p, x2 q)  A fusion move is submodular [16], and, therefore, can be optimized with a graph cut if  q)  fpq(x0  p, x0  p, x1  q)+fpq(x1  q) (3) fpq(x0 Eq.|
|||q)+fpq(x1  p, x1  p, x0  Most related for our work, in [21] they propose a parallel implementation of the expansion algorithm using a mixture of expansion and fusion moves.|
|||Two results are then combined using a fusion move: x1 = argminxF use(x1,x2) f (x).|
|||This algorithm can be generalized from two processors to m processors by splitting the label set into m subsets and combining the results of m expansion sub-problems with the fusion moves.|
|||It is easy to see that the fusion step on line 7 is not guaranteed to be submodular even for the Potts model.|
|||No matter how the initial x1 and x2 are chosen, at the second execution of the fusion step on line 7, current x1 and x2 can be  Algorithm 3: PARALLEL FUSION 1 x1, x2 := arbitrary labelings 2 L = L1  L2 // even split of the label set 3 repeat 4 5 6  for each i = 1, 2 in parallel do  for each   Li  xi := argminxM(xi) f (x) x1 := argminxF use(x1,x2) f (x) x2 := x1  7  8 9 until converged 10 return x1  Label tree illustration.|
|||Hierarchical Fusion Algorithm  The hierarchical fusion (h-fusion) algorithm of [9] was developed for a special class of energies with h-metric pairwise terms.|
|||Hierarchical fusion starts at tree level D  1 and proceeds upward to level 0, see Fig.|
|||The pseudo-code for hierarchical fusion is given below.|
|||The advantage of this structure is that for each internal node, fusion needs to be performed just one time, that is no iterations are needed.|
|||It is easy to see that with the structure above, for Potts model, the fusion move is performed optimally at each node.|
|||Efficient Parallel Implementation  We now develop our efficient parallel algorithm based on hierarchical fusion discussed in Sec.|
|||We explain how we choose the label tree structure, prove optimality bounds, and extend the hiearchical fusion algorithm to be iterative, since the original algorithm in [9, 19] is not iterative.|
|||Suppose a processor fusing labellings at nodes 8 and 9 is done, and, therefore, fusion at node 12 is ready to be executed.|
|||To achieve maximum parallel performance, we repeatedly iterate over all non-leaf nodes in the label tree, check if the fusion at the children nodes has already completed, and, if yes, signal that fusion at this node is ready to proceed.|
|||we find the optimal fusion xa of labellings xc and xd.|
|||5, based on the original hierarchical fusion algorithm in [9, 19] is not iterative.|
||25 instances in total. (in cvpr2015)|
|28|FASON_ First and Second Order Information Fusion Network for Texture Recognition|We propose an effective fusion architecture FASON that combines second order information flow and first order information flow.|
|||This multiple level fusion architecture further improves recognition performance.|
|||Our experiments show the proposed fusion architecture achieves consistent improvements over state-of-the-art methods on several benchmark datasets across different tasks such as texture recognition, indoor scene recognition and fine-grained object classification.|
|||The contribution of our work is two fold:   We design a deep fusion architecture that effectively combines second order information (calculated from a bilinear model) and first order information (preserved through our leaking shortcut) in an end-to-end deep network.|
||| We extend our fusion architecture to take advantage of the multiple features from different convolution layers.|
|||In section 3, we present the design of our fusion architecture, give details and explanations of each building block.|
|||FASON  In this section, we describe our proposed framework FASON (First And Second Order information fusion Network).|
|||We first introduce the basic components of our first order and second order fusion building blocks.|
|||First order information fusion by gradient leak(cid:173)  ing  (x) =  sign(x)p|x| ksign(x)p|x|k2  (2)  3.2.|
|||Figure 1: The core building block of our first and second order information fusion architecture compared to original bilinear model.|
|||First and second order fusion with multiple  levels of convolutional features  One benefit of our fusion framework is that we can fuse more convolutional features into bilinear layers and conduct an effective end-to-end training as shown in Figure 2.|
|||We investigate two major network architectures: a single fusion at conv5 level (equivalent to features generated from conv5 4 of VGG-19 network) and a multiple fusion at conv4, conv5 layers (equivalent to features generated from conv4 4 and conv5 4 of VGG-19 network).|
|||For fair comparison, we also conduct experiments using typical bilinear networks without fusion on these same two setups.|
|||Figure 3: The detailed configurations of our first and second order information fusion architectures.|
|||Effectiveness of fusion  We first evaluate the effectiveness of our fusion architecture by comparing two networks with and without first order information fusion on single (conv5) and multiple  Figure 4: Comparison of learning curves on bilinear model using single level of convolutional feature (conv5) with and without our first order information fusion on DTD dataset.|
|||Figure 5: Comparison of learning curves on bilinear model using two levels of convolutional features (conv4+conv5) with and without our first order information fusion on DTD dataset.|
|||We also evaluate the effectiveness of our fusion architecture on different deep networks such as VGG-16 and VGG19.|
|||The performance further boosts with our multiple layer fusion when two level of convolutional layers conv4 and conv5 are combined.|
|||Combining the improvements from first and second order information fusion with multi-layer feature fusion, we obtain a 2% improvement from a strong bilinear CNN baseline for both VGG-16 and VGG-19.|
|||We compare the performance of our fusion architecture with several state-of-the-art methods such as [3, 4, 20] on the DTD dataset.|
|||We also evaluate the performance of our fusion architecture on KTH-T2b dataset with several state-of-the-art methods such as [30, 3, 4, 20].|
|||We compare our fusion architectures with the standard VGG network (using the weights learned form the DTD dataset for classification task) on different combinations of content and style images in Figure 6.|
|||Furthermore, our multi-layer fusion architecture is even better than our single-layer fusion architecture.|
|||Experiments show that our fusion architecture consistently improves over the standard bilinear networks.|
||25 instances in total. (in cvpr2017)|
|29|Multi-View 3D Object Detection Network for Autonomous Driving|We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths.|
|||detection by employing early or late fusion schemes.|
|||The multi-view fusion network extracts regionwise features by projecting 3D proposals to the feature maps from mulitple views.|
|||We design a deep fusion approach to enable interactions of intermediate layers from different views.|
|||Combined with drop-path training [14] and auxiliary loss, our approach shows superior performance over the early/late fusion scheme.|
|||A deep fusion network is used to combine region-wise features obtained via ROI pooling for each view.|
|||Related Work  We briefly review existing work on 3D object detection from point cloud and images, multimodal fusion methods and 3D object proposals.|
|||In this paper, we design a deep fusion approach inspired by FractalNet [14] and Deeply-Fused Net [26].|
|||Each 3D box  1909  C  C  (a) Early Fusion  (b) Late Fusion  M  M  M  M  Input  Intermediate layers Output  (c) Deep Fusion  C  M  Concatenation  Element-wise Mean  Figure 3: Architectures of different fusion schemes: We instantiate the join nodes in early/late fusion with concatenation operation, and deep fusion with element-wise mean operation.|
|||Region(cid:173)based Fusion Network  We design a region-based fusion network to effectively combine features from multiple views and jointly classify object proposals and do oriented 3D box regression.|
|||To combine information from different features, prior work usually use early fusion [1] or late fusion [22, 12].|
|||A comparison of the architectures of our deep fusion network and early/late fusion networks are shown in Fig.|
|||For a network that has L layers, early fusion combines features {fv} from multiple views in the input stage:  fL = HL(HL1(   H1(fBV  fF V  fRGB)))  (4)  {Hl, l = 1,    , L} are feature transformation functions and  is a join operation (e.g., concatenation, summation).|
|||In contrast, late fusion uses seperate subnetworks to learn feature transformation independently and combines their outputs in the prediction stage:  1  L (   HBV L (   HF V  fL =(HBV (HF V (HRGB  1  L  (fBV )))  (fF V )))  (5)  (   HRGB  1  (fRGB)))  To enable more interactions among features of the intermediate layers from different views, we design the following deep fusion process:  f0 =fBV  fF V  fRGB fl =HBV (fl1)  HF V  l  l  (fl1)  HRGB  l  (fl1),  (6)  l = 1,    , L  We use element-wise mean for the join operation for deep fusion since it is more flexible when combined with droppath training [14].|
|||In particular,  Oriented 3D Box Regression Given the fusion features of the multi-view network, we regress to oriented 3D boxes from 3D proposals.|
|||Network Regularization We employ two approaches to regularize the region-based fusion network: drop-path training [14] and auxiliary losses.|
||| In the muti-view fusion network, we add an extra fully connected layer f c8 in addition to the original f c6 and f c7 layer.|
|||loss  AP3D (IoU=0.5)  APloc (IoU=0.5)  AP2D (IoU=0.7)  Easy Moderate 93.92 93.53 94.21 96.02  87.60 87.70 88.29 89.05  Hard 87.23 86.88 87.21 88.38  Easy Moderate 94.31 93.84 94.57 96.34  88.15 88.12 88.75 89.39  Hard 87.61 87.20 88.02 88.67  Easy Moderate 87.29 87.47 88.64 95.01  85.76 85.36 85.74 87.59  Hard 78.77 78.66 79.06 79.90  Table 3: Comparison of different fusion approaches: Peformance are evaluated on KITTI validation set.|
|||We first compare our deep fusion network with early/late fusion approaches.|
|||As commonly used in literature, the join operation is instantiated with concatenation in the early/late fusion schemes.|
|||As shown in Table 3, early and late fusion approaches have very similar performance.|
|||Adding auxiliary loss further improves deep fusion network by around 1%.|
|||A region-based fusion network is presented to deeply fuse multi-view information and do oriented 3D box regression.|
||24 instances in total. (in cvpr2017)|
|30|cvpr18-Residual Dense Network for Image Super-Resolution|Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network.|
|||After extracting multi-level local dense features, we further conduct global feature fusion (GFF) to adaptively preserve the hierarchical features in a global way.|
|||The accumulated features are then adaptively preserved by local feature fusion (LFF).|
||| We propose global feature fusion to adaptively fuse hierarchical features from all RDBs in the LR space.|
|||2, our RDN mainly consists four parts: shallow feature extraction net (SFENet), redidual dense blocks (RDBs), dense feature fusion (DFF), and finally the up-sampling net (UPNet).|
|||After extracting hierarchical features with a set of RDBs, we further conduct dense feature fusion (DFF), which includes global feature fusion (GFF) and global residual  learning (GRL).|
|||Our RDB contains dense connected layers, local feature fusion (LFF), and local residual learning, leading to a contiguous memory (CM) mechanism.|
|||Local feature fusion is then applied to adaptively fuse the states from preceding RDB and the whole Conv layers in current RDB.|
|||Dense Feature Fusion  After extracting local dense features with a set of RDBs, we further propose dense feature fusion (DFF) to exploit hierarchical features in a global way.|
|||Our DFF consists of global feature fusion (GFF) and global residual learning.|
|||Global feature fusion is proposed to extract the global  feature FGF by fusing features from all the RDBs  FGF = HGF F ([F1,    , FD]) ,  (9)  where [F1,    , FD] refers to the concatenation of featuremaps produced by residual dense blocks 1,    , D. HGF F is a composite function of 1  1 and 3  3 convolution.|
|||All the other layers before global feature fusion are fully utilized with our proposed residual dense blocks (RDBs).|
|||We would also demonstrate the effectiveness of global feature fusion in Section 5.|
|||Shallow feature extraction layers, local and global feature fusion layers have G0=64 filters.|
|||While in RDN, we combine dense connected layers with local feature fusion (LFF) by using local residual learning, which would be demonstrated to be effective  2475  in Section 5.|
|||Last not the least, we adopt global feature fusion to fully use hierarchical features, which are neglected in DenseNet.|
|||Our RDB allow larger growth rate by using local feature fusion (LFF), which stabilizes the training of wide network.|
|||Instead we use global feature fusion (GFF) and global residual learning to extract global features, because our RDBs with contiguous memory have fully extracted features locally.|
|||Ablation investigation of contiguous memory (CM), local residual learning (LRL), and global feature fusion (GFF).|
|||Ablation Investigation  Table 1 shows the ablation investigation on the effects of contiguous memory (CM), local residual learning (LRL), and global feature fusion (GFF).|
|||We find that local feature fusion (LFF) is needed to train these networks properly, so LFF isnt removed by default.|
|||The local feature fusion (LFF) not only stabilizes the training wider network, but also adaptively controls the preservation of information from current and preceding RDBs.|
|||By fully using local and global features, our RDN leads to a dense feature fusion and deep supervision.|
||24 instances in total. (in cvpr2018)|
|31|Spindle Net_ Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion|In order to make better use of the region features, a tree-structured feature fusion strategy is adopted in our approach instead of directly concatenating the region features together.|
|||Moreover, a competitive strategy is also used in the feature fusion process.|
|||Then the regions features of different semantic levels are merged by a treestructured fusion network with a competitive strategy.|
|||A fusion unit is proposed for the feature fusion process, which takes two or more feature vectors of the same size as input and outputs one merged feature vector.|
|||3, and each fusion unit is represented by one green block.|
|||(b-d) Three input feature vectors of the body fusion unit.|
|||The fusion unit has two main processes.|
|||2) The feature transformation process is conducted by a inner product layer, so that the transformed result can be used for later fusion units.|
|||3, in the first stage, the features of the two leg regions, and the features of the two arm regions, are merged by two fusion units, separately.|
|||Then, a fusion unit takes the two fusion results of the previous stage, together with the feature vector of the head-shoulder region as input and compute the merged feature vector of the whole body.|
|||5 to demonstrate the feature competition and fusion strategy based on the element-wise max operation.|
|||In this example, we focus on the fusion unit which takes three feature vectors, i.e.|
|||Moreover, the feature selection and fusion strategy also helps obtain good compact features.|
|||Investigations on FFN  There are two key factors of the proposed FFN, i.e., the tree fusion structure and the feature competition strategy.|
|||For the tree fusion structure, the results of using only one region feature are evaluated and listed in Table 5.|
|||Thus the tree-structured fusion technology is adopted and better features are merged in later stages.|
|||On the other hand, such fusion structure is also consistent with  Table 6.|
|||Comparison results of different fusion structures and competition strategies on Market-1501 [33] datasets.|
|||The proposed fusion structure (Tree) is compared with some other possible ones, including the Linear structure (features are merged one by one) and the inverse tree (iTree) structure (macro features are merged first).|
|||The performance of different fusion structures and competition strategies are reported in Table 6.|
|||Features of different body regions are separated by a multi-stage ROI pooling network and merged by a treestructured fusion network.|
|||Strong capacity of the proposed feature competition and fusion network is also verified.|
||23 instances in total. (in cvpr2017)|
|32|Minjun_Li_Unsupervised_Image-to-Image_Translation_ECCV_2018_paper|Moreover, to properly exploit the information from the previous stage, an adaptive fusion block is devised to learn a dynamic integration of the current stages output and the previous stages output.|
|||Secondly, we introduce a novel adaptive fusion block to dynamically integrate the current stages output and the previous stages output, which outperforms directly stacking multiple stages.|
|||Diferent form the existing works, this work exploits stacked image-to-image translation networks combined with a novel adaptive fusion block to tackle the unsupervised image-to-image translation problem.|
|||2  G2 consists of two parts: a newly initialized image translation network GT and an adaptive fusion block GF 2 .|
|||Illustration of the linear combination in an adaptive fusion block.|
|||The fusion block applies the fusion weight map  to ind defects in the previous result y1 and correct it precisely using y2 to produce a reined output y2.|
|||Speciically, the adaptive fusion block learns a pixel-wise linear combination of the previous results:  GF  (5) where  denotes element-wise product and   (0, 1)HW represents the fusion weight map, which is predicted by a convolutional network hx:  2 (y1, y2) = y1  (1  x) + y2  x,  (6) Figure 4 shows an example of adaptively combining the outputs from two stages.|
|||The adaptive fusion block is a simple 3-layer convolutional network, which calculates the fusion weight map  using two Convolution-InstanceNormReLU blocks followed by a Convolution-Sigmoid block.|
|||To examine the efectiveness of the proposed fusion block, we compare it with several variants: 1) Learned Pixel Weight (LPW), which is our proposed fusion block; 2) Uniform Weight (UW), in which the two stages are fused with the same weight at diferent pixel locations y1(1  w) + y2w, and during training w gradually increases from 0 to 1; 3) Learned Uniform Weight (LUW), which is similar to UW, but w is a learnable parameter instead; 4) Residual Fusion (RF),  Unsupervised Image-to-Image Translation with SCAN  11  Table 2.|
|||FCN Scores and Segmentation Scores of several variants of the fusion block on the Cityscapes dataset.|
|||which uses a simple residual fusion y1 + y2.|
|||It can be observed that our proposed LPW fusion yields the best performance among all alternatives, which indicates that the LPW approach can learn better fusion of the outputs from two stages than approaches with uniform weights.|
|||Distributions of fusion weights over all pixels in diferent epochs.|
|||Dashed arrows indicate the average weights of fusion maps.|
|||4.6 Visualization of Fusion Weight Distributions To illustrate the role of the adaptive fusion block, we visualize the three average distributions of fusion weights (x in Equation 5) over 1000 samples from Cityscapes dataset in epoch 1, 10, and 100, as shown in Figure 9.|
|||We observed that the distribution of the fusion weights gradually shifts from left to right.|
|||It indicates a consistent increase of the weight values in the fusion maps, which implies more and more details of the second stage are bought to the inal output.|
|||Class IoU  Method SCAN Stage-1 128 SCAN Stage-2 128-256 w/o Skip,Fusion SCAN Stage-2 128-256 w/o Skip SCAN Stage-2 128-256 w/o Fusion SCAN Stage-2 128-256  0.457 0.513 0.593 0.613 0.637  0.188 0.186 0.184 0.194 0.201  0.124 0.125 0.136 0.137 0.157   SCAN w/o Adaptive Fusion Block: remove the inal adaptive fusion block  in the Stage-2 model , denoted by SCAN w/o Fusion.|
|||Table 4 shows the results of the ablation study, in which we can observe that removing the adaptive fusion block as well as removing the skip connection both downgrade the performance.|
|||Note that the fusion block only consists of three convolution layers, which have a relatively small size compared to the whole network.|
|||Thus, the improvement of the fusion block does not simply come from the added capacity.|
|||Therefore, we can conclude that using our proposed SCAN structure, which consists of the skip connection and the adaptive fusion block, is critical for improving the overall translation performance.|
||23 instances in total. (in eccv2018)|
|33|Yuan_3D_R_Transform_2013_CVPR_paper|In addition, we propose a new fusion strategy to combine the R feature with the BOVW representation for further improving recognition accuracy.|
|||We utilize a context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the videos.|
|||To take this benefit, we propose a new context-aware fusion strategy to combine the two features.|
|||Section 2 gives a review of the improved BOVW approaches and fusion approaches for action recognition.|
|||Section 4 discusses the video representations based on spatio-temporal interest points and describes the proposed fusion method.|
|||Therefore, a number of approaches, which propose feature fusion for improving action recognition in video sequences, have recently appeared in the literature [11, 12, 13, 14].|
|||[15] employ the kernel-level fusion approach and utilize a multi-kernel classifier for combining different features.|
|||The above fusion approaches rely only on the pairwise similarities of videos without considering the highorder correlations among videos.|
|||Addressing this issue, we propose a context-aware feature fusion method which not only combines the two feature representations but also captures the underlying contextual information from videos.|
|||The proposed context-aware feature fusion method includes two steps: context selection and context-aware kernel construction.|
|||Parameter Evaluation of the Context-aware  Feature Fusion  We evaluate the parameter r, which is the number of neighbors for context construction in fusion method, on the KTH dataset.|
|||Namely, BOVW+R and R+BOVW are our proposed fusion approaches; while BOVW+BOVW and R+R are the one feature based and context aware approaches.|
|||Experiments on the KTH Database  To evaluate the two proposed fusion methods, we compared our fusion approaches with two other feature fusion approaches: the feature-level fusion approach [11][12] and the similarity kernel-level fusion approach.|
|||All of these fusion approaches combine the proposed two features.|
|||Specifically, the feature-level fusion approach concatenates the two normalized feature vectors to form a larger feature vector as input to the SVM classifier.|
|||For the similarity kernel-level fusion approach, we separately compute the similarity matrix by each kind of feature, and then utilize the weighted sum of the two obtained similarity matrices as the final kernel of the SVM, which is formulated as follows:  k2(Vi, Vj) =   min(V 1  i (n), V 1  j (n)) +      n  (1  )  min(V 2  i (m), V 2  j (m)).|
|||(15)  m  Table 1 lists the recognition accuracies of eight approaches on KTH dataset, including one feature based approaches (e.g., BOVW, R), One feature based and context aware approaches (e.g., BOVW+BOVW, R+R), two other feature fusion approaches (e.g., feature-level fusion, kernel-level fusion), and our proposed fusion approaches (e.g., BOVW+R, R+BOVW).|
||| Our fusion approach (R+BOVW) obtains higher accuracy than the other two common fusion approaches, which demonstrates the effectiveness of our proposed fusion strategy.|
|||Moreover, our fusion approach with the R feature for context calculation and the BOVW feature for kernel calculation (e.g., R+BOVW) achieves the best accuracies.|
|||The similar results as in KTH are obtained in the Table 2, which demonstrate the effectiveness of our proposed cloud feature and fusion method on the realistic and complicated dataset.|
|||We then proposed a new fusion strategy to combine the local cuboid feature and the global R feature for action recognition.|
|||Experimental results on several datasets have demonstrated the effectiveness of our proposed R feature and fusion method.|
||23 instances in total. (in cvpr2013)|
|34|Bahrampour_Quality-based_Multimodal_Classification_2014_CVPR_paper|Quality-based Multimodal Classification Using Tree-Structured Sparsity  Soheil Bahrampour  Asok Ray  Pennsylvania State University  Pennsylvania State University  soheil@psu.edu  axr2@psu.edu@psu.edu  Nasser M. Nasrabadi  Army Research Laboratory  Kenneth W. Jenkins  Pennsylvania State University  nasser.m.nasrabadi.civ@mail.mil  jenkins@engr.psu.edu  Abstract  Recent studies have demonstrated advantages of information fusion based on sparsity models for multimodal classification.|
|||An accelerated proximal algorithm is proposed to solve the optimization problem, which is an efficient tool for feature-level fusion among either homogeneous or heterogeneous sources of information.|
|||This approach provides a general framework for quality-based fusion that offers added robustness to several sparsity-based multimodal classification algorithms.|
|||Introduction  Information fusion using multiple sensors often results in better situation awareness and decision making [7].|
|||While the information from a single sensor is generally localized and can be corrupted, sensor fusion provides a framework to obtain sufficiently local information from different perspectives, which is expected to be more tolerant to the errors of individual sources.|
|||Fusion algorithms are usually categorized into two levels: feature fusion [22] and classifier fusion [20, 23].|
|||Feature fusion methods combine various features extracted from different sources into a single feature set, which are then used for classification.|
|||While classifier fusion has been well studied, feature fusion is a relatively less-studied problem, mainly due to the incompatibility of feature sets [21].|
|||A naive way of feature fusion is to concatenate features into a longer one [30], which may suffer from the curse of dimensionality.|
|||However, if above limitations are mitigated, feature fusion can potentially outperform the classifier fusion [12].|
|||Feature level fusion using sparse representation has also been recently introduced and is often referred to as multi-task learning in which the general goal is to represent samples jointly from several tasks/sources using different sparsity priors [24, 25, 29].|
|||A more generalized approach is proposed in [11] for multi-task regression in which different tasks can be grouped in a tree-structured sparsity providing flexibility in fusion of different sources.|
|||This can significantly limits the performance of fusion algorithms in dealing with occasional perturbation or malfunction of individual modalities.|
|||The proposed learning facilitates feature level fusion of homogeneous/heterogeneous sources at multiple granularities.|
|||Section 3 proposes the quality-based fusion which is followed by comparative studies in Section 4 and conclusions in Section 5.|
|||Ideally, a fusion scheme should adaptively weight the In [24], a quality modalities based on their reliabilities.|
|||The proposed composite optimization problem to achieve quality-based multimodal fusion is posed as:  (s)m  2  kys  X ssk2  l2 +  (A) +  argmin  A=[1,...,S ],s  S Xs=1 Xs=1  S  s (1  s)m!|
|||Algorithm 2 summarizes the proposed quality-based multimodal fusion method.|
|||The performance of the proposed feature-level fusion algorithms is compared with that of several state-ofthe-art decision-level and feature-level fusion algorithms.|
|||The proposed methods are also evaluated using existing featurelevel fusion methods that include holistic sparse representation classifier (HSRC) [30], JSRC [18], joint dynamic sparse representation classifier (JDSRC) [30] and relaxed collaborative representation (RCR) [29].|
|||New fusion architectures for performance enhancement of a pca-based fault diagnosis and isolation system.|
|||This formulation provides a framework for robust fusion of available sources based on their respective reliability.|
||23 instances in total. (in cvpr2014)|
|35|Wu_Harnessing_Object_and_CVPR_2016_paper|To address these problems, we propose a novel objectand scene-based semantic fusion network and representation.|
|||Further, by examining and back propagating information through the fusion network, semantic relationships (correlations) between video classes and objects/scenes can be discovered.|
|||OSF combines three streams of information using a three-layer fusion neural network: (i) frame-based low-level CNN features, (ii) object features from a stateof-the-art large-scale CNN object-detector with 20K classes and (iii) scene features from a state-of-the-art CNN scenedetector trained to recognize 205 scenes.|
|||First, it is defined as an end-to-end network and hence joint training of all streams and the fusion layers is possible.|
|||This procedure is not as trivial as it may sound, as unlike with linear models, discovering such relations in our non-linear fusion architecture requires optimization.|
|||We use output of the last fully connected layer (F C8) as the input for the fusion network; in other words, for the j-th frame of video i, fi,j , this stream outputs fi,j 7 xO  i,j  R20574.|
|||We again use the output of the last fully connected layer (F C8) as the input for the fusion network; in other words, for the j-th frame of video i, fi,j , this stream outputs fi,j 7 xS  i,j  R205.|
|||However, for this stream we take features of the first (not last) fully connected layer as input to the fusion network.|
|||Since we do not fine-tune the network endto-end, this is done explicitly; but can be equivalently implemented by a pooling operation inserted between each stream and the first layer of the fusion network.|
|||For example, video Vi we represent as  xO i = Pni  k=1 The averaged representations  xO  i are fed into a first hidden layer of the fusion network, consisting of 250, 50, and 250 neurons respectively for each stream (550 neurons total).|
|||We denote f () as the non-linear function approximated by semantic fusion network and fz ( xi) as the score of video instance Vi belong to the class z.|
|||More formally, let fz ( xi) be the score of the class z computed by the fusion network for video Vi.|
|||One of the reasons is that contextual information among all three streams and the fusion network are not utilized and hence the individual predictions obtained using the object and scene streams tend to be noisier.|
|||Probability Calibration: We first employ Platt Scaling [24] to calibrate the output of the fusion network f () into a probability distribution p (), defined for each of the training classes.|
|||We start by showing that our object-scene semantic fusion (OSF) network is effective for supervised action and video categorization (Sec.|
|||OSF Network for Supervised Recognition  In this section we focus on exploring the effectiveness of  our object-scene fusion network.|
|||We note that most of these baselines are very strong as they are using exactly the same features as our fusion network and complex state-of-the-art non-linear classifiers.|
|||Late fusion suffers from the heterogeneous classification scores coming from each stream; each stream has varying discriminative capacity and (may) results in incomparable classification scores.|
|||We adopt a 3-layer NN classifier for each single stream (similar in structure to our fusion network); and a variant network with two streams.|
|||Our fusion network combines three streams (i.e., object, scene and generic feature) of information using a three-layer neural network to model object and scene dependencies.|
|||Further, by examining and back propagating information through the fusion layers, semantic relationships (correlations) between video classes (or activities) and objects/scenes can be identified.|
|||Multimodal feature fusion for robust event detection in web videos.|
||23 instances in total. (in cvpr2016)|
|36|Beier_Fusion_Moves_for_2015_CVPR_paper|To this end we define fusion moves for the correlation clustering problem.|
|||Third, we introduce cluster-fusion moves, which extend the original fusion moves [24] used in supervised segmentation to the unsupervised case and give a polyhedral interpretation of this algorithm.|
|||For energy minimization problems fusion moves have become increasingly popular [24, 19].|
|||For many large scale computer vision applications fusion moves lead to good approximations with state of the art anytime performance [19].|
|||3 and our proposed correlation clustering fusion moves in Sec.|
|||In the following, we propose a more suitable fusion move for correlation clustering which works on the edge domain.|
||| yij = max{y E  y  0 = {ij  E |  yij = 0}  ij, y  ij}  ij  E  (3)  (4)  The fusion move for correlation clustering is solving Eq.|
|||A further difference is how we efficiently calculate the correlation clustering fusion move and how we generate proposals.|
|||5.  g n i l e b a l  e g d e  l a s o p o r p  0  w01  1    y  w02  w13  2  w23  3     y  0  w01  1  w02  w13  2  w23  3  g n i l e b a l  e d o n l a s o p o r p  0  w01  1   l  w02  w13  2  w23  3  All these node labelings encode the same partition  0  w01  1  0  w01  1  0  w01  1  0  w01  1  0  w01  1  0  w01  1  0  w01  1  w02  w13  w02  w13  w02  w13  w02  w13  w02  w13  w02  w13  w02  w13  2  w23  3  2  w23  3  2  w23  3  2  w23  3  2  w23  3  2  w23  3  2  w23  3  w01  w01  w01  w01  w01  w01  w01  w02  w13  w02  w13  w02  w13  w02  w13  w02  w13  w02  w13  w02  w13  w23  w23  w23  w23  w23  w23  w23  For all different colorings l, the binary fusion move subproblem is different.|
|||The mapping from edge labels to node labels is ambiguous and even for this small graph there are seven node labels which result in different binary fusion move problems.|
|||Fast Optimization of CC(cid:173)Fusion Moves  In general the auxiliary fusion problem 5 is, as for classical fusion [24], NP-hard.|
|||Polyhedral Interpretation  A polyhedral interpretation of fusion moves is shown in Fig.|
|||Figure 6: Each fusion move can be interpreted as an optimization of an inner polytope.|
|||For correlation clustering fusion we add a third property: size.|
|||In one extreme case, where each node is in a separate connnected component, the fusion move is equivalent to solving the original problem.|
|||The actual fusion move is solving ( 2) for Gy as in 5e and projecting the result back to G as in (5f).|
|||In summary, the correlation clustering fusion move algorithm iteratively fuses the current best solution with different proposals.|
|||The proposed correlation cluster fusion algorithm with EHC-based and watershed-based proposals and MC-I and CGC as subproblem solvers ( CC-Fusion-HC-MC, -HC-CGC, -WSMC, and -WS-CGC) respectively.|
|||The best solution is iteratively improved by a fusion with proposal solutions.|
|||The fusion move itself is formulated as correlation clustering on a smaller graph with fewer edges and nodes and can therefore be solved much faster than the original problem.|
|||On all instances except for modularity clustering, it is better to solve the fusion move to optimality (Fusion-HC-MC) than using approximations (FUSION-HC-GCG).|
||22 instances in total. (in cvpr2015)|
|37|Kyungmin_Kim_Multimodal_Dual_Attention_ECCV_2018_paper|Multimodal fusion is performed after the dual attention processes (late fusion).|
|||We confirm the best performance of the dual attention mechanism combined with late fusion by ablation studies.|
|||3) At the multimodal fusion step, the question, caption, and frame information are fused using residual learning.|
|||During the whole inference process, multimodal fusion occurs only once.|
|||Therefore, the use of multimodal fusion methods such as concatenation [15,8] or Multimodal Bilinear Pooling [3,16,11] along with time axis might be prohibitively expensive and have the risk of over-fitting.|
|||The experimental results demonstrate two hypotheses of our model that 1) maximize QA related information through the dual attention process considering high-level video contents, and 2) multimodal fusion should be applied after high-level latent information is captured by our early process.|
|||(4) The fused representation o is calculated using residual learning fusion (Section 3.4 and Fig.|
|||Note that our multimodal fusion is applied to the latent variables instead of the early fusion in this work for high-level reasoning process.|
|||4) These attentively refined frames and captions, and a question are fused using the residual function in the multimodal fusion module.|
|||A schematic diagram of the multimodal fusion module with the two deep residual blocks.|
|||3.4 Multimodal Fusion  During the entire QA process, multimodal fusion occurs only once in this module.|
|||4 illustrates an example of our multimodal fusion module.|
|||3.5 Answer Selection  This module learns to select the correct answer sentence using the basic elementwise calculation between the output of multimodal fusion module, o  R512,  10  K.M.|
|||1) MDAM-MulFusion: model using element-wise multiplication instead of the residual learning function in the multimodal fusion module (self-attention is used).|
|||Lm denotes the depth of the learning blocks in the multimodal fusion module.|
|||Due to the small size of the MovieQA data set, the overall performance pattern shows a tendency to decrease as the depth of the attention layers Lattn and the depth of the learning blocks in the multimodal fusion module Lm increase.|
|||In addition, even if multimodal fusion occurs late, the performance is degraded where a simple element-wise multiplication is used as the fusion method (MDAM-MulFusion).|
|||The self-attention module helps MDAM achieve better performance (48.9 % for MDAM vs. 47.3 % for MDAM-NoSelfAttn), and multimodal fusion with high-level latent information by our module performs better than early fusion baseline (46.1 % for MDAM-EarlyFusion).|
|||The fundamental idea of MDAM is to provide the dual attention structure that captures a high-level abstraction of the full video content by learning the latent variables of the video input, i.e., frames and captions, then, late multimodal fusion is applied to get a joint representation.|
|||Exploring various alternative models in our ablation studies, we conjecture the following two points: 1) The position of multimodal fusion in our QA pipeline is important to increase the performance.|
|||On the other hand, the late fusion model were faster in convergence, leading to better performance results.|
||22 instances in total. (in eccv2018)|
|38|Zolfaghari_Chained_Multi-Stream_Networks_ICCV_2017_paper|This trend was initiated by Simonyan and Zisserman [36], who proposed a simple fusion of the action class scores obtained with two separate convolutional networks, where one was trained on raw images and the other on optical flow.|
|||Experiments on multiple benchmarks consistently show the benefit of the sequential refinement approach over alternative fusion strategies.|
|||[30] proposed a gated fusion approach.|
|||[50] presented an adaptive fusion approach, which uses two regularization terms to learn fusion weights.|
|||In the present work, we introduce a new, flexible fusion technique for early or late fusion via a Markov chain and show that it outperforms previous fusion methods.|
|||Baseline fusion architecture (left) and the proposed approach (right).|
|||At each fusion stage, we concatenate the output of the function 3DCNN() with the hidden state and the outputs from the previous stream and apply the non-linearity f before feeding them to Nets.|
|||This is in contrast to the fusion approaches that combine features from different, independently trained streams.|
|||Here we placed the fusion after the first fully-connected layer, but the fusion could also be applied to the earlier convolutional layers.|
|||Thus, it is well-suited for evaluating the contribution of optical flow, body part segmentation, and the fusion of all cues via a Markov chain.|
|||Integration via the proposed Markov chain clearly outperforms the baseline fusion approach.|
|||Action Classification  Table 1 shows that fusion with the sequential Markov chain model outperforms the baseline fusion consistently across all datasets.|
|||The baseline fusion is shown in Figure 4 and can be considered a strong baseline.|
|||Again, the Markov chain fusion is advantageous with a large margin.|
|||This conclusion can be drawn independently of the fusion method.|
|||Finally, the temporal multi-granularity fusion (MG) further improves results.|
|||5.2.3 Fusion location  In principle the chained fusion can be applied to any layer in the network.|
|||Classification performance for different fusion locations on UCF101, HMDB51 and J-HMDB datasets (split1).|
|||We have shown that this sequential fusion clearly outperforms other ways of fusion, because it can consider the mutual dependencies of cues during training while avoiding overfitting due to very large network models.|
|||Convolutional two-stream network fusion for video action recognition.|
||21 instances in total. (in iccv2017)|
|39|cvpr18-What Have We Learned From Deep Representations for Action Recognition |First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features.|
|||Experiments  For sake of space, we focus all our experimental studies on a VGG-16 two-stream fusion model [8] that is illustrated in Fig.|
|||Emergence of spatiotemporal features  We first study the conv5 fusion layer (i.e.|
|||2 for the overall architecture), which takes in features from the appearance and motion streams and learns a local fusion representation for subsequent fully-connected layers with global receptive fields.|
|||At conv5 fusion we see the emergence of both class specific and class agnostic units (i.e.|
|||This fact empirically verifies that the fusion unit also expects specific appearance when confronted with particular motion signals.|
|||We now consider unit f004 at conv5 fusion in Fig.|
|||Studying the Billiards unit at layer conv5 fusion from Fig.|
|||To begin, we consider filters f006 and f009 at the conv5 fusion layer that fuses from the motion into the appearance stream, as shown in Fig.|
|||6, we similarly show general feature examples for the conv5 fusion layer that seem to capture general spatiotemporal patterns for recognizing classes corresponding to mul 57848  appearance  = 0  flow  = 10  flow  = 5  flow  = 1  flow  = 0  appearance  flow  = 10  flow  = 5  flow  = 1  flow  = 0  7 0 0 f n o i s u f  5 v n o c  9 0 0 f n o i s u f  5 v n o c  YoYo 1  flow  Nunchucks 1  flow  Nunchucks 2  flow  Figure 5.|
|||Two general units at the convolutional fusion layer.|
|||General units at the convolutional fusion layer that could be useful for representing ball sports.|
|||Visualization of fusion layers.|
|||We now briefly re-examine the convolutional fusion layer (as in the previous Sect.|
|||8, we show the filters at the conv5 fusion layer, which fuses from the motion into the appearance stream, while varying the temporal regularization and keeping the spatial regularization constant.|
|||The visualizations reveal that these first 3 fusion filters at this last convolutional layer show reasonable combinations of appearance and motion information, a qualitative proof that the fusion model in [8] performs as desired.|
|||For example, the receptive field centre of conv5 fusion f002 seems matched to lip like appearance with a juxtaposed elongated horizontal structure, while the motion is matched to slight up and down motions of the elongation (e.g.|
|||Visualization of 3 filters of the conv5 fusion layer.|
|||fully-connected layers that operate on top of the convolutional fusion layer illustrated above.|
|||Convolutional two-stream network fusion for video action recognition.|
||21 instances in total. (in cvpr2018)|
|40|Shrivastava_Class_Consistent_Multi-Modal_2015_CVPR_paper|Another approach to feature level fusion is multiple kernel learning (MKL), which learns a linear combination of multiple kernels.|
|||In such situations, it is useful to make the prediction based on score level or  1  decision level fusion and not rely entirely on training accuracy.|
|||Score level fusion can be achieved by averaging the scores of decision functions and decision level fusion can be performed by taking a majority vote from all the modalities.|
|||The predicted scores from multiple modalities can also be combined using late fusion methods [23, 40].|
|||Recently, [37] proposed a sparse representation-based multimodal biometric fusion method, which represents the test data by a sparse linear combination of training data, while constraining the observations from different modalities of the test subject to share their sparse representations.|
|||Most algorithms for feature fusion have been developed for continuous features.|
|||To the best of our knowledge, ours is the first work proposing multi-modal fusion using binary codes.|
||| Based on this notion of class consistency, we develop  an efficient binary feature fusion algorithm.|
|||First, we use the fusion algorithm to combine image and depth data for object categorization.|
|||As SLR and SVM methods cannot handle multiple modalities, [37] explored score-level and decision level fusion to combine modalities.|
|||Score level fusion was achieved by adding probability outputs of all the modalities to obtain a final score vector.|
|||The score level fusion using SLR is called SLR-Sum and the decision level fusion is called as SLR-Major; for SVM, they are called SVM-Sum and SVM-Major, respectively.|
|||We present the comparison of different multi-modal fusion algorithms in Table 3, which shows that CCMM outperforms the competing algorithms despite the fact that, for individual modalities, the performance of CCMM is lower than SMBR.|
|||Acknowledgement  We described a multi-modal fusion algorithmCCMMbased on class consistency and demonstrated that it per This work was partially supported by an ONR grant  N00014-12-1-0124.|
|||Multimodal fusion for multimedia analysis: a survey.|
|||Sample-specific late fusion for visual category recognition.|
|||Feature level fusion of face and fingerprint biometrics.|
|||Feature level fusion using hand and face biometrics.|
|||Feature fusion of face and gait for human recognition at a distance in video.|
||20 instances in total. (in cvpr2015)|
|41|Yantao_Shen_Person_Re-identification_with_ECCV_2018_paper|Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information.|
|||Unlike most previous GNNs designs, in SGGNN, the weights for feature fusion are determined by similarity scores by gallery image pairs, which are directly supervised by training labels.|
|||With these similarity guided feature fusion weights, SGGNN will fully exploit the valuable label information to generate discriminative person image features and obtain robust similarity estimations for probe-gallery image pairs.|
|||(2) Different from most Graph Neural Network (GNN) approaches, SGGNN exploits the training label supervision for learning more accurate feature fusion weights for updating the nodes features.|
|||This similarity guided manner ensures the feature fusion weights to be more precise and conduct more reasonable feature fusion.|
|||With gallery-gallery similarity scores, the probe-gallery relation feature fusion could be deduced as a message passing and feature fusion schemes, which is defined as Eq.|
|||The node features are then updated as a weighted addition fusion of all  8  Y. Shen, H. Li, S. Yi, D. Chen and X. Wang  input messages and the nodes original features.|
|||This relation feature fusion could be deduced as a message passing and feature fusion scheme.|
|||After obtaining the edge weights Wij and deep message ti from each node,  the updating scheme of node relation feature di could be formulated as  d(1) i = (1  )d(0)  i +   Wij t(0)  j  N  Xj=1  for i = 1, 2, ..., N,  (4)  denotes the i-th refined relation feature, d(0)  i  i  where d(1) relation feature and t(0) weighting parameter that balances fusion feature and original feature.|
|||denotes the i-th input j denotes the deep message from node j.  represents the  Noted that such relation feature weighted fusion could be performed itera tively as follows,  d(t) i = (1  )d(t1)  i  +   N  Xj=1  Wijt(t1)  j  for i = 1, 2, ..., N,  (5)  where t is the iteration number.|
|||(4) as our relation feature fusion in both training and testing stages.|
|||Person Re-ID with Deep Similarity-Guided Graph Neural Network  9  3.3 Relations to Conventional GNN  In our proposed SGGNN model, the similarities among gallery images are served as fusion weights on the graph for nodes feature fusion and updating.|
|||In conventional GNN [66, 45] models, the feature fusion weights are usually modeled as a nonlinear function h(di, dj) that measures compatibility between two nodes di and dj.|
|||To overcome such limitation, we propose to use similarity scores S(gi, gj) between gallery images gi and gj with directly training label supervision to serve as the node feature fusion weights in Eq.|
|||(6), these direct and rich supervisions of gallery-gallery similarity could provide feature fusion with more accurate information.|
|||HydraPlus-Net [39] is proposed for better exploiting the global and local contents with multi-level feature fusion of a person image.|
|||We also validate the importance of learning visual feature fusion weight with gallery-gallery similarities guidance.|
|||We therefore remove the directly gallery-gallery supervisions and train the model with weight fusion approach in Eq.|
|||For conventional Graph Neural Network setting, the rich gallery-gallery similarity labels are ignored while our approach utilized all valuable labels to ensure the weighted deep message fusion is more effective.|
||20 instances in total. (in eccv2018)|
|42|Ma_Multiple_Feature_Fusion_ICCV_2015_paper|In this paper, we propose a new data-adaptive visual tracking approach by using multiple feature fusion via weighted entropy.|
|||Over the past few years, some researchers have proposed several multiple feature fusion based visual tracking methods, where different features were concatenated directly for object representation [22].|
|||To make the tracking model discriminative and robust to the drastic background variation, we propose a new dataadaptive visual tracking approach by using multiple feature fusion via weighted entropy, where both the generative and discriminative information are exploited in our approach.|
|||To better highlight different contributions of different features in object representation, we employ the weighted entropy [11, 12, 18] to represent the difference or disorderness of the candidate states, and the most dis 13128  criminative feature fusion strategy can be obtained by minimizing the weighted entropy.|
|||generative model model In our work, we utilize the generative model to represent the object appearance, and discriminatively compute the optimal multiple feature fusion strategy by weighted entropy.|
|||They concatenated multiple features to represent the object appearance and used no complementary information between features, while we obtained a better fusion result by using the complementary information.|
|||Specifically, we utilize the gradient descent method to seek the optimal fusion coefficient .|
|||Let a be the step length, then the new fusion coefficient at step k+1 is k+1 = k  a.|
|||(6)  Based on (6), we make the new fusion coefficient stay within the acceptable range.|
|||(3) to obtain the optimal fusion coefficient.|
|||10 Update fusion coefficients whenever a sample is saved.|
|||Secondly, we validate our multiple feature fusion based method with four methods which use single feature for state evaluation.|
|||Figures 4 and 5 show sample frames and the fusion coefficients for two video sequences, respectively.|
|||With the weighted entropy based fusion strategy, our method obtains more discriminative state evaluation and performs more robustly than other four comparing methods.|
|||We update the fusion coefficient whenever a new subimage is saved.|
|||We set  = [0.33, 0.33]T initially when updating the fusion coefficients, and set s = 0.98.|
|||Non(cid:173)weighted Entropy  In this section, we compare our weighted entropy with entropy based fusion method (Ent) on ten video sequences  (the same as those in Sec 5.2).|
|||Conclusion and Future Work  In this paper, we have proposed a new multiple feature fusion based state evaluation method with weighted entropy.|
|||Moreover, how to employ our feature fusion method to other vision applications such as image classification and action recognition is also interesting future topic.|
||20 instances in total. (in iccv2015)|
|43|Qian_Multi-Scale_Deep_Learning_ICCV_2017_paper|This is achieved by introducing two novel layers: multi-scale stream layers that extract images features by analyzing the person images in multi-scale; and saliency-based learning fusion layer, which selectively learns to fuse the data streams of multi-scale and generate the more discriminative features of each branch in MuDeep.|
|||our MuDeep generalizes the convolutional layers with multi-scale strategy and proposed multi-scale stream layers and saliency-based learning fusion layer, which is different from the ideas of combing multiple sub-networks [51] or channels [4] with pairwise or triplet loss.|
|||In this work, we use saliency-based learning strategy in a saliency-based learning fusion layer to exploit both visual saliency and attention mechanism.|
|||(2) We propose a saliency-based learning fusion layer which can learn to weight important scales in the data streams in a saliency-based learning strategy.|
|||Saliency(cid:173)based learning fusion layer  This layer is proposed to fuse the outputs of multi-scale stream layers.|
|||A fully connected layer is appended after saliency-based learning fusion layer, which extracts features of 4096dimensions of each image.|
|||Note that (1) . indicates the element-wise multiplication; the idea behind using elementwise subtraction is that if an input image pair is labelled same person, the features generated by multi-scale stream layers and saliency-based learning fusion layers should be similar; in other words, the output values of feature difference layer should be close to zero; otherwise, the values have different responses.|
|||Since the primary focus of this paper is on image-based person re-id, we employ the simplest feature fusion scheme for video re-id: Given a video sequence, we compute features of each frame which are aggregated by max-pooling to form video level representation.|
|||In contrast, most of the state-ofthe-art video-based re-id methods [40, 37, 52, 23, 30, 22] utilized the RNN models such as LSTM to perform temporal/sequence video feature fusion from each frame.|
|||This validates the efficacy of our architectures and suggests that the proposed multiscale and saliency-based learning fusion layer can help extract discriminative features for person re-id.|
|||This suggests that our framework can better analyze the multiscale patterns from data than Gated Sia [50], again thanks to the novel multi-scale stream layers and saliency-based learning fusion layers.|
|||This further validates that our multi-scale stream layers can augment the training data to exploit more information from medium-scale dataset rather than scaling up the size of training data; and saliency-based learning fusion layers can better fuse the output of multiscale information which can be used for person re-id.|
|||This means that more available related training data can always help improve the performance of deep learning model; and this also validates that our multi-scale stream layers and saliency-based fusion layers can help extract and fuse the multi-scale information and thus cope well with less training data.|
|||Our saliency-based learning fusion layer can automatically learn the optimal  weights from training data.|
|||This shows that our MuDeep is the most powerful at learning discriminative patterns than the Inceptions variants, since the multi-scale stream layers can more effectively extract multi-scale information and the saliency-based learning fusion layer facilitates the automatic selection of the important feature channels.|
|||Saliency-based learning fusion layer and classification subset.|
|||In Table 7, the  Fusion denotes our MuDeep without using the fusion layer; and ClassNet indicates our MuDeep without the classification subnet; and  Fusion  ClasNet means that MuDeep has neither fusion layer nor classification subnet.|
|||The feature fusion is done by concatenation.|
|||Note that Fusion means that saliency-based learning fusion layer is not used; ClassNet indicates that the classification subnet is not used in the corresponding structure.|
||20 instances in total. (in iccv2017)|
|44|Wang_Designing_Deep_Networks_2015_CVPR_paper|We separately learn global and local processes and use a fusion network to fuse the contradictory beliefs into a final interpretation.|
|||We combine their predictions with a fusion network that greatly outperforms either alone.|
|||Our fusion network can be viewed as a form of learned reasoning that replaces previous optimization-based attempts to reconcile evidence [12, 24, 29, 10] from conflicting sources.|
|||Inspired by these approaches, our global network predicts a box layout in addition to coarse geometry, and we provide the vanishingpoints to our fusion network.|
|||This enables the fusion network to softly apply the Manhattan or box constraint as the data dictates (e.g., on walls and floors but not chairs), and our results show that this leads to improved predictions.|
|||them in learning of the local network and as an input in the fusion network.|
|||Because the global and local processes have complementary errors, we combine their output with a fusion network that consolidates their predictions (Section 4.5).|
|||Additionally, both coarse and local networks treat every pixel independently; our fusion network can also be thought of applying a form of learned reasoning akin to [26, 35] on our outputs.|
|||Input: As input to this fusion network, we concatenate outputs of the global and local networks with the input image.|
|||As the probabilities sum to 1, we do not pass the no-edge output to the fusion network.|
|||Output: The fusion network is also applied in a sliding window scheme on the 195  260 image.|
|||By taking the inputs with size of 55 55, we estimate the surface normals of the Mb  Mb center patch via the fusion network.|
|||At testing time, we apply the fusion network on the feature maps with the stride of Mb.|
|||By combining both normal outputs we obtain better results via fusion network.|
|||With more information feeding in, the full fusion network reasons among them and improve the performance.|
|||The fusion network, which combines the raw images, surface normal predictions from the local and global networks provides a significant boost in performance.|
|||By combining all of them together in the full fusion network, we obtain better results in all metrics, and a 4.6% gain in the most strict metric.|
|||While it is easy to improve bad systems, it is difficult to improve on a strong system like the fusion network: by itself, it would be state-of-the-art in most metrics.|
|||We report our results of our full fusion network in Table 3 and some some qualitative results in Figure 9.|
||20 instances in total. (in cvpr2015)|
|45|Eleftheriadis_Multi-Conditional_Latent_Variable_ICCV_2015_paper|Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection  Stefanos Eleftheriadis  Ognjen Rudovic  Maja Pantic  Department of Computing, Imperial College London, UK  EEMCS, University of Twente, The Netherlands  {s.eleftheriadis, orudovic, m.pantic}@imperial.ac.uk  Abstract  We propose a novel multi-conditional latent variable model for simultaneous facial feature fusion and detection of facial action units.|
|||On the other hand, methods that do attempt to model the AU co-occurrences (e.g., [26, 33, 35]) fail to perform efficient fusion of different types of facial features.|
|||However, neither of these methods can perform simultaneous feature fusion and modeling of a large number of AUs.|
|||To this end, we propose a Multi-Conditional Latent Variable Model (MC-LVM) that performs jointly the fusion of different facial features and detection of multiple AUs.|
|||Instead of performing the AU detection in the original feature space, as done in existing works [29, 34, 36], the MCLVM attains the fusion by learning a low-dimensional subspace (i) shared across different feature sets, learned via the framework of Gaussian processes (GPs) [21], and (ii) constrained by the local dependencies among multiple AUs, encoded by means of string kernels [21], and the global dependencies, encoded via the AU co-occurrence structure.|
|||As evidenced by our results, the resulting model achieves superior performance compared to existing methods for multiple AU detection, and other methods for feature fusion and multi-label classification.|
|||Yet, the fusion task is not addressed, while the AU-dependencies are regarded only between predefined pairs.|
|||Since the latent variables are not connected to the feature space, they cannot model correlations between the inputs, hence,  3793  concatenation is used for the fusion task.|
|||Our approach significantly differs from the above works, since the fusion of the features is performed in a continuous latent space.|
|||The learning of the output dependencies is performed simultaneously to the fusion task by combining both generative and discriminative learning within a single model.|
|||However, none of these methods perform simultaneous feature fusion and MLC.|
|||To mitigate the limitations of the above methods, recent works in the GP context [28, 6] try to combine multitask learning and feature fusion via subspace learning.|
|||Contrary to [28, 6], MC-LVM employs multi-conditional learning strategies to re-weight the generative and discriminative conditionals, in order to unravel a more suitable subspace for joint feature fusion and MLC.|
|||We continue by evaluating the effectiveness of the proposed MC-LVM on the feature fusion task.|
|||The effect of the relational constraints (left), and  the feature fusion (right) to the joint AU detection task.|
|||HRBM cannot handle simultaneously the fusion of the concatenated features and the modeling of the AU dependenlp-MTMKL, due to its cies using binary latent variables.|
|||space learning for efficient feature fusion and joint AU detection.|
|||By assuming conditional independence given the subspace of AUs, MC-LVM allows each feature set to be described via feature-specific GPs, resulting in more accurate fusion in the manifold, and hence, more discriminative features for the detection task.|
|||We demonstrated the effectiveness of these properties on three publicly available datasets by showing that the proposed model outperforms the existing works for multiple AU detection, and several methods for feature fusion and multi-label learning.|
||20 instances in total. (in iccv2015)|
|46|Lan_Multi-Cue_Visual_Tracking_2014_CVPR_paper|Therefore, feature level fusion should be performed to take advantage of the more informative cues for tracking.|
|||Recently, multi-task joint sparse representation (MTJSR) [19, 22] has been proposed for feature-level fusion in visual classification and promising results have been reported.|
|||Based on joint sparse representation, we propose and develop a new robust feature-level fusion method for visual tracking.|
|||To the best of our knowledge, this is the first joint sparse representation based multiple feature-level fusion method for visual tracking.|
|||The contributions of this paper are as follows:  This paper develops a new visual tracking algorithm based on feature-level fusion using joint sparse representation.|
||| We propose to detect the unreliable visual cues for the robustness in the feature-level fusion process.|
|||And, fusion via concatenation may not improve the performance when some source data are corrupted.|
|||Robust Feature-Level Fusion for Multi-Cue  Tracking This section presents the details of the proposed tracking algorithm using robust feature-level fusion based on joint sparse representation.|
|||The proposed method consists of two major components: feature-level fusion based on joint sparse representation and detecting unreliable visual cues for robust fusion.|
|||In other words, the feature-level fusion is given by discovering the relationship between visual cues y1, , yK to determine weight vectors w1, , wK dynamically.|
|||In this case, we cannot obtain robust fusion result by minimizing optimization problem (2) with the regularization function (4).|
|||Experiment  In this section, we evaluate the proposed robust joint sparse representation based feature-level fusion (RJSR-FFT) tracking algorithm using both synthetic data and real videos for experiments.|
|||This mainly attribute to the fusion of local information in our proposed method so its less sensitive to partial occlusion.|
|||Conclusion  In this paper, we have successfully formulated a featurelevel fusion visual tracker based on joint sparse representation.|
|||This paper has demonstrated that using proposed robust feature-level fusion of multiple features can improve the tracking accuracy.|
|||Experimental results on publicly available videos show that the performance of the proposed tracker using robust joint sparse representation based feature-level fusion model outperforms seven state-of-theart tracking methods.|
|||Linear dependency modeling for classifier fusion and feature combination.|
|||Multiple source data fusion via sparse representation for robust visual tracking.|
||19 instances in total. (in cvpr2014)|
|47|cvpr18-Gesture Recognition  Focus on the Hands|It has three main components: 1) a focus of attention mechanism, 2) 12 separate global and focused channels, and 3) a fusion mechanism.|
|||Finally, fusion occurs through a sparse network that learns which channels are important for each gesture.|
|||Section 3.2 explains the sparse fusion network that combines information across channels.|
|||With 12 channels, the concatenated feature vector would be over 24,000 elements long, and the fusion layer as a whole would have to learn over 6 million weights.|
|||To measure the effectiveness of spatial attention channels and gesture-based fusion relative to other techniques, we compare the recognition accuracy of FOANet as shown in Figure 2 to those of previous systems on the ChaLearn IsoGD (this section) and NVIDIA (next section) data sets.|
|||To learn the weights of the fusion layer, the softmax scores of different channels of training data are precomputed.|
|||The average fusion version of FOANet achieves better results than previous methods (67.38% vs 64.40% on validation set and 70.37% vs 67.71% on test set), as shown in Table 2.|
|||Therefore, sparse network fusion improves performance by 11.7%.|
|||With sparse network fusion the system learns which channels to include for each gesture type, with the result that sparse network fusion benefits from the presence of channels that hurt performance when averaging channels.|
|||Unfortunately, this method doesnt perform on par with sparse network fusion or even simply averaging the softmax out 5240  Fusion  Sparse Average  Concatenation  Valid  Test  12 Channels  7 Channels  12 Channels  7 Channels  80.96 67.38 56.03  77.31 69.06 55.29  82.07 70.37 59.44  78.90 71.93 58.84  Table 2.|
|||Comparison of fusion strategies.|
|||However, overall performance is best when all channels are combined, suggesting that the left hand is important for two-handed gestures and that sparse network fusion is able to learn when to pay attention to the left hand.|
|||From the first two fusion columns, we can see that the combination of focus channels is better than the combination of global channels.|
|||In fact, the fusion of focus channels is the best combination, short of combining all channels.|
|||We also notice that the fusion of RGB and RGB flow nets is better than the fusion of depth and depth flow nets on validation set.|
|||Next, we see that the fusion of RGB and depth channels performs on par with the fusion of RGB flow and depth flow channels.|
|||The accuracy of FOANet drops to 85.26% when sparse network fusion is replaced by average fusion, emphasizing the importance of sparse network fusion even in domains with only one hand and no significant background changes.|
|||The current architecture does not address temporal fusion in a sophisticated way.|
||19 instances in total. (in cvpr2018)|
|48|Park_RDFNet_RGB-D_Multi-Level_ICCV_2017_paper|To learn the optimal fusion of multimodal features, this paper presents a novel network that extends the core idea of residual learning to RGB-D semantic segmentation.|
|||Our network effectively captures multilevel RGB-D CNN features by including multi-modal feature fusion blocks and multi-level feature refinement blocks.|
|||Feature fusion blocks learn residual RGB and depth features and their combinations to fully exploit the complementary characteristics of RGB and depth data.|
|||Our network consists of two feature fusion blocks: multi-modal feature fusion (MMF) block and multi-level feature refinement (Refine) block (Figure 1).|
|||Our multi-modal feature fusion block enables efficient end-to-end training of discriminative RGB-D features on a single GPU by taking full advantage of residual learning with skip-connection.|
|||[5] extended multi-scale RGB CNN architecture [8] to RGBD situation by simply concatenating input color and depth channels, i.e., early fusion (Figure 2 (a)).|
|||It does not exploit any informative intermediate features of both modalities and it adopts simple score fusion of two modalities at the end of the network for final prediction.|
|||Our network is trained to obtain optimal fusion of two complementary modality features through residual learning with skip-connection and iteratively refines the fused features.|
|||The multi-path residual feature fusion with skip-connection allows the backward gradient to easily propagate to both RGB and depth layers.|
|||The multiresolution fusion block fuses the multi-path input into a  (cid:56) (cid:47) (cid:72) (cid:53)  (cid:56) (cid:47) (cid:72) (cid:53)  (cid:3)  (cid:3)  (cid:3)  (cid:3)  (cid:3)  (cid:14)  (cid:14)  (cid:79) (cid:82) (cid:82) (cid:51) (cid:24) (cid:3400) (cid:24)  (cid:89) (cid:81) (cid:82) (cid:38) (cid:22) (cid:3400) (cid:22)  (cid:89) (cid:81) (cid:82) (cid:38) (cid:22) (cid:3400) (cid:22)  (cid:14) (cid:53)(cid:72)(cid:47)(cid:56)  (cid:89) (cid:81) (cid:82) (cid:38) (cid:22) (cid:3400) (cid:22) higher-resolution feature map.|
|||Diagram of our multi-modal feature fusion (MMF) network.|
|||In this paper, we employ a similar architecture for multi-modal CNN feature fusion while retaining the advantage of skip connection.|
|||Our RDFNet extends the RefineNet to handle multimodal feature fusion and includes RefineNet blocks for fused feature refinement.|
|||Our feature fusion block consists of the same components as in RefineNet but with different inputs, from which desired operations are slightly different.|
|||As color features generally have better discrimination power than depth features for semantic segmentation, the summation fusion in the block mainly works to learn supplementary or residual depth features which might improve RGB features to discriminate  confusing patterns.|
|||For the comparison, we replace our MMFNet with feature concatenation fusion with additional dropout layer and one convolution layer for dimension reduction.|
|||Here we only compare with the multi-level concatenation fusion (Refine-Concat) because we found that it generally shows better accuracy than other fusion architectures (early fusion, late fusion, and other variations).|
|||Lstmcf: Unifying context modeling and fusion with lstms for rgbd scene labeling.|
||19 instances in total. (in iccv2017)|
|49|cvpr18-Language-Based Image Editing With Recurrent Attentive Models|The fusion module takes as input the image features that encode the source image via a convolutional neural network, and the textual features that encode the natural language expression via an LSTM, and  outputs the fused features to be upsampled by a deconvolutional network into the target image.|
|||In the fusion module, recurrent attentive models are employed to extract distinct textual features based on the spatial features from different regions of an image.|
|||A high-level diagram of our model, composed of a convolutional image encoder, an LSTM text encoder, a fusion module, a deconvolutional upsampling layer, with an optional convolutional discriminator.|
|||The framework is composed of a convolutional image encoder, an LSTM text encoder, a fusion network that generates a fusion feature map by integrating image and text features, a deconvolutional network that generates pixelwise outputs (the target image) by upsampling the fusion feature map, and an optional convolutional discriminator used for training colorization models.|
|||Recurrent attentive fusion module The fusion network fuses text information in U into the M  N image feature map V , and outputs an M  N fusion feature map, with each position (image region) containing an editing feature vector, O = {oi : i = 1, .|
|||The fusion network is devised to mimic the human image editing process.|
|||Note that Ot is the intermediate output of the fusion feature map at time step t.  i; tg)).|
|||Each termination gate generates a binary random variable according to the current internal state of its image region:  t i  p(ftg(st i = 1, the fusion process for the image region vi stops at t, and the editing feature vector for this image region is set as oi = ot i.|
|||When all terminate gates are true, the fusion process for the entire image is completed, and the fustion network outputs the fusion feature map O.|
|||k<t  the probability of stopping the fusion process at the i-th image region of the feature map at time t.  Inference Algorithm 1 describes the stochastic inference process of the fusion network.|
|||The fusion network outputs for each image region vi an editing feature vector oi at the ti-th step, where ti is controlled by the ith termination gate, which varies from region to region.|
|||It takes as input the M N fusion feature map O produced by the fusion module, and unsamples from O to produce a H  W  De editing map E of the same size as the target image, where De is the number of classes in segmentation and 2 (ab channels) in colorization.|
|||In the fusion network, the attention model has 16 units, the GRU cells use 16 units, and the termination gate uses a linear map on top of the hidden state of each GRU cell.|
|||In the fusion network, the attention model uses 512 units and the GRU cells 1, 024 units, on top of which is a classifier and an upsampling layer similar to the implementation in Section 4.1.|
|||We attribute the superior performance to the unique attention mechanism used by our fusion network.|
|||In the fusion network, the attention model uses 128 units and the GRU cells 128 units.|
|||The image encoder is composed of 2 deconvolutional layers, each followed by 2 convolutional layers, to upsample the fusion feature map to the target image space of 256  256  2.|
|||At the heart of the proposed framework is a fusion module that uses recurrent attentive models to dynamically decide, for each region of an image, whether to continue the text-to-image fusion process.|
||19 instances in total. (in cvpr2018)|
|50|Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper|For the same input image, we show the result of the MUTAN fusion process when integrated into an attention mechanism [28]: we can see that the regions with larger attention scores (in red) indicate a very fine understanding of the image and question contents, enabling MUTAN to properly answer the question (see detailed maps in experiments section).|
|||On the contrary, [5] and [8] developed their own fusion methods that they use for global and attention-based strategies.|
|||In this paper, we use the attentional modeling, proposed in [5], as a tool that we integrate in our new fusion strategy for both the global fusion and the attentional modeling.|
|||To control the number of model parameters, MUTAN reduces the size of the mono-modal embeddings, while modeling their interaction as accurately as possible with a full bilinear fusion scheme.|
|||Our submission therefore encompasses the following contributions:   New fusion scheme for VQA relying on a Tucker tensor-based decomposition, consisting in a factorization into three matrices and a core tensor.|
|||2613  Figure 2: MUTAN fusion scheme for global Visual QA.|
|||Bilinear models [5, 8] are recent powerful solutions to the fusion problem, since they encode fully-parametrized bilinear interactions between the vectors q and v:  with the full tensor T  Rdqdv|A|, and the operator i designing the i-mode product between a tensor and a matrix (here a vector).|
|||The multimodal Tucker fusion is depicted in Figure 2.|
|||As we can see in Table 1, MUTAN obtains the best results, validating our intuition of having a nice tradeoff between the projection dimensions and a reasonable number of useful bilinear interaction parameters in the core tensor T c. Finally, a naive late fusion MUTAN(2)+MLB5 further improves performances (about +1pt on test-dev).|
|||Other All All 79.25 36.18 46.69 58.91 56.92 80.81 35.91 46.43 59.40 57.39 82.02 36.61 46.65 60.08 57.91 81.44 36.42 46.86 59.92 57.94 81.45 37.32 47.17 60.17 58.16  MUTAN(2)+MLB 17.5 82.29 37.27 48.23 61.02 58.76  Table 1: Comparison between different fusion under the same setup on the test-dev split.|
|||MCB [5] and MLB [8] have a strong edge over other methods with a less powerful fusion scheme.|
|||This validates the relevance of the proposed fusion scheme, which models precise interactions between modalities.|
|||the benefit of the fusion scheme directly translates for the whole VQA task.|
|||Finally, we also evaluated an ensemble of 3 models based on the MUTAN fusion scheme (without MLB), that we denote as MUTAN (3).|
|||Here, we examine under different aspects the fusion between q and v with the Tucker decomposition of tensor T .|
|||With our experimental setup, we just focus on the effect of adding parameters to our fusion scheme.|
|||Figure 4: The improvements given by MUTAN noR over a model trained with the identity tensor as a fusion operator between  q and  v.|
|||The number of parameters in the fusion is lower, and the accuracy on the val split is higher.|
||19 instances in total. (in iccv2017)|
|51|Fan_A_Paradigm_for_CVPR_2016_paper|Thus, through data fusion we get extra performance for free.|
|||Based on structure-revealing data fusion and statistical modeling, we  5762  Figure 1: Overview of our approach.|
|||Second, we built a generalized and expandable human perception model through structural-revealing statistical modeling, which also informs data fusion (c).|
|||In many disciplines, from Data fusion and inference: computer vision to neuroscience, data from multiple sources are acquired and jointly modeled for enhanced knowledge discovery [27], but joining data often results in missing data.|
||| Selected as auxiliary variables [30] in data fusion in Sec.|
|||Such structures also guide data fusion in Sec.|
|||Datasets with human perception  We used the following two datasets in empirical modeling,  and the data fusion in Sec.|
|||Objects_Combo_Natural  Objects_Natural  Common_Perspective  Image_Sharp Image_Quality  Dynamic_Scene  Storyline  Static  Mysterious Strange  Light_Natural Color_Natural  Cluttered  Open_space  .67  .82  .89  .95  .96  .85  .48  .68  .77  .86  .83  .80  Familiar  .41  Artistic  -.03  -.07  -.44  Dynamic  .12  .85  .15  .07  .39  Weird  -.06  .27  -.41  .14  Natural  .19  .06  .46  .14  .35  .44  -.10  .14  .44  -.91  Space  .82  .91  Liking  Interesting  Exciting  Figure 3: Human perception model (fused model 1) based on the fusion of two datasets.|
|||Data fusion was enabled by shared perceptions across the datasets.|
|||In other words, data fusion allows more latent factors to be included, thus generating a visual perception model more gerneral than its constituents.|
|||Objects_Combo_Natural  Objects_Natural  Common_Perspective  Image_Sharp  Image_Quality  Dynamic_Scene  Storyline  Static  Mysterious  Strange  Light_Natural  Color_Natural  Cluttered  Open_space  .69  .84  .87  .93  .97  .85  .49  .67  .74  .88  .81  .81  Familiar  .38  Artistic  .08  .37  .20  .31  .37  -.14  .17  -.05  .44  Dynamic  .11  .83  .07  .43  .16  Weird  -.03  .28  -.37  .14  Natural  .17  .11  .45  -.92  Space  Liking  .82  .91  Interesting  Exciting  Figure 5: Human perceptual model (fused model 2) based on the fusion of three datasets.|
|||Perception model based on fusion of three datasets: The fusion and modeling procedures were the same as in Sec.|
|||We then combined the features in later fusion according to the weights of the links towards liking in fused model 2 (Table 2).|
|||6a, using all the  attributes based on the fusion of three datasets produced the best performance, suggesting the advantage of data fusion.|
|||Late fusion was performed as weighted average of the regression outputs for respective perceptual factors.|
|||As shown in Table 3, our complete attributes set from data fusion (1st row) performed the best among all evaluation metrics.|
|||Discussion  Power of data fusion: Using all data in fused datasets was consistently better than using only observed data, and the performance based on the fusion of three datasets was better than that based on fusions of two datasets (Fig.|
|||The information gain most likely comes from the expansion of observables due to the data fusion per se, and data imputation makes the usage of off-the-shelf classifiers possible.|
||19 instances in total. (in cvpr2016)|
|52|Zerong_Zheng_HybridFusion_Real-Time_Performance_ECCV_2018_paper|Our method combines nonrigid surface tracking and volumetric fusion to simultaneously reconstruct challenging motions, detailed geometries and the inner human body of a clothed subject.|
|||Significant fusion artifacts are reduced using a new confidence measurement for our adaptive TSDF-based fusion.|
|||Combining IMUs with depth sensors within a non-rigid depth fusion framework is non-trivial.|
|||), we propose an adaptive TSDF fusion method that considers all the factors above in one tracking confidence measurement to get more robust and detailed TSDF fusion results.|
|||DoubleFusion [46] leveraged parametric body model (SMPL [18]) in non-rigid surface integration to improve the tracking, loop closure and fusion performance, and achieved the state-of-the-art single-view human performance capture results.|
||| Adaptive Geometry Fusion To improve the robustness of the fusion step, we propose an adaptive fusion method that utilizes tracking confidence to adjust the weight of TSDF fusion adaptively.|
|||Besides voxel collision, the surface fusion still suffers from inaccurate motion tracking, which is a factor that previous fusion methods do not consider.|
|||Since the quality of depth input is inversely proportional to body-camera distance and the low quality depth will significantly deteriorate the tracking and fusion performance, the tracking confidence of all nodes declines when the body is far from the camera (Fig.|
|||For a voxel v, D(v) denotes the TSDF value of the voxel, W(v) denotes its accumulated fusion weight, d(v) is the projective signed distance  10  Z. Zheng et al.|
|||function (PSDF) value, and (v) is the fusion weight of v at current frame:  (v) = XxkN (v)  Ctrack (xk) , (v) =(0  (v)  (v) <  , otherwise.|
|||The majority of the running time is spent on the joint motion tracking (23 ms) and the adaptive geometric fusion (6 ms).|
|||We also evaluate the effectiveness of the adaptive geometric fusion method.|
|||We then compare our adaptive geometry fusion method against previous fusion method used in [26, 10, 45, 46].|
|||In Fig.9, the results of the previous fusion method are presented on the left side of each sub-figure, while the reconstruction results with adaptive fusion are shown on the right.|
|||4, the fusion weights in our system can be automatically adjusted (set to a very small value or skip the fusion step) in all the situations, resulting in more plausible and detailed surface fusion results.|
|||Evaluation of adaptive fusion under far body-camera distance (a), occlusions (b) and fast motions (c).|
|||In each sub-figure, the left mesh is fused by previous fusion method and the right one is fused using our adaptive fusion method.|
||18 instances in total. (in eccv2018)|
|53|Zhiwen_Fan_A_Segmentation-aware_Deep_ECCV_2018_paper|In this paper, we proposed a segmentation-aware deep fusion network called SADFN for compressed sensing MRI.|
|||Then, the aggregated feature maps containing semantic information are provided to each layer in the reconstruction network with a feature fusion strategy.|
|||We prove the utility of the cross-layer and cross-task information fusion strategy by comparative study.|
|||In this paper, we propose a segmentation-aware deep fusion network (SADFN) architecture for compressed sensing MRI to fuse the semantic supervision information in the different depth from the segmentation label and propagate the semantic features to each layer in the reconstruction network.|
||| The semantic information from the segmentation network is provided to reconstruction network using a feature fusion strategy, helping the recon 4  W. Fan et al.|
|||3 The Proposed Architecture  To incorporate the information from segmentation label into the MRI reconstruction, we proposed the segmentation-aware deep fusion network (SADFN).|
|||Then a segmentation-aware feature extraction module is designed to provide features with rich segmentation information to reconstruction network using a feature fusion strategy.|
|||3.3 Deep Fusion Network  With the well-trained Pre-RecNet and Pre-SegNet, we can construct the segmentationaware deep fusion network with N blocks (SADFNN ) by integrating the features  8  W. Fan et al.|
|||from the Pre-RecNet and Pre-SegNet, which involving a cross-layer multilayer feature aggregation strategy and a cross-task feature fusion strategy.|
|||Also, in the Figure 2, the feature fusion strategy is also utilized in each block of the Pre-RecNet.|
|||The Fine-tuning Strategy With the well-constructed deep fusion network, we further fine-tune the resulting architecture.|
|||Meanwhile, the zero-filled MR image is also input to the deep fusion network.|
|||During the optimization, the parameters in the Pre-RecNetN and Pre-SegNet are kept fixed, while we only adjust the parameters in the deep fusion network.|
|||The selected feature maps from the feature tensors produced by the feature fusion in the deep fusion network.|
|||Again, we note that during the fine-tuning of the SADFN model, compressed feature tensor is yielded by multilayer features aggregation (MLFA) and the feature tensor is propagated to the Pre-RecNet before the feature fusion in each block.|
|||6 Conclusion  In this paper, we proposed a segmentation-aware deep fusion network (SADFN) for compressed sensing MRI.|
|||The multilayer feature aggregation is adopted to fuse crosslayer information in the MRI segmentation network and the feature fusion strategy is utilized to fuse cross-task information in the MRI reconstruction network.|
||18 instances in total. (in eccv2018)|
|54|Liong_Cross-Modal_Deep_Variational_ICCV_2017_paper|Our model is trained under two main steps: First, we perform binary code inference to learn unified binary codes for each training pair using a cross-modal fusion network such that we obtain a common hamming space for the two modalities and the modality gap can be implicitly reduced.|
|||Second, we model the modality-specific hashing networks which have a probabilistic interpretation such that the latent variable is modeled similar to the inferred binary code from the fusion network through a log likelihood criterion, which is also sampled based on an approximate posterior distribution regularized by a prior through a KullbackLiebler Divergence (KLD) criterion.|
|||Given a gallery set represented by two modalities (image and text), we learn a fusion hashing network and a joint binary code matrix, simultaneously.|
|||First, given the image-text pair, the latent variable is forced to be similar as possible to the inferred binary code from the fusion network through a negative log likelihood criterion.|
|||Our implementation composes of a fusion network for binary code inference that learns binary codes from image and text data discretely and discriminatively, and a generative modality-specific network to encode the image/text sample to representative binary codes.|
|||The output of the fusion network would then be h  R1 .|
|||We let the output for the whole training set of the fusion network be H  R  , the learned binary code matrix be B = [b1, b2,    , b ]  {1, 1}  , the label data be defined as Y = [y1, y2,    , y ]  {1, 0}  where y, = 1 if the -th sample belongs to class  and 0 otherwise, and a multi-class projection matrix be defined as M = [m1, m2,    , m ]  R .|
|||We obtain a closed-form solution as follows:  b = sgn(yM + h)  (6)  Update  with fixed M and B: We obtain the resulting formulation:  min    () = B  H2   (7)  Algorithm 1: CMDVH cross-modal fusion network  Input: Training set X and X, network learning  parameters, iterative number , objective function parameter  and convergence error .|
|||Step 2 (Fusion Network and Binary Code Learning): for  = 1, 2,    ,  do  Compute H using the initial fusion network.|
|||Algorithm 1 summarizes the detailed procedure of our the cross-modal fusion network of our CMDVH.|
|||Modality-Specific Networks: After learning a representative binary code for the training cross-modal pairs from a fusion network, we can now learn generative modality-specific networks for encoding out-of-sample input.|
|||The aim of modality-specific networks is to directly map each cross-modal sample pair into similar binary code inferred from the fusion network as follows :   : R  {1, 1} ,   : R  {1, 1}  (9)  Inspired by the success of variational encoders [12], we employ a probabilistic interpretation for the modality-specific network to make it more general and suitable for out-ofsample extension.|
|||For the fusion network, the image hashing network used the pre-trained CNN-F from [4] as our initial convolution and pooling layers up to FC7, and stack a number of new FC layers with dimensions of [4096  500  200] for all datasets, while the text hashing network is designed with fully-connected networks and use the pre-processed text features, given by each experiment, as input.|
|||CMDVH1 ignores the latent network in the cross-modal fusion network which assumes that simply combining the outputs of the image  10Results are obtained from the respective authors papers.|
|||We see that a fusion network is still important to perform the nonlinear transformation to make the learned codes more representative.|
|||Our method learns a fusion network to learn binary codes from cross-modal training pairs which exploits class label information, which learn a generative modalityspecific hash network for the out-of-sample extension.|
|||Data fusion through cross-modality metric learning using similarity-sensitive hashing.|
||18 instances in total. (in iccv2017)|
|55|Cheng_Query_Adaptive_Similarity_ICCV_2015_paper|To capture such a dynamic characteristic, a group of matchers equipped with various fusion weights is constructed, to explore the responses of dense matching under different fusion configurations.|
|||They usually share similar visual appearances or geometric shapes, which makes the choosing of fusion weight a tough problem.|
|||Here, we argue that such an ideally golden fusion weight doesnt exist at all.|
|||Through varying the weight, we construct a group of dense matchers, whose outputs together characterize the trend of similarity change under different fusion configurations.|
|||(3)  Here, frgb(p|I) and fdepth(p|I) are respectively the RGB and depth feature descriptors extracted at the point p from the object I, and   [0, 1] is a specified fusion weight to combine RGB and depth cues.|
|||The score explicitly evaluates the local appearance and shape agreements between the two objects, pixel-by-pixel, adopting a certain fusion weight  to balance the two cues.|
|||However, just like we explained in Section 1, it is hard to further improve the performance as a constant fusion weight cannot deal with all the complicated situations when comparing objects from various categories.|
|||Through studying the behaviors of a single dense matcher, it is found the trend of matching cost varying along with the fusion weight  also reveals some sort of information.|
|||That is, the combined similarity score between the query Iq and the reference Ir can be written as  ssum(Ir|Iq) = (cid:2)  w  s(Ir|Iq) + b = w  Ir|Iq , (5)    where the fusion weight  is uniformly sampled from the interval [0.0, 1.0] with a stride  (i.e.,  = [0.0 :  : 1.0]), Ir|Iq = {s0.0(Ir|Iq),    , s1.0(Ir|Iq), 1} is the vector of  dense matcher scores, and w = {w0.0,    , w1.0, b} is the  Training data collection.|
|||The learning-to-combination works on 11 dense matchers whose fusion weights  ranged from [0, 1] with the stride  = 0.1.|
|||The top-1 accuracy of object recognition based on a single dense matcher equipped with fusion weight   [0, 1].|
|||Following the same setting of the experiment setup, we evaluate the performance of a single dense matcher with different fusion weight  on the Washington RGB-D dataset.|
|||Clearly that optimal fusion weights on one set lead to bad performance on the other set.|
|||Even belonging to the same class, the object instances can depend on very different fusion weights to distinguish themselves with other categories.|
|||The results also demonstrate that devising multiple dense matchers equipped with different fusion weights for object recognition is necessary.|
|||Our similarity measure has two advantages: (1) through dense matching, the two objects in comparison can be transformed and aligned with each other, of which the similarity measure is more meaningful; (2) with a learning-to-combination strategy, the measure can explore a dynamic fusion way to combine RGB and depth cues effectively, which can offers surprisingly good generalization to apply the proposed measure for object recognition.|
||17 instances in total. (in iccv2015)|
|56|Jiang_Combination_Features_and_2015_CVPR_paper|We apply the HOG-III features and weighted-NMS fusion algorithm to (1) the Grammar model and Poselet model for human detection, and to (2) the deformable part-based model and deepCNN-based model for the detection of the whole VOC 20 object categories, and they lead to competitive improvements in both cases, which are indeed demonstrated by the experiments.|
|||Section 3 presents the model fusion method based on the weighted-NMS algorithm.|
|||Weighted-NMS based model fusion method  It is almost impossible for a single human detection model to detect all types of human bodies precisely.|
|||The detailed fusion procedure based on weightedNMS is shown in Algorithm 1.|
|||Input  : Detections of model A: {(pi, si)}, Detections of model B: {(pj, sj)} ; Output: Fused detections, i.e., the updated U  // sj has already been calibrated  1 Merge {(pi, si)}M 2 Normalize the scores sk to interval (0, 1) with the sigmoid function  j=1 to a union set U : {pk, sk}M +N k=1 ;  i=1 and {(pj, sj)}N   sk = 1/ (1 + exp{  (sk  )}) ;  // ,  are fixed hyper-parameters  3 Sort the tuples {(pk,  sk)}M +N 4 for h  1 to end(U ) do 5  for l  h + 1 to end(U ) do  k=1  in U by descending order of  sk;  6  7  8  9  10  11  12  Compute overlap(ph, pl) = area(ph T pl) area(ph S pl) ; if overlap(ph, pl) > T then whl  overlap(ph, pl) ;  sh   sh + whl   sl; Delete (pl,  sl) from U ;  // T : threshold for overlapped detections decay weight for score absorption  // w:  end  end  13 end 14 return The updated detection set U  Algorithm 1: Weighted-NMS based fusion procedure for model A and model B.|
|||Experimental results  To test the performance of the HOG-III features and weighted-NMS based model fusion method, we conduct a series of experiments on the PASCAL VOC datasets.|
|||We combine the person Grammar model [15] and Poselet model [3] with different fusion methods, to show the effects for different fusion methods.|
|||The hyper-parameters  and  in the fusion step are fixed to  = 2,  = 0, which are optimized on the validation set.|
|||The results for model combination are shown in Table 1(b), where G-P Model denotes the combination model of Grammar model and Poslet model, obtained by our weighted-NMS fusion algorithm.|
|||However, when we combine the Poselet model and Grammar model together with our weightedNMS based fusion method, the fineness of the Poselet model has negligible effect on the performance of our G-P model.|
|||The resultant fusion model shows very good performance.|
|||is 51.3% for Grammar(HOG-III), 58.7% for R-CNN, and 65.2% for the fusion model of Grammar and R-CNN, which is indeed a significant improvement.|
|||Extension to the whole VOC 20 classes  Our initial aim is just to find good features or detection models for the human detection task, so the proposed HOGIII features and weighted-NMS based fusion method are initially designed and evaluated only for person class.|
|||To investigate this generalization problem, we extend the HOG-III features and weightedNMS fusion algorithm to the detection of the whole VOC 20 object categories.|
|||Specifically, the mean AP on VOC2007 testset is 33.7% for DPM, 58.4% for R-CNN, while 60.5% for the fusion model of DPM and R-CNN.|
|||The experiments on PASCAL VOC datasets have demonstrated that, both the HOG-III features and the weighted-NMS fusion algorithm can boost the performance significantly for human detection, as well as gain competitive improvements for the detection of the whole VOC classes.|
||17 instances in total. (in cvpr2015)|
|57|Zheng_Packing_and_Padding_2014_CVPR_paper|To address this problem, this paper proposes a coupled MultiIndex (c-MI) framework to perform feature fusion at indexing level.|
|||Specifically, we exploit the fusion of local color feature into c-MI.|
|||To enhance the discriminative power of SIFT visual words, we present a coupled Multi-Index (c-MI) framework to perform local feature fusion at indexing level.|
|||We show in the experiments that c-MI is well compatible with methods such as rootSIFT [17], Hamming Embedding [4], burstiness weighting [5], graph fusion [25], etc.|
|||Feature Fusion The fusion of multiple cues has been proven to be effective in many tasks [18, 30, 13].|
|||Since the SIFT descriptor used in most image retrieval systems only describes the local gradient distribution, feature fusion can be performed to capture complementary information.|
|||To perform feature fusion between global and local features, Zhang et al.|
|||[25] combine BoW and global features by graph fusion and maximizing weighted density, while coindexing [26] expands the inverted index according to global attribute consistency.|
|||The primary reason lies in that feature fusion works better for features with low correlation, such as SIFT and color.|
|||Moreover, contrary to [1] we couple different features into a multiindex, so that feature fusion is performed at indexing level.|
|||Graph Fusion As a post-processing step, we implemented the graph fusion algorithm proposed in [25].|
|||In addition, we add a post-processing step, i.e.,, the graph fusion of global HSV histogram [25] to the Holidays and Ukbench datasets.|
|||As the database is scaled up, the performance gap between c-MI and the baseline seems to become larger: the feature fusion scheme works better for large databases.|
|||Moreover, in Table 6, we present a comparison with results obtained by various post-processing algorithms, including RANSAC verification [14], kNN re-ranking [19], graph fusion [25], and RNN re-ranking [15], etc.|
|||We show that, followed by graph fusion [25] of modified global HSV feature (HSV*), we have set new records on both Ukbench  4327  1041051062.72.82.933.13.23.33.43.53.63.7database sizeNS scorecMI + HEsHEscMIbaseline1031041051062030405060708090database sizemean average precision (%)cMI + HEsHEscMIbaseline1031041051062030405060708090database sizemean average precision (%)cMI + HEsHEscMIbaseline1031041051062030405060708090100database sizetop1 hit rate (%)cMI + HEsHEscMIbaselineand Holidays datasets.|
|||Query spe cific fusion for image retrieval.|
||17 instances in total. (in cvpr2014)|
|58|Guo_Learning_Dynamic_Siamese_ICCV_2017_paper|We then present elementwise multi-layer fusion to adaptively integrate the network outputs using multi-level deep features.|
|||Instead, we propose to offline learn the elementwise fusion weight maps.|
|||Hence, such offline trained elementwise fusion truly reflects the complementary role of response maps from different layers, thus is helpful to obtain better target localization ability (see section 4.3).|
|||(9), we have two advantages over HCF [21]: 1) elementwise fusion is much more effective that allows spatiallyvariant integration; 2) the weight maps can be offline learned, instead of artificially setting.|
|||4 for an example of two real offline learned fusion weight maps.|
|||The DSiam network can be further extended to a multi-layer version DSiamM using elementwise fusion in Eq.|
|||The above process can also be used to calculate FZ Lt, FG Lt, F  G Lt and w Lt from   FZ Lt. For elementwise multilayer fusion Eq.|
|||For elementwise fusion weight maps, we initialize the weight map of conv5 to be the matrix of ones and that of conv4 to be the matrix of zeros.|
|||DSiam and DSiamM use the feature network introduced in section 3.6 as f l. Specifically, DSiam only uses layer conv5; DSiamM fuses the responses of layers conv5 and conv4 with offline learned elementwise fusion weight maps; DSiamM Vgg19 uses the pre-trained VGG19 network [25] as f l and adopts the deep features from conv5-4 and conv4-4 layers.|
|||DSiamM-VT, DSiamM-BS and DSiamM-Multi denote the trackers whose target appearance variation transformation, background suppression transformation and elementwise multi-layer fusion component are removed from DSiamM, respectively.|
|||twise multi-layer fusion (Multi).|
|||Elementwise fusion vs. fixed fusion weight.|
|||An alternative fusion solution is to artificially select some proper combination weights for multi-layer response maps through generic optimization [10] or exhaustive testing.|
|||We compare our elementwise multi-layer fusion with this simpler fusion strategy.|
|||9, elementwise fusion indeed gets better tracking performance.|
|||Second, our DSiam model can work on multi-level deep features, the outputs of which can be adaptively integrated through a particular elementwise fusion layer.|
||17 instances in total. (in iccv2017)|
|59|Werghi_Representing_3D_Texture_2015_CVPR_paper|In addition, it allows earlylevel fusion of the geometry and photometric texture modalities.|
|||To the best of our knowledge, this work is the first one to propose texture and shape fusion for face recognition using LBP patterns constructed on the mesh.|
|||Fusion of mesh-LBP descriptors  Four levels of fusion are typically considered in biometry applications, namely, data, feature, score, and decision [30].|
|||[4], it is believed that low-level fusion performs better than its higher level counterparts (score and decision) [14].|
|||To the best of our knowledge, this work is the first one to propose texture and shape fusion for face recognition using LBP patterns constructed on the mesh.|
|||The purpose of using the BU-3DFE is to assess the performance of our method, in particular our fusion schemes, with respect to facial expressions.|
|||The distribution of the best scores, highlighted in bold, clearly indicates the recognition enhancement brought by the fusion schemes.|
|||Also, we can observe that most of the best scores have been ob tained with the feature-level fusion variants.|
|||The same can also be observed for 1 fusion variants, which show a better overall scores.|
|||The 2 fusion scores better across all the subset instances, reflecting thus a neatly superior performance.|
|||The experiments conducted with BU-3DFE and Bosphorus database showcased the boosting of the recognition performance brought by our fusion framework, and its superiority with regard to the most closest approaches.|
|||We have showed that our framework can be  easily adapted to different fusion schemes, in particular the early stage fusion.|
|||In both applications, despite using a basic minimum distance classifier, we showcased the powerfulness of the mesh-LBP descriptors for shape analysis, and the performance enhancement that the meshLBP fusion framework can bring in recognition tasks.|
|||Spatially optimized data-level fusion of texture and shape for face recognition.|
|||An efficient 3D face recognition approach based on the fusion of novel local low-level features.|
|||Information fusion in biometrics.|
||17 instances in total. (in cvpr2015)|
|60|Zhenli_Zhang_ExFuse_Enhancing_Feature_ECCV_2018_paper|In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution.|
|||Intuitively, the fusion of high-level features with such pure low-level features helps little, because low-level features are too noisy to provide sufficient highresolution semantic guidance.|
|||In contrast, if low-level features include more semantic information, for example, encode relatively clearer semantic boundaries, then the fusion becomes easy  fine segmentation results could be obtained by aligning high-level feature maps to the boundary.|
|||In other words, feature fusion could be enhanced by introducing more semantic concepts into low-level features or by embedding more spatial information into high-level features.|
|||2 Related Work  Feature fusion in semantic segmentation.|
|||Feature fusion is frequently employed in semantic segmentation for different purposes and concepts.|
|||3 Approach  In this work we mainly focus on the feature fusion problem in U-Net segmentation frameworks [12, 2, 28, 26, 25, 22].|
|||In Sec 1 we argue that feature fusion could become less effective if there is a large semantic or resolution gap between low-level and high-level features.|
|||It is clear that even though the segmentation quality increases with the fusion of more feature levels, the performance tends to saturate quickly.|
|||Especially, the lowest two feature levels (1 and 2) only contribute marginal improvements (0.24% for ResNet 50 and 0.05% for ResNeXt 101), which implies the fusion of low-level and high-level features is rather ineffective in this framework.|
|||To address the drawback, we generalize the fusion as follows:  yl = U psample (yl+1) + F(xl, xl+1, .|
|||At the beginning of Sec 3 we demonstrate that feature fusion in our baseline architecture (GCN [26]) is ineffective.|
|||Despite the improved performance, a question raises: is feature fusion in the framework really improved?|
|||The comparison implies our insights and methodology enhance the feature fusion indeed.|
|||5 Conclusions  In this work, we first point out the ineffective feature fusion problem in current U-Net structure.|
|||Eventually, better feature fusion is demonstrated by the performance boost when fusing with original low-level features and the overall segmentation performance is improved by a large margin.|
||17 instances in total. (in eccv2018)|
|61|Kuan-Chuan_Peng_Zero-Shot_Deep_Domain_ECCV_2018_paper|We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating task-relevant target-domain representations with task-relevant source-domain data.|
|||To the best of our knowledge, ZDDA is the first domain adaptation and sensor fusion method which requires no taskrelevant target-domain data.|
|||Such impractical assumption is also assumed true in the existing works of sensor fusion such as [31, 48], where the goal is to obtain a dual-domain (source and target) TOI solution which is robust to noise in either domain.|
|||This unsolved issue motivates us to propose zero-shot deep domain adaptation (ZDDA), a DA and sensor fusion approach which learns from the task-irrelevant dual-domain training pairs without using the task-relevant target-domain training data, where we use the term task-irrelevant data to refer to the data which is not task-relevant.|
|||(2) Given no task-relevant target-domain training data, we show that ZDDA can perform sensor fusion and that ZDDA is more robust to noisy testing data in either source or target or both domains compared with a naive fusion approach in the scene classification task from the SUN RGB-D [36] dataset.|
|||Although certain progress about sensor fusion is achieved in the previous works [31, 48], we are unaware of any existing sensor fusion method which overcomes the issue of lacking T-R target-domain training data, which is the issue that ZDDA is designed to solve.|
|||ZDDA simulates the target-domain representation using the source-domain data, builds a joint network with the supervision from the source domain, and trains a sensor fusion network.|
|||2, where we simulate the RGB representation using the depth image, build a joint network with the supervision of the TOI in depth images, and train a sensor fusion network in step 1, step 2, and step 3 respectively.|
|||For the baseline of sensor fusion, we compare ZDDA3 with a naive fusion method by predicting the label with the highest probability from CRGB and CD in Sec.|
|||Traditionally, training a fusion model requires the T-R training data in both modalities.|
|||However, we show that without the T-R training data in the RGB domain, we can still train an RGB-D fusion model, and that the performance degrades smoothly when the noise increases.|
|||In addition to using black images as the noise model, we evaluate the same trained joint classifier in ZDDA3 using another noise model (adding a black rectangle with a random location and size to the clean image) at testing time, and the result also supports that ZDDA3 outperforms the naive fusion method.|
|||Performance comparison between the two sensor fusion methods with black images as the noisy images.|
|||(c) shows that ZDDA3 outperforms the naive fusion under most conditions  use black images as the noise model for ZDDA3 at training time, we expect that adding different noise models can improve the robustness of ZDDA3.|
|||6 Conclusion and Future Work  We propose zero-shot deep domain adaptation (ZDDA), a novel approach to perform domain adaptation (DA) and sensor fusion with no need of the taskrelevant target-domain training data which can be inaccessible in reality.|
||16 instances in total. (in eccv2018)|
|62|gao_peng_Question-Guided_Hybrid_Convolution_ECCV_2018_paper|The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods.|
|||State-of-the-art feature fusion methods, such as Multimodal Compact Bilinear pooling (MCB) [10], utilize bilinear pooling to learn multi-model features.|
|||To solve these problems, we propose a feature fusion scheme that generates multi-modal features by applying question-guided convolutions on the visual features (see Figure 1).|
|||Our experiments on VQA datasets validate the effectiveness of our approach and show advantages of the proposed feature fusion over the state-of-the-arts.|
|||1) We propose a novel multi-modal feature fusion method based on question-guided convolution kernels.|
|||Early methods utilize feature concatenation [9] for multi-modal feature fusion [15, 27, 34].|
|||Instead of fusing the textual and visual information in high level layers, such as feature concatenation in the last layer, we propose a novel multi-modal feature fusion method, named Question-guided Hybrid Convolution (QGHC).|
|||Conventional ImageQA systems focus on designing robust feature fusion functions to generate multi-modal image-question features for answer prediction.|
|||Most state-of-the-art feature fusion methods fuse 1-d visual and language feature vectors in a symmetric way to generate the multi-modal representations.|
|||3.5 QGHC network with bilinear pooling and attention  Our proposed QGHC network is also complementary with the existing bilinear pooling fusion methods and the attention mechanism.|
|||To combine with the MLB fusion scheme [11], the multi-modal features extracted from the global average pooling layer could be fused with the RNN question features again using a MLB.|
|||The second stage fusion of textual and visual features brings a further improvement on the answering accuracy in our experiments.|
|||The output feature maps of our QGHC module utilize the textual information to guide the learning of visual features and outperform state-of-the-art feature fusion methods.|
|||Our feature fusion is performed before the spatial pooling and can better capture the spatial information than previous methods.|
|||Stacked Attention (SA) [18] adopts multiple attention models to refine the fusion results and utilizes linear transformations to obtain the attention maps.|
||16 instances in total. (in eccv2018)|
|63|Lin_3D_Sub-query_Expansion_2013_ICCV_paper|Then a fusion function is designed and applied to the multi-query feature vector, all dataset images are ranked with the final fusion score.|
|||1st and 3rd rows are the top 10 retrieved samples using 28 synthesized car and bicycle query sketches respectively with max fusion scheme.|
|||We then create a fusion function f; for each dataset image, it outputs the score f (xj).|
|||The dataset images are then ranked by the fusion score.|
|||j  j  There have been several commonly used fusion functions, e.g., average or max fusion scheme, that averages the similarity scores or pick the best one as the final fusion score.|
|||However, these simple fusion methods obtain poor retrieval performance (Section 4.2) due to the ambiguity problem.|
|||Given the learned weight w, the final fusion score is defined as:  f (xj) =  1  1 + e(wT xj )  ,  (2)  where xj is the feature vector for each dataset image as defined above.|
|||Those more discriminative views would contribute more to the final fusion score and thus improve the retrieval performance.|
|||Average and max fusion scheme are also evaluated, which averages the similarity scores or pick the best one as the final fusion score.|
|||Mean average precision (MAP) of our approach, stateof-the-art method: SHoG [14], and different fusion strategies.|
|||In all cases, a disjunct set of query sketches from a single user are used as test samples, while the remaining sketches are training samples for learning our categoryspecific fusion function (cf.|
|||3493 3500  leads the max fusion scheme to have unacceptable even worser retrieval performance than the baseline approach.|
|||Average fusion scheme does not perform well either, since the views within a object category are not equally important and may include some noise responses from those less discriminative views.|
|||The experimental results also show that the use of synthesized views with learned fusion function can significantly improve the retrieval performance and shows best MAP = 0.36 compared to the state-of-the method (MAP = 0.21 and 0.14 for frontal view and side view case).|
|||It can be seen that the proposed method, 3D sub-query expansion (capturing more information than a single query) and fusion function (emphasizing on those more discriminative sub-queries), can get more accurate and diversified results.|
||16 instances in total. (in iccv2013)|
|64|Chen_Image_Fusion_with_2014_CVPR_paper|Therefore, we expect to obtain images in both high spatial resolution and high spectral resolution via image fusion (also called pan-sharpening).|
|||Image fusion is a typical inverse problem and generally difficult to solve.|
|||The first variational method P+XS [3] is based on the linear combination assumption in Brovey [10] and also assumes the upsampled MS image is the fusion result after blurring.|
|||However, due to the lack of an effective model to preserve spatial information, visible artifacts may appear on the fusion results.|
|||In this paper, we propose a new variational model for image fusion to bridge this gap.|
|||This strategy is common for comparing fusion algorithms (e.g.|
|||It can be further applied to image fusion from different sources or different the proposed dynamic gradient sparsity only forces the support sets to be the same, while the sign of the gradients as well as the magnitudes of the signal are not required to be the same.|
|||Visual Comparison  First, we compare the fusion result by our method with those of previous works [6][11][25][10][3][18].|
|||Figure 2 shows the fusion results as well as the original images captured by the Quickbird satellite.|
|||Wavelet fusion [25] suffers from both spectral distortion and blocky artifacts (e.g.|
|||Moreover, the fusion from the upsampled MS image often  results in inaccuracy.|
|||The fusion results are impressively good from these visual observations.|
|||To evaluate the fusion quality of different methods, we use four metrics that measure spectral quality and one metric that measures spatial quality.|
|||In addition, peak signal-to-noise ratio (PSNR), and root mean squared error (RMSE) and mean structural similarity (MSSIM) [23] are used to evaluate the fusion accuracy when compared with the ground-truth.|
|||A new intensity-hue-saturation fusion approach to image fusion with a tradeoff parameter.|
||16 instances in total. (in cvpr2014)|
|65|Ge_Robust_3D_Hand_CVPR_2016_paper|We first describe the methods of multiview projection and learning in Section 3.1 and then describe the method of multi-view fusion in Section 3.2.|
|||Multi(cid:173)view Fusion  The objective for multi-view fusion is to estimate the 3D hand joint locations from three views heat-maps.|
|||Self(cid:173)comparisons  For self-comparison, we implement two baselines: the single view regression approach and the multi-view regression approach using a coarse fusion method.|
|||The multi-view regression approach using a coarse fusion method can be considered as a degenerated variant of our fine fusion method.|
|||We compare the accuracy performance of these two approaches with the multi-view fine fusion method described in Section 3.|
|||In addition, the fine fusion method is better than the coarse fusion method when considering the mean error performance, which is about 13 mm on the dataset in [21].|
|||When considering the worst case accuracy, the fine fusion method performs worse than the coarse fusion method only when the error tolerance is large.|
|||Thus, the fine fusion method is overall better than the coarse fusion method and we apply this fusion method in the following experiments.|
|||However, the multi-view fine fusion method fuses the heat-maps of three views and estimates the 3D location with high accuracy.|
|||view coarse fusion method gives an estimation in between the results of the above two methods due to its underutilization of heat-maps information.|
|||9 shows qualitative results of these three methods on several challenging examples to further illustrate the superiority of the multi-view fine fusion method over the other two methods.|
|||Comparison with State(cid:173)of(cid:173)the(cid:173)art  We compare our multi-view fine fusion method with two state-of-the-art methods on the dataset in [21].|
|||7, our multi-view regression with fine fusion method significantly outperforms the method in [29] for the worst case accuracy.|
|||Note that the process of multi-view projection and multi-view fusion is performed on CPU without parallelism, and the process of CNNs forward propagation is performed on GPU with parallelism for three views.|
|||73599  Figure 9: Qualitative results for dataset in [21] of three approaches: single view regression (in the first line), our multi-view regression with coarse fusion (in the second line) and our multi-view regression with fine fusion (in the third line).|
||16 instances in total. (in cvpr2016)|
|66|Hengshuang_Zhao_ICNet_for_Real-Time_ECCV_2018_paper|We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve highquality segmentation.|
|||Then cascade feature fusion unit and cascade label guidance strategy are proposed to integrate medium and high resolution features, which refine the coarse semantic map gradually.|
||| The developed cascade feature fusion unit together with cascade label guidance can recover and refine segmentation prediction progressively with a low computation cost.|
|||2, along with the cascade feature fusion unit and cascade label guidance, for fast semantic segmentation.|
|||CFF stands for cascade feature fusion detailed in Sec.|
|||Instead it takes cascade image inputs (i.e., low-, mediumand high resolution images), adopts cascade feature fusion unit (Sec.|
|||Because weights and computation (in 17 layers) can be shared between lowand medium-branches, only 6ms is spent to construct the fusion map.|
|||3.3 Cascade Feature Fusion  To combine cascade features from differentresolution inputs, we propose a cascade feature fusion (CFF) unit as shown in Fig.|
|||Effectiveness of cascade feature fusion unit (CFF) and cascade label guidance (CLG).|
|||Cascade Structure We also do ablation study on cascade feature fusion unit and cascade label guidance.|
|||Compared to the deconvolution layer with 3  3 and 5  5 kernels, with similar inference efficiency, cascade feature fusion unit gets higher mIoU performance.|
|||Compared to deconvolution layer with a larger kernel with size 77, the mIoU performance is close, while cascade feature fusion unit yields faster processing speed.|
|||With proposed gradual feature fusion steps and cascade label guid 4 https://youtu.be/qWl9idsCuLQ  ICNet for Real-Time Semantic Segmentation  13  Fig.|
|||The major contributions include the new framework for saving operations in multiple resolutions and the powerful fusion unit.|
||15 instances in total. (in eccv2018)|
|67|Johannes_Schoenberger_Learning_to_Fuse_ECCV_2018_paper|proposals and the WTA step should be replaced by a more general fusion step.|
|||Based on this insight, we formulate the fusion step as the task of selecting the best amongst all the scanline optimization proposals at each pixel in the image.|
|||SGM-Forest uses this fusion method instead of SGMs sumbased aggregation and WTA steps and our results shows that it consistently outperforms SGM in many different settings.|
|||In contrast, we use regular scanline optimization but propose a learning-based fusion step using random forests.|
|||Stereo matching has been solved by combining multiple disparity maps using MRF fusion moves [24, 4, 44].|
|||Unlike MRF fusion moves [24], our fusion method is not general.|
|||To overcome this problem, we propose a novel fusion method to robustly compute the disparity dp from the multiple scanline costs Lr(p, d).|
|||4 Learning To Fuse Scanline Optimization Solutions  We start by analyzing some difficult examples for scanline optimization in order to motivate our fusion method and then describe the method in detail.|
|||This insight forms the basis of our fusion model which is described next.|
|||The main challenge for robust and accurate scanline fusion is to identify the scanlines which agree on the correct estimate.|
|||In our proposed approach, we cast the fusion of scanlines as a classification problem that chooses the optimal estimate from the given set of candidate scanlines.|
|||Both methods perform worse than baseline SGM, underlining the need for a more sophisticated fusion approach.|
|||While the biggest accuracy improvement stems from the initial fusion step (see Table 1), the final filtering further improves the results by eliminating spatially inconsistent outliers.|
|||In contrast to most learning-based methods, we demonstrate that our learned fusion approach is general and extremely robust across different domains and settings: SGM-Forest performs well outdoors when trained on indoor scenes, handles different image resolutions, disparity ranges and diverse matching costs, and consistently outperforms baseline SGM by a large margin.|
||15 instances in total. (in eccv2018)|
|68|Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper|Multimodal fusion is thus an important longstanding strategy for robustness.|
|||In this work, we tested our attentional multimodal fusion using MSR-VTT and precisely analyzed significance of improvements.|
|||Attention-Based Multimodal Fusion  We propose an attention model to handle fusion of multiple modalities, where each modality has its own sequence of feature vectors.|
|||Furthermore, combinations of multiple  yi+1   yi-1   yi   gi   si-1   si   Multimodal  Naive fusion   di   WC1   c1,i   1,i,1   1,i,L   1,i,2   WC2   2,i,1   c2,i   2,i,2   x11   x12   x1L   x21   x22   x11   x12   x1L   x21   x22   2,i,L   x2L   x2L   Figure 2.|
|||yi-1   yi   gi   si-1   si   yi+1   1,i   2,i   Attentional   multimodal fusion   d1,i   WC1   c1,i   d2,i   WC2   c2,i   1,i,1   1,i,L   1,i,2   2,i,1   2,i,L   2,i,2   x11   x12   x1L   x21   x22   x11   x12   x1L   x21   x22   x2L   x2L   Figure 3.|
|||This is performed in the fusion layer, in which the following activation vector is computed instead of Eq.|
|||Our attention-based feature fusion is performed using  gi = tanh W (D)  s  si1 +  k,idk,i + b(D)  s !|
|||Unlike in the Na ve multimodal fusion method shown in Figure 2, in our method (shown in Figure 3) the multimodal attention weights can change according to the decoder state and the feature vectors.|
|||Datasets  We evaluated our proposed feature fusion using the YouTube2Text [9] and MSR-VTT [32] video datasets.|
|||a girl is riding a horse  Attentional fusion is best.|
|||On each dataset, we compare the performance of our multimodal attention model (Attentional Fusion), which integrates temporal and multimodal attention mechanisms, to a na ve additive multimodal fusion (Na ve Fusion).|
|||On the other hand, the audio feature contributed to the performance for both Na ve and Attentional fusion models.|
|||The results show that whereas the Na ve fusion baseline did not make good use of the audio features in these videos, our proposed Attentional Fusion method does, yielding a significant score improvement over the baseline for all metrics.|
|||Attention-based multimodal fusion for video description.|
||15 instances in total. (in iccv2017)|
|69|cvpr18-Multi-Level Fusion Based 3D Object Detection From Monocular Images|In addition, another module is introduced to estimate the disparity information and adopt multi-level fusion method for accurate 3D localization, constituting our 3D object detection procedure.|
|||The late fusion ensures the accurate 3D localization in the network, which is the most important part of the whole 3D object detection framework.|
|||This  2349  can be regarded as an early fusion or a pre-processing step for enhancing the input.|
|||In the estimation of 3D location, another fusion for different feature maps is proposed.|
|||In total, there are three levels of fusion in the network.|
|||The earliest fusion is the concatenation between front view feature maps and the corresponding RGB image.|
|||Generally, the last fusion is necessary for the framework, while the other two can improve the whole performance to a certain extent.|
|||We also measure the effect of fusion methods in our framework.|
|||All experiments are done with estimation fusion for 3D localization, which is also the core part for the 3D detection pipeline.|
|||Typically, more fusion requires more parameters.|
|||In particular, changing 3-channel RGB input to 6channel RGB+FV input only introduces 0.009% additional weights, and adding fusion of XYZ map only increases 0.79% weights.|
|||FV indicates the fusion between the front view feature maps and the RGB image.|
|||FV indicates the fusion between the front view feature maps and the RGB image.|
|||FF means the fusion between Fmax and Fmean.|
||15 instances in total. (in cvpr2018)|
|70|cvpr18-Gated Fusion Network for Single Image Dehazing|We exploit a gated fusion network for single image deblurring.|
|||The proposed neural network is built on a fusion strategy which aims to seamlessly blend several input images by preserving only the specific features of the composite output image.|
|||The main idea of image fusion is to combine several images into a single one, retaining only the most significant features.|
|||Gated Fusion Network  This section presents the details of our gated fusion network that employs an original hazy image and three derived images as inputs.|
|||We perform an early fusion by concatenating the original hazy image and three derived inputs in the input layer.|
|||Figure 4 shows the proposed multi-scale fusion network, in which the coarsest level network is shown in Figure 2.|
|||Effectiveness of the gated fusion network.|
|||Effectiveness of Gating Strategy  Image fusion is a method to blend several images into a single one by retaining only the most useful features.|
|||Consequently, in our gated fusion network, the derived inputs are gated by three pixel-wise confidence maps that aim to preserve the regions with good visibility.|
|||Our fusion network has two advantages: the first one is that it can reduce patch-based artifacts (e.g.|
|||To show the effectiveness of fusion network, we also train an end-to-end network without fusion process.|
|||In addition, we also conduct a experiment based on equivalent fusion strategy, i.e., all the three derived inputs are weighted equally using 1/3.|
|||In these examples, the approach without gating generates very dark images in Figure 8(b), and the method without fusion strategy generates results with color distortion and dark regions as shown in  (a) Hazy input  (b) DCP [9]  (b) DehazeNet [4]  (d) GFN  Figure 9.|
|||Conclusions  In this paper, we addressed the single image dehazing problem via a multi-scale gated fusion network (GFN), a fusion based encoder-decoder architecture, by learning confidence maps for derived inputs.|
||15 instances in total. (in cvpr2018)|
|71|Ciprian_Corneanu_Deep_Structure_Inference_ECCV_2018_paper|(b) Each fusion unit is a stack of 2 FC layers.|
|||Finally, py(y, p, ) is defined as the log probability of P (y|p, ) which is modeled by a set of independent functions, so called fusion functions {j(sj; j)}N j=1, where sj  p corresponds to the set of j-th AU predictions from all patches and j is function parameters.|
|||On the fusion and structure inference outputs we apply a binary cross-entropy loss (denoted by L(f, y) and L(y, y)).|
|||i=1, y}  i=1  Training data: {{I}P Model parameters: patch prediction: {i}P inference {i}N Step 0: random initialization around 0: , ,   N (0, 2) Step 1: train patch prediction: i  min(L (i(Ii; i)), y), i  {1, ..., P } Step 2: freeze patch prediction; train fusion:   min L((; ), y) Step 3: train patch prediction and fusion jointly:  i=1, fusion {i}N  i=1, structure  ,   min,(L ((I; )), y) + L((; ), y))  Step 4: freeze patch prediction and fusion; train structure inference:    min L((; ), y)  Step 5. train all: , ,   min,,(w1L ((I; )), y) + w2L((; ), y) + w3L((; ), y)) Output: optimized parameter: opt, opt, opt  a regularization on the correction factors (denoted by  in Eq.|
|||F stands for the fusion and DSIN is the final model.|
|||2 and 3 show results of AU-wise fusion for BP4D and DISFA (PP+F).|
|||On both, patch learning through fusion is beneficial, but on DISFA benefits are higher.|
|||Overall on BP4D the fusion improves results on almost all AUs compared to face prediction.|
|||However, the fusion is not capable to replicate the result of the mouth prediction on AU14.|
|||On DISFA, in almost every case fusion gets close or higher to the best patch prediction.|
|||In both cases, fusion has greater problems in improving individual patches in cases where input predictions are already very noisy.|
|||When we add patch prediction fusion (PP+F) we get just 0.5% lower than ROI while the addition of the structure inference and threshold tuning improves ROI performance.|
|||In the first 3 column examples, AU06 and AU07 are not correctly classified by the fusion model (middle row).|
|||8: (a) Examples of AU predictions: ground-truth (top), fusion module (middle) and structure inference (bottom) prediction (: true positive, : false positive).|
||15 instances in total. (in eccv2018)|
|72|cvpr18-Rethinking the Faster R-CNN Architecture for Temporal Action Localization|TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important.|
|||However, there has been limited work in exploring such feature fusion for Faster R-CNN.|
|||We propose a late fusion scheme and empirically demonstrate its edge over the common early fusion scheme.|
|||We propose to extend the input extent of SoI  Proposal Logits   Averaging  Proposal Logits (RGB)  Proposal Logits (Flow)  Segment Proposal Network  1D Feature Map (RGB)  1D Feature Map (Flow)  Proposal Generation  Figure 6: The late fusion scheme for the two-stream Faster RCNN framework.|
|||We hypothesize such two-stream input and feature fusion may also play an important role in temporal action localization.|
|||Therefore we propose a late fusion scheme for the two-stream Faster R-CNN framework.|
|||Conceptually, this is equivalent to performing the conventional late fusion in both the proposal generation and action classification stage (Fig.|
|||Note that a more straightforward way to fuse two features is through an early fusion scheme: we concatenate the two 1D feature maps in the feature dimension, and apply the same pipeline as before (Sec.|
|||We show by experiments that the aforementioned late fusion scheme outperforms the early fusion scheme.|
|||used two-stream features, but either did not perform fusion [15] or only tried the early fusion scheme [8, 14].|
|||[54]  66.0 59.4 51.9 41.0 29.8  Ours  59.8 57.1 53.2 48.5 42.8 33.8 20.8  Table 4: Results for late feature fusion in mAP (%).|
|||4 reports the action localization results of the two single-stream networks and the early and late fusion schemes.|
|||Finally, the late fusion scheme outperforms the early fusion scheme except at tIoU threshold 0.1, validating  our proposed design.|
||14 instances in total. (in cvpr2018)|
|73|Kolev_Turning_Mobile_Phones_2014_CVPR_paper|Based on that, the proposed fusion technique justifies the integrity of the depth estimates and resolves visibility conflicts.|
|||Nevertheless, we drew some inspiration from the utilized depth map fusion scheme, originally published in [4].|
|||However, the developed fusion scheme is designed for measurements stemming from active sensors, which are considerably more accurate than stereo-based ones.|
|||Confidence-Based Depth Map Fusion  A central issue in the design of a depth map fusion approach is the representation of the modeled scene.|
|||The proposed depth map fusion approach relies on the following scheme: When a new depth map becomes available, a weight is assigned to each pixel measurement reflecting its expected accuracy.|
|||The color is set to the color of the pixel with the largest weight w(x) used in the fusion process for the surfel.|
|||One could wonder why the normals are integrated in the proposed depth map fusion scheme.|
|||Moreover, note that the proposed depth map fusion procedure is incremental and lends itself to online applications.|
|||To evaluate the viability of the confidence-based weighting approach, we combined the developed fusion scheme with the weight computation proposed in [4].|
|||For the other two approaches we used the normal estimates obtained online from the fusion process.|
|||This proves the importance of a depth map fusion scheme.|
|||From left to right: Reconstructions with the depth map merging technique in [20], the developed fusion scheme with the weighting suggested in [4] and the complete approach proposed in this paper.|
|||Real-time visibility-based fusion of depth maps.|
||14 instances in total. (in cvpr2014)|
|74|Sun_Human_Action_Recognition_ICCV_2015_paper| In addition, we propose a novel score fusion scheme based on the sparsity concentration index (SCI).|
|||Experiments show that this score fusion scheme consistently improves over existing ones.|
|||This presumption suggests that we may use the sparsity degrees of each of {pk,i}C M i=1 to derive a weighted score fusion scheme.|
|||Given C cropped videos of the ith sampled video clip pair and their score outputs {pk,i}C k=1, our proposed SCI based score fusion scheme computes the final score of class probability as  pi =  PC  k=1 SCI(pk,i)pk,i PC  .|
|||The SCI based score fusion scheme.|
|||Our score fusion scheme also provides the compensation of the misalignment problem since maximized values of video clips are taken.|
|||We illustrate in Figure 3 the idea of our proposed SCI based score fusion scheme.|
|||Note that these results are obtained without using our score fusion scheme.|
|||Table 2 tells that our proposed data augmentation scheme by sampling video clips, and also the SCI based score fusion scheme effectively improve the recognition performance.|
|||Compared with the state-of-the-art CNN based method [10], our method outperforms it by about 1% on both datasets, when averaging fusion is adopted.|
|||When a supervised learning  4603  based SVM score fusion scheme is used in [10], our method still achieves better or comparable performance on the two datasets.|
|||Methods  UCF-101  HMDB-51  Improved dense trajectories (IDT) [30] IDT higher-dimensional encodings [23] Spatio-temporal HMAX network [6] [14]  Slow fusion spatio-temporal ConvNet [11]  Two-stream model (averaging fusion) [10] Two-stream model (SVM fusion) [10] *  FSTCN (averaging fusion)  FSTCN (SCI fusion)  85.9% 87.9%  -%  65.4% 86.9% 88.0% 87.9% 88.1%  57.2% 61.1% 22.8%  -%  58.0% 59.4% 58.6% 59.1%  * Additional videos are fed into the network to train the optical flow stream since  multi-task learning strategy is applied.|
|||Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice.|
||14 instances in total. (in iccv2015)|
|75|Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper|We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before.|
|||More importantly, when aggregating patch-level labels to an image-level label, simple decision fusion methods such as voting and max-pooling are not robust and do not match the decision process followed by pathologists.|
|||Bottom: An image-level decision fusion model is trained on histograms of patchlevel predictions, to predict the image-level label.|
|||We propose using a patch-level CNN and training a decision fusion model as a two-level model, shown in Fig.|
|||However, it has been shown that in many applications, learning decision fusion models can significantly improve performance compared to voting [42, 45, 24, 47, 26, 46].|
|||Furthermore, such a learned decision fusion model is based on the Count-based Multiple Instance (CMI) assumption which is the most general MIL assumption [49].|
|||Image-level decision fusion model  We combine the patch-level classifiers of Sec.|
|||Third, because the patch-level model is never perfect and probably biased, an image-level decision fusion model may learn to correct the bias of patch-level decisions.|
|||Depending on method, training patches are further divided into i) CNN and ii) decision fusion model training sets.|
|||CNN-Fea-SVM: We apply feature fusion instead of decision level fusion.|
|||Conclusions  We presented a patch-based Convolutional Neural Network (CNN) model with a supervised decision fusion model that is successful in Whole Slide Tissue Image (WSI) classification.|
|||Data vs. deci sion fusion in the category theory framework.|
|||a decision level fusion method for object recognition using multi-angular imagery.|
||14 instances in total. (in cvpr2016)|
|76|Yu_BodyFusion_Real-Time_Capture_ICCV_2017_paper|To reduce the ambiguities of the non-rigid deformation parameterization on the surface graph nodes, we take advantage of the internal articulated motion prior for human performance and contribute a skeleton-embedded surface fusion (SSF) method.|
|||Experimental results show that our method exhibits substantially improved nonrigid motion fusion performance and tracking robustness compared with previous state-of-the-art fusion methods.|
|||Introduction  Recently, volumetric depth fusion methods for dynamic scene reconstruction, such as DynamicFusion [27], VolumeDeform [18] and Fusion4D [11], have attracted considerable attentions from both academia and industry in the fields of computer vision and computer graphics.|
|||The dynamic fusion module in such a reconstruction method enables quality improvements over temporal reconstruction models in terms of both the accuracy and completeness of the surface geometry.|
|||Among all of these works, fusion methods using a single depth camera [27, 18] are low in cost and easy to set up and therefore show more promise for popularization.|
|||Aiming at a more robust real-time fusion method for dynamic scenes, we make two observations.|
|||Based on these observations, we propose BodyFusion, i.e., a skeleton-embedded surface fusion (SSF) approach, to improve the reconstruction quality of a dynamic human motion.|
|||On one hand, integrating the articulated body prior into the dynamic fusion framework assists in the reconstruction of human motions; on the other hand, including a non-rigid surface registration method in a skeleton-based tracking technique improves the quality of fused geometry.|
|||With the intent of solving the above problems, we have carefully designed our BodyFusion system, to achieve the fully automatic, real-time fusion of natural human motion and surface geometry using a single depth video input.|
||| The BodyFusion system is presented based on the proposed skeleton-embedded surface fusion (SSF) approach, which outperforms other recent dynamic fusion techniques [27] and [18] in handling human body motions, and enables the more convenient, real-time generation of a full-body 3D self-portrait using a single depth camera.|
||| We contribute a dataset captured using a marker-based Mocap system for the quantitative evaluation of dynamic fusion algorithms.|
|||Moreover, we believe that our SSF approach will open the door for the study of leveraging the high-level semantic information in real-time non-rigid surface fusion and dynamic scene reconstruction.|
|||From these experiments, we can see that when nonrigid registration is not included, the system cannot achieve faithful fusion due to the lack of tracking of detailed nonrigid surface motion.|
||14 instances in total. (in iccv2017)|
|77|Hou_DualNet_Learn_Complementary_ICCV_2017_paper|To the best of our knowledge, this work is the first to focus on the cooperation of multiple DCNNs, with the same input and only simple fusion method considered, such as SUM, Max and Concat.|
|||Secondly, the bilinear pooling is taken as the fusion method in [23], but in DualNet only simple SUM is adopted.|
|||Particularly, the performance achieved by DualNet is state-of-the-art on CIFAR-100 with SUM as the fusion method.|
|||The highlight of DualNet is to coordinate two networks to learn complementary features from input images, i.e., one network is able to learn details about the objects of interest which are missing in the other, such that after fusion richer and more accurate image representation can be extracted for recognition.|
|||Particularly, in the design of DualNet, we follow the principles listed below:  P1 The features after fusion are expected to be the most discriminative compared to the features extracted by each subnetwork, which exactly indicates the complementary learning embedded in DualNet.|
|||P4 Only simple fusion methods, such as SUM, MAX and Concat, are considered to ensure the generalization ability and computation efficiency, and the focus is the cooperation and complementarity of two subnetworks.|
|||The fusion layer pool5 is empirically selected for the following reasons.|
|||So it is implied that the fusion is better performed on the local patch, not the full image scope.|
|||SUM is chosen as the fusion method for simplicity, and also for transferring the parameters from the last fully connected layer, i.e., classifier, of the original CaffeNet.|
|||Further discussion about the selection of fusion method is provided in Section 5 according to P2, P3 and P4.|
|||Referring to P2 and P4, we deal with the issue considering only simple fusion methods, such as SUM, MAX and Concat, to ensure the generalization ability.|
|||In the future, we plan to explore the utilization of more advanced fusion methods and apply the network compression techniques to further optimize DualNet.|
||13 instances in total. (in iccv2017)|
|78|Cai_Multi-View_Super_Vector_2014_CVPR_paper|Two widely applied fusion pipelines are descriptor concatenation and kernel average.|
|||In practice, however, different descriptors are neither fully independent nor fully correlated, and previous fusion methods may not be satisfying.|
|||Experiments on video based action recognition tasks show that MVSV achieves promising results, and outperforms FV and VLAD with descriptor concatenation or kernel average fusion strategy.|
|||Kernel-level fusion utilizes a linear combination of kernel matrices belonging to each local descriptor to capture the structure of video data [12, 33].|
|||[4] reported that kernel average is particularly effective compared to more sophisticated kernel-level fusion methods when only limited kernels are considered.|
|||The last fusion method is score-level fusion, which trains classifiers for each descriptor and fuses the confidence scores [39, 30, 38].|
|||Descriptor-level fusion and kernel average are widely applied in action recognition [12, 33].|
|||When the adopted descriptors have strong dependency, descriptor fusion is probably better, because the correlation among different descriptors are taken into account.|
|||Previous fusion methods such as descriptor concatenation and kernel average assume that different descriptors are either fully correlated or fully independent.|
|||In the first experiment on HMDB51 database, HOG is combined with MBH via several fusion methods, as is shown in Table 1.|
|||When it comes to three or more descriptors, the fusion procedure becomes tedious.|
|||Multimodal feature fusion for robust event detection in web videos.|
||13 instances in total. (in cvpr2014)|
|79|Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper|Finally, we introduce a new fusion approach by using the predicted view probabilities as the weights for fusing the classification results from multiple view-specific classifiers to output the final prediction score for action classification.|
|||3) A new view-prediction-guided fusion method for combining action classification scores from multiple branches is proposed.|
|||(3) In the view-prediction-guided fusion module, we design several view-specific action classifiers for each branch.|
|||The details for (a) inter-view message passing module discussed in Section 3.3, and (b) view-prediction-guided fusion module described in Section 3.4.|
|||Specifically, for the video xi, the fused  (a) Message passing module(b) View-prediction-guided fusion module............1,1C1,Cv1,CV,1Cu,Cuv,CuV.|
|||The cross-view multi-branch module with view-prediction-guided fusion module forms our Dividing and Aggregating Network (DA-Net).|
|||We first train the network based on the basic multi-branch module to learn the basic  Inception 5a output1x1 convolutions1x1 convolutions1x1 convolutions1x1 convolutions3x3 convolutions3x3 convolutions3x3 convolutionsInception 5b outputpoolingShared CNNCNN Branch10  D. Wang, W. Ouyang, W. Li and D. Xu  features of each branch and then fine-tune the learnt network by additionally adding the message passing module and view-prediction-guided fusion module.|
|||The scores indicate the similarity between the videos from the target view and those from the source views, based on which we can still obtain the weighted fusion scores that can be used for classifying videos from the target view.|
|||In particular, in the first variant, we remove the view-prediction-guided fusion module, and only keep the basic multi-branch module and message passing module, which is referred to as DA-Net (w/o fus.).|
|||Similarly in the second variant, we remove the message passing module, and only keep the basic multi-branch module and view-predictionguided fusion module, which is referred to as DA-Net (w/o msg.).|
|||), since the fusion part is ablated, we only train one classifier for each branch, and we equally fuse the prediction scores from all branches for obtaining the action recognition results.|
|||In the view-prediction-guided fusion module, all the view-specific classifiers integrate the total V  V types of cross-view information.|
||13 instances in total. (in eccv2018)|
|80|Kalogeiton_Action_Tubelet_Detector_ICCV_2017_paper|For combining the two streams at test time we compare two approaches: union fusion and late fusion.|
|||For the union fusion [30], we consider the set union of the outputs from both streams: the tubelets from the RGB stream with their associated scores and the tubelets from the flow stream with their scores.|
|||For the late fusion [6], we average the scores from both streams for each anchor cuboid, as the set of anchors is the same for both streams.|
|||Our experiments show that late fusion outperforms the union fusion (Section 4.3).|
|||After presenting the datasets used in our experiments (Section 4.1), we provide an analysis of our ACTdetector: we validate anchor cuboids (Section 4.2), evaluate input modalities (RGB and flow) and their fusion (Section 4.3), and examine the impact of the length K of the  sequence of frames (Section 4.4).|
|||Tubelet modality  In this section, we examine the impact of the RGB and flow modalities and their fusion on the performance of our ACT-detector.|
|||For all datasets, we examine the frame-mAP when using (i) only RGB data, (ii) only flow data, (iii) union of RGB + flow data [27], and (iv) late fusion of RGB + flow data for varying sequence length from 1 to 10 frames.|
|||Frame-mAP of our ACT-detector on the three datasets when varying K for RGB data (blue line), flow (red line), union and late fusion of RGB + flow data (black and green lines, resp.).|
|||We observe that late fusion of the scores (green line) performs consistently better than union fusion (black line), with a gain between 1% and 4% in terms of frame-mAP.|
|||Instead, the late fusion re-scores every detection by taking into account both RGB and flow scores.|
|||Given that late fusion delivers the best performance, we use it in the remainder of this paper.|
|||Convolutional two-stream network fusion for video action recognition.|
||13 instances in total. (in iccv2017)|
|81|Wang_Multi-feature_Spectral_Clustering_2014_CVPR_paper|To solve the minimax optimization, an iterative solution is proposed to update the universal embedding, individual embedding, and fusion weights, separately.|
|||However, it is diffi cult to determine the fusion coefficients for different feature modalities.|
|||Kernel fusion k-means [39] and affinity aggregation spectral clustering [12] are recent additions to this family.|
|||In such a case, similarity matrices are weighted and combined for graph structure fusion [41, 36, 12].|
|||Instead of manually selecting weights ij for Qij, our objective function can optimize the fusion weights too.|
|||NMI performance of each feature embedding and the fusion result on the body motion and Oxford flower datasets.|
|||To further illustrate the advantage of our approach, we study how different regularization methods influence the performance of each feature embedding, as well as the performance of the fusion result.|
|||In the initialization, i.e., the first iteration, the fusion result approaches/exceeds the performance of the best feature type.|
|||As can be seen, different values of  do not influence much for appearance and motion feature fusion on the body motion dataset.|
|||Unsupervised metric fusion by cross diffusion.|
|||Heterogeneous visual  features fusion via sparse multimodal machine.|
|||Optimized data fusion for kernel kmeans clustering.|
||13 instances in total. (in cvpr2014)|
|82|Sanroma_Learning-Based_Atlas_Selection_2014_CVPR_paper|Next, a label fusion procedure is often used to determine the label on each target image point based on the label information from the registered atlases  L.|
|||Many label fusion strategies have been developed.|
|||We apply the following label fusion methods: MV, LWV and NLWV.|
|||Figures 4, 5 and 6 show the average segmentation results in both hippocampi obtained by different selection methods using the label fusion methods MV, LWV and NLWV, respectively.|
|||Label fusion with majority voting  Figure 5.|
|||Label fusion with local weighted voting  051015202530354045500.730.740.750.760.770.780.790.80.81Number of atlases KAverage Dice ratioHippocampus  Majority Voting  NMIHOGHOG+SVMRank051015202530354045500.730.740.750.760.770.780.790.80.81Number of atlases KAverage Dice ratioHippocampus  Local weighted voting  NMIHOGHOG+SVMRankgyrus, parietal gyrus, occipital gyrus, temporal gyrus and parahippocampal gyrus.|
|||Label fusion with majority voting  Figure 8.|
|||Label fusion with local weighted voting  with respect to NMI for K = 7 are  1.1%,  1.5% and  1.2% for MV, LWV and NLWV label fusion, respectively.|
|||Label fusion with non-local weighted voting  Regarding the different atlas selection methods, best results by our method are  2% better than best results by NMI for MV label fusion and  1% for both LWV and NLWV label fusions.|
|||Best results for NMI selection in all the label fusion modalities are achieved when using K (cid:39) 25 best atlases.|
|||Label fusion with non-local weighted voting  NMI in 9, 11 and 10 out of the 16 structures for MV, LWV and NLWV label fusions, respectively.|
|||Our method presents improvements with respect to NMI-based selection of  2% for MV label fusion and  1% for LWV and NLWV label fusions.|
||13 instances in total. (in cvpr2014)|
|83|Goutam_Bhat_Unveiling_the_Power_ECCV_2018_paper|Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy.|
|||As our second contribution, we propose a novel fusion strategy to combine the deep and shallow predictions in order to exploit their complementary characteristics.|
|||We propose a novel adaptive fusion approach that aims at fully exploiting their complementary nature, based on a quality measure described in section 4.1.|
|||4.2 Target Prediction  We present a fusion approach based on the quality measure (1), that combines the deep and shallow model predictions to find the optimal state.|
|||4: An illustration of our fusion approach, based on solving the optimization problem (7).|
|||For the fusion method presented in section 4, the regularization parameter  in (6) is set to 0.15.|
|||These results are also compared with the baseline ECO (orange) and our adaptive fusion (blue).|
|||Figure 5 also shows the results of our proposed adaptive fusion approach (section 4), where the model weights  are dynamically computed in each frame.|
|||Figure 6 shows a qualitative example of our adaptive fusion approach.|
|||6: Qualitative example of our fusion approach.|
|||In (c), where the target undergoes scale changes, our fusion exploits the shallow model for better accuracy.|
|||These results show that our analysis in section 3 and the fusion approach proposed in section 4 generalizes across different network architectures.|
||13 instances in total. (in eccv2018)|
|84|cvpr18-Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation|The decoder features a novel architecture, consisting of blocks, that (i) capture context information, (ii) generate semantic features, and (iii) enable fusion between different output resolutions.|
|||The dense decoder connections allow for effective information propagation from one decoder block to another, as well as for multi-level feature fusion that significantly improves the accuracy.|
|||The dense decoder shortcut connections allow for effective information propagation from one block of a decoder to another, and for multi-level feature fusion that significantly improves the accuracy.|
|||Moreover, the blocks dec1, dec2, and dec3 have a fusion stage in between them.|
|||We call these 3 blocks the fusion blocks, as they fuse the output of the previous decoder with the output from the encoder adaptation.|
|||Finally, if a decoders block performs a fusion (i.e.|
|||The fusion is the second stage of the blocks dec1, dec2 and dec3.|
|||  decoder A-d in    decoder B-d in          33, D  UPSCALE    encoder C-d in  33, D  +  D-d out    Figure 3: Decoders fusion stage.|
|||If we generate semantic features for further processing, we then apply a 3  3 convolution, as explained in the fusion stage.|
|||Moreover, we do not use the ReLU right after the last convolution of the encoder adaptation stage and semantic feature generation stage, where features are being adopted for the fusion stage or final prediction, and before the pooling layer in the semantic feature generation stage.|
|||Moreover, we have proposed a novel decoders architecture, consisting of blocks, each capturing context information, generating semantic features, and enabling fusion between different output resolutions.|
|||The dense decoder shortcut connections allow for effective information propagation from one decoder block to another, and for multi-level feature fusion that significantly improves the accuracy.|
||13 instances in total. (in cvpr2018)|
|85|Spatiotemporal Multiplier Networks for Video Action Recognition|Extensions to that work that investigated convolutional fusion [9] and residual connections [8] are of particular relevance for the current work, as they serve as points of departure.|
|||Each stream performs video recognition on its own and prediction layer outputs are combined by late fusion for final classification.|
|||Connecting the two streams  The original two-stream architecture only allowed the two processing paths to interact via late fusion of their respective softmax predictions [28].|
|||3.2.3 Discussion  Inclusion of the multiplicative interaction increases the order of the network fusion from first to second order [10].|
|||During backpropagation, instead of the fusion gradient flowing through the appearance, (3), and motion, (4), streams being distributed uniformly due to additive forward interaction (2), it now is multiplicatively scaled by the opposing streams current inputs, f (xm l in equations (6) and (7), respectively.|
|||In previous work, different fusion functions have been discussed [9] where it has been shown that additive fusion performed better than maxout or concatenation of feature channels from the two paths.|
|||Additive fusion of ResNets has been used in [8], but was not compared to alternatives.|
|||This overly aggressive change is induced in two ways: via the forwarded signal as it passes through the deep layers; via the backpropagated signal in all preceding layers that emit the fusion signal.|
|||Late fusion is implemented by averaging the prediction layer outputs.|
|||As a final experiment, we are interested if there is still something to gain from a fusion with hand-crafted IDT features [37].|
|||These results indicate that the degree of complementary between hand-crafted representations and our end-to-end learned ConvNet approach is vanishing for UCF101, given the fact that other representations see much larger gains by fusion with IDT.|
|||Convolutional two-stream network fusion for video action recognition.|
||13 instances in total. (in cvpr2017)|
|86|cvpr18-Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering|This progress has been mainly brought about by two lines of research, the development of better attention mechanisms and the improvement in fusion of features extracted from an input image and question.|
|||Meanwhile, researchers have proposed several methods for  feature fusion [5, 14, 32], where the aim is to obtain better fused representation of image and question pairs.|
|||This is particularly the case with the studies of feature fusion methods, where attention is considered to be optional, even though the best performance is achieved with it.|
|||Motivated by this, we propose a novel co-attention mechanism for improved fusion of visual and language representations.|
|||Related Work  In this section, we briefly review previous studies of VQA with a special focus on the developments of attention mechanisms and fusion methods.|
|||[32] combined the mechanism with a novel multi-modal feature fusion of image and question.|
|||In early studies, researchers employed simple fusion methods such as the concatenation, summation, and element-wise product of the visual and language features, which are fed to fully connected layers to predict answers.|
|||[5] that a more complicated fusion method does improve prediction accuracy; they introduced the bilinear (pooling) method that uses an outer product of two vectors of visual and language features for their fusion.|
|||We adopt a similar approach that uses multiple attention maps here, but we use average instead of concatenation for fusion of the multiple attended features, because we found it works better in our case.|
|||Vl = softmax  A(i)  A(i)  (8)  (9)  and also normalize Al in column-wise to derive attention maps on image regions conditioned by each question word as  As we employ multiplicative (or dot-product) attention as explained below, average fusion of multiple attended features is equivalent to averaging our attention maps as  AVl = softmax(A  l ).|
|||The core of the network is the dense co-attention layer, which is designed to enable improved fusion of visual and language representations by considering dense symmetric interactions between the input image and question.|
||12 instances in total. (in cvpr2018)|
|87|Shi_Yan_DDRNet_Depth_Map_ECCV_2018_paper|From this perspective, our denoising net is learning a deep fusion step, which is able to achieve better depth accuracy than heuristic smoothing.|
|||These fusion methods are able to effectively reduce the noises in the scanning by integrating the TSDF.|
|||Recent progresses have extended the fusion to dynamic scenes [26, 11].|
|||The scan from these depth fusion methods can achieve very clean 3D reconstruction, which improves the accuracy of the original depth map.|
|||Based on this observation, we employ depth fusion to generate a training data for our denoising net.|
|||By feeding lots of the fused depth as our training data to the the network, our denoising net effectively learns the fusion process.|
|||To achieve this, we use the non-rigid dynamic fusion pipeline proposed by [11], which is able to reconstruct complete and good quality geometries of dynamic scenes from single RGB-D camera.|
|||Then we run the non-rigid fusion pipeline [11] to produce a complete and improved mesh, and deform it using the estimated motion to each corresponding frame.|
|||The temporal window in fusion systems would smooth out noise, but it will also wipe out high-frequency details.|
|||We proposed a near-groundtruth training data generation pipeline, based on the depth fusion techniques.|
|||Enabled by the separation of low/high frequency parts in network design, as well as the collected fusion data, our cascaded CNNs achieves state-of-the-art result in realtime.|
||12 instances in total. (in eccv2018)|
|88|cvpr18-Visual Question Generation as Dual Task of Visual Question Answering|So we formulate the dual training of VQA and VQG as learning an invertible cross-modality fusion model that can infer Q or A when given the counterpart conditioned on the given image.|
|||Apart from proposing new frameworks, some focus on designing effective multimodal fusion schemes [5, 13].|
|||Therefore, VQG can be modeled as a multi-modal fusion problem like VQA.|
|||Different from one-to-one translation problems, where there exists large quantities of available unpaired data, visual question answering is a multimodal fusion problem, which is hard to model as an unsupervised learning problem.|
|||With attention and MUTAN fusion module, predicted features are obtained.|
|||Then another MUTAN fusion module is used for obtaining the answer features a  Rda by fusing vq and q.|
|||We will briefly review the core part, MUTAN fusion module, which takes an image feature vq and a question feature q as input, and predicts the answer feature a.|
|||3.1.1 Review on MUTAN fusion module  Since language and visual representations are in different modalities, merging visual and linguistic features is crucial in VQA.|
|||Bilinear models are recently used in the multimodal fusion problem, which encodes bilinear interactions between q and vq as follows:  a = (T 1 q) 2 vq  (1)  where the tensor T  Rdq dv da denotes the fullyparametrized operator for answer feature inference, and i denotes the mode-i product between a tensor X and a matrix  6118  M:  Di  (X i M) [d1, ...di1, j, di+1...dN ] =  X [d1...dN ]M[di, j]  Xdi=1  (2) To reduce the complexity of the full tensor T , Tucker decomposition [3] is introduced as an effective way to factorize T as a tensor product between factor matrices Wq, Wv and Wa, and a core tensor Tc:  T = ((Tc 1 Wq) 2 Wv) 3 Wa  (3)  with Wq  Rtq dq , Wv  Rtv dv and Wa  Rtada , and Tc  Rtq tv ta .|
|||Since there is no parameter for fusion part, Dual Training only requires decoder & encoder weight sharing and duality regularizers.|
|||Mutan: Multimodal tucker fusion for visual question answering.|
||12 instances in total. (in cvpr2018)|
|89|Bhattacharya_Recognition_of_Complex_2014_CVPR_paper|The representations generated by SSID-S and H-S are individually compact, discriminative and complementary, enabling us to perform late fusion in order to achieve better accuracies in complex event recognition.|
|||Baseline Methods  Since, the datasets are relatively new and research on concept detection is still in its infancy, it is very difficult to compare our work with published methods [2, 20] that involve fusion of multiple low-level feature representations.|
|||For our approach, we consider SSID-S alone, H-S alone and a weak fusion of the two.|
|||We also observe that the late fusion of SSID-S and H-S (denoted combined temporal representation) helps more on the MED12EC dataset.|
|||However, a natural question is whether we can further improve performance by performing late fusion using SSID-S and H-S with video-level complex event predictions generated directly from the mid-level concept detectors.|
|||Largest SV2 Largest SV4 Largest SV8 Largest SVr=4r=8r=1600.10.20.30.40.50.60.70.80.91  Largest SV2 Largest SV4 Largest SV8 Largest SVAmount of OverlapFigure 7. mAP for complex event recognition of all 25 categories in MED 11-12 EC using three baselines (DCT, Discrete HMM, Continuous HMM) and three variants of the proposed method (SSID-S alone, H-S alone, weak fusion of both signals).|
|||All variants of the proposed method consistently dominate the baselines, with weak fusion providing a slight benefit among variants of the proposed method.|
|||Results of fusion with low-level event classifiers.|
|||The remaining two entries, F1 and F2 show the fusion of CTR with video-level predictions from BoC and BoVW, respectively.|
|||Our results are shown in the last three columns, with CTR denoting late fusion of SSID-S and H-S.|
|||Multimodal feature fusion for robust event detection in web videos.|
||12 instances in total. (in cvpr2014)|
|90|Surveillance Video Parsing With Single Frame Supervision|The three components of SVP, namely frame parsing, optical flow estimation and temporal fusion are integrated in an end-to-end manner.|
|||Based on the mined correspondences and their confidences, the temporal fusion sub-network fuses the parsing results of the each frame, and then outputs the final parsing result.|
|||Moreover, the feature learning, pixelwise classification, correspondence mining and the temporal fusion are updated in a unified optimization process and collaboratively contribute to the parsing results.|
|||The temporal fusion sub-network (Section 3.4) applies the obtained optical flow Ft,tl and Ft,ts to  Ptl and  Pts, producing Ptl,t and Pts,t.|
|||They are fused with  Pt via a temporal fusion layer with several 1  1 filters to produce the final Pt.|
|||(ii) we train the frame parsing sub-network and the temporal fusion sub-network together using the optical flow estimated in step (i).|
|||The temporal fusion sub-network is initialized via standard Gaussian distribution (with zero mean and unit variance).|
|||The major reason of training the optical flow subnetwork at the beginning is that, the temporal fusion subnetwork depends on the optical flow results.|
|||Component Analysis  Temporal fusion weights: We visualize the learned weights for the temporal fusion layers for R-arm and L-shoe in Figure 3 in the Indoor dataset.|
|||The vertical axis illustrates the fusion weights.|
|||Thanks to the fusion from Ptl,t, the womens left shoe is labelled correctly in the final prediction Pt.|
||12 instances in total. (in cvpr2017)|
|91|Li_Shape_Driven_Kernel_2015_CVPR_paper|The contributions of the paper include: 1) propose shape driven kernel adaptation in CNN framework, which helps learning robust face representations that are invariant to non-rigid appearance variations; 2) propose a tree-structured deep convolutional fusion hierarchy, which further enhances the power of kernel adaptation and 3) achieve the state-of-the-art performance in various facial trait recognition tasks, including identity, age and gender recognition.|
|||Although our method also benefits from similar fusion hierarchy, we use such a hierarchy mainly because the kernel adaptation function is not be shared over the whole face image with non-rigid variation.|
|||The convolved features learned by multiple local subnetworks are then combined as the middle-level representations to learn high-level features with the fusion subnetworks, i.e.|
|||multiple part fusion subnetworks {C 2 i=1 and a global fusion subnetwork C 3.|
|||Subnetworks in both fusion stages, i.e.|
|||Convolutional fusion is used in both the part-fusion subnetworks and the global-fusion subnetwork.|
|||Now the forward and backward propagations through the convolutional fusion networks, i.e.|
|||As shown, the part fusion subnetwork can effectively fuse multiple local subnetworks.|
|||As we adopt stage-wise supervised initialization, we first show the performance of 36 location adaptive subnetworks and 6 part fusion subnetworks in the first and second stages of the tree-CNN.|
|||5, the convolutional fusion of local CNN can consistently improve the classification accuracy.|
|||Our 3 layer tree-aCNN also outperforms Krizhevskys 8 layer AlexNet, more specifically tree-a-CNN/Alexnet has 0.34M/60M parameters and 23K/650K neurons, which further validate the effectiveness of the shape driven kernel adaptation and treestructured convolutional fusion architecture.|
||12 instances in total. (in cvpr2015)|
|92|Taniai_Graph_Cut_based_2014_CVPR_paper|Second, this optimality property and spatial propagation allow randomized search, rather than employ external methods to generate plausible initial proposals as done in previous fusion approaches [17, 25, 19], which may limit the possible solutions.|
|||Some methods [25, 19] use fusion moves [17], an operation that combines two disparity maps to make a better one (binary fusion) by solving a non-submodular binary-labeling problem using QPBO-GC [13, 17].|
|||Our method is also based on fusion moves but generates proposals using locally shared labels, which enable spatial propagations of local candidate planes and, more importantly, they make fusion moves submodular, i.e., each binary fusion is optimally solved via GC (subproblem optimal).|
|||Consider the essential function of fusion is to make a good solution f by fusing a number of proposal disparity maps {g(0), g(1), .|
|||The fusion is performed using the proposals generated from both pixel and region labels.|
|||This particular form of proposal construction guarantees that a binary fusion of an arbitrary solution f and any of the proposals g = g(j) is submodular, thus it is exactly solved via GC.|
|||Lets consider a simple case where gp takes the same value for all pixels p. A fusion move with such globally-constant g is called an expansion move [6], which is submodular if pairwise terms pq satisfy Eq.|
|||2c, our proposal g is made locally-constant by shared regions, thereby a fusion move with our proposals is, virtually, many mutually-disjoint local-alpha-expansions with different alpha for each shared region.|
|||Lp  best K1 candidate labels c  (Lp  Cp)\{f (t) p } that minimize Ep(c|f (t)) Lp  Lp  {f (t) p }  4: 5: 6: 7:  end for for all regions r  R do  8: 9: 10: 11: 12: 13: until convergence  end for  Rr  random KR candidate labels from {f (t)  p |p in r}  this submodularity guarantee, we only need to use a standard GC [16, 5] instead of employing expensive QPBOGC [13] used in usual fusion approaches [17, 19, 25].|
|||We iterate twice for each proposal in fusion stage, and iterate the outer-loop process ten times.|
|||Unlike previous approaches that use fusion [17, 19, 25], our method is subproblem optimal and only requires randomized initial proposals.|
||12 instances in total. (in cvpr2014)|
|93|Yang_Du_Interaction-aware_Spatio-temporal_Pyramid_ECCV_2018_paper|(3)  F is a fusion function for yj of all layers in the pyramid.|
|||Here we respectively investigate three fusion functions, element-wise maximum, element-wise sum and element-wise multiplication.|
|||4.1 Evaluations of the Proposed Attention Layer  We investigate our interaction-aware spatio-temporal pyramid attention on the following five parts: (1) layer position of feature maps used for aggregation,  CONV+Classification LossATTENTIONRELUCONV12KK group of feature mapsOne group of aggregated feature maps12KSptaio-Temporal PyramidFlow or RGB is used as inputSalientregionsK++Attention LossInteractive LossISTPAN for Action Classification  9  (2) different fusion functions F of feature maps of pyramid, (3) numbers of layers in pyramid, (4) loss functions with ablated regularization items, and (5) the generality of our layer applied in different deep networks, including popular architectures VGGNet-16 [9], BN-Inception [52] and Inception-ResNet-V2 [10].|
|||We explore different fusion functions F of feature maps of pyramid evaluated on RGB stream.|
|||Element-wise multiplication performs better than other candidate functions, and is therefore selected as a default fusion function.|
|||Evaluations of (a) position of the top layer of pyramid; (b) different scales with Inception-ResNet-V2 (I-R-V2); (c) fusion functions with 3 scales; (d) loss functions with I-R-V2 and our attention layer; and 3) our layer on VGGNet-16, BN-Inception and Inception-ResNet-V2.|
|||Block A (35  35  320) Block B (17  17  1088)  Block (Inception-ResNet-V2) RGB Flow 85.8% 83.5% 86.1% 83.7% 86.3% 84.0% 85.5% 83.4%  Block C (8  8  2080)  FC (1536)  Scale RGB Flow 1 scale 86.3% 84.0% 2 scales 86.8% 84.8% 3 scales 87.3% 85.5% 4 scales 86.5% 85.0%  (c) Performance of different fusion functions.|
|||Fusion Function (F )  Accuracy #RGB  Element-wise Maximum  Element-wise Sum  Element-wise Multiplication  85.7% 86.4% 87.3%  Stream linter RGB Flow  lattn no loss 87.8% 87.5% 87.3% 86.1% 85.7% 85.5% Late fusion 94.7% 94.4% 94.2%  (e) Performance of the proposed attention layer on popular networks.|
|||Stream/(linter + lattn) VGGNet-16 BN-Inception Inception-ResNet-V2  RGB  RGB (3 scales)  Flow  Flow (3 scales)  Late fusion  Late fusion (3 scales)  80.4% 83.8% 85.5% 87.1% 90.7% 92.8%  84.5% 86.7% 87.2% 87.9% 92.0% 94.6%  85.2% 88.2% 83.1% 86.5% 92.6% 95.1%  by improving the performance 0.9%/1.0%/0.9% on RGB/Flow/Late fusion (3 scales).|
|||The late fusion approach means that the prediction scores of the RGB and Flow streams are averaged as the final video classification, as other methods [26,28,39,40] do.|
|||For VGGNet-16, our attention network respectively promotes 3.4%/1.6%/2.1% on RGB/Flow/Late fusion on the UCF101 split 1.|
||12 instances in total. (in eccv2018)|
|94|Gan_A_Multimodal_Deep_ICCV_2017_paper|The main stream of current affective video content analyses adopt either feature-level fusion or decision-level fusion to integrate visual and audio features.|
|||In addition to feature-level fusion, some work adopts decision-level fusion to integrate visual features and audio features for emotion classification or regression from videos.|
|||[19] adopted kernellevel fusion to linearly combine kernels computed on the individual features for video emotion tagging.|
|||In addition to performing feature-level and decision-level fusion separately, hybrid methods have also been proposed to combine feature-level and decision-level fusion.|
|||[43]  proposed a hybrid multilevel fusion approach to take advantage of both feature-level fusion and decision-level fusion.|
|||Although feature-level fusion and decision-level fusion can combine the information from visual content and audio content for emotion classification and regression, they can not successfully capture the structures embedded in video content and the inherent interactions among visual content, audio content and emotion labels by simply concatenating all the visual and audio features to one feature vector as the input of a classifier, or directly combining the classification results from visual features and audio features.|
|||To demonstrate the advantage of the multimodal process in our method, we compare the proposed model with the early fusion and late fusion methods.|
|||In early fusion method, we merge the two modalities in to one vector, and train a network based on merged feature vectors.|
|||The late fusion method learns two separated inference networks using the proposed method.|
|||Comparison with different fusion methods.|
|||2 shows the comparison with the early fusion and  late fusion methods.|
||12 instances in total. (in iccv2017)|
|95|Fix_A_Primal-Dual_Algorithm_2014_CVPR_paper|The most widely used graph cut techniques, including -expansion [4] and its generalization to fusion moves [22], repeatedly solve a first-order binary MRF in order to minimize the original multilabel energy function.|
|||When a move-making algorithm, such as -expansion or fusion moves, is used to solve a higher-order MRF, the optimal move is computed by solving a higher-order binary MRF.|
|||At each iteration, much like the -expansion or fusion move algorithms, we have a current labeling x and a proposed labeling y.|
|||Update-Duals-Primals  To begin with, we need notation for fusion moves [22].|
|||Consider an individual clique C for now, and let us examine what our labeling x(cid:48) C could look like after a fusion step.|
|||The convergence of our method is not guaranteed for arbitrarily bad fusion moves (for instance, we could have a bad proposal generator which always suggests labels which have greater height than xi).|
|||Data was obtained by running the code2 for [30] and recording the proposed fusion moves and corresponding unary terms.|
|||Both methods cycle through the pre-generated proposals and perform fusion move.|
|||The most important step consists of pre-generating a set of 14 piecewiseplanar proposed disparity maps, and then using these as proposals to the fusion move algorithm to improve the current disparity until convergence.|
|||We compare against fusion move with the same proposals, and the reductions of [6] and [10].|
|||Higher-order gradient descent by fusion move  graph cut.|
||12 instances in total. (in cvpr2014)|
|96|Detailed, Accurate, Human Shape Estimation From Clothed 3D Scan Sequences|Since the model factors pose and shape, all cloth alignment templates live in a common unposed space; we call the union of these unposed alignments the fusion scan.|
|||Since the cloth should lie outside the body for all frames we minimize the single-frame objective using the fusion scan as input and obtain an accurate shape template (fusion shape) for the person.|
|||Finally, to obtain the pose and the time varying shape details, we optimize again the single objective function using the fusion shape as a regularizer.|
|||The fusion scan is the union of the frame wise unposed alignments.|
|||From the fusion scan c) we obtain the fusion shape d).|
|||The union of the unposed templates creates the fusion scan (Fig.|
|||Since all temporal information is fused into a single fusion scan, we can estimate the fusion shape using the same single-frame objective function.|
|||(7)  The obtained fusion shapes are already quite accurate because the fusion scan carves the volume where the naked shape should lie.|
|||Pose and Shape Tracking  Finally we use the fusion shape to perform tracking regularizing the estimated shapes to remain close to the fusion shape.|
|||We achieve that by coupling the estimations towards the fusion shape instead of the SMPL model shape space.|
|||1 we show the numerical results obtained by [45], our fusion mesh, and our detailed mesh.|
||12 instances in total. (in cvpr2017)|
|97|Wu_Multiple_Non-rigid_Surface_2013_ICCV_paper|In [29], a fusion approach based on the triangulation mesh is proposed which takes advantage of both appearance information and local features.|
|||In this paper, we propose a fusion approach using the TPS warp which maps the points in a holistic way.|
|||The fusion approach can deal with large distortions in which correct feature correspondences are difficult to obtain.|
|||The adopted vector parameterization facilitates solving the fusion optimization problem which will be discussed later.|
|||In this paper, we propose a fusion approach based on the TPS warp by exploiting both appearance and local features.|
|||(13) By using all the energy terms in (13), (10) and (8), we have the fusion energy function:  E(h) = Ea + f Ef + sEs = (cid:2)I(W(p; h))  T(cid:2)2+  f Ni  (cid:2)Ah  u(cid:2)2+s(cid:2)Zh(cid:2)2.|
|||Thus, the fusion energy is linearized to  1996 1996  after obtaining the propagated matches, the proposed outlier rejection algorithm is used again to refine the matches.|
|||Finally, we use the feature-based approach to estimate the TPS warp (100 control points are used) for the deformed instance, which is further refined by the fusion approach.|
|||When the initial warp is far from the ground truth (Figure 2(b)), the refined warp obtained by the fusion approach is still not accurate (Figure 2(c)).|
|||On the other hand, if the initial warp is close to the ground truth (Figure 2(e)), the fusion approach facilitates obtaining a more accurate warp (Figure 2(f)).|
|||The proposed fusion approach exploits both appearance information and local features to register each nonrigid instance to the corresponding template.|
||12 instances in total. (in iccv2013)|
|98|Dense Captioning With Joint Inference and Visual Context|Context fusion for accurate description  Visual context is important for understanding a local region in an image, where it has already shown to benefit tasks such as object detection and semantic segmentation [3] [7] [33].|
|||tation via a fusion operator for both variants.|
|||Such fusion designs can be easily integrated with any of the models in Fig.|
|||Integrated model  The aforementioned model structures of joint inference and context fusion can be easily plugged together to produce an integrated model.|
|||Integrated models  We evaluate the integrated models with different designs for both joint inference and context fusion in this section.|
|||For context fusion, we compare the different settings proposed in Section 3.3, where we evaluate early-fusion and late-fusion with different fusion operators: concatenation, summation, and multiplication.|
|||The three types of fusion methods all yield improvements in mAP for different models.|
|||9 shows example predictions for comparison of T-LSTM without context fusion and T-LSTM-mult.|
|||Also, early fusion only outperforms  Table 5: The chosen hyper-parameters and the performance on Visual Genome V1.0 and V1.2 respectively.|
|||Here, we see similar results as on V1.0, which further verifies the advantage of T-LSTM over S-LSTM (mAP 8.16 vs 6.44 for no-context), and that context fusion greatly improves performance for both models.|
||11 instances in total. (in cvpr2017)|
|99|Li_A_Two-Streamed_Network_ICCV_2017_paper|The two streams follow the same format: an image parsing block, flowed by a feature fusion block and finally a refinement block.|
|||The output from the second fully connected layer is then reshaped into a 5575D feature map to be passed onto the feature fusion block, where D = 1 for the depth stream and D = 2 for the gradient stream.|
|||The feature fusion block consists of one 99 convolution  and pooling, followed by eight successive 55 convolutions without pooling.|
|||The output of the feature fusion block is a coarse 5575D depth or depth gradient map.|
|||The refinement block, similar to the feature fusion block, consists of one 99 convolution and pooling and five 55 convolutions without pooling.|
|||It takes as input a downsampled RGB input and then fuses together a bilinearly upsampled output of the feature fusion block via a skip connection (concatenation) to the third convolutional layer.|
|||The depth and gradient fusion block brings together the depth and depth gradient estimates from the two separate  3374  streams into a single coherent depth estimate.|
|||First, the image parsing and feature fusion blocks are trained with a loss on the depth and depth gradients.|
|||Otherwise, for the end-to-end network fusion, the image parsing, feature fusion and refinement blocks are fixed while the fusion block is trained using the combined loss (Equation 6).|
|||When the depth maps are projected into 3D (see Figure 4), there is little difference between the two fusion methods.|
||11 instances in total. (in iccv2017)|
|100|Wang_Modality_and_Component_CVPR_2016_paper|To this end, we propose a modality and component aware feature fusion framework for RGB-D scene classification on the extracted multi-modal FV features.|
|||In this research, we combine both group lasso and exclusive group lasso in our feature fusion framework to solve the scene classification problem.|
|||To further leverage global features, we also adapt the proposed feature fusion framework to the multi-modal CNN features applied on full images.|
|||Compared with the proposal-based feature fusion framework, the only difference is that the full-image based framework does not have components, because it is a single measurement rather than modeled as a distribution.|
|||Experiments  To evaluate the effectiveness of our proposed modality and component aware feature fusion framework, we perform scene classification experiments on the SUNRGBD Dataset [23] and the NYU Depth Dataset V2 [22].|
|||The parameters 1, 2 and 3 in (3) for proposal-based feature fusion are set at 0.005, 0.01 and 0.001 respectively with standard 5-fold crossvalidation.|
|||For full-image-based feature fusion (1 = 0), we empirically set 2 and 3 to be 0.001 and 0.0001 respectively.|
|||The learning rates  in (8) are set at 104 and 108 for proposal-based and full-image-based feature fusion respectively.|
|||Table 2 shows the impact of our modality and component aware feature fusion frameworks with the added regularization terms.|
|||Conclusion  In this paper, we proposed a modality and component aware feature fusion framework that effectively makes use of high-dimensional FV features from RGB, HHA and surface normal modalities.|
||11 instances in total. (in cvpr2016)|
|101|Chen_Constructing_Adaptive_Complex_2013_ICCV_paper|A fusion weight is associated with each complex cell type to preserve the global distinctiveness.|
|||m is the fusion weight associated with each complex cell type, while wj is the adaptive weight associated with each complex cell.|
|||The timevarying curve of the fusing weights  for the four types of complex cells, where the fusion weights automatically adjust to different challenges.|
|||Fusion Weights  The fusion weights  balance between different complex cell types to preserve global distinctiveness.|
|||With fusion weight m, we can weight more on distinctive complex cell types, which are less prone to be confounded by the background and improve the global distinctiveness of object model.|
|||1116 1116  m  Sm((cid:5)xt1)  median  M AD  (cid:4)  (8)  t  }  1,i t  1,i t  1,i t  1,i t  1,i t ) N , x   q(x1  1,i t ) and    qn(xr,i  (cid:3)xt1, w, T  Algorithm 1 Complex Cell Tracker Input: |(cid:3)xt) 1: sample x  S(x 2: compute the score S(x } to get N particles { 1 3: resample { 1,i , x t 4: for r = 2 to R do 5:  S(xr,i 6: t ) } to get N particles { 1 N , xr,i 7: 8: end for 9: estimate the optimal state (cid:3)xt = maxr,i S(xr,i t ) 10: determine the fusion weight  using (8).|
|||The average VORs and CLEs of the trackers with and without fusion weights.|
|||The success plots and the precision plots for investigating the effect of fusion weights and for the comparison of different algorithms respectively.|
|||Performance of fusion weights To justify the effectiveness of adaptive weights , we construct a tracker -T ignoring the fusion weights , which combines the score of four types of complex cells equally.|
|||Equipped with a two-layer template, the complex cells were further weighted by adaptive weights and fusion weights to cope  2 For ASLA, we evaluate them using a fixed motion model as [29].|
||11 instances in total. (in iccv2013)|
|102|Deng_Visual_Reranking_through_2013_ICCV_paper|Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs.|
|||[9] proposed a graph-based query specific fusion approach, where multiple retrieval sets from different visual cues were merged and reranked by link analysis on a fused graph.|
|||CoRMGL can be interpreted as a multiple graphs fusion algorithm via graph anchor alignment, as described in Eq.|
|||Our formulation is general enough to unify several existing graph fusion techniques developed for reranking and beyond [2][9].|
|||Following the state-of-the-art setting in multi-feature fusion based reranking, we design the following feature channels.|
|||Figure 3 shows the performance on Oxford5k when we change N. when N becomes larger, the mAP of each feature channel and fusion continues to decrease.|
|||With N increasing from 20 to 200, the mAP of fusion drops from 0.81 to 0.56.|
|||The mAP of direct fusion is lower than the one of GIST-based reranking since the complementary properties of different feature channels are not exploited.|
|||In Figure 5 and Figure 6, graph alignment refers to the direct fusion after aligning multiple graphs guided by a set of anchors that are determined by clustering centers; while anchor learning also refers to direct fusion where multiple graphs are aligned through a set of anchors that are selected by attribute intersection.|
|||Query  specific fusion for image retrieval.|
||11 instances in total. (in iccv2013)|
|103|Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation|The fusion module uses continuous CRFs to integrate multiple side output maps of the front-end CNN.|
|||The second component of our model is a fusion block.|
|||The main idea behind the proposed fusion block is to use CRFs to effectively integrate the side output maps of our front-end CNN for robust depth prediction.|
|||Specifically, we introduce and compare two different multi-scale models, both based on CRFs, and corresponding to two different version of the fusion block.|
|||Multi(cid:173)scale models as sequential deep networks  In this section, we describe how the two proposed CRFsbased models can be implemented as sequential deep networks, enabling end-to-end training of our whole network model (front-end CNN and fusion module).|
|||Comparison of different multiscale fusion schemes.|
|||Experimental Results  Analysis of different multi-scale fusion methods.|
|||Specifically, we consider: (i) the HED method in [33], where the sum of multiple side output losses is jointly minimized with a fusion loss (we use the square loss, rather than the cross-entropy, as our problem involves continuous variables), (ii) Hypercolumn [10], where multiple score maps are concatenated and (iii) a CRF applied on the prediction of the front-end network (last layer) a posteriori (no end-to-end training).|
|||It is evident that with our CRFs-based models more accurate depth maps can be obtained, confirming our idea that integrating complementary information derived from CNN side output maps within a graphical model framework is more effective than traditional fusion schemes.|
|||As discussed above, the proposed multi-scale fusion models are general and different deep neural architectures can be employed in the front end network.|
||11 instances in total. (in cvpr2017)|
|104|Yi_Zhou_Semi-Dense_3D_Reconstruction_ECCV_2018_paper|Based on the derived uncertainty, a fusion strategy is developed and is incrementally applied as sparse reconstructions of new RVs are obtained.|
|||5: Depth map fusion strategy.|
|||Using the fusion from RV5 to RV3 as an example, the fusion rules are illustrated in the dashed square, in which a part of the image plane is visualized.|
|||Using xi  j}4 1 as an example, the fusion is performed based on the following rules:  a) to xi  1.|
|||An illustration of the fusion strategy is given in Fig.|
|||Additionally, the depth fusion process is illustrated to highlight how it improves the density of the reconstruction while reducing depth uncertainty.|
|||Semi-dense depth maps (after fusion with several neighboring RVs) are given in the third column, pseudo-colored from red (close) to blue (far).|
|||To show how the fusion strategy improves the density of the reconstruction as well as reduces the uncertainty, we additionally perform an experiment that visualizes the fusion process incrementally.|
|||Semi-dense depth maps (after fusion with several neighboring RVs) are given in the third column, colored according to depth, from red (close) to blue (far).|
|||8: Illustration of how the fusion strategy increasingly improves the density of the reconstruction while reducing depth uncertainty.|
||11 instances in total. (in eccv2018)|
|105|Ti_Simultaneous_Time-of-Flight_Sensing_2015_CVPR_paper|The first group is based on image super-resolution techniques that utilize temporal redundancy, while the second group is mostly based on fusion with other devices to take advantage of spatial redundancy.|
|||(2) an algorithm to estimate time of flight from dis Depth fusion schemes have been proposed to overcome the need of multiple frames captured temporally by incorporating extra devices to provide measurements captured simultaneously [6].|
|||It has been shown that the level of details can be greatly boosted via this fusion scheme.|
|||As will be illustrated in the next section, our hardware implementation of the depth fusion scheme utilizes distributed light sources for the ToF sensor.|
|||Our algorithm consists of three main components: ToF sensing with distributed light sources, surface normal estimation via Photometric Stereo and finally a fusion scheme to combine these two sources of information.|
|||Fusion results  We perform the fusion algorithm on a number of objects, and results from various stages of the pipeline are displayed here.|
|||Holes in the depth maps from the distributed lights setup are due to 1) long faces in the mesh are removed by the fusion algorithm, and 2) non-Lambertian parts of the surface (such as the eyes in the human head model) causes wrong depth and normal.|
|||From top down: intensity image, mesh from original ToF, mesh from ToF with distributed lighting, normal map, final mesh from fusion with PS.|
|||From top down: intensity image, mesh from original ToF, mesh from ToF with distributed lighting, normal map, final mesh from fusion with PS.|
|||From top down: intensity image, mesh from original ToF, mesh from ToF with distributed lighting, normal map, final mesh from fusion with PS.|
||11 instances in total. (in cvpr2015)|
|106|Pan_A_Divide-and-Conquer_Method_2013_CVPR_paper|Robust late fusion (RLF) is a recent proposed method that fuses multiple output score lists from different models via pursuing a shared low-rank latent matrix.|
|||The proposed method is evaluated on various fusion problems in object categorization and video event detection.|
|||In this paper, we focus on the problem of Robust Late Fusion (RLF) proposed in [21], in which the authors formulated late fusion as an optimization problem of Low-rank Latent Matrix Pursuit (LLMP).|
|||The strategies of feature combination can be divided into early fusion and late fusion.|
|||A representative method of early fusion is MKL [1], which simultaneously learns kernel matrices and their associated combination weights.|
|||Many methods of late fusion have been proposed.|
|||The most related work to ours is the recently proposed Robust Late Fusion in [21], where the authors formulated late fusion as a problem of pursuing a shared low-rank latent matrix, and then proposed to the corresponding optimization problem via an algorithm based on Augmented Lagrange Multiplier method [9].|
|||Problem Formulation  The robust  late fusion problem [21] is to fuse n confidence score lists {s(i)}n Each list s(i) = (s(i) 1 , s(i) m )T is an intermediate output of a learned model (e.g., a classifier) from a specific kind of features/cues.|
|||To compare MAP, we include the results of three additional early and late fusion baselines: (1) kernel average, an early method which averages multiple kernel matrices to a single kernel matrix for learning; (2) SimpleMKL [16], a representative of Multiple Kernel Learning; (3) average late fusion, which obtains the final model by directly averaging the intermediate output scores from different models.|
|||And all the three robust late fusion methods have superior performance on MAP over the other three early and late fusion baselines.|
||11 instances in total. (in cvpr2013)|
|107|Jiang_Pan-Sharpening_With_a_ICCV_2015_paper|A suite of model-based fusion methods have also been proposed to address the spectral distortion issue.|
|||These methods treat the fusion problem as an image restoration model, with several additional regularization schemes [25, 4, 11, 7, 3].|
|||Figure 4 shows the fusion results and Figure 5 the residuals on the blue and green bands.|
|||Therefore we quantitatively evaluate both the spatial and spectral quality of our fusion results using the following five standard quality metrics: relative dimensionless global error in synthesis (ERGAS), spectral angle mapper (SAM) [2], universal image quality index (QAVE) [26], relative average spectral error (RASE) [8] and correlation coefficient  544  Table 1.|
|||Introduction of sensor spectral response into image fusion methods: Application to wavelet-based methods.|
|||A fast intensity-hue-saturation fusion technique with spectral adjustment for IKONOS imagery.|
|||A sparse image fusion algorithm with application to pansharpening.|
|||Comparison of pansharpening algorithms: Outcome of the 2006 grs-s data fusion contest.|
|||Image fusion with local spectral consistency and dynamic gradient sparsity.|
|||A new intensity-hue-saturation fusion approach to image fusion with a tradeoff parameter.|
||11 instances in total. (in iccv2015)|
|108|Barman_SHaPE_A_Novel_ICCV_2017_paper|Moreover, our algorithm outperforms other state-of-the-art score fusion techniques used in the context of person re-identification.|
|||Using the results from many algorithms to make a consensus-based decision has also been explored through the use of rank aggregation [8], fusion of features [13, 17], sum of weighted scores [25], and the use of false alarm rate (FAR) for supervised score fusion [12].|
|||The estimation of posterior probabilities from raw scores for unsupervised fusion [2] and Query-Adaptive late fusion [37] exhibit some of the most promising results in recent years.|
|||The performance of l-UPPSF [2] for score fusion is also shown here.|
|||Further, we compare the performance of SHaPE with the results of two state-of-the-art score fusion algorithms  l-UPPSF [2] and Query-Adaptive Fusion (QAF) [37].|
|||Moreover, the performance of SHaPE also supersedes the performance of lUPPSF, a state-of-the-art score fusion method.|
|||A marked improvement in performance can be observed over the individual re-identification algorithms and their fusion using l-UPPSF.|
|||Improving person reidentification systems: A novel score fusion framework for rank-n recognition.|
|||Evaluation of multi feature fusion at score-level for appearance-based person re-identification.|
|||Query-adaptive late fusion for image search and person reidentification.|
||11 instances in total. (in iccv2017)|
|109|cvpr18-MiCT  Mixed 3D 2D Convolutional Tube for Human Action Recognition|We argue that the high training complexity of spatio-temporal fusion and the huge memory cost of 3D convolution hinder current 3D CNNs, which stack 3D convolutions layer by layer, by outputting deeper feature maps that are crucial for high-level tasks.|
|||The performance of 3D CNNs is further improved by employing more complex spatio-temporal fusion strategies [7].|
|||Spatiotemporal fusion is achieved by both the 2D convolution block to generate stationary features and 3D convolution to extract temporal residual information.|
|||In other words, feature maps of a 3D input V using G() are achieved by coupling the 3D convolution with the 2D convolution block serially in which the 3D convolution enables spatio-temporal information fusion while the 2D convolution block deepens feature learning for each 2D output of the 3D convolution.|
|||It introduces a 2D convolution between the input and output of the 3D convolution to further reduce spatio-temporal fusion complexity and facilitate the optimization of the whole network.|
|||Regarding the baseline C3D architecture [30, 31], the MiCT-Net contains fewer 3D convolutions for spatiotemporal fusion while it produceing deeper feature maps and limiting the complexity of the entire deep model.|
|||Method  UCF101 HMDB51  Method  UCF101 HMDB51  Slow fusion [15]  C3D [30] LTC [31]  Two-stream [25]  Two-stream fusion [11] Two-stream+LSTM [40]  Transformations [36]  TSN [35]  FST CN [28] ST-ResNet [9]  Key-volume mining CNN [41]  TLE(C3D CNN) [7]  TLE(BN-Inception) [7]  I3D [5]  P3D ResNet [22]  MiCT-Net  65.4% 44.0%1 59.9% 73.0% 82.6% 82.6% 81.9% 85.7% 71.3% 82.2% 84.5% 86.3% 86.9% 84.5% 88.6%  88.9%   43.92%   40.5% 47.1% 47.1% 44.1% 54.6%3 42.0% 43.4%   60.3% 63.2% 49.8%   63.8%  Table 3.|
|||Even some of these referred works adopt advanced spatio-temporal fusion methods to the feature maps  C3D + IDT [30] TDD + IDT [34]  LTC [31]  LTC + IDT [31]  ST-ResNet + IDT [9]  P3D ResNet + IDT [22] Two-stream+LSTM [40]  Two-stream(conv.|
|||Convolutional two-stream network fusion for video action recognition.|
|||Two-stream 3d convnet fusion for action recognition in videos with arbitrary size and length.|
||11 instances in total. (in cvpr2018)|
|110|cvpr18-Zero-Shot Sketch-Image Hashing|The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively.|
||| We propose an end-to-end three-network structure for deep generative hashing, handling the train-test cat egory exclusion and search efficiency with attention model, Kronecker fusion and graph convolution.|
|||find the Kronecker product fusion layer suitable for our model, which is discussed in Sec.|
|||Fusing sketch and image with Kronecker layer  Sketch-image feature fusion plays an important role in our task as is addressed in problem (b) of Sec.|
|||To this end, we utilize the recent advances in Kronecker-product-based feature learning [22] as the fusion network.|
|||Denoting the attention model outputs of a sketch-image pair {y, x} from the same category as h(sk)  R256 and h(im)  R256, a non-linear data fusion operation can be derived as  W  1h(sk)  3h(im).|
|||(1)  Here W is a third-order tensor of fusion parameters and  denotes tensor dot product.|
|||(2)  Kronecker layer [22] is supposed to be a better choice in feature fusion for ZSIH than many conventional methods such as layer concatenation or factorized model [71].|
|||Data fusion through cross-modality metric learning using similarity-sensitive hashing.|
|||Attributeenhanced face recognition with neural tensor fusion networks.|
||11 instances in total. (in cvpr2018)|
|111|cvpr18-Multi-Cue Correlation Filters for Robust Visual Tracking|Although feature-level fusion methods [26, 31, 38] have been widely used or extended to boost the performance, there still leaves room for improvement.|
|||In HCF [31] or other methods [56, 38] that follow such a fusion strategy, the initial weight of high-level features is usually high such that semantic features play the dominant role in general.|
|||Furthermore, it is quite difficult to handle various challenging variations using a single model, and relying on a certain feature-level fusion strategy limits the model diversity to some extent.|
|||Since it is quite difficult to design a satisfying feature-level fusion method that suits various challenging scenes, it is intuitive to design an adaptive switch mechanism to achieve better performance, which can flexibly switch to the reliable tracker depending on what kind of challenging factors it is expert at handling.|
|||In other words, the performance of a single tracker can sometimes be unstable but the decision-level fusion of the outputs from multiple trackers can enhance the robustness effectively.|
|||Different from the DCF based methods mentioned above, our algorithm considers not only feature-level fusion but also decision-level fusion to better explore the relationship of multiple features, and adaptively selects the expert that is suitable for a particular tracking task.|
|||In [19], a partition fusion framework is proposed to cluster reliable trackers for target state prediction.|
|||Specially, the fusion methods [19, 24] by analyzing the forward and backward trajectories require each tracker to run at least twice; (2) the trackers are just regarded as independent black boxes and their fusion result in each frame does not feedback to the trackers [1, 44, 19], which fails to make full use of the reliable fusion outputs; (3) if the fusion tracker number increases, the dynamic programming based fusion methods [25, 1] still bring obvious computational burden (e.g., O(T N 2) for T frames and N trackers).|
|||The MCCT-PSR (MCCTH-PSR) represents the tracker adopts the fusion method and uses PSR measurement for adaptive update (Sec.|
|||MCCT-H MLDF SSAT TCNN C-COT CSR-DCF ECO MCCT  Accuracy Failure Rate EAO  0.57 1.24 0.305  0.54 0.48 0.96 0.83 0.311 0.321 0.325  0.57 1.04  0.52 0.85 0.331  0.51 0.85 0.338  0.58 0.54 0.73 0.72 0.374 0.393  feature-level fusion but also decision-level fusion to fully explore the strength of multiple features.|
||11 instances in total. (in cvpr2018)|
|112|Chang_Chen_Deep_Boosting_for_ECCV_2018_paper|Furthermore, we propose a pathwidening fusion scheme cooperated with the dilated convolution to derive a lightweight yet efficient convolutional network as the boosting unit, named Dilated Dense Fusion Network (DDFN).|
|||Last but not least, we further propose a path-widening fusion scheme cooperated with the dilated convolution to make the boosting unit more efficient.|
|||Cooperating with the dilated convolution, we propose a path-widening fusion scheme to expand the capacity of each boosting unit.|
|||4.3 Path-widening Fusion  We further propose a path-widening fusion scheme to make the boosting unit more efficient.|
|||The proposed path-widening fusion exploits the potential of these two orders at the same time, and thus promotes the possibility to learn better representations.|
|||Path-widening fusion (DDFN).|
|||4, we further propose the path-widening fusion which aggregates the concatenated features of preceding layers using a 1  1 convolution in the dense block, as shown in Fig.|
|||This fusion can further promote the denoising performance, i.e., DDFN as shown in Fig.|
|||Based on the densely connected structure, we further propose the path-widening fusion cooperated with the dilated convolution to optimize the DDFN for efficiency.|
|||Also, the idea of path-widening fusion is demonstrated to be useful in the task of spectral reconstruction from RGB images [37].|
||11 instances in total. (in eccv2018)|
|113|cvpr18-Look at Boundary  A Boundary-Aware Face Alignment Algorithm|Boundary heatmap fusion scheme is introduced to incorporate boundary information into the feature learning of regressor.|
|||Mi(x, y) =(exp( Di(x,y)2  22  0,  ),  if Di(x, y) < 3 otherwise  (1)  In order to fully utilise the rich information contained in boundary heatmaps, we propose a multi-stage boundary heatmap fusion scheme.|
|||Boundary heatmap fusion is conducted at the input and every stage of the network.|
|||Boundary  Heatmaps  Concatenation  M  (N+13)*32*32  Sigmoid  S  13*64*64  Down  Sampling  13*32*32  T  N*32*32  Feature Map Fusion  F  N*32*32  Input Feature   Maps  Concatenation  N*32*32  H  Elementwise Dot Product  (N+N)*32*32 Refined   Feature Maps  Figure 4: An illustration of the feature map fusion scheme.|
|||have shown that the more fusion we conducted to the baseline network, the better performance we can get.|
|||Boundary information fusion is one of the key steps in our algorithm.|
|||To evaluate the relationship between the quantity of boundary information fusion and the final prediction accuracy, we vary the number of fusion levels from 1 to 4 and report the mean error results in Table 6.|
|||Method  Mean Error  BL 7.12  BL+L1 BL+L1&2 BL+L1&2&3 BL+L1&2&3&4  6.56  6.32  6.19  6.13  Table 6: Mean error (%) on 300W Challenging Subset for various fusion levels.|
|||Method  Mean Error  BL 7.12  BL+HG/B BL+CL BL+HG  6.95  6.24  6.13  Table 7: Mean error (%) on 300W Challenging Set for different settings of boundary fusion scheme.|
|||To verify the effectiveness of the fusion scheme shown in Fig.|
||11 instances in total. (in cvpr2018)|
|114|Sohn_Unsupervised_Domain_Adaptation_ICCV_2017_paper|We further improve performance through a discriminator-guided feature fusion that boosts high-quality frames while eliminating those degraded by video domainspecific factors.|
|||Consequently, Section 3.4 proposes a discriminator-guided weighted feature fusion to aggregate frames in each video, by assigning higher weights to image-like frames, that potentially have better quality among the others.|
||| We use the confidence score of the discriminator to develop an unsupervised feature fusion method that suppresses low quality frames.|
|||Thus, the discriminator serves a dual role of guiding both the feature-level domain adaptation and a fusion weighted by confidence in the fitness of a frame for a face recognition engine.|
|||Six networks (A-F) with different combinations of feature matching (FM), feature restoration (FR) with various data augmentation methods, adversarial training (Adv) and discriminator-guided feature fusion are presented in Table 1.|
|||To reflect the quality of each frame, the feature fusion module aggregates all frames of a video using a weighted average of feature vectors based on the normalized likelihood as in (11).|
|||We quantitatively and qualitatively evaluate the discriminator-guided fusion in Table 1 and Figure 3.|
|||In contrast, the fusion strategy for the two-way network in model E (second sub-row) has marginal effect.|
|||In Figure 3, we demonstrate qualitative results for the feature fusion us 3215  Table 2.|
|||Furthermore, we propose a discriminator-guided feature fusion method to effectively aggregate features from multiple frames and effectively rank them in accordance to their suitability for face recognition.|
||11 instances in total. (in iccv2017)|
|115|Ji_SurfaceNet_An_End-To-End_ICCV_2017_paper|In available multiview stereo pipelines, sparse features are first detected and then propagated to a dense point cloud for covering the whole surface [8, 11], or multiview depth maps are first computed followed with a depth map fusion step to obtain the 3D reconstruction of the object [15, 17].|
|||Other approaches have focused on more advanced depth fusion algorithms [13].|
|||Besides of binarization and thinning, SurfaceNet does not require any additional post-processing or depth fusion to obtain an accurate and complete reconstruction.|
|||Related Works  Works in the multiview stereopsis (MVS) field can be roughly categorised into volumetric methods and depth maps fusion algorithms.|
|||As our method is more related to the second category, our survey mainly covers depth map fusion algorithms.|
|||The depth map fusion algorithms first recover depth maps [30] from view pairs by matching similarity patches [2, 18, 33] along epipolar line and then fuse the depth maps to obtain a 3D reconstruction of the object [27, 4, 9].|
|||In order to improve the fusion accuracy, [4] mainly learns several sources of the depth map outliers.|
|||The depth fusion methods usually contain several manually engineered steps, such as point matching, depth map denoising, and view pair selection.|
|||Compared with the mentioned depth fusion methods, the proposed SurfaceNet infers the 3D surface with thin structure directly from multiview images without the need of manually engineering separate processing steps.|
|||Real-time visibilitybased fusion of depth maps.|
||11 instances in total. (in iccv2017)|
|116|Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper|At the inference time, we adopt a late fusion scheme to connect the two visual graphs in our designed GCN-LSTM architecture.|
|||In addition, by utilizing both spatial and semantic graphs in a late fusion manner, our GCN-LSTM further boosts up the performances.|
|||One is to perform early fusion scheme by concatenating each pair of region features from graphs before attention module or the attended features from graphs after attention module.|
|||The other is our adopted late fusion scheme to linearly fuse the predicted word distributions from two decoders.|
|||Figure 6 depicts the three fusion schemes.|
|||Different schemes for fusing spatial and semantic graphs in GCN-LSTM: (a) Early fusion before attention module, (b) Early fusion after attention module and (c) Late fusion.|
|||The fusion operator could be concatenation or summation.|
|||performances of our GCN-LSTM in the three fusion schemes (with cross-entropy loss).|
|||The results are 116.4%, 116.6% and 117.1% in CIDEr-D metric for early fusion before/after attention module and late fusion, respectively, which indicate that the adopted late fusion scheme outperforms other two early fusion schemes.|
||10 instances in total. (in eccv2018)|
|117|Olsson_In_Defense_of_2013_CVPR_paper|One reason for their popularity is that when applying movemaking algorithms such as -expansion [4] or fusion moves [8] they often result in submodular moves, allowing efficient computation using min-cut/max-flow algorithms [4].|
|||Submodularity of Fusion Moves  In this section we will show that fusion moves [8] with our interactions are often submodular.|
|||Candidate Planes  We first show that fusion moves where the candidate function P is a plane result in submodular terms.|
|||General Candidates  Next we derive some more general sufficient conditions  for submodularity of the fusion move.|
|||Proposition 3.2 If both D and P are convex (or alternatively both concave) between p and q then the interactions Vpq and Vqp are submodular for the fusion move.|
|||As in Proposal 3.1 it is easy to see that if our proposals fulfill  ApP (q) = P (q) ,  (32)  then the fusion move will be submodular.|
|||For example if we only use the zero order expansion (constant functions/ fronto-parallel planes) then we find that fusion moves with constant depth proposals are submodular.|
|||In the fusion moves we use improve [11] after running RD to label all unlabelled variables.|
|||Note that the fusion move for our method is only submodular if we fuse one planar function at a time.|
||10 instances in total. (in cvpr2013)|
|118|Liu_Multi-View_Complementary_Hash_ICCV_2015_paper|have enjoyed the benefits of complementary hash tables and information fusion over multiple views.|
|||Owing to exemplar based feature fusion and its reweighing scheme, the proposed method is powerful to mutually capture the underlying neighbor structures using multi-view complementary tables, and computationally feasible for large-scale learning and fast online search.|
|||This fact indicates that the similarity induced from our exemplar based feature fusion is equivalent to the linear combination of corresponding approximated similarities in each view.|
|||The proof can be completed using the exemplar based feature fusion (please see the supplementary material).|
|||The theorem states that for a new sample its hash code can be generated simply by a composite operation: nonlinear feature map z(x), linear projection using W and sgn binarization, which implies explicit hash functions H: hj(x) = sgn(wT Now, we can summarize that by using the exemplar based feature fusion the proposed multi-view hash table can be learnt explicitly and efficiently at the offline training stage, and promises fast out-of-sample extension for any new sample at the online searching stage.|
|||SETTINGS  F1 F1+ER F2 F2+ER F1F2 F1+F2+ER  RH2  PH2  L = 1  2.960.09 2.960.09 3.290.23 3.290.23 3.120.17 5.55 0.20  L = 4  4.160.10 3.440.12 4.750.25 4.410.20 4.400.21 9.18 0.28  L = 8  5.160.12 3.920.13 5.830.23 5.320.35 5.420.21 11.19 0.31  L = 16 6.970.13 5.050.19 7.750.23 6.460.35 7.280.21 13.32 0.39  L = 1  16.620.24 16.620.24 24.990.58 24.990.58 19.940.20 26.21 0.78  L = 4  15.520.15 16.500.23 22.110.54 24.530.48 18.040.16 25.02 0.75  L = 8  14.420.10 16.400.24 19.390.39 24.140.38 16.310.17 24.38 0.69  L = 16  13.280.08 16.170.22 16.710.25 23.640.35 14.620.14 23.76 0.59    ) )(5 ) )(5 )) ))(5     # 3 $                4.3.1 On Feature Fusion  To illustrate the benefits from exemplar based feature fusion, on CIFAR-10 we compare our method with different settings (but all with exemplar reweighting, ER for short): using exemplar based fusion of two views (F1+F2+ER, i.e., the standard MVCH in Sec.|
|||Figure 3 shows the precision using Hamming distance ranking of different methods, where we can obtain the similar observation that feature fusion without considering their correlations even performs worse than single view.|
|||Moreover, both Table 2 and Figure 3 demonstrate that our MVCH can build complementary tables and achieves the best performance by simultaneously integrating both adaptive multiple feature fusion and exemplar reweighting.|
|||The exemplar based feature fusion was first introduced to capture the inherent neighbor structures among data by linearly combining the nonlinear feature transformation in each view.|
||10 instances in total. (in iccv2015)|
|119|Wang_Online_Reconstruction_of_CVPR_2016_paper|Camera Tracking: We extends the voxel hashing based volumetric fusion [19] approach to achieve robust real-time camera tracking.|
|||After that, we make use of the volumetric fusion to fuse these depth images into a final 3D model with refined trajectory.|
|||Camera Tracking with Volumetric Fusion  To perform real-time 6DOF camera tracking, we adopt the framework of volumetric fusion with voxel hashing [19], which can handle large-scale reconstruction with fine-grained details.|
|||The volumetric fusion framework stores the TSDF value (p) and an associated weighting factor W (p) in a voxel whose center locates at point p. And it has two important components, i.e., frame-to-model registration and model integration.|
|||Finally, we apply the volumetric fusion described in section 4.2 to reconstruct the final model.|
|||We also provide our camera tracking results with volumetric fusion only (without pose graph optimization).|
|||For DVO-SLAM, we use its optimized trajectory as inputs for volumetric fusion to obtain the reconstructed model.|
|||We also evaluate the performance of our volumetric fusion without pose graph optimization.|
|||Our system is composed of tracking and fusion threads (Tr), pose graph construction  73277  Table 4.|
||10 instances in total. (in cvpr2016)|
|120|Choe_Exploiting_Shading_Cues_2014_CVPR_paper|Exploiting Shading Cues in Kinect IR Images for Geometry Refinement  Gyeongmin Choe  Korea Advanced Institute of Science and Technology, Republic of Korea  Jaesik Park Yu-Wing Tai  In So Kweon  [gmchoe,jspark]@rcv.kaist.ac.kr,yuwing@kaist.ac.kr,iskweon77@kaist.ac.kr  Abstract  In this paper, we propose a method to refine geometry of 3D meshes from the Kinect fusion by exploiting shading cues captured from the infrared (IR) camera of Kinect.|
|||To resolve ambiguity in our model between normals and distance, we utilize an initial 3D mesh from the Kinect fusion and multi-view information to reliably estimate surface details that were not reconstructed by the Kinect fusion.|
|||a scene which allows higher quality reconstruction than the popular Kinect fusion [8] that uses only the estimated depth map for 3D reconstruction.|
|||Therefore, we utilize an initial 3D mesh from the Kinect fusion and shading images from different view point.|
|||The result is a high quality mesh model which captures surface details that were not reconstructed by the Kinect fusion as shown in Figure 1.|
|||We use the Kinect fusion to obtain a base mesh of the sphere, and then capture the IR shading images of the sphere.|
|||We use the Kinect fusion to obtain an initial base mesh.|
|||The first term ensures the change of displacement is locally smooth between neighboring vertices, xi, xj  Ni, and the second term in Equation (6) is to ensure the estimated displacement i would not be too large since the initial mesh from the Kinect fusion is already quite accurate.|
|||From the left, each column represents IR shading images, initial mesh from the Kinect fusion and our mesh result respectively.|
||10 instances in total. (in cvpr2014)|
|121|cvpr18-Two Can Play This Game  Visual Dialog With Discriminative Question Generation and Answering|A baseline for simple models is set using the late fusion architecture.|
|||While late fusion has a simple architecture, the other two complex models obtain better performance.|
|||Similarly, to obtain an embedding for a questionanswer pair, we use a question and an answer LSTM to encode all question-answer pairs in the history set H. Upon encoding the question and the answer of a question-answer  5756  Figure 2: Overview of the proposed approach: Joint similarity scoring of answer option and fusion of all input features.|
|||We perform both fusion and similarity scoring together using a multilayer perceptron network.|
|||As mentioned before, unlike previous methods, we perform similarity scoring and feature fusion jointly.|
|||Network Training  To describe training more formally, let Fw(Oi) denote the score for answer option i obtained from the similarity scoring + fusion network, and let w denote all the parameters of the architecture illustrated in Fig.|
|||Our similarity scoring + fusion (SF) performs best in all three scenarios.|
|||This includes models proposed in [6], based on late fusion (LF), hierarchical LSTM net (HRE), and memory networks (MN).|
|||Mutan: Multimodal tucker fusion for visual question answering.|
||10 instances in total. (in cvpr2018)|
|122|cvpr18-Path Aggregation Network for Instance Segmentation|But this method extracted feature maps on input with different scales and then conducted feature fusion (with the max operation) to improve feature selection from the input image pyramid.|
|||In following sub-networks, pooled feature grids go through one parameter layer independently, which is followed by the fusion operation, to enable network to adapt features.|
|||We apply the fusion operation after the first layer.|
|||Based on our re-implemented baseline (RBL), we gradually add multi-scale training (MST), multi-GPU synchronized batch normalization (MBN), bottom-up path augmentation (BPA), adaptive feature pooling (AFP), fullyconnected fusion (FF) and heavier head (HHD) for ablation study.|
|||Fully-connected fusion predicts masks with better quality.|
|||As shown in Table 4, adaptive feature pooling is not sensitive to the fusion operation type.|
|||Ablation study on fully-connected fusion on val-2017 in terms of mask AP.|
|||They clearly show that staring from conv3 and taking sum for fusion produce the best results.|
||9 instances in total. (in cvpr2018)|
|123|Sridhar_Fast_and_Robust_2015_CVPR_paper|In a final late fusion step the best pose is chosen based on the pose fitting energy.|
|||Pose estimates obtained with each energy are used by a late fusion approach to find the final pose estimate (Section 5).|
|||We also motivate the need for our fusion strategy, parameter selection in optimization, and analyze the effects of different components of our objective function.|
|||To demonstrate how our late fusion strategy and the different terms in the energy help achieve this, we show the frame-wise error over the flexex1 sequence (Figure 6).|
|||The results from the late fusion approach show large gains in accuracy.|
|||The late fusion approach, when used with 2 particles (10 iterations per particle), achieved a framerate of 50 fps.|
|||Plot of the error for the depth-only tracking and late fusion approach.|
|||The late fusion strategy is robust and prevents error accumulation.|
||9 instances in total. (in cvpr2015)|
|124|Pfister_Flowing_ConvNets_for_ICCV_2015_paper|To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps; (ii) spatial fusion layers that learn an implicit spatial model; (iii) optical flow is used to align heatmap predictions from neighbouring frames; and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map.|
|||We show that these spatial fusion layers remove pose estimation failures that are kinematically impossible.|
|||Spatial fusion layers  Vanilla heatmap pose nets do not learn spatial dependencies of joints, and thus often predict kinematically impossible poses (see examples in the extended arXiv version of this paper).|
|||These spatial fusion layers (normal convolutional layers) take as an input pre-heatmap activations (conv7), and learn dependencies between the human body parts locations represented by these activations.|
|||This is done in three steps: (1) the confidences from nearby  1915  k jointsIteration 262,400 Iteration 1,146,400SpatialNet  conv1 5x5x128  pool 2x2  conv2 5x5x128 pool 2x2  conv3 5x5x128  conv4 5x5x256  conv5 9x9x512  conv6 1x1x256  conv7 1x1x256  conv8 1x1x7  Loss 1  Spatial fusion layers  conv1_f 7x7x64  conv2_f 13x13x64  conv3_f 13x13x128  conv4_f 1x1x256  conv5_f  1x1x7  Loss 2  Figure 4.|
|||Spatial fusion layers.|
|||The fusion layers learn to encode dependencies between human body parts locations, learning an implicit spatial model.|
|||The proposed ConvNet is a simple, direct method for regressing heatmaps, and its performance is improved by combining it with optical flow and spatial fusion layers.|
||9 instances in total. (in iccv2015)|
|125|Kyoungoh_Lee_Propagating_LSTM_3D_ECCV_2018_paper|2(e) shows the proposed p-LSTM which consists of one LSTM network and one depth fusion layer.|
|||Then, the estimated 3D joints are merged into the input 2D pose in the depth fusion layer of the first  6  K. Lee et al.|
|||A1: Algorithm of p-LSTMs  Input: X (2D pose) Output: Y (3D pose)  Variables k: index of the p-LSTM K: number of the p-LSTM Y k: output of the kth LSTM network X k: output of the kth depth fusion layer LSTMk: kth LSTM network Depthk: kth depth fusion layer FC: fully connected layer YPred: output of 3D pose  Propagating Connection: To reflect the joint interdependency (JI) into our method, the body part based structural connectivity is carefully dealt with.|
|||To prevent the initial 2D pose from disappearing, each p-LSTM uses the input 2D pose as ancillary data and merges it with its own output in the depth fusion layer.|
|||In other words, the pose depth cue is created by integrating the 2D with 3D poses in the depth fusion layer, as shown in Fig.|
|||2(d), and creates the pose depth cue for each depth fusion layer of the p-LSTMs.|
|||One stage of p-LSTMs consists of 9 LSTM blocks, 9 depth fusion layers and 2 FCs.|
|||How to set the propagating direction: The pose depth cue is created through a depth fusion layer of a p-LSTM.|
||9 instances in total. (in eccv2018)|
|126|Li_A_Multi-Level_Contextual_CVPR_2016_paper|Another option for fusion is to assign instance specific weights based on the prediction confidence scores.|
|||An alternative to score-level fusion is to concatenate the features from all regions to represent the instance and proceed with the other components for prediction.|
|||We observe that this fusion method performs worse than any of the score fusion methods, as shown in Table 1.|
|||The weighted average fusion method works best for the original splits.|
|||The max pooling fusion gives consistent improvements after fusion.|
|||The performance of the confidence-aware fusion method is stable across all setups.|
||| baseline method: the confidence-aware fusion of the  global face and body SVMs is our baseline method;   baseline method with photo-level context: we directly update the results from the baseline method with the iterative image label joint inference CRF;  1302  y c a r u c c A n o     i t i  n g o c e R   0.95   0.9   0.85   0.8   0.75   0.7   0.65   10   20   30   40   50   60   70   80   90   100  # of nearest neighors  Figure 6.|
|||We propose to use a confidence-aware fusion method to integrate the discriminative information from all the regions.|
||9 instances in total. (in cvpr2016)|
|127|Action Unit Detection With Region Adaptation, Multi-Labeling Learning and Optimal Temporal Fusing|Many proposed approaches face challenging problems in dealing with the alignments of different face regions, in the effective fusion of temporal information, and in training a model for multiple AU labels.|
|||Due to the fusion of both spatial CNN and temporal features, the AU detection performance in this work has improved significantly compared to existing approaches.|
|||To see if LSTM is useful in AU detection, we have conducted experiments to compare LSTM-based temporal fusion versus static image AU prediction.|
|||That implies that the global information does have an important impact on the fusion learning.|
|||Comparison of static image and temporal fusion in AU detection on BP4D  Table 2.|
|||To obtain the spatiotemporal fusion features, the last layer features of the CNN and LSTM nets are concatenated.|
|||Conclusion  In this paper, we looked into three essential problems, the region adaption learning, temporal fusion and single/multi-label AU learning, in AU detection and proposed a novel approach to address these problems.|
|||We finally explored the LSTMbased temporal fusion approach, which boosted the AU detection performance significantly compared to static imagebased approaches.|
||9 instances in total. (in cvpr2017)|
|128|Sun_Human_Pose_Estimation_ICCV_2017_paper|An additional minor contribution is that we em pirically show the improvement by using an architecture with multi-scale supervision and fusion for joint detection.|
|||In our work, we introduce multi-scale supervision and fusion to further improve performance with gradual up-sampling.|
|||Multi-scale fusion exploits the information at different scales.|
|||We introduce multi-scale supervision (with losses LD1, LD2, and LD3) and multi-scale fusion (with losses LF 1, LF 2) (see Figure 6).|
|||5605  Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  FCN 32s  FCN 16s  FCN 8s  93.7  93.9  94.2  FCN 16s (Extra)  94.2  FCN 16s (Fusion) 94.3  FCN 8s (Extra)  94.2  FCN 8s (Fusion)  94.3  85.2  85.9  86.2  87.5  87.7  87.5  87.8  74.4  65.2 86.2 81.2  77  80.4  50  75.1  68.3 86.3 83.4  78.5  81.6 52.1  75.8  68.8 86.5 83.8  78.3  82  53.7  76.8  69.2 87.5 82.8  78.4  82.4 53.2  77.0  69.5 87.6 83.4  78.6  82.6 53.2  77.2  69.6 87.2 83.5  79.7  82.6 54.2  77.1  69.8 87.7 83.7  79.7 82.8 54.2  Table 11: Evaluation of multi-scale supervision and multiscale fusion on top of FCN on the LSP testing set with the OC annotation (@PCK0.2) trained on the LSP training set.|
|||For FCN, we add multi-scale supervision and multi-scale score map fusion to improve accuracy.|
|||Here, we evaluate the efficiency of the extra supervision and fusion respectively.|
|||FCN 16s and FCN 8s denote the results of the original FCN without extra loss and fusion at the middle and high resolution respectively.|
||9 instances in total. (in iccv2017)|
|129|Deep Multimodal Representation Learning From Temporal Data|Deep Multimodal Representation Learning from Temporal Data  Xitong Yang1, Palghat Ramesh2, Radha Chitta3, Sriganesh Madhvanath3,  Edgar A. Bernal4 and Jiebo Luo5  1University of Maryland, College Park 2PARC 3Conduent Labs US  4United Technologies Research Center  5University of Rochester  1  xyang35@cs.umd.edu, 2Palghat.Ramesh@parc.com, 3{Radha.Chitta,  Sriganesh.Madhvanath}@conduent.com, 4bernalea@utrc.utc.com, 5jluo@cs.rochester.edu  Abstract  In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications.|
|||Generally speaking, and from the standpoint of dynamicity, fusion frameworks can be classified based on the type of data they support (e.g., temporal vs. non-temporal data) and the type of model used to fuse the data (e.g., temporal vs. non-temporal model) as illustrated in Fig.|
|||Multimodal Deep Learning  Within the context of data fusion applications, deep learning methods have been shown to be able to bridge the gap between different modalities and produce useful joint representations [13, 21].|
|||The multimodal inputs are first mapped to separate hidden layers before being fed to a common layer called the fusion layer.|
|||Specifically, the activations of the fusion layer in the encoder at the last time step is output as the sequence feature representation.|
|||Conclusions  In this paper, we have proposed CorrRNN, a new model for multimodal fusion of temporal inputs such as audio, video and sensor data.|
|||We have demonstrated that the CorrRNN model achieves state-of-the-art accuracy in a variety of temporal fusion applications.|
|||Multimodal fusion using dynamic hybrid models.|
||9 instances in total. (in cvpr2017)|
|130|Saleh_Bringing_Background_Into_ICCV_2017_paper|An early, trainable fusion that puts in correspondence the spatial and temporal information, and a late fusion that merges the resulting spatio-temporal stream with the appearance one for final prediction.|
|||Below, we discuss how we encode optical flow and describe our fusion strategy.|
|||In particular, fusion occurs after the fifth convolutional layer (Conv5-3) of each stream, which has been shown to contain a rich semantic representation of the input [45, 3].|
|||The first, early fusion puts in correspondence the activations of both streams corresponding to the same pixel location.|
|||As [14], instead of performing sumor max-fusion, we rely on a convolutional fusion strategy.|
|||The second, late fusion of our network merges the spatio-temporal stream resulting from early fusion with the appearance stream.|
|||In the future, we plan to investigate other fusion strategies within our two-stream formalism.|
|||Convolutional two-stream network fusion for video action recognition.|
||9 instances in total. (in iccv2017)|
|131|Hwang_Markov_Network-Based_Unified_2013_ICCV_paper|The score fusion rule is the sum rule and the test protocol is the UvC test.|
|||Test  Method  ROC1  ROC2  ROC3  Avg  CvC  UvC  BGL  RSG (Sum) ECG (LR)  Proposed (RSG) Proposed (ECG)  BGL  RSG (Sum) ECG (LR)  Proposed (RSG) Proposed (ECG)  95.86% 96.69% 97.21% 98.53% 98.93% 73.58% 83.15% 86.86% 92.15% 94.06%  94.16% 94.92% 95.69% 98.01% 98.44% 71.24% 80.85% 84.51% 91.85% 93.85%  93.76% 94.39% 95.36% 97.77% 98.17% 73.54% 82.63% 85.64% 92.54% 94.42%  94.59% 95.33% 96.09% 98.10% 98.52% 72.79% 82.21% 85.67% 92.18% 94.11%  ple ones (e.g., MIN, MAX, and Sum) [7], RANK based fusion [11], likelihood rate (LR) based fusion [16], Gaussian mixture model-based LR (GMLR) method [13], and fisher classifier (LDA) based score fusion method [18].|
|||In this respect, we can know that the proposed method achieves the best result among the published fusion methods.|
|||The performances of the proposed method are compared with well-known fusion methods that combined the ten proposed RSG classifiers.|
|||Figure 9 summarizes the performances of the wellknown fusion methods for the ten RSG classifiers: the sim 1957 1957  curacies of the proposed method decrease in both the XvX and BvB tests.|
|||Information fusion in biometrics.|
|||A comparison of score, rank and probability-based fusion methods for video shot retrieval.|
|||Decision-level fusion in fingerprint verification.|
||9 instances in total. (in iccv2013)|
|132|Newcombe_DynamicFusion_Reconstruction_and_2015_CVPR_paper|A key contribution of our work is an approach for non-rigid transformation and fusion that retains the optimality properties of volumetric scan fusion [5], developed originally for rigid scenes.|
|||t , 1)(cid:62) = Wt(xc)(x(cid:62)  We extend the projective TSDF fusion approach originally introduced by [6] to operate over non-rigidly deforming scenes.|
|||Given the live depth image Dt, we transform each voxel center xc  S by its estimated warp into the live c , 1)(cid:62), and carry through the frame (x(cid:62) TSDF surface fusion operation by directly projecting the warped center into the depth frame.|
|||(5) Unlike the static fusion scenario where the weight w(x) encodes the uncertainty of the depth value observed at the projected pixel in the depth frame, we also account for uncertainty associated with the warp function at xc.|
|||Furthermore, given a correct warp field, then, since all TSDF updates are computed using distances in the camera frame, the non-rigid projective TSDF fusion approach maintains the optimality guarantees for surface reconstruction from noisy observations originally proved for the static reconstruction case in [6].|
|||3.3.2 Warp-field Regularization  It is crucial for our non-rigid TSDF fusion technique to estimate a deformation not only of currently visible surfaces, but over all space within S. This enables reconstruction of new regions of the scene surface that are about to come into view.|
|||Inserting New Deformation Nodes into Nwarp: After performing a non-rigid TSDF fusion step, we extract the surface estimate in the canonical frame as the polygon mesh Vc.|
|||We achieved this by generalising the volumetric TSDF fusion technique to the non-rigid case, as well as developing an efficient approach to estimate a volumetric 6D warp field in real-time.|
||9 instances in total. (in cvpr2015)|
|133|Xu_Event_Detection_using_2014_CVPR_paper|There has been plenty of research focusing on multiple feature fusion [16, 15, 13], but none of the existing algorithms can tackle multiple relevance levels of the training data.|
|||have designed a local expert forest model for score fusion from multiple classifiers in complex event detection [9].|
|||Regarding the algorithms of combining multiple features, we apply early fusion and late fusion on these features.|
|||For early fusion methods, we apply average early fusion.|
|||For late fusion methods, we use average late fusion and LPBoost fusion [3].|
|||We compare to the results from average late fusion of SVM (SVMlate), average late fusion of KR (KRlate), average early fusion of SVM (SVMearly), LPBoost fusion of SVM (SVMLP), and LPBoost fusion of KR (KRLP).|
|||Multimodal feature fusion for robust event detection in web videos.|
|||Robust late fusion with  rank minimization.|
||9 instances in total. (in cvpr2014)|
|134|What Is and What Is Not a Salient Object_ Learning Salient Object Detector by Ensembling Linear Exemplar Regressors|As a result, their fusion can adaptively handle the SOD tasks in various scenarios, and the usage of shape descriptor and high-objectness proposals ensure the well suppression of non-salient objects.|
|||However, the scores of each linear exemplar regressor may fall in different dynamic ranges so that their direct fusion will lead to inaccurate saliency maps (see Fig.|
|||The enhancement-based fusion strategy for combining exemplar scores makes the learned salient detector emphasize more on the most relevant linear exemplar  43264147  Figure 6.|
|||Different fusion strategy for SOD.|
|||(a) Image, (b) groundtruth, (c) direct fusion by computing the maximum saliency value, (d) direct fusion by computing the mean saliency value, (e) enhanced fusion by computing the mean saliency value after an enhancement operation using a sigmoid function.|
|||In the third experiment, we test various fusion strategies of exemplar scores.|
|||We compare 3 different fusion ways as shown in Fig.|
|||We find that the weighted F-measure of using the max and mean value of raw exemplar scores as the final saliency value of a proposal is 0.540 and 0.588, while the weighted F-measure of using enhancement-based fusion is 0.649.|
||9 instances in total. (in cvpr2017)|
|135|Deep Temporal Linear Encoding Networks|They exploit fusion techniques like trajectory-constrained pooling [37], 3D pooling [8], and  12329  consensus pooling [38].|
|||The fusion methods of spatial and motion information lie at the heart of the state-of-the-art two-stream ConvNets.|
|||Similar to [25] and [33] is Feichtenhofer et al.s [8] work, where they employ 3D Conv fusion and 3D pooling to fuse spatial and temporal networks using RGB images and a stack of 10 optical flow frames as input.|
|||[38] use multiple clips sparsely sampled from the whole video as input for both streams, and then combine the scores for all clips in a late fusion approach.|
|||Finally, the scores for the two ConvNets are combined in a late fusion approach as averaging.|
|||Unlike IDTs, these techniques use ConvNets with late fusion to combine spatial and temporal cues, but they still fail to efficiently encode all frames together.|
|||The prediction scores of the spatial and temporal ConvNets are combined in a late fusion approach via averaging.|
|||Convolutional two-stream network fusion for video action recognition.|
||9 instances in total. (in cvpr2017)|
|136|Tz-Ying_Wu_Liquid_Pouring_Monitoring_ECCV_2018_paper|Given many success and failure demonstrations of liquid pouring, we train a hierarchical LSTM [8] with late fusion to incorporate rich sensories inputs without significantly increasing the model parameters as compared to early fusion models.|
|||Vanilla RNN: Our fusion RNN without auxiliary tasks.|
|||RNN w/ IOSC: Our fusion RNN with an auxiliary task, initial object state classification (IOSC).|
|||RNN w/ TF: Our fusion RNN with an auxiliary task, trajectory forecasting (TF).|
|||: Our fusion RNN with two proposed auxiliary tasks, initial object state classification and trajectory forecasting.|
|||Ours: Our fusion RNN with two proposed auxiliary tasks, initial object state classification and trajectory forecasting.|
|||The latter one is an early fusion method that data from different modalities is directly concatenated together and fed into the 2-layer LSTM.|
|||The results in Table 4 show that the hierarchical LSTM with late fusion outperforms the naive 2-layer LSTM in all tasks and this may be due to the capability of the hierarchical LSTM to handle scale difference and imbalanced dimension among multimodal inputs.|
||9 instances in total. (in eccv2018)|
|137|Wang_SORT_Second-Order_Response_ICCV_2017_paper|Right: in a residuallearning building block [16], we can also modify the fusion stage from x + F(x) to x + F(x) + px  F(x).|
|||2 xi, and the fusion is  2 h  The core idea of SORT is extremely simple.|
|||element-wise product, to the linear term, leading to a new fusion strategy:  yS = F1(x) + F2(x) + g[F1(x)  F2(x)].|
|||Then, we  perform element-wise fusion (1) beyond F1(x) and F2(x) by setting g[] to be an identity mapping function.|
|||As a direct variant of (1), SORT modifies the original fusion function from yR = x + F(x) to yS =  x + F(x) +px  F(x) + .|
|||Recognition error rate (%) on the CIFAR10 dataset with different fusion strategies.|
|||We experimentally verify the effectiveness of nonlinearity by considering three fusion strategies, i.e., F1(x) +  F2(x), max {F1(x) , F2(x)} and pF1(x)  F2(x).|
|||To  compare their performance, we apply different fusion strategies on different networks, and evaluate them on the CIFAR10 dataset (detailed settings are elaborated in Section 4.1).|
||9 instances in total. (in iccv2017)|
|138|Nguyen_Fast_and_Effective_ICCV_2015_paper|Our method uses a descent approach based on region fusion that converges faster than other methods while providing a better approximation of the optimal L0 norm.|
|||Section 3 describes our descent and fusion approach for approximating L0 gradient minimization.|
|||Our method uses a fusion technique that examines neighboring regions in the signal that have nearly similar values and decides whether to fuse them to have the same value.|
|||Their work, however, used a more complicated optimization mechanism that separated the fusion step from the coordinate descent step in a way that did not guarantee the objective function to monotonically decrease during the fusion step (see accompanying supplemental material for more details).|
|||In contrast, our approach combines the fusion and descent into a single step that guarantees a decrease of the objective function and allows our method to converge quickly.|
|||We call Equation 9 the fusion criterion.|
|||According to this fusion criterion, we will decide whether to fuse these two elements into one group or not.|
|||After each fusion step, all the information of the fused group Gj is deleted and the number of remaining groups is reduced by one.|
||9 instances in total. (in iccv2015)|
|139|cvpr18-Spline Error Weighting for Robust Visual-Inertial Fusion|We demonstrate the effectiveness of the prediction in a synthetic experiment, and apply it to visual-inertial fusion on rolling shutter cameras.|
|||Visual-inertial fusion using splines has traditionally balanced the sensor modalities using inverse noise covariance  Figure 1.|
|||SEW makes visual-inertial fusion robust on real sequences, acquired with rolling shutter cameras.|
|||Related work  Visual-inertial fusion on rolling shutter cameras has classically been done using Extended Kalman-filters (EKF).|
|||[8] study visual-inertial fusion with preintegration of IMU measurements between keyframes, with a global shutter camera model.|
|||Visual-inertial fusion  We will use the residual error prediction introduced in section 2 to balance visual-inertial fusion on rolling shutter cameras.|
|||We plan to release our spline error weighting framework  for visual-inertial fusion under an open source license.|
|||A splinebased trajectory representation for sensor fusion and rolling shutter cameras.|
||9 instances in total. (in cvpr2018)|
|140|Chen_3D_Model-Based_Continuous_2015_CVPR_paper|Moreover, via the reconstructed 3D facial model, temporal information and user-independent emotion presentations are also taken into account through our image fusion process.|
|||Yang and Bhanu [37, 38] showed their image fusion method which used SIFT-flow algorithm combining images from one video clip into one image.|
|||With the help of the 3D model, we propose a real-time image fusion method to represent continuous emotions and user-independent emotions respectively.|
|||Figure 3 shows the pipeline of our image fusion method.|
|||Image fusion pipeline.|
|||After transforming all the facial parts of original images to the unified facial coordinate system, these images are superposed and result in one fusion presentation.|
|||For different goals, the image fusion method is used to generate user-specific continuous emotion presentation and user-independent emotion presentations in our work.|
|||With the reconstructed 3D facial shape, an image fusion method is proposed to generate a users continuous emotion expressions (CEP) and user-independent emotion expressions (UIEP).|
||9 instances in total. (in cvpr2015)|
|141|Huang_Predicting_Gaze_in_ECCV_2018_paper|The late fusion module combines the results of saliency prediction and attention transition to generate a final gaze map.|
|||Since our task is different from [12], we modify the kernel sizes of the fusion part, which can be seen in detail in Section 3.7.|
|||3.5 Late Fusion  We build the late fusion module (LF) on top of the saliency prediction module and the attention transition module, which takes Gs t as input and outputs the predicted gaze map Gt.|
|||3.6 Training  For training gaze prediction in saliency prediction module and late fusion module, the ground truth gaze map G is given by convolving an isotropic Gaussian over the measured gaze position in the image.|
|||The late fusion module consists of 4 convolution layers followed by sigmoid activation.|
|||After training the attention transition module, we fix the saliency prediction and the attention transition module to train the late fusion module in the end.|
||| SP+AT d: The late fusion on top of SP and AT d.  SP+AT s: The late fusion on top of SP and AT s.  Quantitative results of different settings are shown in Figure 4.|
|||However, SP+AT d with the late fusion module can still improve the performance compared with SP and AT s, even with the context learned from different tasks.|
||9 instances in total. (in eccv2018)|
|142|Murray_Generalized_Max_Pooling_2014_CVPR_paper|Late fusion results were obtained by evaluating an unweighted summation of the scores given by the SIFT and color-based classifiers.|
|||EMK results for SIFT descriptors, color descriptors, and late fusion of SIFT + color.|
|||Our GMP consistently performs significantly better  10% better on average for late fusion and G = 256  than when no power normalization is applied.|
|||The average improvement for late fusion is 2.8%.|
|||Note that, on the Flowers dataset with late fusion and G = 256, we obtain 83.5% and 82.2% respectively for the power normalization and GMP.|
|||Also, on the Pets dataset with late fusion and G = 256, GMP obtains top-1 accuracy of 56.1%, compared to 54.2% with power normalization  an increase in performance of 1.9%.|
|||For instance, with late fusion and G = 256, GMP obtains 62.0% compared to 60.2% for the power baseline  a 1.8% increase in performance.|
|||FV results for SIFT descriptors, color descriptors, and late fusion of SIFT + color.|
||9 instances in total. (in cvpr2014)|
|143|Galliani_Massively_Parallel_Multiview_ICCV_2015_paper|However, we prefer to carefully exploit the multiview information at the level of photo-consistency, and then use a rather basic fusion scheme to merge them into a consistent 3D point cloud.|
|||This is in contrast to some other methods that start from efficiently computable, but noisy depth maps and merge them with sophisticated fusion algorithms, which (at least implicitly) have to solve the additional problem of surface fitting [21, 28, 39].|
|||This can be used to improve the subsequent point cloud fusion (e.g.|
|||Accuracy vs. completeness The fusion parameters fo , fang and fcon filter out 3D points that are deemed unreliable, and thus balance accuracy against completeness of the multiview reconstruction.|
|||Note that the fusion step is very fast ( 15 seconds for 49 depthmaps of size 1600  1200) and does not change the depthmaps.|
|||For fusion we used the parameters fo = 0.3 and fcon = 3.|
|||Fast and high quality fusion of depth maps.|
||8 instances in total. (in iccv2015)|
|144|cvpr18-Matryoshka Networks  Predicting 3D Geometry via Nested Shape Layers|In general, view-based methods are able to generate shapes at high resolutions, but occasionally suffer from noisy estimates, which need to be addressed in the fusion step.|
|||Our proposed method addresses the fusion step and handling of occlusions in a simple, but efficient formulation.|
|||Through careful alignment of the depth maps and an appropriate loss function, we avoid noisy estimates, a costly fusion via optimization, and minimize the dimensionality of the final network layer.|
|||This fusion process and the placement of the three orthogonal views vi is motivated by our observation that depth map predictions are often less accurate near the silhouette of an object.|
|||To that end, let T1:L  S be the (true) target shape and  : S  (D) be the projection from an arbitrary shape to the space of shapes that can be represented by the depth map fusion process  from Eq.|
|||We leave this and learning the shape fusion [21] for future work.|
|||Octnetfusion: Learning depth fusion from data.|
||8 instances in total. (in cvpr2018)|
|145|Ferstl_Image_Guided_Depth_2013_ICCV_paper|In general, they can be separated in three main classes: (1) fusion of multiple depth sensors, (2) temporal and spatial fusion and (3) upsampling by combining depth and intensity sensors.|
|||Multiple Depth Sensor Fusion Recent works addressed the fusion of different depth sensing techniques to increase resolution and quality.|
|||Additionally to this spatial fusion also a temporal fusion was performed by measuring the frame-to-frame displacement acquired with high speed intensity cameras.|
|||[14] proposed a method for simultaneous camera localization and depth fusion in real time.|
|||Further, most sensor fusion techniques have to calculate a depth map from passive stereo in a preprocessing step before the actual fusion is able to start.|
|||Contrary, temporal and spatial fusion approaches rely on multiple acquisitions from a single depth sensor.|
|||The major drawback of these methods is that changing environments during these acquisitions will harm the fusion result.|
||8 instances in total. (in iccv2013)|
|146|Zhang_Scale-Adaptive_Convolutions_for_ICCV_2017_paper|In order to address the limitation of fixed-size receptive fields, many approaches based on multi-scale fusion are presented.|
|||Besides, multi-scale fusion can be regarded as data argumentation and model ensemble, so that it can be applied together with the proposed scale-adaptive convolutions to boost the stability and robustness of the predictions and further improve the performance.|
|||MS: Multi-scale fusion during testing.|
|||MS: Multi-scale fusion during testing.|
|||MS: Multi-scale fusion during testing.|
|||We also employ the multi-scale fusion during testing, which further improves the performance of SAC-single and SAC-multiple to 78.2% and 78.7%.|
|||The proposed SAC-multiple with multi-scale fusion achieves 78.1% in terms of mean IoU.|
||8 instances in total. (in iccv2017)|
|147|cvpr18-Robust Depth Estimation From Auto Bracketed Images|We compare exposure fusion results from input images (L) and aligned images using our depth (R).|
|||4d and exposure fusion algorithm [23] in Fig.|
|||We also found that our accurate depth can be additionally useful for exposure fusion and depth-aware photographic editing applications, such as digital refocusing and image stylization in Fig.|
|||Exposure fusion assembles the multi-exposure sequence into a high quality image using a weighted blending of the input images [23].|
|||On the other hand, exposure fusion with real bracketing can cover all of the areas of the input image, as shown in Fig.|
|||(b) Our noise-free exposure fusion results.|
|||(d) Our noise-free exposure fusion results.|
||8 instances in total. (in cvpr2018)|
|148|Guanying_Chen_PS-FCN_A_Flexible_ECCV_2018_paper|PS-FCN is composed of three components, namely a shared-weight feature extractor for extracting feature representations from the input images, a fusion layer for aggregating features from multiple input images, and a normal regression network for inferring the normal map (see Fig.|
|||Given a variable number of inputs, a shared-weight feature extractor can be used to extract features from each of the inputs (e.g., siamese networks), but an additional fusion layer is required to aggregate such features into a representation with a fixed number of channels.|
|||A convolutional layer is applicable for multi-feature fusion only when the number of inputs is fixed.|
|||4.2 Network architecture  PS-FCN is a multi-branch siamese network [37] consisting of three components, namely a shared-weight feature extractor, a fusion layer, and a normal regression network (see Fig.|
|||Hence, the input to our model has a dimension of q  6  h  w. We separately feed the image-light pairs to the shared-weight feature extractor to extract a feature map from each of the inputs, and apply a max-pooling operation in the fusion layer to aggregate these feature maps.|
|||In particular, we first validated the effectiveness of max-pooling in multi-feature fusion by  4 In our experiment, for each object in the Light Stage Data Gallery, we only used the  133 pairs with the front side of the object under illumination.|
|||Similarly, experiments with IDs 2, 5 & 6 showed that fusion by convolutional layers on the concatenated features was sub-optimal.|
||8 instances in total. (in eccv2018)|
|149|Xu_A_Discriminative_CNN_2015_CVPR_paper|Results for Fusing Multiple Layers Extracted  from the Same Model  We investigate average late fusion [39] to fuse the prediction results from different layers with PQ compression, i.e., VLAD encoded LCD with SPP, fc6 and fc7.|
|||From Table 8 we can see that the simple fusion pushes the performance further beyond the single layers on MEDTest 13 and MEDTest 14, and achieves significant advantages over improved Dense Trajectories (IDT).|
|||We provide results for average pooling on CNN descriptors with late fusion of three layers as well, denoted as CNNavg.|
|||To show that our representation is complementary to features from other modalities, we perform average late fusion of our proposed representation with IDT and MFCC, and generate a lightweight system with static, motion and acoustic features, which achieves 48.6% mAP on 100Ex, and 32.2% mAP on 10Ex.|
|||Multimodal feature fusion for robust event detection in web videos.|
|||Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice.|
|||Early versus late fusion in semantic video analysis.|
||8 instances in total. (in cvpr2015)|
|150|cvpr18-CNN in MRF  Video Object Segmentation via Inference in a CNN-Based Higher-Order Spatio-Temporal MRF|This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step.|
|||The entire inference algorithm alternates between a temporal fusion step and a feed-forward pass of the CNN.|
|||The algorithm alternates between a temporal fusion operation and a feed-forward CNN to progressively refine the segmentation results.|
|||(11) in step 2 only considers spatial dependencies for each frame c. The two steps are essentially performing temporal fusion and mask refinement, respectively.|
|||The temporal fusion step in our algorithm is performed locally and the runtime is almost ignorable.|
|||TF represents the temporal fusion step in our algorithm, while MR represents the mask refinement step.|
|||By performing inference in the MRF model, we developed an algorithm that alternates between a temporal fusion operation and a mask refinement feed-forward CNN, progressively inferring the results of video object segmentation.|
||8 instances in total. (in cvpr2018)|
|151|Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper|A recent wave of work using CNN for patch-based edge prediction [10, 34, 2, 19] contains an alternative common thread that focuses on three aspects: automatic feature learning (property 1), multi-scale response fusion (property 2), and possible engagement of different levels of visual perception (property 3).|
|||The multiple side outputs also give us the flexibility to add an additional fusion layer if a unified output is desired.|
|||, |Y |} are activations of the side-output of layer m.  side ), where A(m)  side = ( A(m)  j  j  To directly utilize side-output predictions, we add a weighted-fusion layer to the network and (simultaneously) learn the fusion weight during training.|
|||Our loss function at the fusion layer Lfuse becomes  Lfuse(W, w, h) = Dist(Y, Yfuse)  (3)  m=1 hm A(m)  Yfuse  (PM  side ) where h = where (h1, .|
|||, hM ) is the fusion weight.|
|||The hyper-parameters (and the values we choose) include: mini-batch size (10), learning rate (1e-6), loss-weight m for each side-output layer (1), momentum (0.9), initialization of the nested filters (0), initialization of the fusion layer weights (1/5), weight decay (0.0002), number of training iterations (10,000; divide learning rate by 10 after 5,000).|
|||Taking advantage of the readily available side outputs in HED, we merge the fusion layer output with the side outputs (at no extra cost) in order to compensate for the loss in average precision.|
||8 instances in total. (in iccv2015)|
|152|Galliani_Just_Look_at_CVPR_2016_paper|On the other hand, the depthmap fusion relies on a consensus mechanism that checks both the consistency of the depth values and of the normal directions across several views.|
|||As a result, points with unreliable normals are discarded during fusion, which is important for our purposes, since  1The integration and fusion steps could potentially be solved jointly.|
|||As a first filter, we remove all normals that did not survive the multi-view fusion (meaning that they did not fit the consensus).|
|||This can be expected to improve the accuracy of the valid normals, because the inliers to the consensus voting are averaged during fusion to suppress noise.|
|||Obviously, the fusion parameters , , K provide a simple interface to tune accuracy vs. completeness of the reconstruction.|
|||To ensure a fair comparison to pure MVS, we set the same fusion parameters (Sec.|
|||The original MVS points are not modified, and depth map fusion is done in a separate step.|
||8 instances in total. (in cvpr2016)|
|153|cvpr18-Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification|Importantly, we design a progressive knowledge fusion mechanism by introducing an Identity Inferred Attribute (IIA) regularisation space for more smoothly transferring the global identity information into the local attribute feature representation space.|
|||Identity Inferred Attribute Space We introduce an intermediate Identity Inferred Attribute (IIA) Space for achieving the knowledge fusion learning on attribute and identity labels in a softer manner (Fig.|
|||The IIA space is jointly learned with the two branches while being exploited to perform information transfer and fusion from the identity branch to the attribute branch simultaneously.|
|||This suggests the overall performance advantages of the proposed TJ-AIDL in the capability of multi-source (attribute and identity) information extraction and fusion for cross-domain unsupervised re-id matching.|
|||Table 2 shows that: (1) The TJ-AIDL outperforms both alternative fusion methods.|
|||Comparing different multi-source fusion methods.|
|||We also compared the TJ-AIDL model with popular multi-supervision fusion methods and provided detailed component analysis with insights into the performance gain of our model design.|
||8 instances in total. (in cvpr2018)|
|154|cvpr18-DoubleFusion  Real-Time Capture of Human Performances With Inner Body Shapes From a Single Depth Sensor|Therefore, we propose a pipeline that executes joint motion tracking, geometric fusion and volumetric shape-pose optimization sequentially (Fig.|
|||Geometric fusion Similar to previous work [28], we nonrigidly integrate depth observation of multiple frames in a reference volume (Sec.|
|||4), geometric fusion (Sec.|
|||Geometric Fusion  Similar to the previous non-rigid fusion works [28, 15, 14], we integrate the depth information into a reference volume.|
|||The geometric fusion takes 6 ms  7292  Figure 5: Example results reconstructed by our system.|
|||The lack of semantic information results in wrong connections (connection between two legs) and erroneous fusion results as shown in Fig.|
|||Only using all the energy terms we can get accurate pose and fusion results as shown in Fig.|
||8 instances in total. (in cvpr2018)|
|155|Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos|As the UCF101 is an extension of the UCF50 dataset, to avoid the risk of overfitting, for any further fusion and for the comparison with the state-of-theart, we excluded TCN features for the UCF50 dataset results.|
|||For these four feature combinations we evaluate different fusion strategies: Early, where after we individually build the final representation for each feature type and normalize it accordingly, we concatenate all resulted representations in a final vector, we apply L2 normalization for making unit length and then perform the classification part; sLate, where we make late fusion by making sum between the classifiers output from each representation; wLate, where we give different weights for each feature representation classifier output, and then we perform the sum.|
|||Table 4 shows that early fusion performs better than late fusion.|
|||Double fusion combines the benefit of both, early and late fusion, and boosts further the accuracy.|
|||The best performance results are in bold for each fusion type over each feature representation combination.|
|||Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice.|
|||Multilayer and multimodal fusion of deep neural networks for video classification.|
||8 instances in total. (in cvpr2017)|
|156|Ionescu_Unmasking_the_Abnormal_ICCV_2017_paper|To combine the anomaly scores from motion and appearance features, we employ a late fusion strategy by averaging the scores for each frame, in step H. Third of all, we take windows at a predefined interval s (stride), where the choice of s can generate overlapping windows (e.g.|
|||Figure 2 illustrates the frame-level anomaly scores, for test video 4 in the Avenue data set, produced by our unmasking framework based on combining motion and appearance features using a late fusion strategy.|
|||Our late fusion strategy is not able to bring any improvements.|
|||For a fair comparison, we evaluated our unmasking framework based on the late fusion strategy in their setting, and obtained a frame-level AUC of 78.1%.|
|||Some qualitative results of our unsupervised framework based on late fusion are illustrated in Figure 7.|
|||On the last scene, the performance of our unmasking framework based on late fusion is less than 2% lower than the best supervised approach [22].|
|||We have adopted a late fusion strategy to combine motion and appearance features, but we did not observe any considerable improvements when using this strategy.|
||8 instances in total. (in iccv2017)|
|157|Maier_Intrinsic3D_High-Quality_3D_ICCV_2017_paper|A very popular strategy to handle strong noise characteristics is volumetric fusion of independent depth frames [7], which has become the core of many state-of-the-art RGB-D reconstruction frameworks [17, 18, 21, 5, 8].|
|||Volumetric fusion is a fast and efficient solution for regularizing out sensor noise; however, due to its l2regularization property, it tends to oversmooth the reconstruction, leaving little fine-scale surface detail in the result.|
|||While most SDFbased fusion methods efficiently regularize noisy depth input, they spend little focus on reconstructing consistent and sharp surface textures.|
|||Our method (right) reveals finer geometric details compared to volumetric fusion (left) and Zollh ofer et al.|
|||Figure 6 shows that, in contrast to fusion and [30], our method is able to reveal even smaller details.|
|||Refined geometry of the Frog dataset: while fusion (a) smooths out high-frequency details, Zollh ofer et al.|
|||Super-resolution keyframe fusion for 3D modeling with high-quality textures.|
||8 instances in total. (in iccv2017)|
|158|cvpr18-Multi-Level Factorisation Net for Person Re-Identification|The MLFN architecture is noteworthy in that:  (i) A compact FS is generated by concatenating the FSM output vectors from all blocks, and therefore multi-level feature fusion is obtained without exploding dimensionality; (ii) Using the FSM output vectors to predict person identity via skip connections and fusion provides deep supervision [21, 14] which ensures that the learned factors are identitydiscriminative, but without introducing a large number of parameters required for conventional deep supervision.|
|||More importantly, it extends both in that it is the selection of which factor modules or experts are active that provides a compact latent semantic feature, and enables the low-dimensional fusion across semantic levels and deep supervision.|
|||Very few fusion architectures on specific tasks, e.g., edge detection [45], fuse features from all layers/levels.|
|||Using their fusion as a representation, we obtain state-ofthe-art results on three large person Re-ID benchmarks.|
|||This suggests that our fusion architecture with deep supervision is more effective than the handcrafted architectures with manual layer selection in [53, 29], which require extra effort but may lead to suboptimal solutions.|
|||MLFN-Fusion: MLFN using dynamic factor selection, but without fusion of the FS feature.|
|||Multilayer and multimodal fusion of deep neural networks for video classification.|
||8 instances in total. (in cvpr2018)|
|159|Saha_AMTnet_Action-Micro-Tube_Regression_ICCV_2017_paper|Subsequently, a bilinear feature pooling (h) and an element-wise feature fusion (i) are used to obtain a fixed sized feature representation for each sampled 3D proposal.|
|||Indeed the method can be easily extended to incorporate motion at the micro-tube level rather than frame level, allowing fusion of appearance and motion at training time, unlike current methods [20, 22].|
|||We then apply element-wise fusion (i) ( 3.4) to these 2 feature maps.|
|||We fuse the two conv feature maps produced by the two parallel VGG-16 networks using element-wise sum fusion ( Figure 1 (c)).|
|||Further, whereas [20, 22] train appearance and motion streams independently, and perform fusion at test time, our model requires one-time training, and feature fusion is done at training time.|
|||Feature fusion of 3D region proposals.|
|||We then apply elementwise sum fusion ( Figure 1 (i)) to these 2 feature maps, producing an output feature map of size [D  kh  kw].|
||8 instances in total. (in iccv2017)|
|160|Kim_Optical_Flow_via_2013_ICCV_paper|Our adaptive data fusion method consists of complementary data models that overcome the limitations of the single data model and provides the similar result to the ground truth flow.|
|||Figure 1 shows that our new optical flow estimation model based on the generalized data fusion framework significantly improves the accuracy.|
|||Thus, designing a locally (pixel-wise) adaptive data term is desirable by the fusion of complementary data models while excluding the invalid data models.|
|||Therefore, we can generalize (1) by employing the adaptive data fusion model as, E = Edata(u, w) + Edscr(u, w) + Ereg(w) + Ereg(u).|
|||Therefore, we study the nature of the data models to be used in the data fusion on the Middlebury training datasets where the ground truth of the motion fields is known.|
|||The comparisons are made using the methods with single data term, which has the best score, mean of data models, and the proposed data fusion with and without the discriminability term.|
|||In the evaluation, the single data model giving the best result, simple mean of data models and their fusion by the proposed method with and without the data discriminability are compared.|
||8 instances in total. (in iccv2013)|
|161|Wei_Dong_Probabilistic_Signed_Distance_ECCV_2018_paper|Many representations built upon appropriate mathematical models are designed for robust data fusion in such a context.|
|||Volumetric grids lack flexibility to some extent, hence corresponding data fusion can be either oversimplified using weighted average [20], or much time-consuming in order to maximize joint distributions [27].|
|||Our framework is able to perform reliable depth data fusion and reconstruct high-quality surfaces in real-time with more details and less noise, as depicted in Fig.1.|
|||Incremental 3D data fusion is built upon less ad-hoc probabilistic computations in a parametric Bayesian updating fashion, contributes to online surface reconstruction, and benefits from iteratively recovered geometry in return.|
|||[20,19] use weight average of Truncated SDF in the data fusion stage by considering a per-voxel Gaussian distribution regarding SDF as a random variable.|
|||As we have discussed, evaluation of inlier ratio was performed, increasing total time of the fusion stage.|
|||The meshing stage is the runtime bottleneck of the approach, in general the saved time compensate for the cost in fusion stage, see Fig.8(e) and Table.2.|
||8 instances in total. (in eccv2018)|
|162|StyleBank_ An Explicit Representation for Neural Image Style Transfer|Because of the explicit representation, we can more conveniently control style transfer and create new interesting style fusion effects.|
|||More specifically, we can either linearly fuse different styles altogether, or produce region-specific style fusion effects.|
|||Figure 9 shows such  linear fusion results of two styles with variant fusion weight wi.|
|||Then region can be described as F = Pm  specific style fusion can be formulated as Equation (6):  eF = Xm  i=1  Ki  (Mi  F ),  (6)  where Ki is the i-th filter bank.|
|||Figure 10 shows such a region-specific style fusion result which exactly borrows styles from two famous paintings of Picasso and Van Goph.|
|||Region-specific style fusion with two paintings of Picasso and Van Gophm, where the regions are automatically segmented with K-means method.|
|||The decoupling allows faster training (for multiple styles, and new styles), and enables new interesting style fusion effects, like linear and region-specific style transfer.|
||8 instances in total. (in cvpr2017)|
|163|Chen_Large_Displacement_Optical_2013_CVPR_paper|Therefore, we perform a fusion between the motion segmentation result under translations and that under the similarity transformations before a final refinement.|
|||We use a novel fusion algorithm to merge the motion result under translations with that under similarity transformations.|
|||Accordingly, we optimize it by a two-stage fusion process which approximates the global minimum.|
|||between u and u to obtain the final result u as:  Then, we apply a fusion algorithm to adaptively choose  u = arg min u  E(u)  s.t.|
|||The final fusion step adaptively selects the best model between u and u, and obtains quite satisfactory results.|
|||In contrast, our algorithm achieves accurate motion estimation by fusion of different motion patterns.|
|||First column: image sequences; second column: ground truth; third column: flow u by translation in Eqn (4); fourth column: similarity flow u  in Eqn (5); fifth column: fusion result u by Eqn (6); last column: further refined optical flow initialized with u.|
||8 instances in total. (in cvpr2013)|
|164|Haque_Multi-View_Non-Rigid_Refinement_ICCV_2017_paper|Mohammadul Haque, Venu Madhav Govindu  Indian Institute of Science  Bengaluru, India  {smhaque, venu}@ee.iisc.ernet.in  Abstract  In recent years, there have been a variety of proposals for high quality 3D reconstruction by fusion of depth and normal maps that contain good low and high frequency information respectively.|
|||Subsequently, normal estimates are assigned to each mesh vertex and a mesh-normal fusion step is carried out.|
|||In [2], the authors obtain 2D normal maps from multiple viewpoints and combine them in a weighted fashion before passing on to a depth-normal fusion step.|
|||We run the MERGE2-3D [2], Priority Selection (PS) [9] and our global, non-rigid refinement and adaptive normal selection steps for the final multi-view fusion and compare their performances.|
|||In Figure 6 (a) we can notice that the simple priority ordering heuristic results in labels that randomly alternate between adjacent mesh vertices which results in a diffusion of errors during the fusion step.|
|||(SEL selection and EST estimation)  Mesh-Normal Fusion: Once we obtain the high quality optimal normals from the previous step, we use the mesh normal fusion [14] to obtain the final results.|
|||We then run our non-rigid refinement  2387  method, adaptive normal selection and fusion steps on these input data and obtain the final multi-view 3D reconstructed models, denoted as OURS.|
||8 instances in total. (in iccv2017)|
|165|Long_Fully_Convolutional_Networks_2015_CVPR_paper|We find learning through upsampling, as described in the next section, to be more effective and efficient, especially when combined with the skip layer fusion described later on.|
|||We compared this fusion with learning only from the pool4 layer, which resulted in poor performance, and simply decreasing the learning rate without adding the skip, which resulted in an insignificant performance improvement without improving the quality of the output.|
|||At this point our fusion improvements have met diminishing returns, both with respect to the IU metric which emphasizes large-scale correctness, and also in terms of the improvement visible e.g.|
|||Setting the pool5 stride to 1 requires our convolutionalized fc6 to have kernel size 14  14 to  6Max fusion made learning difficult due to gradient switching.|
|||RGB-HHA is the jointly trained late fusion model that sums RGB and HHA predictions.|
|||[13], we try the three-dimensional HHA encoding of depth, training nets on just this information, as well as a late fusion of RGB and HHA where the predictions from both nets are summed at the final layer, and the resulting two-stream net is learned end-to-end.|
|||Finally we upgrade this late fusion net to a 16-stride version.|
||8 instances in total. (in cvpr2015)|
|166|Ruoxi_Deng_Learning_to_Predict_ECCV_2018_paper|In a refinement module, the mask-encoding is fused with the side-output features and then reduces its channels by a factor of 2 and double its resolution to prepare for the fusion in the next refinement module.|
|||Ours-w/o-FL refers to our method without the fusion loss.|
|||Moreover, we trained two versions of the proposed network via the balanced cross-entropy loss (Ours-w/o-FL) and the proposed fusion loss (Ours), respectively.|
|||Lastly, both the quantitative and the qualitative results have demonstrated the effectiveness of the proposed fusion loss.|
|||By simply using the proposed fusion loss, the ODS f-score (before NMS) of our network is increased  Learning to predict crisp boundaries  11  (a) Input Image  (b) GT  (c) Ours  (d) RCF [13]  (e) CED [45]  Fig.|
|||One may ask a question: Does the fusion loss only work on the convolutional encoder-decoder network?|
|||Similar to the ablation experiments, we evaluate two versions of HED: one is trained by means of the proposed fusion loss, the other is applying the balanced cross-entropy loss.|
||8 instances in total. (in eccv2018)|
|167|Mahabadi_Segment_Based_3D_2015_CVPR_paper|Bottom: Standard volumetric fusion result (left) and our result using the proposed segment based shape prior (right).|
|||From left to right: example input image, example depth map, baseline TV-flux fusion result, result using our shape prior formulation, PMVS+PSR result.|
|||We compare all our results to a baseline approach which reconstructs the same unary terms s using an isotropic regularization (TV-flux fusion of [25]).|
|||From left to right: Example input image, example depth map, baseline TV-flux fusion result, result using our shape prior formulation.|
|||Top row, from left to right: example input image, example depth map, baseline TV-flux fusion result.|
|||From left to right: Example input image, example depth map, TV-Flux fusion result, result of our shape prior formulation.|
|||Fast and high quality fusion of depth maps.|
||8 instances in total. (in cvpr2015)|
|168|cvpr18-Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network|Finally, we design a fusion and recurrent reconstruction network to recurrently refine the preceding blur detection maps.|
|||Then, we develop a fusion and recursive reconstruction network (FRRNet) to recursively refine the preceding blur detection maps.|
|||[8] propose a spatially-varying blur detection method based on a high-frequency multiscale fusion and sort transform of gradient magnitudes to determine the level of blur at each location.|
|||Finally, FRRNet consisting of the fusion network (FNet) and recursion reconstruction network (RRNet) is used to refine the predicted DBD maps, generating the final DBD map.|
|||Then, the fusion and recurrent reconstruction network (FFRNet) is described in Section 3.2.|
|||Comparison of multi-stream DBD map fusion results.|
|||F) [20], spectral and spatial approach (SS) [27], deep and hand-crafted features (DHCF) [17], kernel-specific feature vector (KSFV) [16], local binary patterns (LBP) [31] and high-frequency multi-scale fusion and sort transform of gradient magnitudes (HiFST) [8].|
||8 instances in total. (in cvpr2018)|
